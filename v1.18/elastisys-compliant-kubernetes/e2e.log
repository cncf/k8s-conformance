I1217 09:18:20.163269      24 test_context.go:410] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-588444857
I1217 09:18:20.163312      24 test_context.go:423] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I1217 09:18:20.163538      24 e2e.go:124] Starting e2e run "a63343c7-62fa-43cd-9ede-fef06ca77e31" on Ginkgo node 1
{"msg":"Test Suite starting","total":277,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1608196698 - Will randomize all specs
Will run 277 of 4993 specs

Dec 17 09:18:20.177: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
E1217 09:18:20.178560      24 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp 127.0.0.1:8099: connect: connection refused
Dec 17 09:18:20.180: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 17 09:18:20.219: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 17 09:18:20.285: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 17 09:18:20.285: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Dec 17 09:18:20.285: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 17 09:18:20.301: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec 17 09:18:20.301: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 17 09:18:20.301: INFO: e2e test version: v1.18.12
Dec 17 09:18:20.304: INFO: kube-apiserver version: v1.18.12
Dec 17 09:18:20.304: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 09:18:20.311: INFO: Cluster IP family: ipv4
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:18:20.311: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-probe
Dec 17 09:18:20.355: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Dec 17 09:18:20.390: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-2917edf8-a096-4e1c-8c3f-5ddb6333c275 in namespace container-probe-500
Dec 17 09:18:30.545: INFO: Started pod liveness-2917edf8-a096-4e1c-8c3f-5ddb6333c275 in namespace container-probe-500
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 09:18:30.551: INFO: Initial restart count of pod liveness-2917edf8-a096-4e1c-8c3f-5ddb6333c275 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:22:31.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-500" for this suite.

• [SLOW TEST:251.076 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":277,"completed":1,"skipped":3,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:22:31.388: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3323
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-3323
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating stateful set ss in namespace statefulset-3323
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3323
Dec 17 09:22:31.618: INFO: Found 0 stateful pods, waiting for 1
Dec 17 09:22:41.624: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 17 09:22:41.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-3323 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 17 09:22:42.133: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 17 09:22:42.133: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 17 09:22:42.133: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 17 09:22:42.141: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 17 09:22:52.148: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 09:22:52.148: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 09:22:52.186: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Dec 17 09:22:52.186: INFO: ss-0  ck8s-conftest-118-workload-cluster-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  }]
Dec 17 09:22:52.186: INFO: ss-1                                               Pending         []
Dec 17 09:22:52.186: INFO: 
Dec 17 09:22:52.186: INFO: StatefulSet ss has not reached scale 3, at 2
Dec 17 09:22:53.194: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984132673s
Dec 17 09:22:54.208: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.976656084s
Dec 17 09:22:55.215: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.962820484s
Dec 17 09:22:56.224: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.955158304s
Dec 17 09:22:57.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.946347343s
Dec 17 09:22:58.239: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.937733614s
Dec 17 09:22:59.246: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.931560541s
Dec 17 09:23:00.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.924870518s
Dec 17 09:23:01.259: INFO: Verifying statefulset ss doesn't scale past 3 for another 917.688852ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3323
Dec 17 09:23:02.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-3323 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 17 09:23:02.581: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 17 09:23:02.581: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 17 09:23:02.581: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 17 09:23:02.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-3323 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 17 09:23:02.859: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 17 09:23:02.859: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 17 09:23:02.859: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 17 09:23:02.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-3323 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 17 09:23:03.137: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 17 09:23:03.137: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 17 09:23:03.137: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 17 09:23:03.145: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec 17 09:23:13.153: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 09:23:13.153: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 09:23:13.153: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 17 09:23:13.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-3323 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 17 09:23:13.430: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 17 09:23:13.430: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 17 09:23:13.430: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 17 09:23:13.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-3323 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 17 09:23:13.715: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 17 09:23:13.715: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 17 09:23:13.715: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 17 09:23:13.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-3323 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 17 09:23:13.994: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 17 09:23:13.995: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 17 09:23:13.995: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 17 09:23:13.995: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 09:23:14.000: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 17 09:23:24.012: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 09:23:24.012: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 09:23:24.012: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 09:23:24.060: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Dec 17 09:23:24.060: INFO: ss-0  ck8s-conftest-118-workload-cluster-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  }]
Dec 17 09:23:24.060: INFO: ss-1  ck8s-conftest-118-workload-cluster-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:24.060: INFO: ss-2  ck8s-conftest-118-workload-cluster-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:24.060: INFO: 
Dec 17 09:23:24.060: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 17 09:23:25.069: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Dec 17 09:23:25.069: INFO: ss-0  ck8s-conftest-118-workload-cluster-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  }]
Dec 17 09:23:25.069: INFO: ss-1  ck8s-conftest-118-workload-cluster-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:25.069: INFO: ss-2  ck8s-conftest-118-workload-cluster-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:25.069: INFO: 
Dec 17 09:23:25.069: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 17 09:23:26.078: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Dec 17 09:23:26.078: INFO: ss-0  ck8s-conftest-118-workload-cluster-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  }]
Dec 17 09:23:26.078: INFO: ss-1  ck8s-conftest-118-workload-cluster-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:26.078: INFO: ss-2  ck8s-conftest-118-workload-cluster-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:26.078: INFO: 
Dec 17 09:23:26.078: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 17 09:23:27.086: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Dec 17 09:23:27.086: INFO: ss-0  ck8s-conftest-118-workload-cluster-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  }]
Dec 17 09:23:27.086: INFO: ss-1  ck8s-conftest-118-workload-cluster-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:27.086: INFO: ss-2  ck8s-conftest-118-workload-cluster-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:27.086: INFO: 
Dec 17 09:23:27.086: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 17 09:23:28.096: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Dec 17 09:23:28.096: INFO: ss-0  ck8s-conftest-118-workload-cluster-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  }]
Dec 17 09:23:28.096: INFO: ss-1  ck8s-conftest-118-workload-cluster-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:28.096: INFO: ss-2  ck8s-conftest-118-workload-cluster-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:28.096: INFO: 
Dec 17 09:23:28.096: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 17 09:23:29.104: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Dec 17 09:23:29.104: INFO: ss-0  ck8s-conftest-118-workload-cluster-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  }]
Dec 17 09:23:29.104: INFO: ss-1  ck8s-conftest-118-workload-cluster-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:29.104: INFO: ss-2  ck8s-conftest-118-workload-cluster-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:29.104: INFO: 
Dec 17 09:23:29.104: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 17 09:23:30.113: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Dec 17 09:23:30.113: INFO: ss-0  ck8s-conftest-118-workload-cluster-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  }]
Dec 17 09:23:30.113: INFO: ss-1  ck8s-conftest-118-workload-cluster-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:30.113: INFO: ss-2  ck8s-conftest-118-workload-cluster-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:30.113: INFO: 
Dec 17 09:23:30.113: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 17 09:23:31.120: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Dec 17 09:23:31.120: INFO: ss-0  ck8s-conftest-118-workload-cluster-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:31 +0000 UTC  }]
Dec 17 09:23:31.120: INFO: ss-1  ck8s-conftest-118-workload-cluster-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:31.120: INFO: ss-2  ck8s-conftest-118-workload-cluster-worker-0  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:31.120: INFO: 
Dec 17 09:23:31.120: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 17 09:23:32.126: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Dec 17 09:23:32.126: INFO: ss-1  ck8s-conftest-118-workload-cluster-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:32.126: INFO: 
Dec 17 09:23:32.126: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 17 09:23:33.133: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Dec 17 09:23:33.133: INFO: ss-1  ck8s-conftest-118-workload-cluster-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:23:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-12-17 09:22:52 +0000 UTC  }]
Dec 17 09:23:33.134: INFO: 
Dec 17 09:23:33.134: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3323
Dec 17 09:23:34.139: INFO: Scaling statefulset ss to 0
Dec 17 09:23:34.155: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Dec 17 09:23:34.159: INFO: Deleting all statefulset in ns statefulset-3323
Dec 17 09:23:34.163: INFO: Scaling statefulset ss to 0
Dec 17 09:23:34.179: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 09:23:34.183: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:23:34.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3323" for this suite.

• [SLOW TEST:62.854 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":277,"completed":2,"skipped":7,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:23:34.243: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-446.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-446.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-446.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-446.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-446.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-446.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 09:23:46.532: INFO: DNS probes using dns-446/dns-test-ae15c2c5-2b97-4178-abfd-147fd361d832 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:23:46.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-446" for this suite.

• [SLOW TEST:12.322 seconds]
[sig-network] DNS
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":277,"completed":3,"skipped":30,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:23:46.565: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5461
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-584984ed-959d-4884-b8ea-ead8e8023ae6
STEP: Creating a pod to test consume secrets
Dec 17 09:23:46.759: INFO: Waiting up to 5m0s for pod "pod-secrets-fa438f4f-34b6-4867-b937-46e0d9ca8708" in namespace "secrets-5461" to be "Succeeded or Failed"
Dec 17 09:23:46.785: INFO: Pod "pod-secrets-fa438f4f-34b6-4867-b937-46e0d9ca8708": Phase="Pending", Reason="", readiness=false. Elapsed: 25.849128ms
Dec 17 09:23:48.794: INFO: Pod "pod-secrets-fa438f4f-34b6-4867-b937-46e0d9ca8708": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034791955s
Dec 17 09:23:50.800: INFO: Pod "pod-secrets-fa438f4f-34b6-4867-b937-46e0d9ca8708": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041107569s
Dec 17 09:23:52.807: INFO: Pod "pod-secrets-fa438f4f-34b6-4867-b937-46e0d9ca8708": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048371047s
STEP: Saw pod success
Dec 17 09:23:52.807: INFO: Pod "pod-secrets-fa438f4f-34b6-4867-b937-46e0d9ca8708" satisfied condition "Succeeded or Failed"
Dec 17 09:23:52.813: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-secrets-fa438f4f-34b6-4867-b937-46e0d9ca8708 container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 09:23:52.862: INFO: Waiting for pod pod-secrets-fa438f4f-34b6-4867-b937-46e0d9ca8708 to disappear
Dec 17 09:23:52.865: INFO: Pod pod-secrets-fa438f4f-34b6-4867-b937-46e0d9ca8708 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:23:52.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5461" for this suite.

• [SLOW TEST:6.320 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":4,"skipped":37,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:23:52.886: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-714
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-31df2bf8-d747-4ab2-bc36-b0fe7b10a808
STEP: Creating a pod to test consume configMaps
Dec 17 09:23:53.093: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39e6e44c-9251-4c2c-9431-c4a063b2e719" in namespace "projected-714" to be "Succeeded or Failed"
Dec 17 09:23:53.102: INFO: Pod "pod-projected-configmaps-39e6e44c-9251-4c2c-9431-c4a063b2e719": Phase="Pending", Reason="", readiness=false. Elapsed: 8.655114ms
Dec 17 09:23:55.108: INFO: Pod "pod-projected-configmaps-39e6e44c-9251-4c2c-9431-c4a063b2e719": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015033871s
Dec 17 09:23:57.116: INFO: Pod "pod-projected-configmaps-39e6e44c-9251-4c2c-9431-c4a063b2e719": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022893075s
STEP: Saw pod success
Dec 17 09:23:57.116: INFO: Pod "pod-projected-configmaps-39e6e44c-9251-4c2c-9431-c4a063b2e719" satisfied condition "Succeeded or Failed"
Dec 17 09:23:57.122: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-projected-configmaps-39e6e44c-9251-4c2c-9431-c4a063b2e719 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 09:23:57.172: INFO: Waiting for pod pod-projected-configmaps-39e6e44c-9251-4c2c-9431-c4a063b2e719 to disappear
Dec 17 09:23:57.176: INFO: Pod pod-projected-configmaps-39e6e44c-9251-4c2c-9431-c4a063b2e719 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:23:57.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-714" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":5,"skipped":46,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:23:57.190: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating replication controller my-hostname-basic-a25fb155-eca7-403c-b8c1-d42408c6a92f
Dec 17 09:23:57.368: INFO: Pod name my-hostname-basic-a25fb155-eca7-403c-b8c1-d42408c6a92f: Found 0 pods out of 1
Dec 17 09:24:02.391: INFO: Pod name my-hostname-basic-a25fb155-eca7-403c-b8c1-d42408c6a92f: Found 1 pods out of 1
Dec 17 09:24:02.391: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a25fb155-eca7-403c-b8c1-d42408c6a92f" are running
Dec 17 09:24:02.399: INFO: Pod "my-hostname-basic-a25fb155-eca7-403c-b8c1-d42408c6a92f-scn6w" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-17 09:23:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-17 09:23:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-17 09:23:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-17 09:23:57 +0000 UTC Reason: Message:}])
Dec 17 09:24:02.399: INFO: Trying to dial the pod
Dec 17 09:24:07.417: INFO: Controller my-hostname-basic-a25fb155-eca7-403c-b8c1-d42408c6a92f: Got expected result from replica 1 [my-hostname-basic-a25fb155-eca7-403c-b8c1-d42408c6a92f-scn6w]: "my-hostname-basic-a25fb155-eca7-403c-b8c1-d42408c6a92f-scn6w", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:24:07.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2741" for this suite.

• [SLOW TEST:10.243 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":6,"skipped":47,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:24:07.434: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 17 09:24:07.626: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3350 /api/v1/namespaces/watch-3350/configmaps/e2e-watch-test-label-changed f950839d-2534-43c9-834f-a6ed52f29858 3373 0 2020-12-17 09:24:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-17 09:24:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 09:24:07.627: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3350 /api/v1/namespaces/watch-3350/configmaps/e2e-watch-test-label-changed f950839d-2534-43c9-834f-a6ed52f29858 3374 0 2020-12-17 09:24:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-17 09:24:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 09:24:07.627: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3350 /api/v1/namespaces/watch-3350/configmaps/e2e-watch-test-label-changed f950839d-2534-43c9-834f-a6ed52f29858 3375 0 2020-12-17 09:24:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-17 09:24:07 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 17 09:24:17.676: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3350 /api/v1/namespaces/watch-3350/configmaps/e2e-watch-test-label-changed f950839d-2534-43c9-834f-a6ed52f29858 3421 0 2020-12-17 09:24:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-17 09:24:17 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 09:24:17.677: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3350 /api/v1/namespaces/watch-3350/configmaps/e2e-watch-test-label-changed f950839d-2534-43c9-834f-a6ed52f29858 3422 0 2020-12-17 09:24:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-17 09:24:17 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 09:24:17.677: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3350 /api/v1/namespaces/watch-3350/configmaps/e2e-watch-test-label-changed f950839d-2534-43c9-834f-a6ed52f29858 3423 0 2020-12-17 09:24:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-12-17 09:24:17 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:24:17.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3350" for this suite.

• [SLOW TEST:10.253 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":277,"completed":7,"skipped":57,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:24:17.690: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4025
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-67a9e298-105c-4a01-a181-ade52bc958e0 in namespace container-probe-4025
Dec 17 09:24:21.876: INFO: Started pod liveness-67a9e298-105c-4a01-a181-ade52bc958e0 in namespace container-probe-4025
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 09:24:21.883: INFO: Initial restart count of pod liveness-67a9e298-105c-4a01-a181-ade52bc958e0 is 0
Dec 17 09:24:39.948: INFO: Restart count of pod container-probe-4025/liveness-67a9e298-105c-4a01-a181-ade52bc958e0 is now 1 (18.064633384s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:24:39.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4025" for this suite.

• [SLOW TEST:22.318 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":8,"skipped":106,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:24:40.008: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1001
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:24:45.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1001" for this suite.

• [SLOW TEST:5.225 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":277,"completed":9,"skipped":110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:24:45.235: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8161
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:25:45.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8161" for this suite.

• [SLOW TEST:60.223 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":277,"completed":10,"skipped":148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:25:45.460: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7181
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-7181
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Dec 17 09:25:45.713: INFO: Found 0 stateful pods, waiting for 3
Dec 17 09:25:55.720: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 09:25:55.720: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 09:25:55.720: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 17 09:25:55.766: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 17 09:26:05.823: INFO: Updating stateful set ss2
Dec 17 09:26:05.836: INFO: Waiting for Pod statefulset-7181/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Dec 17 09:26:15.917: INFO: Found 2 stateful pods, waiting for 3
Dec 17 09:26:25.927: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 09:26:25.927: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 09:26:25.927: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 17 09:26:25.960: INFO: Updating stateful set ss2
Dec 17 09:26:25.971: INFO: Waiting for Pod statefulset-7181/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 17 09:26:36.009: INFO: Updating stateful set ss2
Dec 17 09:26:36.034: INFO: Waiting for StatefulSet statefulset-7181/ss2 to complete update
Dec 17 09:26:36.034: INFO: Waiting for Pod statefulset-7181/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 17 09:26:46.051: INFO: Waiting for StatefulSet statefulset-7181/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Dec 17 09:26:56.047: INFO: Deleting all statefulset in ns statefulset-7181
Dec 17 09:26:56.052: INFO: Scaling statefulset ss2 to 0
Dec 17 09:27:16.078: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 09:27:16.084: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:27:16.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7181" for this suite.

• [SLOW TEST:90.669 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":277,"completed":11,"skipped":174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:27:16.131: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-886
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 09:27:16.758: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 09:27:18.775: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794037, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794037, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794037, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794036, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 09:27:21.811: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:27:21.816: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2312-crds.webhook.example.com via the AdmissionRegistration API
Dec 17 09:27:22.396: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:27:23.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-886" for this suite.
STEP: Destroying namespace "webhook-886-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.595 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":277,"completed":12,"skipped":279,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:27:23.727: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 09:27:23.923: INFO: Waiting up to 5m0s for pod "downwardapi-volume-17695bf4-23a3-4a55-ad78-ca3507572278" in namespace "downward-api-6609" to be "Succeeded or Failed"
Dec 17 09:27:23.939: INFO: Pod "downwardapi-volume-17695bf4-23a3-4a55-ad78-ca3507572278": Phase="Pending", Reason="", readiness=false. Elapsed: 15.495771ms
Dec 17 09:27:25.944: INFO: Pod "downwardapi-volume-17695bf4-23a3-4a55-ad78-ca3507572278": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020689445s
Dec 17 09:27:27.952: INFO: Pod "downwardapi-volume-17695bf4-23a3-4a55-ad78-ca3507572278": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028409125s
STEP: Saw pod success
Dec 17 09:27:27.952: INFO: Pod "downwardapi-volume-17695bf4-23a3-4a55-ad78-ca3507572278" satisfied condition "Succeeded or Failed"
Dec 17 09:27:27.957: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-17695bf4-23a3-4a55-ad78-ca3507572278 container client-container: <nil>
STEP: delete the pod
Dec 17 09:27:28.021: INFO: Waiting for pod downwardapi-volume-17695bf4-23a3-4a55-ad78-ca3507572278 to disappear
Dec 17 09:27:28.028: INFO: Pod downwardapi-volume-17695bf4-23a3-4a55-ad78-ca3507572278 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:27:28.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6609" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":13,"skipped":319,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:27:28.054: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 17 09:27:32.250: INFO: &Pod{ObjectMeta:{send-events-cd95adce-adee-4442-9fc2-e2dfc47317e6  events-538 /api/v1/namespaces/events-538/pods/send-events-cd95adce-adee-4442-9fc2-e2dfc47317e6 190e6a09-b524-4a08-8f36-494ed1fcf93c 4566 0 2020-12-17 09:27:28 +0000 UTC <nil> <nil> map[name:foo time:205480348] map[cni.projectcalico.org/podIP:192.168.192.81/32 cni.projectcalico.org/podIPs:192.168.192.81/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2020-12-17 09:27:28 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 116 105 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 112 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 114 116 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 99 111 110 116 97 105 110 101 114 80 111 114 116 92 34 58 56 48 44 92 34 112 114 111 116 111 99 111 108 92 34 58 92 34 84 67 80 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 99 111 110 116 97 105 110 101 114 80 111 114 116 34 58 123 125 44 34 102 58 112 114 111 116 111 99 111 108 34 58 123 125 125 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:27:29 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-12-17 09:27:31 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 50 46 56 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-h48mr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-h48mr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-h48mr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:27:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:27:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:27:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:192.168.192.81,StartTime:2020-12-17 09:27:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 09:27:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://3c42c23b718a7828b31f2c89df25bf7cd04a408c639ba7808f79376537d61a9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 17 09:27:34.258: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 17 09:27:36.264: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:27:36.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-538" for this suite.

• [SLOW TEST:8.244 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":277,"completed":14,"skipped":325,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:27:36.299: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-831
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:27:36.506: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 17 09:27:40.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-831 create -f -'
Dec 17 09:27:41.523: INFO: stderr: ""
Dec 17 09:27:41.523: INFO: stdout: "e2e-test-crd-publish-openapi-2617-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 17 09:27:41.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-831 delete e2e-test-crd-publish-openapi-2617-crds test-cr'
Dec 17 09:27:41.629: INFO: stderr: ""
Dec 17 09:27:41.629: INFO: stdout: "e2e-test-crd-publish-openapi-2617-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 17 09:27:41.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-831 apply -f -'
Dec 17 09:27:41.894: INFO: stderr: ""
Dec 17 09:27:41.894: INFO: stdout: "e2e-test-crd-publish-openapi-2617-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 17 09:27:41.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-831 delete e2e-test-crd-publish-openapi-2617-crds test-cr'
Dec 17 09:27:42.013: INFO: stderr: ""
Dec 17 09:27:42.014: INFO: stdout: "e2e-test-crd-publish-openapi-2617-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 17 09:27:42.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 explain e2e-test-crd-publish-openapi-2617-crds'
Dec 17 09:27:42.239: INFO: stderr: ""
Dec 17 09:27:42.239: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2617-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:27:45.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-831" for this suite.

• [SLOW TEST:9.641 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":277,"completed":15,"skipped":338,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:27:45.942: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4846
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl label
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1206
STEP: creating the pod
Dec 17 09:27:46.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-4846'
Dec 17 09:27:46.463: INFO: stderr: ""
Dec 17 09:27:46.463: INFO: stdout: "pod/pause created\n"
Dec 17 09:27:46.463: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 17 09:27:46.463: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4846" to be "running and ready"
Dec 17 09:27:46.469: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.318572ms
Dec 17 09:27:48.475: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012405869s
Dec 17 09:27:50.482: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.019033919s
Dec 17 09:27:50.482: INFO: Pod "pause" satisfied condition "running and ready"
Dec 17 09:27:50.482: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 17 09:27:50.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 label pods pause testing-label=testing-label-value --namespace=kubectl-4846'
Dec 17 09:27:50.617: INFO: stderr: ""
Dec 17 09:27:50.617: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 17 09:27:50.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pod pause -L testing-label --namespace=kubectl-4846'
Dec 17 09:27:50.733: INFO: stderr: ""
Dec 17 09:27:50.733: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 17 09:27:50.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 label pods pause testing-label- --namespace=kubectl-4846'
Dec 17 09:27:50.863: INFO: stderr: ""
Dec 17 09:27:50.863: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 17 09:27:50.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pod pause -L testing-label --namespace=kubectl-4846'
Dec 17 09:27:50.975: INFO: stderr: ""
Dec 17 09:27:50.975: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1213
STEP: using delete to clean up resources
Dec 17 09:27:50.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 delete --grace-period=0 --force -f - --namespace=kubectl-4846'
Dec 17 09:27:51.111: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 09:27:51.111: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 17 09:27:51.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get rc,svc -l name=pause --no-headers --namespace=kubectl-4846'
Dec 17 09:27:51.230: INFO: stderr: "No resources found in kubectl-4846 namespace.\n"
Dec 17 09:27:51.230: INFO: stdout: ""
Dec 17 09:27:51.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -l name=pause --namespace=kubectl-4846 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 17 09:27:51.334: INFO: stderr: ""
Dec 17 09:27:51.334: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:27:51.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4846" for this suite.

• [SLOW TEST:5.408 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1203
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":277,"completed":16,"skipped":348,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:27:51.351: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Dec 17 09:27:51.507: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:27:59.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1355" for this suite.

• [SLOW TEST:8.089 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":277,"completed":17,"skipped":396,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:27:59.441: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2231
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-462fef03-b051-4e0c-abd6-781047178b7d
STEP: Creating a pod to test consume configMaps
Dec 17 09:27:59.609: INFO: Waiting up to 5m0s for pod "pod-configmaps-1bdfb057-3a97-4cd8-aa2c-6cac2db535a2" in namespace "configmap-2231" to be "Succeeded or Failed"
Dec 17 09:27:59.616: INFO: Pod "pod-configmaps-1bdfb057-3a97-4cd8-aa2c-6cac2db535a2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.77942ms
Dec 17 09:28:01.623: INFO: Pod "pod-configmaps-1bdfb057-3a97-4cd8-aa2c-6cac2db535a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014004664s
Dec 17 09:28:03.631: INFO: Pod "pod-configmaps-1bdfb057-3a97-4cd8-aa2c-6cac2db535a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021763332s
STEP: Saw pod success
Dec 17 09:28:03.631: INFO: Pod "pod-configmaps-1bdfb057-3a97-4cd8-aa2c-6cac2db535a2" satisfied condition "Succeeded or Failed"
Dec 17 09:28:03.637: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-configmaps-1bdfb057-3a97-4cd8-aa2c-6cac2db535a2 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 09:28:03.688: INFO: Waiting for pod pod-configmaps-1bdfb057-3a97-4cd8-aa2c-6cac2db535a2 to disappear
Dec 17 09:28:03.692: INFO: Pod pod-configmaps-1bdfb057-3a97-4cd8-aa2c-6cac2db535a2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:28:03.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2231" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":18,"skipped":442,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:28:03.708: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 09:28:03.878: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fcca7a40-0685-4c8f-894e-7184dda05b2f" in namespace "projected-7611" to be "Succeeded or Failed"
Dec 17 09:28:03.890: INFO: Pod "downwardapi-volume-fcca7a40-0685-4c8f-894e-7184dda05b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.671275ms
Dec 17 09:28:05.896: INFO: Pod "downwardapi-volume-fcca7a40-0685-4c8f-894e-7184dda05b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017621792s
Dec 17 09:28:07.903: INFO: Pod "downwardapi-volume-fcca7a40-0685-4c8f-894e-7184dda05b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024947418s
STEP: Saw pod success
Dec 17 09:28:07.903: INFO: Pod "downwardapi-volume-fcca7a40-0685-4c8f-894e-7184dda05b2f" satisfied condition "Succeeded or Failed"
Dec 17 09:28:07.907: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-fcca7a40-0685-4c8f-894e-7184dda05b2f container client-container: <nil>
STEP: delete the pod
Dec 17 09:28:07.939: INFO: Waiting for pod downwardapi-volume-fcca7a40-0685-4c8f-894e-7184dda05b2f to disappear
Dec 17 09:28:07.943: INFO: Pod downwardapi-volume-fcca7a40-0685-4c8f-894e-7184dda05b2f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:28:07.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7611" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":19,"skipped":472,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:28:07.957: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5807
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 17 09:28:14.255: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:28:14.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5807" for this suite.

• [SLOW TEST:6.350 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    on terminated container
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:133
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":20,"skipped":479,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:28:14.307: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-7661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Dec 17 09:28:14.471: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 17 09:29:14.541: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:29:14.546: INFO: Starting informer...
STEP: Starting pod...
Dec 17 09:29:14.793: INFO: Pod is running on ck8s-conftest-118-workload-cluster-worker-0. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec 17 09:29:14.818: INFO: Pod wasn't evicted. Proceeding
Dec 17 09:29:14.818: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec 17 09:30:29.841: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:30:29.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7661" for this suite.

• [SLOW TEST:135.552 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":277,"completed":21,"skipped":482,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:30:29.860: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2367
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2367
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2367
I1217 09:30:30.107186      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2367, replica count: 2
I1217 09:30:33.157910      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1217 09:30:36.158468      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1217 09:30:39.158862      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 17 09:30:39.159: INFO: Creating new exec pod
Dec 17 09:30:44.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=services-2367 execpodcgw4j -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 17 09:30:44.519: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 17 09:30:44.519: INFO: stdout: ""
Dec 17 09:30:44.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=services-2367 execpodcgw4j -- /bin/sh -x -c nc -zv -t -w 2 10.105.209.150 80'
Dec 17 09:30:44.826: INFO: stderr: "+ nc -zv -t -w 2 10.105.209.150 80\nConnection to 10.105.209.150 80 port [tcp/http] succeeded!\n"
Dec 17 09:30:44.826: INFO: stdout: ""
Dec 17 09:30:44.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=services-2367 execpodcgw4j -- /bin/sh -x -c nc -zv -t -w 2 172.0.10.205 31225'
Dec 17 09:30:45.125: INFO: stderr: "+ nc -zv -t -w 2 172.0.10.205 31225\nConnection to 172.0.10.205 31225 port [tcp/31225] succeeded!\n"
Dec 17 09:30:45.125: INFO: stdout: ""
Dec 17 09:30:45.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=services-2367 execpodcgw4j -- /bin/sh -x -c nc -zv -t -w 2 172.0.10.72 31225'
Dec 17 09:30:45.395: INFO: stderr: "+ nc -zv -t -w 2 172.0.10.72 31225\nConnection to 172.0.10.72 31225 port [tcp/31225] succeeded!\n"
Dec 17 09:30:45.395: INFO: stdout: ""
Dec 17 09:30:45.395: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:30:45.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2367" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:15.614 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":277,"completed":22,"skipped":501,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:30:45.475: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7409
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:30:45.635: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 17 09:30:49.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-7409 create -f -'
Dec 17 09:30:50.121: INFO: stderr: ""
Dec 17 09:30:50.122: INFO: stdout: "e2e-test-crd-publish-openapi-2246-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 17 09:30:50.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-7409 delete e2e-test-crd-publish-openapi-2246-crds test-cr'
Dec 17 09:30:50.249: INFO: stderr: ""
Dec 17 09:30:50.249: INFO: stdout: "e2e-test-crd-publish-openapi-2246-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 17 09:30:50.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-7409 apply -f -'
Dec 17 09:30:50.524: INFO: stderr: ""
Dec 17 09:30:50.524: INFO: stdout: "e2e-test-crd-publish-openapi-2246-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 17 09:30:50.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-7409 delete e2e-test-crd-publish-openapi-2246-crds test-cr'
Dec 17 09:30:50.661: INFO: stderr: ""
Dec 17 09:30:50.661: INFO: stdout: "e2e-test-crd-publish-openapi-2246-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 17 09:30:50.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 explain e2e-test-crd-publish-openapi-2246-crds'
Dec 17 09:30:51.006: INFO: stderr: ""
Dec 17 09:30:51.006: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2246-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:30:55.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7409" for this suite.

• [SLOW TEST:10.204 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":277,"completed":23,"skipped":516,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:30:55.680: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1386
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:30:55.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-1386'
Dec 17 09:30:56.247: INFO: stderr: ""
Dec 17 09:30:56.247: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Dec 17 09:30:56.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-1386'
Dec 17 09:30:56.465: INFO: stderr: ""
Dec 17 09:30:56.465: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Dec 17 09:30:57.473: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 09:30:57.473: INFO: Found 0 / 1
Dec 17 09:30:58.472: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 09:30:58.472: INFO: Found 0 / 1
Dec 17 09:30:59.473: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 09:30:59.473: INFO: Found 1 / 1
Dec 17 09:30:59.473: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 17 09:30:59.479: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 09:30:59.479: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 17 09:30:59.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 describe pod agnhost-master-nm6w4 --namespace=kubectl-1386'
Dec 17 09:30:59.617: INFO: stderr: ""
Dec 17 09:30:59.617: INFO: stdout: "Name:         agnhost-master-nm6w4\nNamespace:    kubectl-1386\nPriority:     0\nNode:         ck8s-conftest-118-workload-cluster-worker-1/172.0.10.72\nStart Time:   Thu, 17 Dec 2020 09:30:56 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 192.168.137.9/32\n              cni.projectcalico.org/podIPs: 192.168.137.9/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           192.168.137.9\nIPs:\n  IP:           192.168.137.9\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://ea137ee9650a39d53fd5e018ed65e3384d1eeb610172f9e21f2c6f7f8ff3a183\n    Image:          us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Image ID:       docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 17 Dec 2020 09:30:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tkjft (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-tkjft:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-tkjft\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-1386/agnhost-master-nm6w4 to ck8s-conftest-118-workload-cluster-worker-1\n  Normal  Pulled     2s    kubelet            Container image \"us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-master\n  Normal  Started    1s    kubelet            Started container agnhost-master\n"
Dec 17 09:30:59.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 describe rc agnhost-master --namespace=kubectl-1386'
Dec 17 09:30:59.759: INFO: stderr: ""
Dec 17 09:30:59.759: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-1386\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-nm6w4\n"
Dec 17 09:30:59.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 describe service agnhost-master --namespace=kubectl-1386'
Dec 17 09:30:59.891: INFO: stderr: ""
Dec 17 09:30:59.891: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-1386\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.101.7.79\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.137.9:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 17 09:30:59.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 describe node ck8s-conftest-118-workload-cluster-master-0'
Dec 17 09:31:00.076: INFO: stderr: ""
Dec 17 09:31:00.076: INFO: stdout: "Name:               ck8s-conftest-118-workload-cluster-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ck8s-conftest-118-workload-cluster-master-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.0.10.59/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.134.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 17 Dec 2020 09:09:20 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ck8s-conftest-118-workload-cluster-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 17 Dec 2020 09:30:54 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 17 Dec 2020 09:10:25 +0000   Thu, 17 Dec 2020 09:10:25 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 17 Dec 2020 09:28:30 +0000   Thu, 17 Dec 2020 09:09:20 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 17 Dec 2020 09:28:30 +0000   Thu, 17 Dec 2020 09:09:20 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 17 Dec 2020 09:28:30 +0000   Thu, 17 Dec 2020 09:09:20 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 17 Dec 2020 09:28:30 +0000   Thu, 17 Dec 2020 09:10:03 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.0.10.59\n  Hostname:    ck8s-conftest-118-workload-cluster-master-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  50633164Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2040716Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  46663523866\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             1938316Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 35bb081d4adc4051b57cd53148f599da\n  System UUID:                35BB081D-4ADC-4051-B57C-D53148F599DA\n  Boot ID:                    ca325949-d636-4a31-a5a3-1f81ab5abcb3\n  Kernel Version:             4.15.0-124-generic\n  OS Image:                   Ubuntu 18.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.13\n  Kubelet Version:            v1.18.12\n  Kube-Proxy Version:         v1.18.12\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                                   ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-59877c7fb4-th6rb                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         21m\n  kube-system                 calico-node-tvwmk                                                      250m (12%)    0 (0%)      0 (0%)           0 (0%)         21m\n  kube-system                 coredns-6f5c7bbdfb-mk92j                                               100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     21m\n  kube-system                 coredns-6f5c7bbdfb-qrvcz                                               100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     21m\n  kube-system                 etcd-ck8s-conftest-118-workload-cluster-master-0                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  kube-system                 kube-apiserver-ck8s-conftest-118-workload-cluster-master-0             250m (12%)    0 (0%)      0 (0%)           0 (0%)         20m\n  kube-system                 kube-controller-manager-ck8s-conftest-118-workload-cluster-master-0    200m (10%)    0 (0%)      0 (0%)           0 (0%)         20m\n  kube-system                 kube-proxy-45g98                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         21m\n  kube-system                 kube-scheduler-ck8s-conftest-118-workload-cluster-master-0             100m (5%)     0 (0%)      0 (0%)           0 (0%)         20m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-mnnrf                0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                1 (50%)     0 (0%)\n  memory             140Mi (7%)  340Mi (17%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From        Message\n  ----    ------                   ----               ----        -------\n  Normal  Starting                 22m                kubelet     Starting kubelet.\n  Normal  NodeHasSufficientPID     22m (x7 over 22m)  kubelet     Node ck8s-conftest-118-workload-cluster-master-0 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  22m                kubelet     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  22m (x8 over 22m)  kubelet     Node ck8s-conftest-118-workload-cluster-master-0 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    22m (x8 over 22m)  kubelet     Node ck8s-conftest-118-workload-cluster-master-0 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 21m                kubelet     Starting kubelet.\n  Normal  NodeHasSufficientMemory  21m                kubelet     Node ck8s-conftest-118-workload-cluster-master-0 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    21m                kubelet     Node ck8s-conftest-118-workload-cluster-master-0 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     21m                kubelet     Node ck8s-conftest-118-workload-cluster-master-0 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  21m                kubelet     Updated Node Allocatable limit across pods\n  Normal  Starting                 21m                kube-proxy  Starting kube-proxy.\n  Normal  NodeReady                20m                kubelet     Node ck8s-conftest-118-workload-cluster-master-0 status is now: NodeReady\n"
Dec 17 09:31:00.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 describe namespace kubectl-1386'
Dec 17 09:31:00.211: INFO: stderr: ""
Dec 17 09:31:00.211: INFO: stdout: "Name:         kubectl-1386\nLabels:       e2e-framework=kubectl\n              e2e-run=a63343c7-62fa-43cd-9ede-fef06ca77e31\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:31:00.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1386" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":277,"completed":24,"skipped":541,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:31:00.229: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Dec 17 09:31:00.406: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:31:04.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1714" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":277,"completed":25,"skipped":571,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:31:04.586: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8958
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-3e252591-9f03-47e3-a91d-9c64699846cf
STEP: Creating a pod to test consume secrets
Dec 17 09:31:04.777: INFO: Waiting up to 5m0s for pod "pod-secrets-6c60ede5-e089-4d7b-a47c-d304b7f60e01" in namespace "secrets-8958" to be "Succeeded or Failed"
Dec 17 09:31:04.787: INFO: Pod "pod-secrets-6c60ede5-e089-4d7b-a47c-d304b7f60e01": Phase="Pending", Reason="", readiness=false. Elapsed: 9.545521ms
Dec 17 09:31:06.795: INFO: Pod "pod-secrets-6c60ede5-e089-4d7b-a47c-d304b7f60e01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017057145s
Dec 17 09:31:08.801: INFO: Pod "pod-secrets-6c60ede5-e089-4d7b-a47c-d304b7f60e01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023369228s
STEP: Saw pod success
Dec 17 09:31:08.801: INFO: Pod "pod-secrets-6c60ede5-e089-4d7b-a47c-d304b7f60e01" satisfied condition "Succeeded or Failed"
Dec 17 09:31:08.806: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-secrets-6c60ede5-e089-4d7b-a47c-d304b7f60e01 container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 09:31:08.865: INFO: Waiting for pod pod-secrets-6c60ede5-e089-4d7b-a47c-d304b7f60e01 to disappear
Dec 17 09:31:08.874: INFO: Pod pod-secrets-6c60ede5-e089-4d7b-a47c-d304b7f60e01 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:31:08.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8958" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":26,"skipped":578,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:31:08.887: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 09:31:09.056: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ede22dc0-13e3-4267-b27e-9b772d9c29a0" in namespace "downward-api-3343" to be "Succeeded or Failed"
Dec 17 09:31:09.067: INFO: Pod "downwardapi-volume-ede22dc0-13e3-4267-b27e-9b772d9c29a0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.086919ms
Dec 17 09:31:11.076: INFO: Pod "downwardapi-volume-ede22dc0-13e3-4267-b27e-9b772d9c29a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019677008s
Dec 17 09:31:13.082: INFO: Pod "downwardapi-volume-ede22dc0-13e3-4267-b27e-9b772d9c29a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026204277s
STEP: Saw pod success
Dec 17 09:31:13.083: INFO: Pod "downwardapi-volume-ede22dc0-13e3-4267-b27e-9b772d9c29a0" satisfied condition "Succeeded or Failed"
Dec 17 09:31:13.087: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-ede22dc0-13e3-4267-b27e-9b772d9c29a0 container client-container: <nil>
STEP: delete the pod
Dec 17 09:31:13.121: INFO: Waiting for pod downwardapi-volume-ede22dc0-13e3-4267-b27e-9b772d9c29a0 to disappear
Dec 17 09:31:13.125: INFO: Pod downwardapi-volume-ede22dc0-13e3-4267-b27e-9b772d9c29a0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:31:13.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3343" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":27,"skipped":590,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:31:13.141: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-2a6d5e2b-630c-425e-a569-bfcdf6640cda
STEP: Creating a pod to test consume secrets
Dec 17 09:31:13.318: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-07dfacbe-8e63-47df-ac97-8e12cd25bb93" in namespace "projected-9225" to be "Succeeded or Failed"
Dec 17 09:31:13.322: INFO: Pod "pod-projected-secrets-07dfacbe-8e63-47df-ac97-8e12cd25bb93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.193506ms
Dec 17 09:31:15.336: INFO: Pod "pod-projected-secrets-07dfacbe-8e63-47df-ac97-8e12cd25bb93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017914558s
Dec 17 09:31:17.342: INFO: Pod "pod-projected-secrets-07dfacbe-8e63-47df-ac97-8e12cd25bb93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024555387s
STEP: Saw pod success
Dec 17 09:31:17.342: INFO: Pod "pod-projected-secrets-07dfacbe-8e63-47df-ac97-8e12cd25bb93" satisfied condition "Succeeded or Failed"
Dec 17 09:31:17.348: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-projected-secrets-07dfacbe-8e63-47df-ac97-8e12cd25bb93 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 17 09:31:17.380: INFO: Waiting for pod pod-projected-secrets-07dfacbe-8e63-47df-ac97-8e12cd25bb93 to disappear
Dec 17 09:31:17.384: INFO: Pod pod-projected-secrets-07dfacbe-8e63-47df-ac97-8e12cd25bb93 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:31:17.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9225" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":28,"skipped":597,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:31:17.398: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-9557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:31:17.547: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9557
I1217 09:31:17.568411      24 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9557, replica count: 1
I1217 09:31:18.619459      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1217 09:31:19.619825      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1217 09:31:20.620220      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 17 09:31:20.744: INFO: Created: latency-svc-m59h8
Dec 17 09:31:20.756: INFO: Got endpoints: latency-svc-m59h8 [35.706817ms]
Dec 17 09:31:20.795: INFO: Created: latency-svc-hf7bn
Dec 17 09:31:20.800: INFO: Got endpoints: latency-svc-hf7bn [44.352637ms]
Dec 17 09:31:20.827: INFO: Created: latency-svc-kfq75
Dec 17 09:31:20.834: INFO: Got endpoints: latency-svc-kfq75 [78.040003ms]
Dec 17 09:31:20.855: INFO: Created: latency-svc-fq59c
Dec 17 09:31:20.869: INFO: Got endpoints: latency-svc-fq59c [113.06411ms]
Dec 17 09:31:20.878: INFO: Created: latency-svc-xwlk4
Dec 17 09:31:20.894: INFO: Got endpoints: latency-svc-xwlk4 [137.110317ms]
Dec 17 09:31:20.907: INFO: Created: latency-svc-vhfcc
Dec 17 09:31:20.923: INFO: Got endpoints: latency-svc-vhfcc [165.814809ms]
Dec 17 09:31:20.940: INFO: Created: latency-svc-6lmzz
Dec 17 09:31:20.982: INFO: Got endpoints: latency-svc-6lmzz [225.373221ms]
Dec 17 09:31:21.015: INFO: Created: latency-svc-wgr85
Dec 17 09:31:21.020: INFO: Got endpoints: latency-svc-wgr85 [263.368498ms]
Dec 17 09:31:21.030: INFO: Created: latency-svc-xzqdv
Dec 17 09:31:21.051: INFO: Got endpoints: latency-svc-xzqdv [294.568903ms]
Dec 17 09:31:21.065: INFO: Created: latency-svc-97ztl
Dec 17 09:31:21.078: INFO: Got endpoints: latency-svc-97ztl [321.435937ms]
Dec 17 09:31:21.089: INFO: Created: latency-svc-hvd5j
Dec 17 09:31:21.100: INFO: Got endpoints: latency-svc-hvd5j [343.214228ms]
Dec 17 09:31:21.134: INFO: Created: latency-svc-vt6tz
Dec 17 09:31:21.167: INFO: Got endpoints: latency-svc-vt6tz [410.199139ms]
Dec 17 09:31:21.325: INFO: Created: latency-svc-l66vb
Dec 17 09:31:21.336: INFO: Got endpoints: latency-svc-l66vb [578.594815ms]
Dec 17 09:31:21.358: INFO: Created: latency-svc-h2qvf
Dec 17 09:31:21.358: INFO: Got endpoints: latency-svc-h2qvf [601.253679ms]
Dec 17 09:31:21.383: INFO: Created: latency-svc-szbgv
Dec 17 09:31:21.391: INFO: Got endpoints: latency-svc-szbgv [634.055493ms]
Dec 17 09:31:21.404: INFO: Created: latency-svc-lw6vd
Dec 17 09:31:21.428: INFO: Got endpoints: latency-svc-lw6vd [670.898906ms]
Dec 17 09:31:21.438: INFO: Created: latency-svc-9zrt9
Dec 17 09:31:21.444: INFO: Got endpoints: latency-svc-9zrt9 [643.241479ms]
Dec 17 09:31:21.500: INFO: Created: latency-svc-qqz6n
Dec 17 09:31:21.502: INFO: Got endpoints: latency-svc-qqz6n [667.052906ms]
Dec 17 09:31:21.545: INFO: Created: latency-svc-pmprw
Dec 17 09:31:21.550: INFO: Got endpoints: latency-svc-pmprw [680.074109ms]
Dec 17 09:31:21.567: INFO: Created: latency-svc-zqfxc
Dec 17 09:31:21.582: INFO: Got endpoints: latency-svc-zqfxc [688.516191ms]
Dec 17 09:31:21.591: INFO: Created: latency-svc-8hhz2
Dec 17 09:31:21.600: INFO: Got endpoints: latency-svc-8hhz2 [677.581541ms]
Dec 17 09:31:21.620: INFO: Created: latency-svc-xl52s
Dec 17 09:31:21.626: INFO: Got endpoints: latency-svc-xl52s [644.385925ms]
Dec 17 09:31:21.634: INFO: Created: latency-svc-spjmb
Dec 17 09:31:21.650: INFO: Got endpoints: latency-svc-spjmb [629.215955ms]
Dec 17 09:31:21.669: INFO: Created: latency-svc-fwpcb
Dec 17 09:31:21.673: INFO: Got endpoints: latency-svc-fwpcb [621.45466ms]
Dec 17 09:31:21.699: INFO: Created: latency-svc-2kzjj
Dec 17 09:31:21.701: INFO: Got endpoints: latency-svc-2kzjj [622.490979ms]
Dec 17 09:31:21.717: INFO: Created: latency-svc-ckqjq
Dec 17 09:31:21.721: INFO: Got endpoints: latency-svc-ckqjq [621.23396ms]
Dec 17 09:31:21.746: INFO: Created: latency-svc-qtzjf
Dec 17 09:31:21.779: INFO: Got endpoints: latency-svc-qtzjf [612.010486ms]
Dec 17 09:31:21.805: INFO: Created: latency-svc-vtqlv
Dec 17 09:31:21.813: INFO: Got endpoints: latency-svc-vtqlv [477.333374ms]
Dec 17 09:31:21.891: INFO: Created: latency-svc-glfsj
Dec 17 09:31:21.919: INFO: Got endpoints: latency-svc-glfsj [561.163824ms]
Dec 17 09:31:21.931: INFO: Created: latency-svc-sx9pn
Dec 17 09:31:21.947: INFO: Got endpoints: latency-svc-sx9pn [556.059938ms]
Dec 17 09:31:21.971: INFO: Created: latency-svc-cvm4l
Dec 17 09:31:21.984: INFO: Got endpoints: latency-svc-cvm4l [556.082307ms]
Dec 17 09:31:21.999: INFO: Created: latency-svc-p5gr9
Dec 17 09:31:22.038: INFO: Got endpoints: latency-svc-p5gr9 [593.998071ms]
Dec 17 09:31:22.049: INFO: Created: latency-svc-25zs8
Dec 17 09:31:22.077: INFO: Got endpoints: latency-svc-25zs8 [575.241712ms]
Dec 17 09:31:22.106: INFO: Created: latency-svc-4xs9j
Dec 17 09:31:22.107: INFO: Got endpoints: latency-svc-4xs9j [557.235671ms]
Dec 17 09:31:22.132: INFO: Created: latency-svc-vfwph
Dec 17 09:31:22.149: INFO: Got endpoints: latency-svc-vfwph [566.623663ms]
Dec 17 09:31:22.169: INFO: Created: latency-svc-4clbq
Dec 17 09:31:22.177: INFO: Got endpoints: latency-svc-4clbq [577.0014ms]
Dec 17 09:31:22.188: INFO: Created: latency-svc-hvwc4
Dec 17 09:31:22.190: INFO: Got endpoints: latency-svc-hvwc4 [563.805638ms]
Dec 17 09:31:22.214: INFO: Created: latency-svc-8824q
Dec 17 09:31:22.218: INFO: Got endpoints: latency-svc-8824q [568.247906ms]
Dec 17 09:31:22.241: INFO: Created: latency-svc-sstns
Dec 17 09:31:22.261: INFO: Created: latency-svc-42cmp
Dec 17 09:31:22.264: INFO: Got endpoints: latency-svc-sstns [590.907023ms]
Dec 17 09:31:22.273: INFO: Got endpoints: latency-svc-42cmp [571.388918ms]
Dec 17 09:31:22.315: INFO: Created: latency-svc-5jxks
Dec 17 09:31:22.321: INFO: Got endpoints: latency-svc-5jxks [599.60004ms]
Dec 17 09:31:22.344: INFO: Created: latency-svc-v8pbd
Dec 17 09:31:22.352: INFO: Got endpoints: latency-svc-v8pbd [572.882734ms]
Dec 17 09:31:22.376: INFO: Created: latency-svc-vlcj6
Dec 17 09:31:22.384: INFO: Got endpoints: latency-svc-vlcj6 [571.339905ms]
Dec 17 09:31:22.395: INFO: Created: latency-svc-lvjcp
Dec 17 09:31:22.404: INFO: Got endpoints: latency-svc-lvjcp [484.312218ms]
Dec 17 09:31:22.466: INFO: Created: latency-svc-w5sjt
Dec 17 09:31:22.470: INFO: Got endpoints: latency-svc-w5sjt [522.290842ms]
Dec 17 09:31:22.494: INFO: Created: latency-svc-ghz6c
Dec 17 09:31:22.498: INFO: Got endpoints: latency-svc-ghz6c [513.405457ms]
Dec 17 09:31:22.777: INFO: Created: latency-svc-5j4zh
Dec 17 09:31:22.790: INFO: Got endpoints: latency-svc-5j4zh [751.73093ms]
Dec 17 09:31:22.810: INFO: Created: latency-svc-45p5d
Dec 17 09:31:22.821: INFO: Got endpoints: latency-svc-45p5d [743.387803ms]
Dec 17 09:31:22.826: INFO: Created: latency-svc-dbplr
Dec 17 09:31:22.832: INFO: Got endpoints: latency-svc-dbplr [724.629998ms]
Dec 17 09:31:22.867: INFO: Created: latency-svc-fvqfg
Dec 17 09:31:22.879: INFO: Got endpoints: latency-svc-fvqfg [730.053201ms]
Dec 17 09:31:22.907: INFO: Created: latency-svc-m2dl9
Dec 17 09:31:22.921: INFO: Got endpoints: latency-svc-m2dl9 [743.930377ms]
Dec 17 09:31:22.941: INFO: Created: latency-svc-hkgxm
Dec 17 09:31:22.950: INFO: Got endpoints: latency-svc-hkgxm [759.559193ms]
Dec 17 09:31:22.968: INFO: Created: latency-svc-7zng5
Dec 17 09:31:22.986: INFO: Got endpoints: latency-svc-7zng5 [768.078623ms]
Dec 17 09:31:22.995: INFO: Created: latency-svc-4n56j
Dec 17 09:31:23.000: INFO: Got endpoints: latency-svc-4n56j [735.480758ms]
Dec 17 09:31:23.010: INFO: Created: latency-svc-q9nnb
Dec 17 09:31:23.020: INFO: Got endpoints: latency-svc-q9nnb [746.952094ms]
Dec 17 09:31:23.042: INFO: Created: latency-svc-5xn7d
Dec 17 09:31:23.054: INFO: Got endpoints: latency-svc-5xn7d [732.542132ms]
Dec 17 09:31:23.072: INFO: Created: latency-svc-nqjmn
Dec 17 09:31:23.083: INFO: Got endpoints: latency-svc-nqjmn [730.402187ms]
Dec 17 09:31:23.102: INFO: Created: latency-svc-4lcf5
Dec 17 09:31:23.112: INFO: Got endpoints: latency-svc-4lcf5 [727.742077ms]
Dec 17 09:31:23.127: INFO: Created: latency-svc-vzvwd
Dec 17 09:31:23.141: INFO: Got endpoints: latency-svc-vzvwd [736.557621ms]
Dec 17 09:31:23.162: INFO: Created: latency-svc-6zg4h
Dec 17 09:31:23.168: INFO: Got endpoints: latency-svc-6zg4h [698.067731ms]
Dec 17 09:31:23.182: INFO: Created: latency-svc-h5plc
Dec 17 09:31:23.207: INFO: Got endpoints: latency-svc-h5plc [709.319968ms]
Dec 17 09:31:23.257: INFO: Created: latency-svc-lzcrl
Dec 17 09:31:23.261: INFO: Got endpoints: latency-svc-lzcrl [471.338264ms]
Dec 17 09:31:23.276: INFO: Created: latency-svc-dcw58
Dec 17 09:31:23.282: INFO: Got endpoints: latency-svc-dcw58 [461.601611ms]
Dec 17 09:31:23.298: INFO: Created: latency-svc-kq5kc
Dec 17 09:31:23.316: INFO: Got endpoints: latency-svc-kq5kc [484.589793ms]
Dec 17 09:31:23.336: INFO: Created: latency-svc-cxpls
Dec 17 09:31:23.344: INFO: Created: latency-svc-qxq7g
Dec 17 09:31:23.345: INFO: Got endpoints: latency-svc-cxpls [465.398258ms]
Dec 17 09:31:23.383: INFO: Got endpoints: latency-svc-qxq7g [461.603904ms]
Dec 17 09:31:23.394: INFO: Created: latency-svc-knszw
Dec 17 09:31:23.404: INFO: Got endpoints: latency-svc-knszw [453.573639ms]
Dec 17 09:31:23.437: INFO: Created: latency-svc-24vf4
Dec 17 09:31:23.444: INFO: Got endpoints: latency-svc-24vf4 [457.881839ms]
Dec 17 09:31:23.456: INFO: Created: latency-svc-fgxx9
Dec 17 09:31:23.495: INFO: Got endpoints: latency-svc-fgxx9 [495.44279ms]
Dec 17 09:31:23.529: INFO: Created: latency-svc-6s2nc
Dec 17 09:31:23.532: INFO: Got endpoints: latency-svc-6s2nc [87.789276ms]
Dec 17 09:31:23.536: INFO: Created: latency-svc-n68v8
Dec 17 09:31:23.552: INFO: Got endpoints: latency-svc-n68v8 [531.454526ms]
Dec 17 09:31:23.566: INFO: Created: latency-svc-pscns
Dec 17 09:31:23.571: INFO: Got endpoints: latency-svc-pscns [516.843258ms]
Dec 17 09:31:23.611: INFO: Created: latency-svc-9fs6l
Dec 17 09:31:23.616: INFO: Got endpoints: latency-svc-9fs6l [532.711086ms]
Dec 17 09:31:23.675: INFO: Created: latency-svc-dw9n5
Dec 17 09:31:23.675: INFO: Got endpoints: latency-svc-dw9n5 [562.650566ms]
Dec 17 09:31:23.715: INFO: Created: latency-svc-nwfxc
Dec 17 09:31:23.715: INFO: Got endpoints: latency-svc-nwfxc [574.682803ms]
Dec 17 09:31:23.775: INFO: Created: latency-svc-655kg
Dec 17 09:31:23.775: INFO: Got endpoints: latency-svc-655kg [606.798369ms]
Dec 17 09:31:23.804: INFO: Created: latency-svc-zwxr2
Dec 17 09:31:23.815: INFO: Got endpoints: latency-svc-zwxr2 [607.739134ms]
Dec 17 09:31:23.852: INFO: Created: latency-svc-mkfv8
Dec 17 09:31:23.862: INFO: Got endpoints: latency-svc-mkfv8 [600.711208ms]
Dec 17 09:31:23.908: INFO: Created: latency-svc-ptgv2
Dec 17 09:31:23.910: INFO: Got endpoints: latency-svc-ptgv2 [627.141463ms]
Dec 17 09:31:23.939: INFO: Created: latency-svc-7mdvp
Dec 17 09:31:23.946: INFO: Got endpoints: latency-svc-7mdvp [629.153751ms]
Dec 17 09:31:23.981: INFO: Created: latency-svc-vw8lv
Dec 17 09:31:23.988: INFO: Got endpoints: latency-svc-vw8lv [643.049641ms]
Dec 17 09:31:24.020: INFO: Created: latency-svc-f662j
Dec 17 09:31:24.020: INFO: Got endpoints: latency-svc-f662j [637.430426ms]
Dec 17 09:31:24.048: INFO: Created: latency-svc-g8hgt
Dec 17 09:31:24.048: INFO: Got endpoints: latency-svc-g8hgt [644.862337ms]
Dec 17 09:31:24.088: INFO: Created: latency-svc-hz2fs
Dec 17 09:31:24.092: INFO: Got endpoints: latency-svc-hz2fs [597.317578ms]
Dec 17 09:31:24.122: INFO: Created: latency-svc-8slhc
Dec 17 09:31:24.134: INFO: Got endpoints: latency-svc-8slhc [601.943087ms]
Dec 17 09:31:24.147: INFO: Created: latency-svc-bp4j8
Dec 17 09:31:24.177: INFO: Got endpoints: latency-svc-bp4j8 [625.908319ms]
Dec 17 09:31:24.194: INFO: Created: latency-svc-bqpdt
Dec 17 09:31:24.203: INFO: Got endpoints: latency-svc-bqpdt [631.909971ms]
Dec 17 09:31:24.219: INFO: Created: latency-svc-6bm6r
Dec 17 09:31:24.234: INFO: Got endpoints: latency-svc-6bm6r [618.122029ms]
Dec 17 09:31:24.268: INFO: Created: latency-svc-9wr9p
Dec 17 09:31:24.284: INFO: Got endpoints: latency-svc-9wr9p [609.363778ms]
Dec 17 09:31:24.311: INFO: Created: latency-svc-hfhss
Dec 17 09:31:24.317: INFO: Got endpoints: latency-svc-hfhss [602.017983ms]
Dec 17 09:31:24.370: INFO: Created: latency-svc-wrxcm
Dec 17 09:31:24.370: INFO: Got endpoints: latency-svc-wrxcm [595.205866ms]
Dec 17 09:31:24.406: INFO: Created: latency-svc-lrrm5
Dec 17 09:31:24.416: INFO: Got endpoints: latency-svc-lrrm5 [601.659427ms]
Dec 17 09:31:24.453: INFO: Created: latency-svc-zh7zv
Dec 17 09:31:24.479: INFO: Got endpoints: latency-svc-zh7zv [616.96184ms]
Dec 17 09:31:24.510: INFO: Created: latency-svc-wrspd
Dec 17 09:31:24.519: INFO: Got endpoints: latency-svc-wrspd [609.776624ms]
Dec 17 09:31:24.541: INFO: Created: latency-svc-h6vhr
Dec 17 09:31:24.569: INFO: Got endpoints: latency-svc-h6vhr [623.400852ms]
Dec 17 09:31:24.578: INFO: Created: latency-svc-jrw8z
Dec 17 09:31:24.596: INFO: Got endpoints: latency-svc-jrw8z [607.883169ms]
Dec 17 09:31:24.644: INFO: Created: latency-svc-559n8
Dec 17 09:31:24.710: INFO: Got endpoints: latency-svc-559n8 [689.391775ms]
Dec 17 09:31:24.758: INFO: Created: latency-svc-k59h4
Dec 17 09:31:24.781: INFO: Got endpoints: latency-svc-k59h4 [732.122508ms]
Dec 17 09:31:24.806: INFO: Created: latency-svc-d7zfg
Dec 17 09:31:24.896: INFO: Got endpoints: latency-svc-d7zfg [803.750095ms]
Dec 17 09:31:24.913: INFO: Created: latency-svc-tf588
Dec 17 09:31:24.917: INFO: Got endpoints: latency-svc-tf588 [782.647655ms]
Dec 17 09:31:24.930: INFO: Created: latency-svc-tfdr7
Dec 17 09:31:24.940: INFO: Got endpoints: latency-svc-tfdr7 [762.543854ms]
Dec 17 09:31:24.972: INFO: Created: latency-svc-nhggp
Dec 17 09:31:24.973: INFO: Got endpoints: latency-svc-nhggp [769.961368ms]
Dec 17 09:31:24.989: INFO: Created: latency-svc-ctqwf
Dec 17 09:31:24.992: INFO: Got endpoints: latency-svc-ctqwf [758.007188ms]
Dec 17 09:31:25.044: INFO: Created: latency-svc-bjkj5
Dec 17 09:31:25.056: INFO: Created: latency-svc-wcqth
Dec 17 09:31:25.073: INFO: Got endpoints: latency-svc-bjkj5 [788.881812ms]
Dec 17 09:31:25.083: INFO: Got endpoints: latency-svc-wcqth [765.801238ms]
Dec 17 09:31:25.090: INFO: Created: latency-svc-cjh7d
Dec 17 09:31:25.098: INFO: Got endpoints: latency-svc-cjh7d [728.333595ms]
Dec 17 09:31:25.108: INFO: Created: latency-svc-fvmcl
Dec 17 09:31:25.112: INFO: Got endpoints: latency-svc-fvmcl [695.848735ms]
Dec 17 09:31:25.138: INFO: Created: latency-svc-ztxc7
Dec 17 09:31:25.143: INFO: Got endpoints: latency-svc-ztxc7 [663.461007ms]
Dec 17 09:31:25.165: INFO: Created: latency-svc-f7hnb
Dec 17 09:31:25.175: INFO: Got endpoints: latency-svc-f7hnb [655.369191ms]
Dec 17 09:31:25.191: INFO: Created: latency-svc-2cdf7
Dec 17 09:31:25.197: INFO: Got endpoints: latency-svc-2cdf7 [627.746453ms]
Dec 17 09:31:25.212: INFO: Created: latency-svc-dr76z
Dec 17 09:31:25.225: INFO: Got endpoints: latency-svc-dr76z [629.079672ms]
Dec 17 09:31:25.236: INFO: Created: latency-svc-m5rpv
Dec 17 09:31:25.243: INFO: Got endpoints: latency-svc-m5rpv [533.485741ms]
Dec 17 09:31:25.263: INFO: Created: latency-svc-ww7zz
Dec 17 09:31:25.267: INFO: Got endpoints: latency-svc-ww7zz [486.687871ms]
Dec 17 09:31:25.288: INFO: Created: latency-svc-t6ffs
Dec 17 09:31:25.298: INFO: Got endpoints: latency-svc-t6ffs [401.969385ms]
Dec 17 09:31:25.321: INFO: Created: latency-svc-zm8zf
Dec 17 09:31:25.327: INFO: Got endpoints: latency-svc-zm8zf [409.671547ms]
Dec 17 09:31:25.328: INFO: Created: latency-svc-qjnp2
Dec 17 09:31:25.337: INFO: Got endpoints: latency-svc-qjnp2 [396.95722ms]
Dec 17 09:31:25.351: INFO: Created: latency-svc-gxdtt
Dec 17 09:31:25.363: INFO: Got endpoints: latency-svc-gxdtt [390.381804ms]
Dec 17 09:31:25.381: INFO: Created: latency-svc-chj45
Dec 17 09:31:25.387: INFO: Got endpoints: latency-svc-chj45 [394.477047ms]
Dec 17 09:31:25.400: INFO: Created: latency-svc-bvt8z
Dec 17 09:31:25.408: INFO: Got endpoints: latency-svc-bvt8z [334.502324ms]
Dec 17 09:31:25.456: INFO: Created: latency-svc-jkrp5
Dec 17 09:31:25.479: INFO: Got endpoints: latency-svc-jkrp5 [396.213415ms]
Dec 17 09:31:25.487: INFO: Created: latency-svc-ss6r5
Dec 17 09:31:25.498: INFO: Got endpoints: latency-svc-ss6r5 [399.304516ms]
Dec 17 09:31:25.506: INFO: Created: latency-svc-dclm6
Dec 17 09:31:25.518: INFO: Got endpoints: latency-svc-dclm6 [405.466052ms]
Dec 17 09:31:25.533: INFO: Created: latency-svc-rt2jf
Dec 17 09:31:25.534: INFO: Got endpoints: latency-svc-rt2jf [390.792305ms]
Dec 17 09:31:25.571: INFO: Created: latency-svc-6tjsk
Dec 17 09:31:25.571: INFO: Got endpoints: latency-svc-6tjsk [396.052929ms]
Dec 17 09:31:25.605: INFO: Created: latency-svc-pfd2b
Dec 17 09:31:25.610: INFO: Got endpoints: latency-svc-pfd2b [412.536232ms]
Dec 17 09:31:25.635: INFO: Created: latency-svc-2chnk
Dec 17 09:31:25.640: INFO: Got endpoints: latency-svc-2chnk [415.092217ms]
Dec 17 09:31:25.669: INFO: Created: latency-svc-fwzmr
Dec 17 09:31:25.678: INFO: Got endpoints: latency-svc-fwzmr [434.207756ms]
Dec 17 09:31:25.704: INFO: Created: latency-svc-c8g7p
Dec 17 09:31:25.708: INFO: Got endpoints: latency-svc-c8g7p [440.768596ms]
Dec 17 09:31:25.737: INFO: Created: latency-svc-fjdjb
Dec 17 09:31:25.745: INFO: Got endpoints: latency-svc-fjdjb [447.009346ms]
Dec 17 09:31:25.772: INFO: Created: latency-svc-gndff
Dec 17 09:31:25.774: INFO: Got endpoints: latency-svc-gndff [447.741474ms]
Dec 17 09:31:25.786: INFO: Created: latency-svc-x7lj5
Dec 17 09:31:25.820: INFO: Got endpoints: latency-svc-x7lj5 [482.848958ms]
Dec 17 09:31:25.827: INFO: Created: latency-svc-z5vdr
Dec 17 09:31:25.841: INFO: Created: latency-svc-4ffs9
Dec 17 09:31:25.849: INFO: Created: latency-svc-pjmq8
Dec 17 09:31:25.858: INFO: Got endpoints: latency-svc-4ffs9 [494.299942ms]
Dec 17 09:31:25.866: INFO: Created: latency-svc-dlnjv
Dec 17 09:31:25.882: INFO: Created: latency-svc-j5mm4
Dec 17 09:31:25.917: INFO: Created: latency-svc-ggwsx
Dec 17 09:31:25.923: INFO: Got endpoints: latency-svc-z5vdr [536.054918ms]
Dec 17 09:31:25.938: INFO: Created: latency-svc-4pmwb
Dec 17 09:31:25.955: INFO: Got endpoints: latency-svc-pjmq8 [546.44462ms]
Dec 17 09:31:25.965: INFO: Created: latency-svc-ltx57
Dec 17 09:31:26.007: INFO: Created: latency-svc-dfcjs
Dec 17 09:31:26.007: INFO: Got endpoints: latency-svc-dlnjv [526.979473ms]
Dec 17 09:31:26.041: INFO: Created: latency-svc-mbbf8
Dec 17 09:31:26.065: INFO: Got endpoints: latency-svc-j5mm4 [567.458209ms]
Dec 17 09:31:26.080: INFO: Created: latency-svc-ktdtv
Dec 17 09:31:26.105: INFO: Created: latency-svc-p29zw
Dec 17 09:31:26.106: INFO: Got endpoints: latency-svc-ggwsx [588.42145ms]
Dec 17 09:31:26.125: INFO: Created: latency-svc-h8bpf
Dec 17 09:31:26.144: INFO: Created: latency-svc-5q4mc
Dec 17 09:31:26.151: INFO: Got endpoints: latency-svc-4pmwb [617.473056ms]
Dec 17 09:31:26.166: INFO: Created: latency-svc-kv2mr
Dec 17 09:31:26.184: INFO: Created: latency-svc-xvtcf
Dec 17 09:31:26.192: INFO: Created: latency-svc-dwbf5
Dec 17 09:31:26.199: INFO: Got endpoints: latency-svc-ltx57 [628.170509ms]
Dec 17 09:31:26.221: INFO: Created: latency-svc-fwgql
Dec 17 09:31:26.229: INFO: Created: latency-svc-kqslx
Dec 17 09:31:26.245: INFO: Created: latency-svc-zh9dt
Dec 17 09:31:26.261: INFO: Got endpoints: latency-svc-dfcjs [651.253837ms]
Dec 17 09:31:26.277: INFO: Created: latency-svc-pnw7l
Dec 17 09:31:26.292: INFO: Created: latency-svc-hdx6f
Dec 17 09:31:26.308: INFO: Got endpoints: latency-svc-mbbf8 [667.561163ms]
Dec 17 09:31:26.319: INFO: Created: latency-svc-w4shb
Dec 17 09:31:26.331: INFO: Created: latency-svc-8x5hk
Dec 17 09:31:26.350: INFO: Created: latency-svc-qltzz
Dec 17 09:31:26.353: INFO: Got endpoints: latency-svc-ktdtv [674.843311ms]
Dec 17 09:31:26.399: INFO: Created: latency-svc-5blmz
Dec 17 09:31:26.430: INFO: Got endpoints: latency-svc-p29zw [721.488139ms]
Dec 17 09:31:26.453: INFO: Got endpoints: latency-svc-h8bpf [707.22241ms]
Dec 17 09:31:26.469: INFO: Created: latency-svc-982r8
Dec 17 09:31:26.513: INFO: Got endpoints: latency-svc-5q4mc [738.354633ms]
Dec 17 09:31:26.531: INFO: Created: latency-svc-b5gxl
Dec 17 09:31:26.542: INFO: Created: latency-svc-rvtpf
Dec 17 09:31:26.555: INFO: Got endpoints: latency-svc-kv2mr [735.255949ms]
Dec 17 09:31:26.575: INFO: Created: latency-svc-6z56n
Dec 17 09:31:26.599: INFO: Got endpoints: latency-svc-xvtcf [741.76693ms]
Dec 17 09:31:26.633: INFO: Created: latency-svc-kzg7q
Dec 17 09:31:26.649: INFO: Got endpoints: latency-svc-dwbf5 [725.734127ms]
Dec 17 09:31:26.691: INFO: Created: latency-svc-xqr5v
Dec 17 09:31:26.702: INFO: Got endpoints: latency-svc-fwgql [747.925494ms]
Dec 17 09:31:26.741: INFO: Created: latency-svc-9gtg2
Dec 17 09:31:26.755: INFO: Got endpoints: latency-svc-kqslx [747.640681ms]
Dec 17 09:31:26.807: INFO: Got endpoints: latency-svc-zh9dt [742.061976ms]
Dec 17 09:31:26.827: INFO: Created: latency-svc-65xvm
Dec 17 09:31:26.856: INFO: Got endpoints: latency-svc-pnw7l [749.585668ms]
Dec 17 09:31:26.884: INFO: Created: latency-svc-4hz4x
Dec 17 09:31:26.902: INFO: Created: latency-svc-47rbw
Dec 17 09:31:26.905: INFO: Got endpoints: latency-svc-hdx6f [754.24514ms]
Dec 17 09:31:26.933: INFO: Created: latency-svc-nnmwl
Dec 17 09:31:26.954: INFO: Got endpoints: latency-svc-w4shb [754.353579ms]
Dec 17 09:31:26.975: INFO: Created: latency-svc-p4f7j
Dec 17 09:31:27.014: INFO: Got endpoints: latency-svc-8x5hk [752.410959ms]
Dec 17 09:31:27.030: INFO: Created: latency-svc-b5v44
Dec 17 09:31:27.059: INFO: Got endpoints: latency-svc-qltzz [751.046998ms]
Dec 17 09:31:27.085: INFO: Created: latency-svc-98v7q
Dec 17 09:31:27.102: INFO: Got endpoints: latency-svc-5blmz [749.454009ms]
Dec 17 09:31:27.127: INFO: Created: latency-svc-fk4l6
Dec 17 09:31:27.150: INFO: Got endpoints: latency-svc-982r8 [720.03041ms]
Dec 17 09:31:27.235: INFO: Got endpoints: latency-svc-b5gxl [782.009995ms]
Dec 17 09:31:27.253: INFO: Created: latency-svc-clrrx
Dec 17 09:31:27.271: INFO: Got endpoints: latency-svc-rvtpf [758.370653ms]
Dec 17 09:31:27.305: INFO: Created: latency-svc-48fkx
Dec 17 09:31:27.311: INFO: Got endpoints: latency-svc-6z56n [755.923818ms]
Dec 17 09:31:27.332: INFO: Created: latency-svc-q29pk
Dec 17 09:31:27.355: INFO: Got endpoints: latency-svc-kzg7q [755.575955ms]
Dec 17 09:31:27.369: INFO: Created: latency-svc-ghrbs
Dec 17 09:31:27.416: INFO: Got endpoints: latency-svc-xqr5v [767.080427ms]
Dec 17 09:31:27.420: INFO: Created: latency-svc-qslft
Dec 17 09:31:27.471: INFO: Created: latency-svc-ljwlx
Dec 17 09:31:27.472: INFO: Got endpoints: latency-svc-9gtg2 [769.765844ms]
Dec 17 09:31:27.535: INFO: Created: latency-svc-xnz8g
Dec 17 09:31:27.552: INFO: Got endpoints: latency-svc-65xvm [797.149074ms]
Dec 17 09:31:27.558: INFO: Got endpoints: latency-svc-4hz4x [750.281383ms]
Dec 17 09:31:27.610: INFO: Got endpoints: latency-svc-47rbw [753.677254ms]
Dec 17 09:31:27.621: INFO: Created: latency-svc-vr89h
Dec 17 09:31:27.638: INFO: Created: latency-svc-5hznx
Dec 17 09:31:27.657: INFO: Got endpoints: latency-svc-nnmwl [751.172409ms]
Dec 17 09:31:27.659: INFO: Created: latency-svc-dpv76
Dec 17 09:31:27.678: INFO: Created: latency-svc-bxxkk
Dec 17 09:31:27.708: INFO: Got endpoints: latency-svc-p4f7j [754.233636ms]
Dec 17 09:31:27.727: INFO: Created: latency-svc-dpqdl
Dec 17 09:31:27.747: INFO: Got endpoints: latency-svc-b5v44 [733.637074ms]
Dec 17 09:31:27.786: INFO: Created: latency-svc-bwbpq
Dec 17 09:31:27.813: INFO: Got endpoints: latency-svc-98v7q [754.724831ms]
Dec 17 09:31:27.831: INFO: Created: latency-svc-bk48k
Dec 17 09:31:27.851: INFO: Got endpoints: latency-svc-fk4l6 [748.372736ms]
Dec 17 09:31:27.871: INFO: Created: latency-svc-2gb9s
Dec 17 09:31:27.899: INFO: Got endpoints: latency-svc-clrrx [748.841007ms]
Dec 17 09:31:27.926: INFO: Created: latency-svc-8t2fk
Dec 17 09:31:27.950: INFO: Got endpoints: latency-svc-48fkx [714.955738ms]
Dec 17 09:31:28.074: INFO: Got endpoints: latency-svc-ghrbs [762.396382ms]
Dec 17 09:31:28.074: INFO: Got endpoints: latency-svc-q29pk [802.558043ms]
Dec 17 09:31:28.113: INFO: Got endpoints: latency-svc-qslft [757.441575ms]
Dec 17 09:31:28.126: INFO: Created: latency-svc-fxrr6
Dec 17 09:31:28.211: INFO: Got endpoints: latency-svc-ljwlx [794.603258ms]
Dec 17 09:31:28.211: INFO: Created: latency-svc-zvtgs
Dec 17 09:31:28.228: INFO: Got endpoints: latency-svc-xnz8g [755.591134ms]
Dec 17 09:31:28.247: INFO: Created: latency-svc-f4gfm
Dec 17 09:31:28.254: INFO: Got endpoints: latency-svc-vr89h [701.926028ms]
Dec 17 09:31:28.264: INFO: Created: latency-svc-pbjz8
Dec 17 09:31:28.278: INFO: Created: latency-svc-r7slg
Dec 17 09:31:28.303: INFO: Got endpoints: latency-svc-5hznx [745.270938ms]
Dec 17 09:31:28.311: INFO: Created: latency-svc-wcvdh
Dec 17 09:31:28.328: INFO: Created: latency-svc-q9lr4
Dec 17 09:31:28.355: INFO: Created: latency-svc-8zbtg
Dec 17 09:31:28.356: INFO: Got endpoints: latency-svc-dpv76 [746.254247ms]
Dec 17 09:31:28.381: INFO: Created: latency-svc-z8h6t
Dec 17 09:31:28.402: INFO: Got endpoints: latency-svc-bxxkk [745.299503ms]
Dec 17 09:31:28.424: INFO: Created: latency-svc-4n8sk
Dec 17 09:31:28.455: INFO: Got endpoints: latency-svc-dpqdl [746.930492ms]
Dec 17 09:31:28.480: INFO: Created: latency-svc-6gccd
Dec 17 09:31:28.518: INFO: Got endpoints: latency-svc-bwbpq [771.102459ms]
Dec 17 09:31:28.551: INFO: Created: latency-svc-545vl
Dec 17 09:31:28.557: INFO: Got endpoints: latency-svc-bk48k [743.779311ms]
Dec 17 09:31:28.583: INFO: Created: latency-svc-hqz44
Dec 17 09:31:28.600: INFO: Got endpoints: latency-svc-2gb9s [748.728877ms]
Dec 17 09:31:28.650: INFO: Got endpoints: latency-svc-8t2fk [750.556691ms]
Dec 17 09:31:28.702: INFO: Got endpoints: latency-svc-fxrr6 [752.047874ms]
Dec 17 09:31:28.760: INFO: Got endpoints: latency-svc-zvtgs [686.256164ms]
Dec 17 09:31:28.803: INFO: Got endpoints: latency-svc-f4gfm [729.231753ms]
Dec 17 09:31:28.851: INFO: Got endpoints: latency-svc-pbjz8 [737.550264ms]
Dec 17 09:31:28.900: INFO: Got endpoints: latency-svc-r7slg [688.897688ms]
Dec 17 09:31:28.949: INFO: Got endpoints: latency-svc-wcvdh [721.226507ms]
Dec 17 09:31:29.020: INFO: Got endpoints: latency-svc-q9lr4 [765.906105ms]
Dec 17 09:31:29.049: INFO: Got endpoints: latency-svc-8zbtg [746.093463ms]
Dec 17 09:31:29.103: INFO: Got endpoints: latency-svc-z8h6t [746.686612ms]
Dec 17 09:31:29.152: INFO: Got endpoints: latency-svc-4n8sk [749.369476ms]
Dec 17 09:31:29.204: INFO: Got endpoints: latency-svc-6gccd [748.742322ms]
Dec 17 09:31:29.252: INFO: Got endpoints: latency-svc-545vl [733.889354ms]
Dec 17 09:31:29.307: INFO: Got endpoints: latency-svc-hqz44 [749.474915ms]
Dec 17 09:31:29.307: INFO: Latencies: [44.352637ms 78.040003ms 87.789276ms 113.06411ms 137.110317ms 165.814809ms 225.373221ms 263.368498ms 294.568903ms 321.435937ms 334.502324ms 343.214228ms 390.381804ms 390.792305ms 394.477047ms 396.052929ms 396.213415ms 396.95722ms 399.304516ms 401.969385ms 405.466052ms 409.671547ms 410.199139ms 412.536232ms 415.092217ms 434.207756ms 440.768596ms 447.009346ms 447.741474ms 453.573639ms 457.881839ms 461.601611ms 461.603904ms 465.398258ms 471.338264ms 477.333374ms 482.848958ms 484.312218ms 484.589793ms 486.687871ms 494.299942ms 495.44279ms 513.405457ms 516.843258ms 522.290842ms 526.979473ms 531.454526ms 532.711086ms 533.485741ms 536.054918ms 546.44462ms 556.059938ms 556.082307ms 557.235671ms 561.163824ms 562.650566ms 563.805638ms 566.623663ms 567.458209ms 568.247906ms 571.339905ms 571.388918ms 572.882734ms 574.682803ms 575.241712ms 577.0014ms 578.594815ms 588.42145ms 590.907023ms 593.998071ms 595.205866ms 597.317578ms 599.60004ms 600.711208ms 601.253679ms 601.659427ms 601.943087ms 602.017983ms 606.798369ms 607.739134ms 607.883169ms 609.363778ms 609.776624ms 612.010486ms 616.96184ms 617.473056ms 618.122029ms 621.23396ms 621.45466ms 622.490979ms 623.400852ms 625.908319ms 627.141463ms 627.746453ms 628.170509ms 629.079672ms 629.153751ms 629.215955ms 631.909971ms 634.055493ms 637.430426ms 643.049641ms 643.241479ms 644.385925ms 644.862337ms 651.253837ms 655.369191ms 663.461007ms 667.052906ms 667.561163ms 670.898906ms 674.843311ms 677.581541ms 680.074109ms 686.256164ms 688.516191ms 688.897688ms 689.391775ms 695.848735ms 698.067731ms 701.926028ms 707.22241ms 709.319968ms 714.955738ms 720.03041ms 721.226507ms 721.488139ms 724.629998ms 725.734127ms 727.742077ms 728.333595ms 729.231753ms 730.053201ms 730.402187ms 732.122508ms 732.542132ms 733.637074ms 733.889354ms 735.255949ms 735.480758ms 736.557621ms 737.550264ms 738.354633ms 741.76693ms 742.061976ms 743.387803ms 743.779311ms 743.930377ms 745.270938ms 745.299503ms 746.093463ms 746.254247ms 746.686612ms 746.930492ms 746.952094ms 747.640681ms 747.925494ms 748.372736ms 748.728877ms 748.742322ms 748.841007ms 749.369476ms 749.454009ms 749.474915ms 749.585668ms 750.281383ms 750.556691ms 751.046998ms 751.172409ms 751.73093ms 752.047874ms 752.410959ms 753.677254ms 754.233636ms 754.24514ms 754.353579ms 754.724831ms 755.575955ms 755.591134ms 755.923818ms 757.441575ms 758.007188ms 758.370653ms 759.559193ms 762.396382ms 762.543854ms 765.801238ms 765.906105ms 767.080427ms 768.078623ms 769.765844ms 769.961368ms 771.102459ms 782.009995ms 782.647655ms 788.881812ms 794.603258ms 797.149074ms 802.558043ms 803.750095ms]
Dec 17 09:31:29.308: INFO: 50 %ile: 637.430426ms
Dec 17 09:31:29.308: INFO: 90 %ile: 757.441575ms
Dec 17 09:31:29.308: INFO: 99 %ile: 802.558043ms
Dec 17 09:31:29.308: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:31:29.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9557" for this suite.

• [SLOW TEST:11.926 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":277,"completed":29,"skipped":606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:31:29.331: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-projected-5q6z
STEP: Creating a pod to test atomic-volume-subpath
Dec 17 09:31:29.521: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5q6z" in namespace "subpath-8117" to be "Succeeded or Failed"
Dec 17 09:31:29.537: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Pending", Reason="", readiness=false. Elapsed: 15.968351ms
Dec 17 09:31:31.543: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021638858s
Dec 17 09:31:33.549: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Running", Reason="", readiness=true. Elapsed: 4.028382295s
Dec 17 09:31:35.554: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Running", Reason="", readiness=true. Elapsed: 6.033319776s
Dec 17 09:31:37.674: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Running", Reason="", readiness=true. Elapsed: 8.15320038s
Dec 17 09:31:39.690: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Running", Reason="", readiness=true. Elapsed: 10.169001047s
Dec 17 09:31:41.698: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Running", Reason="", readiness=true. Elapsed: 12.177012335s
Dec 17 09:31:43.706: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Running", Reason="", readiness=true. Elapsed: 14.184549202s
Dec 17 09:31:45.714: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Running", Reason="", readiness=true. Elapsed: 16.192683031s
Dec 17 09:31:47.720: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Running", Reason="", readiness=true. Elapsed: 18.198910019s
Dec 17 09:31:49.726: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Running", Reason="", readiness=true. Elapsed: 20.204864608s
Dec 17 09:31:51.744: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Running", Reason="", readiness=true. Elapsed: 22.223214265s
Dec 17 09:31:53.750: INFO: Pod "pod-subpath-test-projected-5q6z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.228717476s
STEP: Saw pod success
Dec 17 09:31:53.750: INFO: Pod "pod-subpath-test-projected-5q6z" satisfied condition "Succeeded or Failed"
Dec 17 09:31:53.756: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-subpath-test-projected-5q6z container test-container-subpath-projected-5q6z: <nil>
STEP: delete the pod
Dec 17 09:31:53.797: INFO: Waiting for pod pod-subpath-test-projected-5q6z to disappear
Dec 17 09:31:53.800: INFO: Pod pod-subpath-test-projected-5q6z no longer exists
STEP: Deleting pod pod-subpath-test-projected-5q6z
Dec 17 09:31:53.800: INFO: Deleting pod "pod-subpath-test-projected-5q6z" in namespace "subpath-8117"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:31:53.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8117" for this suite.

• [SLOW TEST:24.491 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":277,"completed":30,"skipped":652,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:31:53.822: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6468
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:32:20.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6468" for this suite.

• [SLOW TEST:26.564 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    when starting a container that exits
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":277,"completed":31,"skipped":653,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:32:20.387: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-6165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test hostPath mode
Dec 17 09:32:20.574: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6165" to be "Succeeded or Failed"
Dec 17 09:32:20.584: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 9.960461ms
Dec 17 09:32:22.592: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017875339s
Dec 17 09:32:24.597: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022575161s
STEP: Saw pod success
Dec 17 09:32:24.597: INFO: Pod "pod-host-path-test" satisfied condition "Succeeded or Failed"
Dec 17 09:32:24.602: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 17 09:32:24.637: INFO: Waiting for pod pod-host-path-test to disappear
Dec 17 09:32:24.641: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:32:24.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6165" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":32,"skipped":663,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:32:24.654: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 09:32:25.056: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 09:32:27.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794345, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794345, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794345, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794345, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 09:32:30.098: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 17 09:32:34.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 attach --namespace=webhook-7118 to-be-attached-pod -i -c=container1'
Dec 17 09:32:34.328: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:32:34.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7118" for this suite.
STEP: Destroying namespace "webhook-7118-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.806 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":277,"completed":33,"skipped":673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:32:34.461: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:32:34.686: INFO: Creating deployment "webserver-deployment"
Dec 17 09:32:34.705: INFO: Waiting for observed generation 1
Dec 17 09:32:36.733: INFO: Waiting for all required pods to come up
Dec 17 09:32:36.746: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 17 09:32:40.782: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 17 09:32:40.790: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 17 09:32:40.800: INFO: Updating deployment webserver-deployment
Dec 17 09:32:40.800: INFO: Waiting for observed generation 2
Dec 17 09:32:42.813: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 17 09:32:42.817: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 17 09:32:42.820: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 17 09:32:42.834: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 17 09:32:42.834: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 17 09:32:42.842: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 17 09:32:42.850: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 17 09:32:42.850: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 17 09:32:42.864: INFO: Updating deployment webserver-deployment
Dec 17 09:32:42.864: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 17 09:32:42.945: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 17 09:32:42.969: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Dec 17 09:32:45.012: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4122 /apis/apps/v1/namespaces/deployment-4122/deployments/webserver-deployment fc6dac44-1fa3-47bf-8dc9-690ecbb17705 8133 3 2020-12-17 09:32:34 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a7cd78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-12-17 09:32:43 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-6676bcd6d4" is progressing.,LastUpdateTime:2020-12-17 09:32:43 +0000 UTC,LastTransitionTime:2020-12-17 09:32:34 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 17 09:32:45.019: INFO: New ReplicaSet "webserver-deployment-6676bcd6d4" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-6676bcd6d4  deployment-4122 /apis/apps/v1/namespaces/deployment-4122/replicasets/webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 8130 3 2020-12-17 09:32:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment fc6dac44-1fa3-47bf-8dc9-690ecbb17705 0xc003a7d517 0xc003a7d518}] []  [{kube-controller-manager Update apps/v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 99 54 100 97 99 52 52 45 49 102 97 51 45 52 55 98 102 45 56 100 99 57 45 54 57 48 101 99 98 98 49 55 55 48 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 6676bcd6d4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a7d5a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 17 09:32:45.019: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 17 09:32:45.019: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-84855cf797  deployment-4122 /apis/apps/v1/namespaces/deployment-4122/replicasets/webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 8117 3 2020-12-17 09:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment fc6dac44-1fa3-47bf-8dc9-690ecbb17705 0xc003a7d627 0xc003a7d628}] []  [{kube-controller-manager Update apps/v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 99 54 100 97 99 52 52 45 49 102 97 51 45 52 55 98 102 45 56 100 99 57 45 54 57 48 101 99 98 98 49 55 55 48 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 84855cf797,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a7d6e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 17 09:32:45.049: INFO: Pod "webserver-deployment-6676bcd6d4-5jm2m" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-5jm2m webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-5jm2m c00459ac-be87-44ff-9e11-89dd2145bac6 8128 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003aa5390 0xc003aa5391}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.049: INFO: Pod "webserver-deployment-6676bcd6d4-5sltj" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-5sltj webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-5sltj 45378785-961c-4e8f-9bfc-c60484401bea 8158 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003aa55e0 0xc003aa55e1}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.050: INFO: Pod "webserver-deployment-6676bcd6d4-5zfdk" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-5zfdk webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-5zfdk 1dbc1b68-a76c-4efa-a100-a3479cdb2599 8054 0 2020-12-17 09:32:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:192.168.192.106/32 cni.projectcalico.org/podIPs:192.168.192.106/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003aa5920 0xc003aa5921}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:41 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:42 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:,StartTime:2020-12-17 09:32:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.051: INFO: Pod "webserver-deployment-6676bcd6d4-8vblr" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-8vblr webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-8vblr 0a5ae339-db43-4994-a4c7-37513b267043 8162 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003aa5bb7 0xc003aa5bb8}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.051: INFO: Pod "webserver-deployment-6676bcd6d4-cljhz" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-cljhz webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-cljhz 455b1f78-420b-4bc6-835a-4b262945cb73 8180 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003aa5e77 0xc003aa5e78}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.052: INFO: Pod "webserver-deployment-6676bcd6d4-dkfwf" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-dkfwf webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-dkfwf e628d14e-775e-499b-ac74-890fcdc6dfe6 8045 0 2020-12-17 09:32:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:192.168.137.16/32 cni.projectcalico.org/podIPs:192.168.137.16/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003af40f0 0xc003af40f1}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:41 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:42 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:,StartTime:2020-12-17 09:32:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.052: INFO: Pod "webserver-deployment-6676bcd6d4-f55xn" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-f55xn webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-f55xn a8293e0f-220d-4ce8-9a15-1a6785674476 8126 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003af4420 0xc003af4421}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.052: INFO: Pod "webserver-deployment-6676bcd6d4-jfq4q" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-jfq4q webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-jfq4q ed4fd26d-0af0-4292-bf58-ae8e4df1fb25 8050 0 2020-12-17 09:32:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:192.168.192.105/32 cni.projectcalico.org/podIPs:192.168.192.105/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003af4670 0xc003af4671}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:41 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:42 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:,StartTime:2020-12-17 09:32:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.053: INFO: Pod "webserver-deployment-6676bcd6d4-lq9fs" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-lq9fs webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-lq9fs f8f72333-eed5-40be-8207-7b92bcaf8a07 8058 0 2020-12-17 09:32:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:192.168.137.17/32 cni.projectcalico.org/podIPs:192.168.137.17/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003af4867 0xc003af4868}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:41 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:42 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:,StartTime:2020-12-17 09:32:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.053: INFO: Pod "webserver-deployment-6676bcd6d4-pgf9f" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-pgf9f webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-pgf9f 874fe6b7-8626-4660-aad3-bc7f03bd2998 8132 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003af4bc0 0xc003af4bc1}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.054: INFO: Pod "webserver-deployment-6676bcd6d4-qgcwc" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-qgcwc webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-qgcwc 8502f998-3e0a-43ba-91ec-98f1e8217e9b 8185 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003af4df0 0xc003af4df1}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.054: INFO: Pod "webserver-deployment-6676bcd6d4-zxnhk" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-zxnhk webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-zxnhk da03109d-07aa-4249-b1d6-117f9ad616db 8174 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003af5097 0xc003af5098}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.054: INFO: Pod "webserver-deployment-6676bcd6d4-zxzwv" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-zxzwv webserver-deployment-6676bcd6d4- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-6676bcd6d4-zxzwv d6c9db94-12a2-4866-be57-06a6509a4e68 8062 0 2020-12-17 09:32:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:192.168.192.107/32 cni.projectcalico.org/podIPs:192.168.192.107/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 3f9fa5be-1d16-48a7-a395-e23b613f1e28 0xc003af5340 0xc003af5341}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 102 57 102 97 53 98 101 45 49 100 49 54 45 52 56 97 55 45 97 51 57 53 45 101 50 51 98 54 49 51 102 49 101 50 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:41 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:,StartTime:2020-12-17 09:32:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.055: INFO: Pod "webserver-deployment-84855cf797-2qqvz" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-2qqvz webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-2qqvz dcafd575-3403-419e-b4a2-ae0a488ae2de 8147 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003af5537 0xc003af5538}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.055: INFO: Pod "webserver-deployment-84855cf797-45tzr" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-45tzr webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-45tzr 066b46aa-e16d-46f7-8777-d20d8f86a85b 8139 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003af5710 0xc003af5711}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.056: INFO: Pod "webserver-deployment-84855cf797-5gmb7" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-5gmb7 webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-5gmb7 49bce3d8-033e-4579-9359-091a1aaeaa8f 7908 0 2020-12-17 09:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.137.11/32 cni.projectcalico.org/podIPs:192.168.137.11/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003af5967 0xc003af5968}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 51 55 46 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:192.168.137.11,StartTime:2020-12-17 09:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 09:32:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://124abfef6b773ca3e80fe51be3c7316ca99ecd800418bb79cb8024604361a81e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.137.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.056: INFO: Pod "webserver-deployment-84855cf797-6dbq2" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-6dbq2 webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-6dbq2 bf9656b4-b7c4-4594-bb7f-2e9a9df5634d 7948 0 2020-12-17 09:32:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.137.15/32 cni.projectcalico.org/podIPs:192.168.137.15/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003af5c20 0xc003af5c21}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:35 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 51 55 46 49 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:192.168.137.15,StartTime:2020-12-17 09:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 09:32:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2aabfd0a56dd613676832190369fa6fe46141f000cb785b93175c9da51cb3678,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.137.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.057: INFO: Pod "webserver-deployment-84855cf797-8rfbv" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-8rfbv webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-8rfbv 73078d9f-6038-440a-b107-b629317ce45e 8127 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003af5ed0 0xc003af5ed1}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.057: INFO: Pod "webserver-deployment-84855cf797-bcxkw" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-bcxkw webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-bcxkw 3d08b61e-cc76-4136-a668-2f21f0e0c0f1 8138 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b22167 0xc003b22168}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.058: INFO: Pod "webserver-deployment-84855cf797-dgpnv" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-dgpnv webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-dgpnv e2a50f7d-dce7-450b-a0f1-5c32138f2241 7931 0 2020-12-17 09:32:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.192.102/32 cni.projectcalico.org/podIPs:192.168.192.102/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b22480 0xc003b22481}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:35 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 50 46 49 48 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:192.168.192.102,StartTime:2020-12-17 09:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 09:32:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e0c20d1f7d0c52a4c17911391eb6b1371ae1cff3c2f6ba7351a1247cbeb88267,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.058: INFO: Pod "webserver-deployment-84855cf797-hjwzd" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-hjwzd webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-hjwzd ff5cfe0f-2da7-4f9c-a674-7eeacb92fd2c 8184 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b226c7 0xc003b226c8}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.065: INFO: Pod "webserver-deployment-84855cf797-hsstb" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-hsstb webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-hsstb 4ef48042-9650-4a14-a726-5887d5f0ee32 7904 0 2020-12-17 09:32:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.137.12/32 cni.projectcalico.org/podIPs:192.168.137.12/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b228f0 0xc003b228f1}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:35 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 51 55 46 49 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:192.168.137.12,StartTime:2020-12-17 09:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 09:32:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://80b48c142683a1922fff0027abc32f1cc1c4a11035ea315b88200feceecfa5a1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.137.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.079: INFO: Pod "webserver-deployment-84855cf797-jmsdb" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-jmsdb webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-jmsdb 79e368a0-efc8-41d0-99ed-3a782250fa92 7928 0 2020-12-17 09:32:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.192.103/32 cni.projectcalico.org/podIPs:192.168.192.103/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b22b70 0xc003b22b71}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:35 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 50 46 49 48 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:192.168.192.103,StartTime:2020-12-17 09:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 09:32:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://49f11b848be6ce523dfd46fb33e1bd08651bd176003bdfff911b952e981f8bb7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.079: INFO: Pod "webserver-deployment-84855cf797-knxrf" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-knxrf webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-knxrf 8a9aa1ed-2f3c-4a2c-b3ae-1626db8f3290 8175 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b22d97 0xc003b22d98}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.080: INFO: Pod "webserver-deployment-84855cf797-kshg6" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-kshg6 webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-kshg6 020c2de0-362d-4737-98d8-7f53d6123cbf 8111 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b22f77 0xc003b22f78}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.080: INFO: Pod "webserver-deployment-84855cf797-l5vfg" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-l5vfg webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-l5vfg 3951df9f-2eb1-43a7-8c6f-c03f8e283590 8197 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b23147 0xc003b23148}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:45 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.081: INFO: Pod "webserver-deployment-84855cf797-nw824" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-nw824 webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-nw824 292b95c2-b08c-456b-a5d3-b419f15b0603 8135 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b233a0 0xc003b233a1}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.081: INFO: Pod "webserver-deployment-84855cf797-qfht4" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-qfht4 webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-qfht4 f291e386-4ab2-40b4-bdd8-5025d11727ff 8196 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.137.19/32 cni.projectcalico.org/podIPs:192.168.137.19/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b235f7 0xc003b235f8}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:45 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.081: INFO: Pod "webserver-deployment-84855cf797-qjclz" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-qjclz webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-qjclz bfb2fe31-fbf2-4355-a4bb-f99ca35fd673 7934 0 2020-12-17 09:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.192.101/32 cni.projectcalico.org/podIPs:192.168.192.101/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b23860 0xc003b23861}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:37 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 50 46 49 48 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:192.168.192.101,StartTime:2020-12-17 09:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 09:32:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1aaf26146a5d210cf8d1a7de2a167712f0deaee1b2ea15dc3466be598df53c6e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.101,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.082: INFO: Pod "webserver-deployment-84855cf797-s9h7d" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-s9h7d webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-s9h7d f085ac63-f7cf-4487-9ae2-3e41fe74ec24 7945 0 2020-12-17 09:32:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.137.14/32 cni.projectcalico.org/podIPs:192.168.137.14/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b23b87 0xc003b23b88}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:35 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 51 55 46 49 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:192.168.137.14,StartTime:2020-12-17 09:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 09:32:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8150f0d85cd50fbe4e0d4eada9c5d549ae21bbac53e204fee1331f377a2b0acd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.137.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.082: INFO: Pod "webserver-deployment-84855cf797-scnd7" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-scnd7 webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-scnd7 d7b30d63-6b6c-4e7b-a0cb-e333b10d30e1 7938 0 2020-12-17 09:32:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.192.100/32 cni.projectcalico.org/podIPs:192.168.192.100/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b23eb0 0xc003b23eb1}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:36 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 50 46 49 48 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:192.168.192.100,StartTime:2020-12-17 09:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 09:32:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c9dd0ce47358b46485c6bf0a857a601da2a99e1ec1860330ce4cbf38ab3fe844,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.083: INFO: Pod "webserver-deployment-84855cf797-vm648" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-vm648 webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-vm648 4933c9fb-fd36-4c32-9c4c-1443bb0a9217 8145 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b500e7 0xc003b500e8}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:,StartTime:2020-12-17 09:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:32:45.083: INFO: Pod "webserver-deployment-84855cf797-vm8bj" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-vm8bj webserver-deployment-84855cf797- deployment-4122 /api/v1/namespaces/deployment-4122/pods/webserver-deployment-84855cf797-vm8bj 1eca5fb6-1a8f-417f-8d19-8dae60aba914 8189 0 2020-12-17 09:32:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:192.168.137.18/32 cni.projectcalico.org/podIPs:192.168.137.18/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 bc79abfb-2a12-43a7-a111-9b35be299495 0xc003b50337 0xc003b50338}] []  [{kube-controller-manager Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 99 55 57 97 98 102 98 45 50 97 49 50 45 52 51 97 55 45 97 49 49 49 45 57 98 51 53 98 101 50 57 57 52 57 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 09:32:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:32:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rrnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rrnlw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rrnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.72,PodIP:,StartTime:2020-12-17 09:32:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:32:45.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4122" for this suite.

• [SLOW TEST:10.640 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":277,"completed":34,"skipped":698,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:32:45.102: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 09:32:45.951: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794366, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794366, loc:(*time.Location)(0x7b675e0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-779fdc84d9\""}}, CollisionCount:(*int32)(nil)}
Dec 17 09:32:47.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794366, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794366, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794366, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794366, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 09:32:49.959: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794366, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794366, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794366, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794366, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 09:32:53.054: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:32:53.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2003" for this suite.
STEP: Destroying namespace "webhook-2003-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.169 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":277,"completed":35,"skipped":709,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:32:53.271: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-559
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 17 09:32:53.515: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:32:53.525: INFO: Number of nodes with available pods: 0
Dec 17 09:32:53.525: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 09:32:54.531: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:32:54.560: INFO: Number of nodes with available pods: 0
Dec 17 09:32:54.560: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 09:32:55.533: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:32:55.541: INFO: Number of nodes with available pods: 0
Dec 17 09:32:55.541: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 09:32:56.531: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:32:56.537: INFO: Number of nodes with available pods: 2
Dec 17 09:32:56.537: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 17 09:32:56.557: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:32:56.561: INFO: Number of nodes with available pods: 1
Dec 17 09:32:56.561: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 09:32:57.567: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:32:57.574: INFO: Number of nodes with available pods: 1
Dec 17 09:32:57.574: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 09:32:58.569: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:32:58.582: INFO: Number of nodes with available pods: 1
Dec 17 09:32:58.582: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 09:32:59.571: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:32:59.576: INFO: Number of nodes with available pods: 1
Dec 17 09:32:59.576: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 09:33:00.568: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:33:00.575: INFO: Number of nodes with available pods: 1
Dec 17 09:33:00.575: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 09:33:01.567: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:33:01.571: INFO: Number of nodes with available pods: 1
Dec 17 09:33:01.571: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 09:33:02.568: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:33:02.574: INFO: Number of nodes with available pods: 1
Dec 17 09:33:02.574: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 09:33:03.569: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:33:03.575: INFO: Number of nodes with available pods: 1
Dec 17 09:33:03.575: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 09:33:04.568: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:33:04.574: INFO: Number of nodes with available pods: 1
Dec 17 09:33:04.574: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 09:33:05.568: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:33:05.574: INFO: Number of nodes with available pods: 2
Dec 17 09:33:05.574: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-559, will wait for the garbage collector to delete the pods
Dec 17 09:33:05.643: INFO: Deleting DaemonSet.extensions daemon-set took: 10.584526ms
Dec 17 09:33:05.744: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.390091ms
Dec 17 09:33:09.449: INFO: Number of nodes with available pods: 0
Dec 17 09:33:09.449: INFO: Number of running nodes: 0, number of available pods: 0
Dec 17 09:33:09.454: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-559/daemonsets","resourceVersion":"8865"},"items":null}

Dec 17 09:33:09.460: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-559/pods","resourceVersion":"8865"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:33:09.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-559" for this suite.

• [SLOW TEST:16.222 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":277,"completed":36,"skipped":710,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:33:09.494: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 17 09:33:10.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794390, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794390, loc:(*time.Location)(0x7b675e0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794390, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794390, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 17 09:33:12.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794390, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794390, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794390, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794390, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 09:33:15.188: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:33:15.193: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:33:16.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1433" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.975 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":277,"completed":37,"skipped":743,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:33:16.470: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-4078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec 17 09:33:16.626: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the sample API server.
Dec 17 09:33:17.337: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 17 09:33:19.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794397, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794397, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794397, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794397, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 09:33:21.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794397, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794397, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794397, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794397, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 09:33:23.417: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794397, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794397, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794397, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794397, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 09:33:26.478: INFO: Waited 1.044634941s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:33:27.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4078" for this suite.

• [SLOW TEST:10.703 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":277,"completed":38,"skipped":786,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:33:27.173: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-4606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Dec 17 09:33:27.371: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 17 09:34:27.445: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:34:27.450: INFO: Starting informer...
STEP: Starting pods...
Dec 17 09:34:27.686: INFO: Pod1 is running on ck8s-conftest-118-workload-cluster-worker-0. Tainting Node
Dec 17 09:34:31.726: INFO: Pod2 is running on ck8s-conftest-118-workload-cluster-worker-0. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec 17 09:34:41.590: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 17 09:35:01.589: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:35:01.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4606" for this suite.

• [SLOW TEST:94.473 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":277,"completed":39,"skipped":793,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:35:01.647: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2719
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:35:08.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2719" for this suite.

• [SLOW TEST:7.236 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":277,"completed":40,"skipped":803,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:35:08.884: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 09:35:09.796: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 09:35:11.810: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794510, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794510, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794510, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794510, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 09:35:14.844: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:35:14.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5024" for this suite.
STEP: Destroying namespace "webhook-5024-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.212 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":277,"completed":41,"skipped":824,"failed":0}
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:35:15.096: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7023
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 17 09:35:20.308: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:35:21.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7023" for this suite.

• [SLOW TEST:6.277 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":277,"completed":42,"skipped":824,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:35:21.373: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6977
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Dec 17 09:35:21.527: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 17 09:35:21.548: INFO: Waiting for terminating namespaces to be deleted...
Dec 17 09:35:21.553: INFO: 
Logging pods the kubelet thinks is on node ck8s-conftest-118-workload-cluster-worker-0 before test
Dec 17 09:35:21.586: INFO: sonobuoy from sonobuoy started at 2020-12-17 09:17:49 +0000 UTC (1 container statuses recorded)
Dec 17 09:35:21.586: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 17 09:35:21.586: INFO: kube-proxy-87hvk from kube-system started at 2020-12-17 09:10:11 +0000 UTC (1 container statuses recorded)
Dec 17 09:35:21.586: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 09:35:21.586: INFO: pod-adoption-release-75xr4 from replicaset-7023 started at 2020-12-17 09:35:20 +0000 UTC (1 container statuses recorded)
Dec 17 09:35:21.586: INFO: 	Container pod-adoption-release ready: false, restart count 0
Dec 17 09:35:21.586: INFO: calico-node-hv857 from kube-system started at 2020-12-17 09:10:11 +0000 UTC (1 container statuses recorded)
Dec 17 09:35:21.586: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 09:35:21.586: INFO: sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-hdzhs from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 09:35:21.586: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 09:35:21.586: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 09:35:21.586: INFO: 
Logging pods the kubelet thinks is on node ck8s-conftest-118-workload-cluster-worker-1 before test
Dec 17 09:35:21.619: INFO: calico-node-dfdx8 from kube-system started at 2020-12-17 09:10:13 +0000 UTC (1 container statuses recorded)
Dec 17 09:35:21.619: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 09:35:21.619: INFO: kube-proxy-xk6gr from kube-system started at 2020-12-17 09:10:13 +0000 UTC (1 container statuses recorded)
Dec 17 09:35:21.619: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 09:35:21.619: INFO: sonobuoy-e2e-job-c7b0b40574f24e1c from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 09:35:21.619: INFO: 	Container e2e ready: true, restart count 0
Dec 17 09:35:21.619: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 09:35:21.619: INFO: sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-nn8jm from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 09:35:21.619: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 09:35:21.619: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 09:35:21.619: INFO: pod-adoption-release from replicaset-7023 started at 2020-12-17 09:35:15 +0000 UTC (1 container statuses recorded)
Dec 17 09:35:21.619: INFO: 	Container pod-adoption-release ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1651774ec9c0f4bb], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:35:22.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6977" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":277,"completed":43,"skipped":824,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:35:22.744: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:35:22.917: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-fcd2b248-9f11-4c9e-9f42-6263d6e3baef" in namespace "security-context-test-3597" to be "Succeeded or Failed"
Dec 17 09:35:22.928: INFO: Pod "alpine-nnp-false-fcd2b248-9f11-4c9e-9f42-6263d6e3baef": Phase="Pending", Reason="", readiness=false. Elapsed: 10.732306ms
Dec 17 09:35:24.935: INFO: Pod "alpine-nnp-false-fcd2b248-9f11-4c9e-9f42-6263d6e3baef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01803628s
Dec 17 09:35:26.943: INFO: Pod "alpine-nnp-false-fcd2b248-9f11-4c9e-9f42-6263d6e3baef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025749937s
Dec 17 09:35:28.949: INFO: Pod "alpine-nnp-false-fcd2b248-9f11-4c9e-9f42-6263d6e3baef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031670247s
Dec 17 09:35:28.949: INFO: Pod "alpine-nnp-false-fcd2b248-9f11-4c9e-9f42-6263d6e3baef" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:35:28.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3597" for this suite.

• [SLOW TEST:6.235 seconds]
[k8s.io] Security Context
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":44,"skipped":841,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:35:28.981: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 09:35:29.166: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd35d7ce-f898-4d56-993a-3663e5d648be" in namespace "downward-api-8003" to be "Succeeded or Failed"
Dec 17 09:35:29.177: INFO: Pod "downwardapi-volume-fd35d7ce-f898-4d56-993a-3663e5d648be": Phase="Pending", Reason="", readiness=false. Elapsed: 10.529659ms
Dec 17 09:35:31.184: INFO: Pod "downwardapi-volume-fd35d7ce-f898-4d56-993a-3663e5d648be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017750157s
Dec 17 09:35:33.190: INFO: Pod "downwardapi-volume-fd35d7ce-f898-4d56-993a-3663e5d648be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023173143s
STEP: Saw pod success
Dec 17 09:35:33.190: INFO: Pod "downwardapi-volume-fd35d7ce-f898-4d56-993a-3663e5d648be" satisfied condition "Succeeded or Failed"
Dec 17 09:35:33.196: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-fd35d7ce-f898-4d56-993a-3663e5d648be container client-container: <nil>
STEP: delete the pod
Dec 17 09:35:33.231: INFO: Waiting for pod downwardapi-volume-fd35d7ce-f898-4d56-993a-3663e5d648be to disappear
Dec 17 09:35:33.239: INFO: Pod downwardapi-volume-fd35d7ce-f898-4d56-993a-3663e5d648be no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:35:33.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8003" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":45,"skipped":850,"failed":0}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:35:33.251: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4845
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's command
Dec 17 09:35:33.426: INFO: Waiting up to 5m0s for pod "var-expansion-49296d3b-75c9-4e06-8719-a1d1a98cbe8d" in namespace "var-expansion-4845" to be "Succeeded or Failed"
Dec 17 09:35:33.433: INFO: Pod "var-expansion-49296d3b-75c9-4e06-8719-a1d1a98cbe8d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.929761ms
Dec 17 09:35:35.439: INFO: Pod "var-expansion-49296d3b-75c9-4e06-8719-a1d1a98cbe8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012952277s
Dec 17 09:35:37.445: INFO: Pod "var-expansion-49296d3b-75c9-4e06-8719-a1d1a98cbe8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019104449s
STEP: Saw pod success
Dec 17 09:35:37.445: INFO: Pod "var-expansion-49296d3b-75c9-4e06-8719-a1d1a98cbe8d" satisfied condition "Succeeded or Failed"
Dec 17 09:35:37.452: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod var-expansion-49296d3b-75c9-4e06-8719-a1d1a98cbe8d container dapi-container: <nil>
STEP: delete the pod
Dec 17 09:35:37.479: INFO: Waiting for pod var-expansion-49296d3b-75c9-4e06-8719-a1d1a98cbe8d to disappear
Dec 17 09:35:37.491: INFO: Pod var-expansion-49296d3b-75c9-4e06-8719-a1d1a98cbe8d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:35:37.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4845" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":277,"completed":46,"skipped":856,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:35:37.508: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:35:50.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1909" for this suite.

• [SLOW TEST:13.290 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":277,"completed":47,"skipped":893,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:35:50.799: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7157
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:35:50.960: INFO: Creating ReplicaSet my-hostname-basic-9475c756-4790-4d75-aff6-8e0a0308f22b
Dec 17 09:35:50.971: INFO: Pod name my-hostname-basic-9475c756-4790-4d75-aff6-8e0a0308f22b: Found 0 pods out of 1
Dec 17 09:35:55.976: INFO: Pod name my-hostname-basic-9475c756-4790-4d75-aff6-8e0a0308f22b: Found 1 pods out of 1
Dec 17 09:35:55.976: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9475c756-4790-4d75-aff6-8e0a0308f22b" is running
Dec 17 09:35:55.980: INFO: Pod "my-hostname-basic-9475c756-4790-4d75-aff6-8e0a0308f22b-dkt6w" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-17 09:35:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-17 09:35:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-17 09:35:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-12-17 09:35:51 +0000 UTC Reason: Message:}])
Dec 17 09:35:55.980: INFO: Trying to dial the pod
Dec 17 09:36:00.996: INFO: Controller my-hostname-basic-9475c756-4790-4d75-aff6-8e0a0308f22b: Got expected result from replica 1 [my-hostname-basic-9475c756-4790-4d75-aff6-8e0a0308f22b-dkt6w]: "my-hostname-basic-9475c756-4790-4d75-aff6-8e0a0308f22b-dkt6w", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:36:00.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7157" for this suite.

• [SLOW TEST:10.215 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":48,"skipped":903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:36:01.016: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-1113
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:36:01.181: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Creating first CR 
Dec 17 09:36:01.825: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-17T09:36:02Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-17T09:36:02Z]] name:name1 resourceVersion:9883 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a7c41a02-5660-4ed8-9264-124ff133ec0d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 17 09:36:11.834: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-17T09:36:12Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-17T09:36:12Z]] name:name2 resourceVersion:9926 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1be3c903-2ab3-480f-9582-e3e5db967223] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 17 09:36:21.843: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-17T09:36:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-17T09:36:22Z]] name:name1 resourceVersion:9951 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a7c41a02-5660-4ed8-9264-124ff133ec0d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 17 09:36:31.853: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-17T09:36:12Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-17T09:36:32Z]] name:name2 resourceVersion:9976 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1be3c903-2ab3-480f-9582-e3e5db967223] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 17 09:36:41.866: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-17T09:36:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-17T09:36:22Z]] name:name1 resourceVersion:10001 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a7c41a02-5660-4ed8-9264-124ff133ec0d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 17 09:36:51.875: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-12-17T09:36:12Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-12-17T09:36:32Z]] name:name2 resourceVersion:10026 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1be3c903-2ab3-480f-9582-e3e5db967223] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:37:02.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1113" for this suite.

• [SLOW TEST:61.401 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":277,"completed":49,"skipped":930,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:37:02.417: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Dec 17 09:37:07.150: INFO: Successfully updated pod "labelsupdateb89e4b1c-f207-47f6-bb1e-e83fccf0e4a5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:37:09.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1588" for this suite.

• [SLOW TEST:6.772 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":50,"skipped":937,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:37:09.189: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override all
Dec 17 09:37:09.363: INFO: Waiting up to 5m0s for pod "client-containers-1393cf8b-5b5d-4af1-a6b4-92117cf0aa3f" in namespace "containers-8284" to be "Succeeded or Failed"
Dec 17 09:37:09.384: INFO: Pod "client-containers-1393cf8b-5b5d-4af1-a6b4-92117cf0aa3f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.079999ms
Dec 17 09:37:11.389: INFO: Pod "client-containers-1393cf8b-5b5d-4af1-a6b4-92117cf0aa3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025617381s
Dec 17 09:37:13.394: INFO: Pod "client-containers-1393cf8b-5b5d-4af1-a6b4-92117cf0aa3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031189189s
STEP: Saw pod success
Dec 17 09:37:13.394: INFO: Pod "client-containers-1393cf8b-5b5d-4af1-a6b4-92117cf0aa3f" satisfied condition "Succeeded or Failed"
Dec 17 09:37:13.399: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod client-containers-1393cf8b-5b5d-4af1-a6b4-92117cf0aa3f container test-container: <nil>
STEP: delete the pod
Dec 17 09:37:13.436: INFO: Waiting for pod client-containers-1393cf8b-5b5d-4af1-a6b4-92117cf0aa3f to disappear
Dec 17 09:37:13.442: INFO: Pod client-containers-1393cf8b-5b5d-4af1-a6b4-92117cf0aa3f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:37:13.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8284" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":277,"completed":51,"skipped":939,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:37:13.453: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:37:29.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6430" for this suite.

• [SLOW TEST:16.327 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":277,"completed":52,"skipped":966,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:37:29.781: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1278
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1278.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1278.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1278.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1278.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1278.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1278.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 09:37:34.034: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:34.040: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:34.044: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:34.049: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:34.063: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:34.068: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:34.074: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:34.078: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:34.088: INFO: Lookups using dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local]

Dec 17 09:37:39.096: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:39.101: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:39.106: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:39.111: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:39.128: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:39.136: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:39.143: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:39.149: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:39.164: INFO: Lookups using dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local]

Dec 17 09:37:44.096: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:44.103: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:44.110: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:44.116: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:44.136: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:44.143: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:44.149: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:44.155: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:44.167: INFO: Lookups using dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local]

Dec 17 09:37:49.096: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:49.103: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:49.110: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:49.115: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:49.135: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:49.142: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:49.148: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:49.154: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:49.165: INFO: Lookups using dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local]

Dec 17 09:37:54.100: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:54.108: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:54.113: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:54.118: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:54.134: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:54.140: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:54.147: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:54.153: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:54.168: INFO: Lookups using dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local]

Dec 17 09:37:59.099: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:59.105: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:59.111: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:59.118: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:59.135: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:59.140: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:59.144: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:59.151: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local from pod dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048: the server could not find the requested resource (get pods dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048)
Dec 17 09:37:59.164: INFO: Lookups using dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1278.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1278.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1278.svc.cluster.local jessie_udp@dns-test-service-2.dns-1278.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1278.svc.cluster.local]

Dec 17 09:38:04.158: INFO: DNS probes using dns-1278/dns-test-ba3c58cb-ff98-44cb-bf39-d959e7516048 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:38:04.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1278" for this suite.

• [SLOW TEST:34.507 seconds]
[sig-network] DNS
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":277,"completed":53,"skipped":979,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:38:04.288: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4405
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Dec 17 09:38:04.477: INFO: Waiting up to 5m0s for pod "downward-api-2f59ba76-68a1-4493-9143-3237f36e1eb9" in namespace "downward-api-4405" to be "Succeeded or Failed"
Dec 17 09:38:04.490: INFO: Pod "downward-api-2f59ba76-68a1-4493-9143-3237f36e1eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.785884ms
Dec 17 09:38:06.495: INFO: Pod "downward-api-2f59ba76-68a1-4493-9143-3237f36e1eb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017810958s
Dec 17 09:38:08.502: INFO: Pod "downward-api-2f59ba76-68a1-4493-9143-3237f36e1eb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025139095s
STEP: Saw pod success
Dec 17 09:38:08.502: INFO: Pod "downward-api-2f59ba76-68a1-4493-9143-3237f36e1eb9" satisfied condition "Succeeded or Failed"
Dec 17 09:38:08.508: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downward-api-2f59ba76-68a1-4493-9143-3237f36e1eb9 container dapi-container: <nil>
STEP: delete the pod
Dec 17 09:38:08.548: INFO: Waiting for pod downward-api-2f59ba76-68a1-4493-9143-3237f36e1eb9 to disappear
Dec 17 09:38:08.552: INFO: Pod downward-api-2f59ba76-68a1-4493-9143-3237f36e1eb9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:38:08.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4405" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":277,"completed":54,"skipped":989,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:38:08.568: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4275
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-4275
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 17 09:38:08.726: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 17 09:38:08.777: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 17 09:38:10.784: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 17 09:38:12.783: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:38:14.784: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:38:16.783: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:38:18.784: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:38:20.783: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:38:22.786: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:38:24.784: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:38:26.784: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:38:28.782: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 17 09:38:28.794: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 17 09:38:32.889: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.192.72 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4275 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 09:38:32.890: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 09:38:34.087: INFO: Found all expected endpoints: [netserver-0]
Dec 17 09:38:34.093: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.137.33 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4275 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 09:38:34.094: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 09:38:35.288: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:38:35.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4275" for this suite.

• [SLOW TEST:26.737 seconds]
[sig-network] Networking
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":55,"skipped":999,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:38:35.306: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9278
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl logs
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1288
STEP: creating an pod
Dec 17 09:38:35.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 run logs-generator --image=us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 --namespace=kubectl-9278 -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 17 09:38:35.605: INFO: stderr: ""
Dec 17 09:38:35.605: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Waiting for log generator to start.
Dec 17 09:38:35.605: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 17 09:38:35.605: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9278" to be "running and ready, or succeeded"
Dec 17 09:38:35.612: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.979042ms
Dec 17 09:38:37.618: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01325157s
Dec 17 09:38:39.623: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.018320165s
Dec 17 09:38:39.623: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 17 09:38:39.623: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 17 09:38:39.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 logs logs-generator logs-generator --namespace=kubectl-9278'
Dec 17 09:38:39.773: INFO: stderr: ""
Dec 17 09:38:39.773: INFO: stdout: "I1217 09:38:37.262238       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/zb4l 500\nI1217 09:38:37.462460       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/2q2 454\nI1217 09:38:37.662529       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/vkm 493\nI1217 09:38:37.862620       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/2lb 514\nI1217 09:38:38.062553       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/qk2 366\nI1217 09:38:38.262572       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/4tx 392\nI1217 09:38:38.462503       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/mwh 262\nI1217 09:38:38.662556       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pbz 210\nI1217 09:38:38.862552       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/dldd 576\nI1217 09:38:39.062541       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/djd 279\nI1217 09:38:39.262519       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/z6d 203\nI1217 09:38:39.462516       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/2mjl 239\nI1217 09:38:39.662426       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/pf5n 357\n"
STEP: limiting log lines
Dec 17 09:38:39.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 logs logs-generator logs-generator --namespace=kubectl-9278 --tail=1'
Dec 17 09:38:39.885: INFO: stderr: ""
Dec 17 09:38:39.885: INFO: stdout: "I1217 09:38:39.862542       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/8pz 296\n"
Dec 17 09:38:39.885: INFO: got output "I1217 09:38:39.862542       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/8pz 296\n"
STEP: limiting log bytes
Dec 17 09:38:39.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 logs logs-generator logs-generator --namespace=kubectl-9278 --limit-bytes=1'
Dec 17 09:38:40.000: INFO: stderr: ""
Dec 17 09:38:40.000: INFO: stdout: "I"
Dec 17 09:38:40.000: INFO: got output "I"
STEP: exposing timestamps
Dec 17 09:38:40.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 logs logs-generator logs-generator --namespace=kubectl-9278 --tail=1 --timestamps'
Dec 17 09:38:40.133: INFO: stderr: ""
Dec 17 09:38:40.133: INFO: stdout: "2020-12-17T09:38:40.062659388Z I1217 09:38:40.062446       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/dt8 497\n"
Dec 17 09:38:40.133: INFO: got output "2020-12-17T09:38:40.062659388Z I1217 09:38:40.062446       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/dt8 497\n"
STEP: restricting to a time range
Dec 17 09:38:42.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 logs logs-generator logs-generator --namespace=kubectl-9278 --since=1s'
Dec 17 09:38:42.766: INFO: stderr: ""
Dec 17 09:38:42.766: INFO: stdout: "I1217 09:38:41.862473       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/tbqr 282\nI1217 09:38:42.062524       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/lcg5 579\nI1217 09:38:42.262597       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/k5j 469\nI1217 09:38:42.462435       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/vc9 457\nI1217 09:38:42.662442       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/nfw 550\n"
Dec 17 09:38:42.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 logs logs-generator logs-generator --namespace=kubectl-9278 --since=24h'
Dec 17 09:38:42.897: INFO: stderr: ""
Dec 17 09:38:42.897: INFO: stdout: "I1217 09:38:37.262238       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/zb4l 500\nI1217 09:38:37.462460       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/2q2 454\nI1217 09:38:37.662529       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/vkm 493\nI1217 09:38:37.862620       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/2lb 514\nI1217 09:38:38.062553       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/qk2 366\nI1217 09:38:38.262572       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/4tx 392\nI1217 09:38:38.462503       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/mwh 262\nI1217 09:38:38.662556       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pbz 210\nI1217 09:38:38.862552       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/dldd 576\nI1217 09:38:39.062541       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/djd 279\nI1217 09:38:39.262519       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/z6d 203\nI1217 09:38:39.462516       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/2mjl 239\nI1217 09:38:39.662426       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/pf5n 357\nI1217 09:38:39.862542       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/8pz 296\nI1217 09:38:40.062446       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/dt8 497\nI1217 09:38:40.262652       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/v45 594\nI1217 09:38:40.462512       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/cmww 596\nI1217 09:38:40.662456       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/sxv 415\nI1217 09:38:40.862499       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/w77x 552\nI1217 09:38:41.062590       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/q5sr 337\nI1217 09:38:41.262464       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/nf8x 393\nI1217 09:38:41.462502       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/nnb 413\nI1217 09:38:41.662555       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/9kh 462\nI1217 09:38:41.862473       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/tbqr 282\nI1217 09:38:42.062524       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/lcg5 579\nI1217 09:38:42.262597       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/k5j 469\nI1217 09:38:42.462435       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/vc9 457\nI1217 09:38:42.662442       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/nfw 550\nI1217 09:38:42.862672       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/g5f9 202\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1294
Dec 17 09:38:42.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 delete pod logs-generator --namespace=kubectl-9278'
Dec 17 09:38:53.247: INFO: stderr: ""
Dec 17 09:38:53.247: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:38:53.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9278" for this suite.

• [SLOW TEST:17.958 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1284
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":277,"completed":56,"skipped":1001,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:38:53.263: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-22
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Dec 17 09:38:54.093: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:38:54.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1217 09:38:54.092875      24 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-22" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":277,"completed":57,"skipped":1004,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:38:54.109: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8853
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-8063ce5a-7474-4033-bea7-efdb0a0b55af
STEP: Creating a pod to test consume configMaps
Dec 17 09:38:54.296: INFO: Waiting up to 5m0s for pod "pod-configmaps-15ff9618-5f96-47fc-b628-fa8fa6ece1d8" in namespace "configmap-8853" to be "Succeeded or Failed"
Dec 17 09:38:54.308: INFO: Pod "pod-configmaps-15ff9618-5f96-47fc-b628-fa8fa6ece1d8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.554651ms
Dec 17 09:38:56.315: INFO: Pod "pod-configmaps-15ff9618-5f96-47fc-b628-fa8fa6ece1d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018458019s
Dec 17 09:38:58.321: INFO: Pod "pod-configmaps-15ff9618-5f96-47fc-b628-fa8fa6ece1d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025265957s
STEP: Saw pod success
Dec 17 09:38:58.321: INFO: Pod "pod-configmaps-15ff9618-5f96-47fc-b628-fa8fa6ece1d8" satisfied condition "Succeeded or Failed"
Dec 17 09:38:58.326: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-configmaps-15ff9618-5f96-47fc-b628-fa8fa6ece1d8 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 09:38:58.387: INFO: Waiting for pod pod-configmaps-15ff9618-5f96-47fc-b628-fa8fa6ece1d8 to disappear
Dec 17 09:38:58.397: INFO: Pod pod-configmaps-15ff9618-5f96-47fc-b628-fa8fa6ece1d8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:38:58.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8853" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":58,"skipped":1005,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:38:58.414: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-c1365086-efec-43a8-a5c1-ce5486cf78e3
STEP: Creating a pod to test consume secrets
Dec 17 09:38:58.601: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f927e34a-eebb-4aa5-bf78-6c3894a385b4" in namespace "projected-7826" to be "Succeeded or Failed"
Dec 17 09:38:58.611: INFO: Pod "pod-projected-secrets-f927e34a-eebb-4aa5-bf78-6c3894a385b4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.28453ms
Dec 17 09:39:00.618: INFO: Pod "pod-projected-secrets-f927e34a-eebb-4aa5-bf78-6c3894a385b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01701883s
Dec 17 09:39:02.624: INFO: Pod "pod-projected-secrets-f927e34a-eebb-4aa5-bf78-6c3894a385b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022941615s
STEP: Saw pod success
Dec 17 09:39:02.624: INFO: Pod "pod-projected-secrets-f927e34a-eebb-4aa5-bf78-6c3894a385b4" satisfied condition "Succeeded or Failed"
Dec 17 09:39:02.630: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-projected-secrets-f927e34a-eebb-4aa5-bf78-6c3894a385b4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 17 09:39:02.670: INFO: Waiting for pod pod-projected-secrets-f927e34a-eebb-4aa5-bf78-6c3894a385b4 to disappear
Dec 17 09:39:02.674: INFO: Pod pod-projected-secrets-f927e34a-eebb-4aa5-bf78-6c3894a385b4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:39:02.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7826" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":59,"skipped":1030,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:39:02.689: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-5826
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 17 09:39:02.863: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 17 09:39:02.939: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 17 09:39:04.947: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 17 09:39:06.946: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:39:08.945: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:39:10.945: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:39:12.944: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:39:14.945: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:39:16.945: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:39:18.946: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:39:20.944: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 17 09:39:20.953: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 17 09:39:25.029: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.192.75:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5826 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 09:39:25.029: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 09:39:25.217: INFO: Found all expected endpoints: [netserver-0]
Dec 17 09:39:25.222: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.137.35:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5826 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 09:39:25.222: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 09:39:25.414: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:39:25.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5826" for this suite.

• [SLOW TEST:22.740 seconds]
[sig-network] Networking
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":60,"skipped":1034,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:39:25.431: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:39:25.589: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-09ef0244-7355-42a0-b1cf-4579a241be5b" in namespace "security-context-test-4498" to be "Succeeded or Failed"
Dec 17 09:39:25.606: INFO: Pod "busybox-readonly-false-09ef0244-7355-42a0-b1cf-4579a241be5b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.165204ms
Dec 17 09:39:27.612: INFO: Pod "busybox-readonly-false-09ef0244-7355-42a0-b1cf-4579a241be5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022570117s
Dec 17 09:39:29.624: INFO: Pod "busybox-readonly-false-09ef0244-7355-42a0-b1cf-4579a241be5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034741213s
Dec 17 09:39:29.624: INFO: Pod "busybox-readonly-false-09ef0244-7355-42a0-b1cf-4579a241be5b" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:39:29.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4498" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":277,"completed":61,"skipped":1049,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:39:29.651: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2123
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Dec 17 09:39:29.872: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:39:50.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2123" for this suite.

• [SLOW TEST:20.923 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":277,"completed":62,"skipped":1057,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:39:50.575: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7493
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Dec 17 09:39:50.787: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:40:09.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7493" for this suite.

• [SLOW TEST:19.340 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":277,"completed":63,"skipped":1077,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:40:09.915: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Dec 17 09:40:10.064: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 17 09:40:10.078: INFO: Waiting for terminating namespaces to be deleted...
Dec 17 09:40:10.082: INFO: 
Logging pods the kubelet thinks is on node ck8s-conftest-118-workload-cluster-worker-0 before test
Dec 17 09:40:10.093: INFO: kube-proxy-87hvk from kube-system started at 2020-12-17 09:10:11 +0000 UTC (1 container statuses recorded)
Dec 17 09:40:10.093: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 09:40:10.093: INFO: sonobuoy from sonobuoy started at 2020-12-17 09:17:49 +0000 UTC (1 container statuses recorded)
Dec 17 09:40:10.093: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 17 09:40:10.093: INFO: calico-node-hv857 from kube-system started at 2020-12-17 09:10:11 +0000 UTC (1 container statuses recorded)
Dec 17 09:40:10.093: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 09:40:10.093: INFO: sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-hdzhs from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 09:40:10.093: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 09:40:10.093: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 09:40:10.093: INFO: 
Logging pods the kubelet thinks is on node ck8s-conftest-118-workload-cluster-worker-1 before test
Dec 17 09:40:10.104: INFO: sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-nn8jm from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 09:40:10.104: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 09:40:10.104: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 09:40:10.104: INFO: calico-node-dfdx8 from kube-system started at 2020-12-17 09:10:13 +0000 UTC (1 container statuses recorded)
Dec 17 09:40:10.104: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 09:40:10.104: INFO: kube-proxy-xk6gr from kube-system started at 2020-12-17 09:10:13 +0000 UTC (1 container statuses recorded)
Dec 17 09:40:10.104: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 09:40:10.104: INFO: sonobuoy-e2e-job-c7b0b40574f24e1c from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 09:40:10.104: INFO: 	Container e2e ready: true, restart count 0
Dec 17 09:40:10.104: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: verifying the node has the label node ck8s-conftest-118-workload-cluster-worker-0
STEP: verifying the node has the label node ck8s-conftest-118-workload-cluster-worker-1
Dec 17 09:40:10.205: INFO: Pod calico-node-dfdx8 requesting resource cpu=250m on Node ck8s-conftest-118-workload-cluster-worker-1
Dec 17 09:40:10.205: INFO: Pod calico-node-hv857 requesting resource cpu=250m on Node ck8s-conftest-118-workload-cluster-worker-0
Dec 17 09:40:10.205: INFO: Pod kube-proxy-87hvk requesting resource cpu=0m on Node ck8s-conftest-118-workload-cluster-worker-0
Dec 17 09:40:10.205: INFO: Pod kube-proxy-xk6gr requesting resource cpu=0m on Node ck8s-conftest-118-workload-cluster-worker-1
Dec 17 09:40:10.205: INFO: Pod sonobuoy requesting resource cpu=0m on Node ck8s-conftest-118-workload-cluster-worker-0
Dec 17 09:40:10.205: INFO: Pod sonobuoy-e2e-job-c7b0b40574f24e1c requesting resource cpu=0m on Node ck8s-conftest-118-workload-cluster-worker-1
Dec 17 09:40:10.205: INFO: Pod sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-hdzhs requesting resource cpu=0m on Node ck8s-conftest-118-workload-cluster-worker-0
Dec 17 09:40:10.205: INFO: Pod sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-nn8jm requesting resource cpu=0m on Node ck8s-conftest-118-workload-cluster-worker-1
STEP: Starting Pods to consume most of the cluster CPU.
Dec 17 09:40:10.205: INFO: Creating a pod which consumes cpu=2625m on Node ck8s-conftest-118-workload-cluster-worker-0
Dec 17 09:40:10.216: INFO: Creating a pod which consumes cpu=2625m on Node ck8s-conftest-118-workload-cluster-worker-1
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8dc88435-d3c8-4f10-8ae8-eefbe66c20c6.16517791ed1d4207], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1326/filler-pod-8dc88435-d3c8-4f10-8ae8-eefbe66c20c6 to ck8s-conftest-118-workload-cluster-worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8dc88435-d3c8-4f10-8ae8-eefbe66c20c6.165177922c53b412], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8dc88435-d3c8-4f10-8ae8-eefbe66c20c6.16517792312748a1], Reason = [Created], Message = [Created container filler-pod-8dc88435-d3c8-4f10-8ae8-eefbe66c20c6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8dc88435-d3c8-4f10-8ae8-eefbe66c20c6.165177923cf34950], Reason = [Started], Message = [Started container filler-pod-8dc88435-d3c8-4f10-8ae8-eefbe66c20c6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea3dfb4b-532b-4c87-af1b-ad956a0aeb12.16517791ec132b73], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1326/filler-pod-ea3dfb4b-532b-4c87-af1b-ad956a0aeb12 to ck8s-conftest-118-workload-cluster-worker-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea3dfb4b-532b-4c87-af1b-ad956a0aeb12.16517792269b961a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea3dfb4b-532b-4c87-af1b-ad956a0aeb12.165177922b5fb633], Reason = [Created], Message = [Created container filler-pod-ea3dfb4b-532b-4c87-af1b-ad956a0aeb12]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea3dfb4b-532b-4c87-af1b-ad956a0aeb12.16517792381f6763], Reason = [Started], Message = [Started container filler-pod-ea3dfb4b-532b-4c87-af1b-ad956a0aeb12]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16517792dd8af3b7], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node ck8s-conftest-118-workload-cluster-worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ck8s-conftest-118-workload-cluster-worker-0
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:40:15.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1326" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:5.433 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":277,"completed":64,"skipped":1087,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:40:15.349: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5697
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:40:31.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5697" for this suite.

• [SLOW TEST:16.298 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":277,"completed":65,"skipped":1105,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:40:31.649: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 09:40:31.817: INFO: Waiting up to 5m0s for pod "downwardapi-volume-152f619e-403f-4a89-a224-ec9be0667ccf" in namespace "projected-7210" to be "Succeeded or Failed"
Dec 17 09:40:31.826: INFO: Pod "downwardapi-volume-152f619e-403f-4a89-a224-ec9be0667ccf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.87587ms
Dec 17 09:40:33.832: INFO: Pod "downwardapi-volume-152f619e-403f-4a89-a224-ec9be0667ccf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015455908s
Dec 17 09:40:35.839: INFO: Pod "downwardapi-volume-152f619e-403f-4a89-a224-ec9be0667ccf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022361314s
STEP: Saw pod success
Dec 17 09:40:35.839: INFO: Pod "downwardapi-volume-152f619e-403f-4a89-a224-ec9be0667ccf" satisfied condition "Succeeded or Failed"
Dec 17 09:40:35.845: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-152f619e-403f-4a89-a224-ec9be0667ccf container client-container: <nil>
STEP: delete the pod
Dec 17 09:40:35.876: INFO: Waiting for pod downwardapi-volume-152f619e-403f-4a89-a224-ec9be0667ccf to disappear
Dec 17 09:40:35.881: INFO: Pod downwardapi-volume-152f619e-403f-4a89-a224-ec9be0667ccf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:40:35.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7210" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":66,"skipped":1118,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:40:35.894: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2996
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-6637
STEP: Creating secret with name secret-test-e2cf43cb-b021-46d2-a334-5a7634d9ef9e
STEP: Creating a pod to test consume secrets
Dec 17 09:40:36.268: INFO: Waiting up to 5m0s for pod "pod-secrets-869d0049-6eae-4e5c-806c-c40f1448a411" in namespace "secrets-2996" to be "Succeeded or Failed"
Dec 17 09:40:36.290: INFO: Pod "pod-secrets-869d0049-6eae-4e5c-806c-c40f1448a411": Phase="Pending", Reason="", readiness=false. Elapsed: 22.267396ms
Dec 17 09:40:38.297: INFO: Pod "pod-secrets-869d0049-6eae-4e5c-806c-c40f1448a411": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029351096s
Dec 17 09:40:40.303: INFO: Pod "pod-secrets-869d0049-6eae-4e5c-806c-c40f1448a411": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035078282s
STEP: Saw pod success
Dec 17 09:40:40.303: INFO: Pod "pod-secrets-869d0049-6eae-4e5c-806c-c40f1448a411" satisfied condition "Succeeded or Failed"
Dec 17 09:40:40.308: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-secrets-869d0049-6eae-4e5c-806c-c40f1448a411 container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 09:40:40.354: INFO: Waiting for pod pod-secrets-869d0049-6eae-4e5c-806c-c40f1448a411 to disappear
Dec 17 09:40:40.357: INFO: Pod pod-secrets-869d0049-6eae-4e5c-806c-c40f1448a411 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:40:40.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2996" for this suite.
STEP: Destroying namespace "secret-namespace-6637" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":277,"completed":67,"skipped":1134,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:40:40.379: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4696
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-32c255fa-4a9b-4eea-9d9c-2801c4b4f2cd
STEP: Creating a pod to test consume secrets
Dec 17 09:40:40.545: INFO: Waiting up to 5m0s for pod "pod-secrets-a513f2a3-a5bd-4385-b633-e73c1738e710" in namespace "secrets-4696" to be "Succeeded or Failed"
Dec 17 09:40:40.556: INFO: Pod "pod-secrets-a513f2a3-a5bd-4385-b633-e73c1738e710": Phase="Pending", Reason="", readiness=false. Elapsed: 10.549356ms
Dec 17 09:40:42.563: INFO: Pod "pod-secrets-a513f2a3-a5bd-4385-b633-e73c1738e710": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017052235s
Dec 17 09:40:44.570: INFO: Pod "pod-secrets-a513f2a3-a5bd-4385-b633-e73c1738e710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024066979s
STEP: Saw pod success
Dec 17 09:40:44.570: INFO: Pod "pod-secrets-a513f2a3-a5bd-4385-b633-e73c1738e710" satisfied condition "Succeeded or Failed"
Dec 17 09:40:44.575: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-secrets-a513f2a3-a5bd-4385-b633-e73c1738e710 container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 09:40:44.616: INFO: Waiting for pod pod-secrets-a513f2a3-a5bd-4385-b633-e73c1738e710 to disappear
Dec 17 09:40:44.620: INFO: Pod pod-secrets-a513f2a3-a5bd-4385-b633-e73c1738e710 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:40:44.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4696" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":68,"skipped":1160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:40:44.635: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-5dd219cd-f670-4bc4-aef8-b44859b0536d
STEP: Creating a pod to test consume configMaps
Dec 17 09:40:44.848: INFO: Waiting up to 5m0s for pod "pod-configmaps-fc06dc60-afa5-4a6d-a903-7646ab174fbc" in namespace "configmap-4802" to be "Succeeded or Failed"
Dec 17 09:40:44.858: INFO: Pod "pod-configmaps-fc06dc60-afa5-4a6d-a903-7646ab174fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.131795ms
Dec 17 09:40:46.863: INFO: Pod "pod-configmaps-fc06dc60-afa5-4a6d-a903-7646ab174fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014400766s
Dec 17 09:40:48.868: INFO: Pod "pod-configmaps-fc06dc60-afa5-4a6d-a903-7646ab174fbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019999943s
STEP: Saw pod success
Dec 17 09:40:48.868: INFO: Pod "pod-configmaps-fc06dc60-afa5-4a6d-a903-7646ab174fbc" satisfied condition "Succeeded or Failed"
Dec 17 09:40:48.873: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-configmaps-fc06dc60-afa5-4a6d-a903-7646ab174fbc container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 09:40:48.923: INFO: Waiting for pod pod-configmaps-fc06dc60-afa5-4a6d-a903-7646ab174fbc to disappear
Dec 17 09:40:48.928: INFO: Pod pod-configmaps-fc06dc60-afa5-4a6d-a903-7646ab174fbc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:40:48.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4802" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":69,"skipped":1182,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:40:48.944: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-266
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service nodeport-service with the type=NodePort in namespace services-266
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-266
STEP: creating replication controller externalsvc in namespace services-266
I1217 09:40:49.182508      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-266, replica count: 2
I1217 09:40:52.234918      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 17 09:40:52.272: INFO: Creating new exec pod
Dec 17 09:40:56.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=services-266 execpodpddpg -- /bin/sh -x -c nslookup nodeport-service'
Dec 17 09:40:56.764: INFO: stderr: "+ nslookup nodeport-service\n"
Dec 17 09:40:56.764: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-266.svc.cluster.local\tcanonical name = externalsvc.services-266.svc.cluster.local.\nName:\texternalsvc.services-266.svc.cluster.local\nAddress: 10.109.166.222\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-266, will wait for the garbage collector to delete the pods
Dec 17 09:40:56.830: INFO: Deleting ReplicationController externalsvc took: 7.212447ms
Dec 17 09:40:57.230: INFO: Terminating ReplicationController externalsvc pods took: 400.432755ms
Dec 17 09:41:01.864: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:41:01.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-266" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:12.973 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":277,"completed":70,"skipped":1188,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:41:01.918: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 17 09:41:02.203: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:41:02.222: INFO: Number of nodes with available pods: 0
Dec 17 09:41:02.222: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 09:41:03.228: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:41:03.234: INFO: Number of nodes with available pods: 0
Dec 17 09:41:03.234: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 09:41:04.228: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:41:04.233: INFO: Number of nodes with available pods: 0
Dec 17 09:41:04.233: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 09:41:05.229: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:41:05.235: INFO: Number of nodes with available pods: 2
Dec 17 09:41:05.235: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 17 09:41:05.276: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:41:05.285: INFO: Number of nodes with available pods: 1
Dec 17 09:41:05.285: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 09:41:06.291: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:41:06.298: INFO: Number of nodes with available pods: 1
Dec 17 09:41:06.298: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 09:41:07.305: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:41:07.310: INFO: Number of nodes with available pods: 1
Dec 17 09:41:07.310: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 09:41:08.291: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 09:41:08.297: INFO: Number of nodes with available pods: 2
Dec 17 09:41:08.297: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2153, will wait for the garbage collector to delete the pods
Dec 17 09:41:08.372: INFO: Deleting DaemonSet.extensions daemon-set took: 11.131472ms
Dec 17 09:41:08.472: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.456265ms
Dec 17 09:41:21.677: INFO: Number of nodes with available pods: 0
Dec 17 09:41:21.677: INFO: Number of running nodes: 0, number of available pods: 0
Dec 17 09:41:21.681: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2153/daemonsets","resourceVersion":"11813"},"items":null}

Dec 17 09:41:21.684: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2153/pods","resourceVersion":"11813"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:41:21.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2153" for this suite.

• [SLOW TEST:19.796 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":277,"completed":71,"skipped":1223,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:41:21.715: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:41:21.870: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:41:25.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6585" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":277,"completed":72,"skipped":1231,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:41:25.962: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5291
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-5291
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5291
STEP: Creating statefulset with conflicting port in namespace statefulset-5291
STEP: Waiting until pod test-pod will start running in namespace statefulset-5291
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5291
Dec 17 09:41:30.198: INFO: Observed stateful pod in namespace: statefulset-5291, name: ss-0, uid: b0247221-526e-49fa-9009-d9eb6919177b, status phase: Pending. Waiting for statefulset controller to delete.
Dec 17 09:41:30.578: INFO: Observed stateful pod in namespace: statefulset-5291, name: ss-0, uid: b0247221-526e-49fa-9009-d9eb6919177b, status phase: Failed. Waiting for statefulset controller to delete.
Dec 17 09:41:30.589: INFO: Observed stateful pod in namespace: statefulset-5291, name: ss-0, uid: b0247221-526e-49fa-9009-d9eb6919177b, status phase: Failed. Waiting for statefulset controller to delete.
Dec 17 09:41:30.596: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5291
STEP: Removing pod with conflicting port in namespace statefulset-5291
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5291 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Dec 17 09:41:36.674: INFO: Deleting all statefulset in ns statefulset-5291
Dec 17 09:41:36.678: INFO: Scaling statefulset ss to 0
Dec 17 09:41:46.716: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 09:41:46.721: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:41:46.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5291" for this suite.

• [SLOW TEST:20.817 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":277,"completed":73,"skipped":1247,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:41:46.781: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6562
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:41:46.949: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 17 09:41:51.956: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 17 09:41:51.957: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Dec 17 09:41:52.011: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6562 /apis/apps/v1/namespaces/deployment-6562/deployments/test-cleanup-deployment b392baaa-1d52-44e4-8a4a-8447522f1f05 12122 1 2020-12-17 09:41:52 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-12-17 09:41:52 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003800d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec 17 09:41:52.033: INFO: New ReplicaSet "test-cleanup-deployment-b4867b47f" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-b4867b47f  deployment-6562 /apis/apps/v1/namespaces/deployment-6562/replicasets/test-cleanup-deployment-b4867b47f cf327164-f6e9-4190-9c2a-34c0084d56f6 12124 1 2020-12-17 09:41:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b392baaa-1d52-44e4-8a4a-8447522f1f05 0xc00382a890 0xc00382a891}] []  [{kube-controller-manager Update apps/v1 2020-12-17 09:41:52 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 51 57 50 98 97 97 97 45 49 100 53 50 45 52 52 101 52 45 56 97 52 97 45 56 52 52 55 53 50 50 102 49 102 48 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: b4867b47f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00382a928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 17 09:41:52.033: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec 17 09:41:52.033: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6562 /apis/apps/v1/namespaces/deployment-6562/replicasets/test-cleanup-controller 1b65d16e-5eb3-41cf-8593-9c6814c21f74 12123 1 2020-12-17 09:41:47 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment b392baaa-1d52-44e4-8a4a-8447522f1f05 0xc00382a707 0xc00382a708}] []  [{e2e.test Update apps/v1 2020-12-17 09:41:47 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-12-17 09:41:52 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 51 57 50 98 97 97 97 45 49 100 53 50 45 52 52 101 52 45 56 97 52 97 45 56 52 52 55 53 50 50 102 49 102 48 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00382a818 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 17 09:41:52.055: INFO: Pod "test-cleanup-controller-mw42p" is available:
&Pod{ObjectMeta:{test-cleanup-controller-mw42p test-cleanup-controller- deployment-6562 /api/v1/namespaces/deployment-6562/pods/test-cleanup-controller-mw42p 2ed724b5-ccdf-4ed6-ace1-a41674162a6f 12088 0 2020-12-17 09:41:47 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:192.168.192.90/32 cni.projectcalico.org/podIPs:192.168.192.90/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 1b65d16e-5eb3-41cf-8593-9c6814c21f74 0xc0038012c7 0xc0038012c8}] []  [{kube-controller-manager Update v1 2020-12-17 09:41:47 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 49 98 54 53 100 49 54 101 45 53 101 98 51 45 52 49 99 102 45 56 53 57 51 45 57 99 54 56 49 52 99 50 49 102 55 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:41:48 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-12-17 09:41:49 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 50 46 57 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xcghl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xcghl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xcghl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:41:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:41:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:41:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:41:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:192.168.192.90,StartTime:2020-12-17 09:41:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 09:41:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9434f596f4685d2f32abec015ef469398af9bee934f8eaffb5b863cf4b0ddf36,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 09:41:52.056: INFO: Pod "test-cleanup-deployment-b4867b47f-cpgb9" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-b4867b47f-cpgb9 test-cleanup-deployment-b4867b47f- deployment-6562 /api/v1/namespaces/deployment-6562/pods/test-cleanup-deployment-b4867b47f-cpgb9 ab9321f1-c945-4dc8-80f3-62a52b2d47cd 12130 0 2020-12-17 09:41:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-b4867b47f cf327164-f6e9-4190-9c2a-34c0084d56f6 0xc0038014d0 0xc0038014d1}] []  [{kube-controller-manager Update v1 2020-12-17 09:41:52 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 102 51 50 55 49 54 52 45 102 54 101 57 45 52 49 57 48 45 57 99 50 97 45 51 52 99 48 48 56 52 100 53 54 102 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xcghl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xcghl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xcghl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:41:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:41:52.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6562" for this suite.

• [SLOW TEST:5.311 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":277,"completed":74,"skipped":1313,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:41:52.093: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-35
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating server pod server in namespace prestop-35
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-35
STEP: Deleting pre-stop pod
Dec 17 09:42:05.341: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:42:05.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-35" for this suite.

• [SLOW TEST:13.273 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":277,"completed":75,"skipped":1322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:42:05.368: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7121
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:42:05.579: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"23e4cd47-8ac3-4c17-b31f-abaa4dfb3042", Controller:(*bool)(0xc0033c360e), BlockOwnerDeletion:(*bool)(0xc0033c360f)}}
Dec 17 09:42:05.594: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"e22938de-9b86-4775-9c0c-29d3a9b0cd16", Controller:(*bool)(0xc0037dd176), BlockOwnerDeletion:(*bool)(0xc0037dd177)}}
Dec 17 09:42:05.614: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3af2bc26-be0a-4d68-828b-173b7d57f031", Controller:(*bool)(0xc003441576), BlockOwnerDeletion:(*bool)(0xc003441577)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:42:10.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7121" for this suite.

• [SLOW TEST:5.297 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":277,"completed":76,"skipped":1350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:42:10.666: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 17 09:42:17.395: INFO: Successfully updated pod "pod-update-f92045c6-8472-4407-b239-24c698c7e733"
STEP: verifying the updated pod is in kubernetes
Dec 17 09:42:17.405: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:42:17.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1717" for this suite.

• [SLOW TEST:6.753 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":277,"completed":77,"skipped":1386,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:42:17.420: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6449
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-c0ab8796-5e57-40e7-aeb8-c00840942cb6
STEP: Creating a pod to test consume configMaps
Dec 17 09:42:17.622: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f87382f-4585-4f3b-9c6f-180c8aaf8984" in namespace "configmap-6449" to be "Succeeded or Failed"
Dec 17 09:42:17.633: INFO: Pod "pod-configmaps-0f87382f-4585-4f3b-9c6f-180c8aaf8984": Phase="Pending", Reason="", readiness=false. Elapsed: 11.144871ms
Dec 17 09:42:19.641: INFO: Pod "pod-configmaps-0f87382f-4585-4f3b-9c6f-180c8aaf8984": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019581274s
Dec 17 09:42:21.664: INFO: Pod "pod-configmaps-0f87382f-4585-4f3b-9c6f-180c8aaf8984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042500718s
STEP: Saw pod success
Dec 17 09:42:21.664: INFO: Pod "pod-configmaps-0f87382f-4585-4f3b-9c6f-180c8aaf8984" satisfied condition "Succeeded or Failed"
Dec 17 09:42:21.669: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-configmaps-0f87382f-4585-4f3b-9c6f-180c8aaf8984 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 09:42:21.726: INFO: Waiting for pod pod-configmaps-0f87382f-4585-4f3b-9c6f-180c8aaf8984 to disappear
Dec 17 09:42:21.730: INFO: Pod pod-configmaps-0f87382f-4585-4f3b-9c6f-180c8aaf8984 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:42:21.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6449" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":78,"skipped":1443,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:42:21.753: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8874
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 17 09:42:24.977: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:42:25.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8874" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":79,"skipped":1461,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:42:25.018: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1137
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 09:42:25.587: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 09:42:27.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794945, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794945, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794946, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743794945, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 09:42:30.631: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 17 09:42:30.665: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:42:30.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1137" for this suite.
STEP: Destroying namespace "webhook-1137-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.809 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":277,"completed":80,"skipped":1504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:42:30.828: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating cluster-info
Dec 17 09:42:30.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 cluster-info'
Dec 17 09:42:31.103: INFO: stderr: ""
Dec 17 09:42:31.103: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:42:31.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9920" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":277,"completed":81,"skipped":1551,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:42:31.157: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 17 09:42:31.335: INFO: Waiting up to 5m0s for pod "pod-ae586bc7-1b82-4e8f-9887-cf27da771f60" in namespace "emptydir-4787" to be "Succeeded or Failed"
Dec 17 09:42:31.350: INFO: Pod "pod-ae586bc7-1b82-4e8f-9887-cf27da771f60": Phase="Pending", Reason="", readiness=false. Elapsed: 14.721365ms
Dec 17 09:42:33.357: INFO: Pod "pod-ae586bc7-1b82-4e8f-9887-cf27da771f60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021610963s
Dec 17 09:42:35.364: INFO: Pod "pod-ae586bc7-1b82-4e8f-9887-cf27da771f60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028034374s
STEP: Saw pod success
Dec 17 09:42:35.364: INFO: Pod "pod-ae586bc7-1b82-4e8f-9887-cf27da771f60" satisfied condition "Succeeded or Failed"
Dec 17 09:42:35.368: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-ae586bc7-1b82-4e8f-9887-cf27da771f60 container test-container: <nil>
STEP: delete the pod
Dec 17 09:42:35.406: INFO: Waiting for pod pod-ae586bc7-1b82-4e8f-9887-cf27da771f60 to disappear
Dec 17 09:42:35.410: INFO: Pod pod-ae586bc7-1b82-4e8f-9887-cf27da771f60 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:42:35.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4787" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":82,"skipped":1551,"failed":0}

------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:42:35.424: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:42:39.688: INFO: Waiting up to 5m0s for pod "client-envvars-7f9a226d-f9eb-42f3-8a44-1371d761de13" in namespace "pods-6064" to be "Succeeded or Failed"
Dec 17 09:42:39.707: INFO: Pod "client-envvars-7f9a226d-f9eb-42f3-8a44-1371d761de13": Phase="Pending", Reason="", readiness=false. Elapsed: 18.87192ms
Dec 17 09:42:41.713: INFO: Pod "client-envvars-7f9a226d-f9eb-42f3-8a44-1371d761de13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024790852s
Dec 17 09:42:43.719: INFO: Pod "client-envvars-7f9a226d-f9eb-42f3-8a44-1371d761de13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030212622s
STEP: Saw pod success
Dec 17 09:42:43.719: INFO: Pod "client-envvars-7f9a226d-f9eb-42f3-8a44-1371d761de13" satisfied condition "Succeeded or Failed"
Dec 17 09:42:43.726: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod client-envvars-7f9a226d-f9eb-42f3-8a44-1371d761de13 container env3cont: <nil>
STEP: delete the pod
Dec 17 09:42:43.760: INFO: Waiting for pod client-envvars-7f9a226d-f9eb-42f3-8a44-1371d761de13 to disappear
Dec 17 09:42:43.764: INFO: Pod client-envvars-7f9a226d-f9eb-42f3-8a44-1371d761de13 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:42:43.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6064" for this suite.

• [SLOW TEST:8.356 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":277,"completed":83,"skipped":1551,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:42:43.780: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-8655
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8655
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8655
Dec 17 09:42:43.994: INFO: Found 0 stateful pods, waiting for 1
Dec 17 09:42:53.999: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 17 09:42:54.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-8655 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 17 09:42:54.277: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 17 09:42:54.277: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 17 09:42:54.277: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 17 09:42:54.282: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 17 09:43:04.289: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 09:43:04.290: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 09:43:04.341: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999051s
Dec 17 09:43:05.350: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.983079943s
Dec 17 09:43:06.357: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.973587358s
Dec 17 09:43:07.364: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.967052793s
Dec 17 09:43:08.371: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.960580368s
Dec 17 09:43:09.378: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.95329294s
Dec 17 09:43:10.386: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.945757892s
Dec 17 09:43:11.394: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.93835821s
Dec 17 09:43:12.400: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.929829014s
Dec 17 09:43:13.407: INFO: Verifying statefulset ss doesn't scale past 1 for another 924.525213ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8655
Dec 17 09:43:14.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-8655 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 17 09:43:14.688: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 17 09:43:14.688: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 17 09:43:14.688: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 17 09:43:14.695: INFO: Found 1 stateful pods, waiting for 3
Dec 17 09:43:24.702: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 09:43:24.702: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 09:43:24.702: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 17 09:43:24.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-8655 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 17 09:43:24.987: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 17 09:43:24.987: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 17 09:43:24.987: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 17 09:43:24.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-8655 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 17 09:43:25.264: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 17 09:43:25.264: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 17 09:43:25.264: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 17 09:43:25.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-8655 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 17 09:43:25.566: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 17 09:43:25.566: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 17 09:43:25.566: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 17 09:43:25.566: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 09:43:25.571: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 17 09:43:35.596: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 09:43:35.596: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 09:43:35.596: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 17 09:43:35.619: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999391s
Dec 17 09:43:36.632: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989695454s
Dec 17 09:43:37.638: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97712301s
Dec 17 09:43:38.644: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.970380391s
Dec 17 09:43:39.653: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964601338s
Dec 17 09:43:40.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.955765196s
Dec 17 09:43:41.672: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.946396708s
Dec 17 09:43:42.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.936790615s
Dec 17 09:43:43.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.927589674s
Dec 17 09:43:44.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 918.49174ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8655
Dec 17 09:43:45.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-8655 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 17 09:43:46.012: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 17 09:43:46.012: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 17 09:43:46.012: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 17 09:43:46.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-8655 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 17 09:43:46.284: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 17 09:43:46.284: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 17 09:43:46.284: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 17 09:43:46.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-8655 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 17 09:43:46.579: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 17 09:43:46.580: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 17 09:43:46.580: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 17 09:43:46.580: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Dec 17 09:43:56.616: INFO: Deleting all statefulset in ns statefulset-8655
Dec 17 09:43:56.621: INFO: Scaling statefulset ss to 0
Dec 17 09:43:56.649: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 09:43:56.654: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:43:56.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8655" for this suite.

• [SLOW TEST:72.956 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":277,"completed":84,"skipped":1565,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:43:56.737: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7042
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-6584a8f0-52d0-4413-91b0-9500aa245e3a
STEP: Creating a pod to test consume configMaps
Dec 17 09:43:56.943: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-35f9479a-c64d-45af-a350-f1134eb11907" in namespace "projected-7042" to be "Succeeded or Failed"
Dec 17 09:43:56.949: INFO: Pod "pod-projected-configmaps-35f9479a-c64d-45af-a350-f1134eb11907": Phase="Pending", Reason="", readiness=false. Elapsed: 6.116004ms
Dec 17 09:43:58.956: INFO: Pod "pod-projected-configmaps-35f9479a-c64d-45af-a350-f1134eb11907": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013370489s
Dec 17 09:44:00.962: INFO: Pod "pod-projected-configmaps-35f9479a-c64d-45af-a350-f1134eb11907": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019036011s
STEP: Saw pod success
Dec 17 09:44:00.962: INFO: Pod "pod-projected-configmaps-35f9479a-c64d-45af-a350-f1134eb11907" satisfied condition "Succeeded or Failed"
Dec 17 09:44:00.966: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-projected-configmaps-35f9479a-c64d-45af-a350-f1134eb11907 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 09:44:01.000: INFO: Waiting for pod pod-projected-configmaps-35f9479a-c64d-45af-a350-f1134eb11907 to disappear
Dec 17 09:44:01.022: INFO: Pod pod-projected-configmaps-35f9479a-c64d-45af-a350-f1134eb11907 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:44:01.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7042" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":85,"skipped":1577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:44:01.034: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9100
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:44:01.182: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:44:05.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9100" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":277,"completed":86,"skipped":1610,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:44:05.413: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-hr6f
STEP: Creating a pod to test atomic-volume-subpath
Dec 17 09:44:05.618: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hr6f" in namespace "subpath-7479" to be "Succeeded or Failed"
Dec 17 09:44:05.647: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.717739ms
Dec 17 09:44:07.655: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036681988s
Dec 17 09:44:09.660: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Running", Reason="", readiness=true. Elapsed: 4.042366151s
Dec 17 09:44:11.666: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Running", Reason="", readiness=true. Elapsed: 6.048017191s
Dec 17 09:44:13.673: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Running", Reason="", readiness=true. Elapsed: 8.055074495s
Dec 17 09:44:15.680: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Running", Reason="", readiness=true. Elapsed: 10.062139841s
Dec 17 09:44:17.686: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Running", Reason="", readiness=true. Elapsed: 12.068472986s
Dec 17 09:44:19.694: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Running", Reason="", readiness=true. Elapsed: 14.076420841s
Dec 17 09:44:21.700: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Running", Reason="", readiness=true. Elapsed: 16.082229642s
Dec 17 09:44:23.707: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Running", Reason="", readiness=true. Elapsed: 18.088594345s
Dec 17 09:44:25.714: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Running", Reason="", readiness=true. Elapsed: 20.09568512s
Dec 17 09:44:27.721: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Running", Reason="", readiness=true. Elapsed: 22.102677141s
Dec 17 09:44:29.729: INFO: Pod "pod-subpath-test-configmap-hr6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.111376206s
STEP: Saw pod success
Dec 17 09:44:29.729: INFO: Pod "pod-subpath-test-configmap-hr6f" satisfied condition "Succeeded or Failed"
Dec 17 09:44:29.735: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-subpath-test-configmap-hr6f container test-container-subpath-configmap-hr6f: <nil>
STEP: delete the pod
Dec 17 09:44:29.770: INFO: Waiting for pod pod-subpath-test-configmap-hr6f to disappear
Dec 17 09:44:29.774: INFO: Pod pod-subpath-test-configmap-hr6f no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hr6f
Dec 17 09:44:29.774: INFO: Deleting pod "pod-subpath-test-configmap-hr6f" in namespace "subpath-7479"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:44:29.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7479" for this suite.

• [SLOW TEST:24.379 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":277,"completed":87,"skipped":1621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:44:29.793: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1306
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Dec 17 09:44:29.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-1306'
Dec 17 09:44:30.277: INFO: stderr: ""
Dec 17 09:44:30.277: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 17 09:44:30.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1306'
Dec 17 09:44:30.401: INFO: stderr: ""
Dec 17 09:44:30.401: INFO: stdout: "update-demo-nautilus-7m8st update-demo-nautilus-lmxf5 "
Dec 17 09:44:30.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-7m8st -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:30.511: INFO: stderr: ""
Dec 17 09:44:30.511: INFO: stdout: ""
Dec 17 09:44:30.511: INFO: update-demo-nautilus-7m8st is created but not running
Dec 17 09:44:35.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1306'
Dec 17 09:44:35.630: INFO: stderr: ""
Dec 17 09:44:35.630: INFO: stdout: "update-demo-nautilus-7m8st update-demo-nautilus-lmxf5 "
Dec 17 09:44:35.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-7m8st -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:35.731: INFO: stderr: ""
Dec 17 09:44:35.731: INFO: stdout: "true"
Dec 17 09:44:35.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-7m8st -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:35.835: INFO: stderr: ""
Dec 17 09:44:35.835: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 09:44:35.835: INFO: validating pod update-demo-nautilus-7m8st
Dec 17 09:44:35.844: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 09:44:35.844: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 09:44:35.844: INFO: update-demo-nautilus-7m8st is verified up and running
Dec 17 09:44:35.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-lmxf5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:35.953: INFO: stderr: ""
Dec 17 09:44:35.953: INFO: stdout: "true"
Dec 17 09:44:35.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-lmxf5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:36.060: INFO: stderr: ""
Dec 17 09:44:36.060: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 09:44:36.060: INFO: validating pod update-demo-nautilus-lmxf5
Dec 17 09:44:36.070: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 09:44:36.070: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 09:44:36.070: INFO: update-demo-nautilus-lmxf5 is verified up and running
STEP: scaling down the replication controller
Dec 17 09:44:36.073: INFO: scanned /root for discovery docs: <nil>
Dec 17 09:44:36.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1306'
Dec 17 09:44:37.223: INFO: stderr: ""
Dec 17 09:44:37.223: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 17 09:44:37.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1306'
Dec 17 09:44:37.353: INFO: stderr: ""
Dec 17 09:44:37.353: INFO: stdout: "update-demo-nautilus-7m8st update-demo-nautilus-lmxf5 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 17 09:44:42.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1306'
Dec 17 09:44:42.462: INFO: stderr: ""
Dec 17 09:44:42.462: INFO: stdout: "update-demo-nautilus-7m8st update-demo-nautilus-lmxf5 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 17 09:44:47.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1306'
Dec 17 09:44:47.570: INFO: stderr: ""
Dec 17 09:44:47.570: INFO: stdout: "update-demo-nautilus-lmxf5 "
Dec 17 09:44:47.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-lmxf5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:47.666: INFO: stderr: ""
Dec 17 09:44:47.666: INFO: stdout: "true"
Dec 17 09:44:47.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-lmxf5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:47.782: INFO: stderr: ""
Dec 17 09:44:47.782: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 09:44:47.782: INFO: validating pod update-demo-nautilus-lmxf5
Dec 17 09:44:47.790: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 09:44:47.790: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 09:44:47.790: INFO: update-demo-nautilus-lmxf5 is verified up and running
STEP: scaling up the replication controller
Dec 17 09:44:47.792: INFO: scanned /root for discovery docs: <nil>
Dec 17 09:44:47.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1306'
Dec 17 09:44:48.944: INFO: stderr: ""
Dec 17 09:44:48.944: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 17 09:44:48.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1306'
Dec 17 09:44:49.049: INFO: stderr: ""
Dec 17 09:44:49.049: INFO: stdout: "update-demo-nautilus-lmxf5 update-demo-nautilus-sxzf6 "
Dec 17 09:44:49.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-lmxf5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:49.156: INFO: stderr: ""
Dec 17 09:44:49.156: INFO: stdout: "true"
Dec 17 09:44:49.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-lmxf5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:49.267: INFO: stderr: ""
Dec 17 09:44:49.267: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 09:44:49.268: INFO: validating pod update-demo-nautilus-lmxf5
Dec 17 09:44:49.273: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 09:44:49.273: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 09:44:49.273: INFO: update-demo-nautilus-lmxf5 is verified up and running
Dec 17 09:44:49.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-sxzf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:49.383: INFO: stderr: ""
Dec 17 09:44:49.383: INFO: stdout: ""
Dec 17 09:44:49.383: INFO: update-demo-nautilus-sxzf6 is created but not running
Dec 17 09:44:54.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1306'
Dec 17 09:44:54.501: INFO: stderr: ""
Dec 17 09:44:54.501: INFO: stdout: "update-demo-nautilus-lmxf5 update-demo-nautilus-sxzf6 "
Dec 17 09:44:54.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-lmxf5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:54.606: INFO: stderr: ""
Dec 17 09:44:54.606: INFO: stdout: "true"
Dec 17 09:44:54.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-lmxf5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:54.720: INFO: stderr: ""
Dec 17 09:44:54.720: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 09:44:54.720: INFO: validating pod update-demo-nautilus-lmxf5
Dec 17 09:44:54.734: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 09:44:54.734: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 09:44:54.734: INFO: update-demo-nautilus-lmxf5 is verified up and running
Dec 17 09:44:54.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-sxzf6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:54.829: INFO: stderr: ""
Dec 17 09:44:54.829: INFO: stdout: "true"
Dec 17 09:44:54.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-sxzf6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1306'
Dec 17 09:44:54.934: INFO: stderr: ""
Dec 17 09:44:54.934: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 09:44:54.934: INFO: validating pod update-demo-nautilus-sxzf6
Dec 17 09:44:54.941: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 09:44:54.941: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 09:44:54.941: INFO: update-demo-nautilus-sxzf6 is verified up and running
STEP: using delete to clean up resources
Dec 17 09:44:54.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 delete --grace-period=0 --force -f - --namespace=kubectl-1306'
Dec 17 09:44:55.069: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 09:44:55.070: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 17 09:44:55.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1306'
Dec 17 09:44:55.185: INFO: stderr: "No resources found in kubectl-1306 namespace.\n"
Dec 17 09:44:55.185: INFO: stdout: ""
Dec 17 09:44:55.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -l name=update-demo --namespace=kubectl-1306 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 17 09:44:55.303: INFO: stderr: ""
Dec 17 09:44:55.303: INFO: stdout: "update-demo-nautilus-lmxf5\nupdate-demo-nautilus-sxzf6\n"
Dec 17 09:44:55.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1306'
Dec 17 09:44:55.911: INFO: stderr: "No resources found in kubectl-1306 namespace.\n"
Dec 17 09:44:55.911: INFO: stdout: ""
Dec 17 09:44:55.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -l name=update-demo --namespace=kubectl-1306 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 17 09:44:56.019: INFO: stderr: ""
Dec 17 09:44:56.019: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:44:56.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1306" for this suite.

• [SLOW TEST:26.244 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":277,"completed":88,"skipped":1649,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:44:56.037: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8468
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-b8c31836-e7b8-459b-9ed6-32dd70dcf89a
STEP: Creating a pod to test consume secrets
Dec 17 09:44:56.252: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c1ed1196-fba7-4c4f-a52d-0fbe91880848" in namespace "projected-8468" to be "Succeeded or Failed"
Dec 17 09:44:56.268: INFO: Pod "pod-projected-secrets-c1ed1196-fba7-4c4f-a52d-0fbe91880848": Phase="Pending", Reason="", readiness=false. Elapsed: 16.251439ms
Dec 17 09:44:58.274: INFO: Pod "pod-projected-secrets-c1ed1196-fba7-4c4f-a52d-0fbe91880848": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022911074s
Dec 17 09:45:00.281: INFO: Pod "pod-projected-secrets-c1ed1196-fba7-4c4f-a52d-0fbe91880848": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029320098s
STEP: Saw pod success
Dec 17 09:45:00.281: INFO: Pod "pod-projected-secrets-c1ed1196-fba7-4c4f-a52d-0fbe91880848" satisfied condition "Succeeded or Failed"
Dec 17 09:45:00.286: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-projected-secrets-c1ed1196-fba7-4c4f-a52d-0fbe91880848 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 17 09:45:00.332: INFO: Waiting for pod pod-projected-secrets-c1ed1196-fba7-4c4f-a52d-0fbe91880848 to disappear
Dec 17 09:45:00.337: INFO: Pod pod-projected-secrets-c1ed1196-fba7-4c4f-a52d-0fbe91880848 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:45:00.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8468" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":89,"skipped":1652,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:45:00.352: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 17 09:45:06.595: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 09:45:06.601: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 09:45:08.602: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 09:45:08.609: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 09:45:10.602: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 09:45:10.609: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 17 09:45:12.601: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 17 09:45:12.607: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:45:12.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9566" for this suite.

• [SLOW TEST:12.291 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":277,"completed":90,"skipped":1672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:45:12.646: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Dec 17 09:45:12.801: INFO: PodSpec: initContainers in spec.initContainers
Dec 17 09:46:00.765: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d3d55024-2891-49dc-bfd4-c2fa35a06c76", GenerateName:"", Namespace:"init-container-6994", SelfLink:"/api/v1/namespaces/init-container-6994/pods/pod-init-d3d55024-2891-49dc-bfd4-c2fa35a06c76", UID:"14068e9c-ec4b-47a4-ae48-d7be51ac932f", ResourceVersion:"13768", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63743795112, loc:(*time.Location)(0x7b675e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"801702786"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.137.47/32", "cni.projectcalico.org/podIPs":"192.168.137.47/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003739d80), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003739da0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003739dc0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003739de0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003739e00), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003739e20)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-bhs4g", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00652ae40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bhs4g", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bhs4g", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bhs4g", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00512c078), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ck8s-conftest-118-workload-cluster-worker-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000212150), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00512c0f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00512c110)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00512c118), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00512c11c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795112, loc:(*time.Location)(0x7b675e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795112, loc:(*time.Location)(0x7b675e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795112, loc:(*time.Location)(0x7b675e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795112, loc:(*time.Location)(0x7b675e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.0.10.72", PodIP:"192.168.137.47", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.137.47"}}, StartTime:(*v1.Time)(0xc003739e40), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000212380)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0002124d0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://2c2ef5fefbe21091b302ded0aca7d8e0ad1cbb3137a455548534b3dc7f98a7fc", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003739e80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003739e60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00512c19f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:46:00.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6994" for this suite.

• [SLOW TEST:48.142 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":277,"completed":91,"skipped":1710,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:46:00.788: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8899
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 17 09:46:00.994: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8899 /api/v1/namespaces/watch-8899/configmaps/e2e-watch-test-resource-version 8255c5b4-d93d-4d10-bcd0-93ec8915ce1c 13778 0 2020-12-17 09:46:00 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 09:46:00.994: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8899 /api/v1/namespaces/watch-8899/configmaps/e2e-watch-test-resource-version 8255c5b4-d93d-4d10-bcd0-93ec8915ce1c 13779 0 2020-12-17 09:46:00 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:46:00.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8899" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":277,"completed":92,"skipped":1713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:46:01.016: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1969
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 17 09:46:01.191: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1969 /api/v1/namespaces/watch-1969/configmaps/e2e-watch-test-configmap-a 74bfb602-705c-4f5d-bf52-d9e12edc6950 13786 0 2020-12-17 09:46:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 09:46:01.192: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1969 /api/v1/namespaces/watch-1969/configmaps/e2e-watch-test-configmap-a 74bfb602-705c-4f5d-bf52-d9e12edc6950 13786 0 2020-12-17 09:46:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 17 09:46:11.203: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1969 /api/v1/namespaces/watch-1969/configmaps/e2e-watch-test-configmap-a 74bfb602-705c-4f5d-bf52-d9e12edc6950 13830 0 2020-12-17 09:46:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 09:46:11.204: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1969 /api/v1/namespaces/watch-1969/configmaps/e2e-watch-test-configmap-a 74bfb602-705c-4f5d-bf52-d9e12edc6950 13830 0 2020-12-17 09:46:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 17 09:46:21.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1969 /api/v1/namespaces/watch-1969/configmaps/e2e-watch-test-configmap-a 74bfb602-705c-4f5d-bf52-d9e12edc6950 13857 0 2020-12-17 09:46:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 09:46:21.216: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1969 /api/v1/namespaces/watch-1969/configmaps/e2e-watch-test-configmap-a 74bfb602-705c-4f5d-bf52-d9e12edc6950 13857 0 2020-12-17 09:46:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 17 09:46:31.229: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1969 /api/v1/namespaces/watch-1969/configmaps/e2e-watch-test-configmap-a 74bfb602-705c-4f5d-bf52-d9e12edc6950 13884 0 2020-12-17 09:46:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 09:46:31.230: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1969 /api/v1/namespaces/watch-1969/configmaps/e2e-watch-test-configmap-a 74bfb602-705c-4f5d-bf52-d9e12edc6950 13884 0 2020-12-17 09:46:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 17 09:46:41.249: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1969 /api/v1/namespaces/watch-1969/configmaps/e2e-watch-test-configmap-b 8f7134ee-42ec-4de8-afdb-c314a456eab2 13909 0 2020-12-17 09:46:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 09:46:41.250: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1969 /api/v1/namespaces/watch-1969/configmaps/e2e-watch-test-configmap-b 8f7134ee-42ec-4de8-afdb-c314a456eab2 13909 0 2020-12-17 09:46:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 17 09:46:51.262: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1969 /api/v1/namespaces/watch-1969/configmaps/e2e-watch-test-configmap-b 8f7134ee-42ec-4de8-afdb-c314a456eab2 13932 0 2020-12-17 09:46:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 09:46:51.262: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1969 /api/v1/namespaces/watch-1969/configmaps/e2e-watch-test-configmap-b 8f7134ee-42ec-4de8-afdb-c314a456eab2 13932 0 2020-12-17 09:46:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-12-17 09:46:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:47:01.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1969" for this suite.

• [SLOW TEST:60.264 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":277,"completed":93,"skipped":1758,"failed":0}
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:47:01.281: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-ff28a5de-736f-4461-8b67-ccdf8e3b8e8f in namespace container-probe-1852
Dec 17 09:47:05.453: INFO: Started pod busybox-ff28a5de-736f-4461-8b67-ccdf8e3b8e8f in namespace container-probe-1852
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 09:47:05.458: INFO: Initial restart count of pod busybox-ff28a5de-736f-4461-8b67-ccdf8e3b8e8f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:51:06.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1852" for this suite.

• [SLOW TEST:244.989 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":94,"skipped":1758,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:51:06.272: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:51:06.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-892" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":277,"completed":95,"skipped":1767,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:51:06.485: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:51:10.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3521" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":96,"skipped":1778,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:51:10.729: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 09:51:11.526: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 09:51:13.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795471, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795471, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795471, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795471, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 09:51:16.566: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:51:16.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5821" for this suite.
STEP: Destroying namespace "webhook-5821-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.960 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":277,"completed":97,"skipped":1850,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:51:16.689: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6254
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:51:16.847: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:51:17.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6254" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":277,"completed":98,"skipped":1855,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:51:17.904: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-3102
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 17 09:51:18.050: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 17 09:51:18.106: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 17 09:51:20.112: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 17 09:51:22.112: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:51:24.113: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:51:26.113: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:51:28.113: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 09:51:30.111: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 17 09:51:30.120: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 17 09:51:32.137: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 17 09:51:36.178: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.137.50:8080/dial?request=hostname&protocol=udp&host=192.168.192.102&port=8081&tries=1'] Namespace:pod-network-test-3102 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 09:51:36.178: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 09:51:36.374: INFO: Waiting for responses: map[]
Dec 17 09:51:36.379: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.137.50:8080/dial?request=hostname&protocol=udp&host=192.168.137.49&port=8081&tries=1'] Namespace:pod-network-test-3102 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 09:51:36.379: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 09:51:36.537: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:51:36.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3102" for this suite.

• [SLOW TEST:18.656 seconds]
[sig-network] Networking
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":277,"completed":99,"skipped":1885,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:51:36.561: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:51:36.742: INFO: (0) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 15.230822ms)
Dec 17 09:51:36.749: INFO: (1) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.717404ms)
Dec 17 09:51:36.754: INFO: (2) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.214865ms)
Dec 17 09:51:36.761: INFO: (3) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.985286ms)
Dec 17 09:51:36.767: INFO: (4) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.297535ms)
Dec 17 09:51:36.775: INFO: (5) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.288815ms)
Dec 17 09:51:36.783: INFO: (6) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.390693ms)
Dec 17 09:51:36.791: INFO: (7) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.968396ms)
Dec 17 09:51:36.797: INFO: (8) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.088824ms)
Dec 17 09:51:36.803: INFO: (9) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.179786ms)
Dec 17 09:51:36.809: INFO: (10) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.894618ms)
Dec 17 09:51:36.815: INFO: (11) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.250757ms)
Dec 17 09:51:36.824: INFO: (12) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.60677ms)
Dec 17 09:51:36.830: INFO: (13) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.246654ms)
Dec 17 09:51:36.840: INFO: (14) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.171123ms)
Dec 17 09:51:36.847: INFO: (15) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.099083ms)
Dec 17 09:51:36.855: INFO: (16) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.408116ms)
Dec 17 09:51:36.863: INFO: (17) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.959295ms)
Dec 17 09:51:36.873: INFO: (18) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.180311ms)
Dec 17 09:51:36.883: INFO: (19) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.885591ms)
[AfterEach] version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:51:36.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-741" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":277,"completed":100,"skipped":1923,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:51:36.906: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 09:51:38.082: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 09:51:40.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795498, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795498, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795498, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795498, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 09:51:42.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795498, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795498, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795498, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795498, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 09:51:45.144: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:51:45.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4750" for this suite.
STEP: Destroying namespace "webhook-4750-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.455 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":277,"completed":101,"skipped":1923,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:51:45.361: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-3973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:51:45.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3973" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":277,"completed":102,"skipped":1934,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:51:45.552: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 09:51:45.758: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78b9696e-c465-4715-b743-6a6590c864fc" in namespace "downward-api-4643" to be "Succeeded or Failed"
Dec 17 09:51:45.765: INFO: Pod "downwardapi-volume-78b9696e-c465-4715-b743-6a6590c864fc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.910193ms
Dec 17 09:51:47.772: INFO: Pod "downwardapi-volume-78b9696e-c465-4715-b743-6a6590c864fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014628819s
Dec 17 09:51:49.779: INFO: Pod "downwardapi-volume-78b9696e-c465-4715-b743-6a6590c864fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020877328s
STEP: Saw pod success
Dec 17 09:51:49.779: INFO: Pod "downwardapi-volume-78b9696e-c465-4715-b743-6a6590c864fc" satisfied condition "Succeeded or Failed"
Dec 17 09:51:49.783: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-78b9696e-c465-4715-b743-6a6590c864fc container client-container: <nil>
STEP: delete the pod
Dec 17 09:51:49.810: INFO: Waiting for pod downwardapi-volume-78b9696e-c465-4715-b743-6a6590c864fc to disappear
Dec 17 09:51:49.816: INFO: Pod downwardapi-volume-78b9696e-c465-4715-b743-6a6590c864fc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:51:49.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4643" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":103,"skipped":1938,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:51:49.828: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:52:01.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6357" for this suite.

• [SLOW TEST:11.293 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":277,"completed":104,"skipped":1942,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:52:01.123: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1995
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-1995
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating statefulset ss in namespace statefulset-1995
Dec 17 09:52:01.336: INFO: Found 0 stateful pods, waiting for 1
Dec 17 09:52:11.342: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Dec 17 09:52:11.398: INFO: Deleting all statefulset in ns statefulset-1995
Dec 17 09:52:11.422: INFO: Scaling statefulset ss to 0
Dec 17 09:52:31.450: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 09:52:31.454: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:52:31.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1995" for this suite.

• [SLOW TEST:30.379 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":277,"completed":105,"skipped":1974,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:52:31.502: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4217
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:52:36.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4217" for this suite.

• [SLOW TEST:5.257 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":277,"completed":106,"skipped":1988,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:52:36.760: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5421
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 17 09:52:36.929: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 17 09:52:51.013: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 09:52:54.700: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:53:09.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5421" for this suite.

• [SLOW TEST:32.273 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":277,"completed":107,"skipped":1992,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:53:09.033: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-d06400f7-80ca-4e49-bdaf-991e557a0981
STEP: Creating a pod to test consume secrets
Dec 17 09:53:09.212: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-31ed36b6-7168-4974-9832-1dd15d3c6c17" in namespace "projected-6820" to be "Succeeded or Failed"
Dec 17 09:53:09.217: INFO: Pod "pod-projected-secrets-31ed36b6-7168-4974-9832-1dd15d3c6c17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.851722ms
Dec 17 09:53:11.224: INFO: Pod "pod-projected-secrets-31ed36b6-7168-4974-9832-1dd15d3c6c17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011279158s
Dec 17 09:53:13.230: INFO: Pod "pod-projected-secrets-31ed36b6-7168-4974-9832-1dd15d3c6c17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017386612s
STEP: Saw pod success
Dec 17 09:53:13.230: INFO: Pod "pod-projected-secrets-31ed36b6-7168-4974-9832-1dd15d3c6c17" satisfied condition "Succeeded or Failed"
Dec 17 09:53:13.235: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-projected-secrets-31ed36b6-7168-4974-9832-1dd15d3c6c17 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 17 09:53:13.268: INFO: Waiting for pod pod-projected-secrets-31ed36b6-7168-4974-9832-1dd15d3c6c17 to disappear
Dec 17 09:53:13.275: INFO: Pod pod-projected-secrets-31ed36b6-7168-4974-9832-1dd15d3c6c17 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:53:13.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6820" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":108,"skipped":2051,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:53:13.298: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-815
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:53:17.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-815" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":277,"completed":109,"skipped":2062,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:53:17.530: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3819
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Dec 17 09:53:18.349: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:53:18.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1217 09:53:18.349761      24 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3819" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":277,"completed":110,"skipped":2094,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:53:18.366: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:53:22.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2534" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":277,"completed":111,"skipped":2097,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:53:22.741: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9581
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:53:22.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9581" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":277,"completed":112,"skipped":2118,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:53:22.919: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8495
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-secret-8xtw
STEP: Creating a pod to test atomic-volume-subpath
Dec 17 09:53:23.167: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8xtw" in namespace "subpath-8495" to be "Succeeded or Failed"
Dec 17 09:53:23.199: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Pending", Reason="", readiness=false. Elapsed: 32.254168ms
Dec 17 09:53:25.215: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04812362s
Dec 17 09:53:27.221: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Running", Reason="", readiness=true. Elapsed: 4.054376586s
Dec 17 09:53:29.226: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Running", Reason="", readiness=true. Elapsed: 6.059561505s
Dec 17 09:53:31.231: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Running", Reason="", readiness=true. Elapsed: 8.064508375s
Dec 17 09:53:33.237: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Running", Reason="", readiness=true. Elapsed: 10.069991813s
Dec 17 09:53:35.245: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Running", Reason="", readiness=true. Elapsed: 12.078621642s
Dec 17 09:53:37.251: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Running", Reason="", readiness=true. Elapsed: 14.084647772s
Dec 17 09:53:39.257: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Running", Reason="", readiness=true. Elapsed: 16.089875041s
Dec 17 09:53:41.263: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Running", Reason="", readiness=true. Elapsed: 18.095997972s
Dec 17 09:53:43.269: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Running", Reason="", readiness=true. Elapsed: 20.102499208s
Dec 17 09:53:45.276: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Running", Reason="", readiness=true. Elapsed: 22.109689352s
Dec 17 09:53:47.284: INFO: Pod "pod-subpath-test-secret-8xtw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.117544433s
STEP: Saw pod success
Dec 17 09:53:47.285: INFO: Pod "pod-subpath-test-secret-8xtw" satisfied condition "Succeeded or Failed"
Dec 17 09:53:47.289: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-1 pod pod-subpath-test-secret-8xtw container test-container-subpath-secret-8xtw: <nil>
STEP: delete the pod
Dec 17 09:53:47.351: INFO: Waiting for pod pod-subpath-test-secret-8xtw to disappear
Dec 17 09:53:47.356: INFO: Pod pod-subpath-test-secret-8xtw no longer exists
STEP: Deleting pod pod-subpath-test-secret-8xtw
Dec 17 09:53:47.356: INFO: Deleting pod "pod-subpath-test-secret-8xtw" in namespace "subpath-8495"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:53:47.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8495" for this suite.

• [SLOW TEST:24.456 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":277,"completed":113,"skipped":2126,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:53:47.376: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4086
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:53:51.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4086" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":114,"skipped":2135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:53:51.622: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 17 09:53:52.151: INFO: Pod name wrapped-volume-race-cd0dbc66-d9c4-487f-b160-776a958e7ace: Found 0 pods out of 5
Dec 17 09:53:57.167: INFO: Pod name wrapped-volume-race-cd0dbc66-d9c4-487f-b160-776a958e7ace: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cd0dbc66-d9c4-487f-b160-776a958e7ace in namespace emptydir-wrapper-9423, will wait for the garbage collector to delete the pods
Dec 17 09:54:09.294: INFO: Deleting ReplicationController wrapped-volume-race-cd0dbc66-d9c4-487f-b160-776a958e7ace took: 15.008404ms
Dec 17 09:54:09.694: INFO: Terminating ReplicationController wrapped-volume-race-cd0dbc66-d9c4-487f-b160-776a958e7ace pods took: 400.37345ms
STEP: Creating RC which spawns configmap-volume pods
Dec 17 09:54:23.537: INFO: Pod name wrapped-volume-race-77f97238-3a21-4751-a2e8-11d18d5ce70c: Found 0 pods out of 5
Dec 17 09:54:28.556: INFO: Pod name wrapped-volume-race-77f97238-3a21-4751-a2e8-11d18d5ce70c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-77f97238-3a21-4751-a2e8-11d18d5ce70c in namespace emptydir-wrapper-9423, will wait for the garbage collector to delete the pods
Dec 17 09:54:40.672: INFO: Deleting ReplicationController wrapped-volume-race-77f97238-3a21-4751-a2e8-11d18d5ce70c took: 11.820059ms
Dec 17 09:54:41.073: INFO: Terminating ReplicationController wrapped-volume-race-77f97238-3a21-4751-a2e8-11d18d5ce70c pods took: 400.339845ms
STEP: Creating RC which spawns configmap-volume pods
Dec 17 09:54:51.809: INFO: Pod name wrapped-volume-race-9561e1ab-bc5a-451b-9644-7102a2d15f01: Found 0 pods out of 5
Dec 17 09:54:56.827: INFO: Pod name wrapped-volume-race-9561e1ab-bc5a-451b-9644-7102a2d15f01: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9561e1ab-bc5a-451b-9644-7102a2d15f01 in namespace emptydir-wrapper-9423, will wait for the garbage collector to delete the pods
Dec 17 09:55:08.941: INFO: Deleting ReplicationController wrapped-volume-race-9561e1ab-bc5a-451b-9644-7102a2d15f01 took: 13.615217ms
Dec 17 09:55:09.441: INFO: Terminating ReplicationController wrapped-volume-race-9561e1ab-bc5a-451b-9644-7102a2d15f01 pods took: 500.632304ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:55:22.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9423" for this suite.

• [SLOW TEST:90.714 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":277,"completed":115,"skipped":2178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:55:22.336: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:55:26.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2609" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":277,"completed":116,"skipped":2201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:55:26.563: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:55:26.743: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Pending, waiting for it to be Running (with Ready = true)
Dec 17 09:55:28.748: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Pending, waiting for it to be Running (with Ready = true)
Dec 17 09:55:30.750: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Running (Ready = false)
Dec 17 09:55:32.750: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Running (Ready = false)
Dec 17 09:55:34.748: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Running (Ready = false)
Dec 17 09:55:36.750: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Running (Ready = false)
Dec 17 09:55:38.751: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Running (Ready = false)
Dec 17 09:55:40.750: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Running (Ready = false)
Dec 17 09:55:42.749: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Running (Ready = false)
Dec 17 09:55:44.750: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Running (Ready = false)
Dec 17 09:55:46.749: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Running (Ready = false)
Dec 17 09:55:48.749: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Running (Ready = false)
Dec 17 09:55:50.749: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Running (Ready = false)
Dec 17 09:55:52.750: INFO: The status of Pod test-webserver-b23c43f3-3a8d-4fca-b784-935ed1252a35 is Running (Ready = true)
Dec 17 09:55:52.755: INFO: Container started at 2020-12-17 09:55:28 +0000 UTC, pod became ready at 2020-12-17 09:55:50 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:55:52.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9452" for this suite.

• [SLOW TEST:26.208 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":277,"completed":117,"skipped":2250,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:55:52.771: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9762
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 17 09:55:52.944: INFO: Waiting up to 5m0s for pod "pod-267d7591-bdac-4002-86c4-3d738e914afd" in namespace "emptydir-9762" to be "Succeeded or Failed"
Dec 17 09:55:52.953: INFO: Pod "pod-267d7591-bdac-4002-86c4-3d738e914afd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.122258ms
Dec 17 09:55:54.958: INFO: Pod "pod-267d7591-bdac-4002-86c4-3d738e914afd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014281603s
Dec 17 09:55:56.966: INFO: Pod "pod-267d7591-bdac-4002-86c4-3d738e914afd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021309556s
Dec 17 09:55:58.972: INFO: Pod "pod-267d7591-bdac-4002-86c4-3d738e914afd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028219826s
STEP: Saw pod success
Dec 17 09:55:58.972: INFO: Pod "pod-267d7591-bdac-4002-86c4-3d738e914afd" satisfied condition "Succeeded or Failed"
Dec 17 09:55:58.979: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-1 pod pod-267d7591-bdac-4002-86c4-3d738e914afd container test-container: <nil>
STEP: delete the pod
Dec 17 09:55:59.027: INFO: Waiting for pod pod-267d7591-bdac-4002-86c4-3d738e914afd to disappear
Dec 17 09:55:59.032: INFO: Pod pod-267d7591-bdac-4002-86c4-3d738e914afd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:55:59.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9762" for this suite.

• [SLOW TEST:6.291 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":118,"skipped":2252,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:55:59.064: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Starting the proxy
Dec 17 09:55:59.218: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-588444857 proxy --unix-socket=/tmp/kubectl-proxy-unix977438936/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:55:59.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6293" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":277,"completed":119,"skipped":2267,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:55:59.322: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:55:59.493: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 17 09:56:04.500: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 17 09:56:04.500: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 17 09:56:06.506: INFO: Creating deployment "test-rollover-deployment"
Dec 17 09:56:06.515: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 17 09:56:08.537: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 17 09:56:08.547: INFO: Ensure that both replica sets have 1 created replica
Dec 17 09:56:08.556: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 17 09:56:08.569: INFO: Updating deployment test-rollover-deployment
Dec 17 09:56:08.569: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 17 09:56:10.582: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 17 09:56:10.593: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 17 09:56:10.602: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 09:56:10.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795768, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 09:56:12.612: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 09:56:12.612: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795770, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 09:56:14.613: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 09:56:14.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795770, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 09:56:16.613: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 09:56:16.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795770, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 09:56:18.612: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 09:56:18.612: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795770, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 09:56:20.615: INFO: all replica sets need to contain the pod-template-hash label
Dec 17 09:56:20.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795770, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743795766, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 09:56:22.614: INFO: 
Dec 17 09:56:22.614: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Dec 17 09:56:22.627: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6948 /apis/apps/v1/namespaces/deployment-6948/deployments/test-rollover-deployment 473665b2-3bd8-4a4f-b390-a541f8b02a9a 17287 2 2020-12-17 09:56:06 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-17 09:56:08 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-12-17 09:56:20 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006127c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-12-17 09:56:06 +0000 UTC,LastTransitionTime:2020-12-17 09:56:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-84f7f6f64b" has successfully progressed.,LastUpdateTime:2020-12-17 09:56:20 +0000 UTC,LastTransitionTime:2020-12-17 09:56:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 17 09:56:22.633: INFO: New ReplicaSet "test-rollover-deployment-84f7f6f64b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-84f7f6f64b  deployment-6948 /apis/apps/v1/namespaces/deployment-6948/replicasets/test-rollover-deployment-84f7f6f64b 0534e9cf-cb84-497d-b3e2-422016ae5ce8 17276 2 2020-12-17 09:56:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 473665b2-3bd8-4a4f-b390-a541f8b02a9a 0xc002fd3647 0xc002fd3648}] []  [{kube-controller-manager Update apps/v1 2020-12-17 09:56:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 55 51 54 54 53 98 50 45 51 98 100 56 45 52 97 52 102 45 98 51 57 48 45 97 53 52 49 102 56 98 48 50 97 57 97 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 84f7f6f64b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002fd36d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 17 09:56:22.633: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 17 09:56:22.634: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6948 /apis/apps/v1/namespaces/deployment-6948/replicasets/test-rollover-controller 4db01ceb-47ed-4d78-9829-addf1100b57f 17285 2 2020-12-17 09:55:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 473665b2-3bd8-4a4f-b390-a541f8b02a9a 0xc002fd33e7 0xc002fd33e8}] []  [{e2e.test Update apps/v1 2020-12-17 09:55:59 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-12-17 09:56:20 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 55 51 54 54 53 98 50 45 51 98 100 56 45 52 97 52 102 45 98 51 57 48 45 97 53 52 49 102 56 98 48 50 97 57 97 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002fd3488 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 17 09:56:22.634: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-5686c4cfd5  deployment-6948 /apis/apps/v1/namespaces/deployment-6948/replicasets/test-rollover-deployment-5686c4cfd5 7849c245-1bd4-4ebb-817d-7e5822e692a3 17225 2 2020-12-17 09:56:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 473665b2-3bd8-4a4f-b390-a541f8b02a9a 0xc002fd3507 0xc002fd3508}] []  [{kube-controller-manager Update apps/v1 2020-12-17 09:56:08 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 55 51 54 54 53 98 50 45 51 98 100 56 45 52 97 52 102 45 98 51 57 48 45 97 53 52 49 102 56 98 48 50 97 57 97 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 114 101 100 105 115 45 115 108 97 118 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5686c4cfd5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002fd35d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 17 09:56:22.640: INFO: Pod "test-rollover-deployment-84f7f6f64b-5hrnm" is available:
&Pod{ObjectMeta:{test-rollover-deployment-84f7f6f64b-5hrnm test-rollover-deployment-84f7f6f64b- deployment-6948 /api/v1/namespaces/deployment-6948/pods/test-rollover-deployment-84f7f6f64b-5hrnm 0f512151-bb62-4528-9ef9-1eb7ca8d729c 17248 0 2020-12-17 09:56:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[cni.projectcalico.org/podIP:192.168.192.71/32 cni.projectcalico.org/podIPs:192.168.192.71/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-84f7f6f64b 0534e9cf-cb84-497d-b3e2-422016ae5ce8 0xc002fd3d07 0xc002fd3d08}] []  [{kube-controller-manager Update v1 2020-12-17 09:56:08 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 48 53 51 52 101 57 99 102 45 99 98 56 52 45 52 57 55 100 45 98 51 101 50 45 52 50 50 48 49 54 97 101 53 99 101 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 09:56:09 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-12-17 09:56:10 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 50 46 55 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7v28q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7v28q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7v28q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:56:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:56:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:56:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 09:56:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:192.168.192.71,StartTime:2020-12-17 09:56:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 09:56:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://d94e43caa4163c5cbdc7bb21355f25b074a5327585ae044873c432e388376511,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:56:22.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6948" for this suite.

• [SLOW TEST:23.333 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":277,"completed":120,"skipped":2279,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:56:22.656: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:56:33.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2803" for this suite.

• [SLOW TEST:11.245 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":277,"completed":121,"skipped":2279,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:56:33.902: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3431
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:56:34.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3431" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":277,"completed":122,"skipped":2310,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:56:34.115: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1241
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 09:56:34.284: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db6b527f-ada9-4bb7-ae36-63d9effc8222" in namespace "downward-api-1241" to be "Succeeded or Failed"
Dec 17 09:56:34.300: INFO: Pod "downwardapi-volume-db6b527f-ada9-4bb7-ae36-63d9effc8222": Phase="Pending", Reason="", readiness=false. Elapsed: 15.683726ms
Dec 17 09:56:36.305: INFO: Pod "downwardapi-volume-db6b527f-ada9-4bb7-ae36-63d9effc8222": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021114627s
Dec 17 09:56:38.311: INFO: Pod "downwardapi-volume-db6b527f-ada9-4bb7-ae36-63d9effc8222": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027318275s
STEP: Saw pod success
Dec 17 09:56:38.311: INFO: Pod "downwardapi-volume-db6b527f-ada9-4bb7-ae36-63d9effc8222" satisfied condition "Succeeded or Failed"
Dec 17 09:56:38.320: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-db6b527f-ada9-4bb7-ae36-63d9effc8222 container client-container: <nil>
STEP: delete the pod
Dec 17 09:56:38.362: INFO: Waiting for pod downwardapi-volume-db6b527f-ada9-4bb7-ae36-63d9effc8222 to disappear
Dec 17 09:56:38.366: INFO: Pod downwardapi-volume-db6b527f-ada9-4bb7-ae36-63d9effc8222 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:56:38.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1241" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":123,"skipped":2329,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:56:38.380: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7328
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-ae95f3df-751d-4f73-9bb0-021a06e7ea63
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-ae95f3df-751d-4f73-9bb0-021a06e7ea63
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:58:09.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7328" for this suite.

• [SLOW TEST:90.930 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":124,"skipped":2349,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:58:09.311: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-336
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 17 09:58:15.539: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
W1217 09:58:15.539637      24 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 17 09:58:15.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-336" for this suite.

• [SLOW TEST:6.258 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":277,"completed":125,"skipped":2350,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:58:15.571: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4430
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service multi-endpoint-test in namespace services-4430
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4430 to expose endpoints map[]
Dec 17 09:58:15.835: INFO: successfully validated that service multi-endpoint-test in namespace services-4430 exposes endpoints map[] (9.329395ms elapsed)
STEP: Creating pod pod1 in namespace services-4430
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4430 to expose endpoints map[pod1:[100]]
Dec 17 09:58:18.958: INFO: successfully validated that service multi-endpoint-test in namespace services-4430 exposes endpoints map[pod1:[100]] (3.064258552s elapsed)
STEP: Creating pod pod2 in namespace services-4430
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4430 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 17 09:58:22.047: INFO: successfully validated that service multi-endpoint-test in namespace services-4430 exposes endpoints map[pod1:[100] pod2:[101]] (3.072343594s elapsed)
STEP: Deleting pod pod1 in namespace services-4430
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4430 to expose endpoints map[pod2:[101]]
Dec 17 09:58:22.106: INFO: successfully validated that service multi-endpoint-test in namespace services-4430 exposes endpoints map[pod2:[101]] (37.089496ms elapsed)
STEP: Deleting pod pod2 in namespace services-4430
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4430 to expose endpoints map[]
Dec 17 09:58:23.136: INFO: successfully validated that service multi-endpoint-test in namespace services-4430 exposes endpoints map[] (1.017124332s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:58:23.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4430" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:7.625 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":277,"completed":126,"skipped":2354,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:58:23.196: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 09:58:23.380: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3585f70-f57f-459e-9040-163cd6207412" in namespace "projected-8545" to be "Succeeded or Failed"
Dec 17 09:58:23.389: INFO: Pod "downwardapi-volume-b3585f70-f57f-459e-9040-163cd6207412": Phase="Pending", Reason="", readiness=false. Elapsed: 8.803161ms
Dec 17 09:58:25.395: INFO: Pod "downwardapi-volume-b3585f70-f57f-459e-9040-163cd6207412": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01482852s
Dec 17 09:58:27.402: INFO: Pod "downwardapi-volume-b3585f70-f57f-459e-9040-163cd6207412": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022365544s
STEP: Saw pod success
Dec 17 09:58:27.402: INFO: Pod "downwardapi-volume-b3585f70-f57f-459e-9040-163cd6207412" satisfied condition "Succeeded or Failed"
Dec 17 09:58:27.408: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-b3585f70-f57f-459e-9040-163cd6207412 container client-container: <nil>
STEP: delete the pod
Dec 17 09:58:27.443: INFO: Waiting for pod downwardapi-volume-b3585f70-f57f-459e-9040-163cd6207412 to disappear
Dec 17 09:58:27.449: INFO: Pod downwardapi-volume-b3585f70-f57f-459e-9040-163cd6207412 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:58:27.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8545" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":127,"skipped":2357,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:58:27.467: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8050
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8050.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8050.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8050.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8050.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8050.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8050.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8050.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8050.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8050.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8050.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 2.16.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.16.2_udp@PTR;check="$$(dig +tcp +noall +answer +search 2.16.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.16.2_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8050.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8050.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8050.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8050.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8050.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8050.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8050.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8050.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8050.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8050.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8050.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 2.16.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.16.2_udp@PTR;check="$$(dig +tcp +noall +answer +search 2.16.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.16.2_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 09:58:31.732: INFO: Unable to read wheezy_udp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:31.739: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:31.745: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:31.751: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:31.799: INFO: Unable to read jessie_udp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:31.806: INFO: Unable to read jessie_tcp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:31.812: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:31.819: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:31.868: INFO: Lookups using dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451 failed for: [wheezy_udp@dns-test-service.dns-8050.svc.cluster.local wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local jessie_udp@dns-test-service.dns-8050.svc.cluster.local jessie_tcp@dns-test-service.dns-8050.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local]

Dec 17 09:58:36.876: INFO: Unable to read wheezy_udp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:36.883: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:36.891: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:36.897: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:36.949: INFO: Unable to read jessie_udp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:36.954: INFO: Unable to read jessie_tcp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:36.958: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:36.964: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:37.000: INFO: Lookups using dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451 failed for: [wheezy_udp@dns-test-service.dns-8050.svc.cluster.local wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local jessie_udp@dns-test-service.dns-8050.svc.cluster.local jessie_tcp@dns-test-service.dns-8050.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local]

Dec 17 09:58:41.877: INFO: Unable to read wheezy_udp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:41.883: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:41.889: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:41.896: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:41.936: INFO: Unable to read jessie_udp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:41.941: INFO: Unable to read jessie_tcp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:41.947: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:41.953: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:41.986: INFO: Lookups using dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451 failed for: [wheezy_udp@dns-test-service.dns-8050.svc.cluster.local wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local jessie_udp@dns-test-service.dns-8050.svc.cluster.local jessie_tcp@dns-test-service.dns-8050.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local]

Dec 17 09:58:46.876: INFO: Unable to read wheezy_udp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:46.883: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:46.888: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:46.892: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:46.943: INFO: Unable to read jessie_udp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:46.948: INFO: Unable to read jessie_tcp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:46.953: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:46.960: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:46.998: INFO: Lookups using dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451 failed for: [wheezy_udp@dns-test-service.dns-8050.svc.cluster.local wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local jessie_udp@dns-test-service.dns-8050.svc.cluster.local jessie_tcp@dns-test-service.dns-8050.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local]

Dec 17 09:58:51.876: INFO: Unable to read wheezy_udp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:51.882: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:51.889: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:51.894: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:51.944: INFO: Unable to read jessie_udp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:51.950: INFO: Unable to read jessie_tcp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:51.956: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:51.963: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:52.000: INFO: Lookups using dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451 failed for: [wheezy_udp@dns-test-service.dns-8050.svc.cluster.local wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local jessie_udp@dns-test-service.dns-8050.svc.cluster.local jessie_tcp@dns-test-service.dns-8050.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local]

Dec 17 09:58:56.878: INFO: Unable to read wheezy_udp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:56.890: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:56.898: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:56.908: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:56.959: INFO: Unable to read jessie_udp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:56.964: INFO: Unable to read jessie_tcp@dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:56.968: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:56.974: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local from pod dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451: the server could not find the requested resource (get pods dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451)
Dec 17 09:58:57.032: INFO: Lookups using dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451 failed for: [wheezy_udp@dns-test-service.dns-8050.svc.cluster.local wheezy_tcp@dns-test-service.dns-8050.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local jessie_udp@dns-test-service.dns-8050.svc.cluster.local jessie_tcp@dns-test-service.dns-8050.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8050.svc.cluster.local]

Dec 17 09:59:01.989: INFO: DNS probes using dns-8050/dns-test-6082c26d-5fb7-41a1-b084-8a37145d6451 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:59:02.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8050" for this suite.

• [SLOW TEST:34.809 seconds]
[sig-network] DNS
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":277,"completed":128,"skipped":2382,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:59:02.279: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1876
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:59:02.444: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:59:10.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1876" for this suite.

• [SLOW TEST:7.905 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":277,"completed":129,"skipped":2398,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:59:10.186: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Dec 17 09:59:50.426: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1217 09:59:50.426676      24 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:59:50.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8285" for this suite.

• [SLOW TEST:40.256 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":277,"completed":130,"skipped":2414,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:59:50.444: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7001
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 09:59:50.600: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:59:51.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7001" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":277,"completed":131,"skipped":2443,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:59:52.008: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5168
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 17 09:59:52.197: INFO: Waiting up to 5m0s for pod "pod-f6b6121c-7c28-49ee-9e56-2a7225f5c7ae" in namespace "emptydir-5168" to be "Succeeded or Failed"
Dec 17 09:59:52.206: INFO: Pod "pod-f6b6121c-7c28-49ee-9e56-2a7225f5c7ae": Phase="Pending", Reason="", readiness=false. Elapsed: 8.314446ms
Dec 17 09:59:54.216: INFO: Pod "pod-f6b6121c-7c28-49ee-9e56-2a7225f5c7ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018526285s
Dec 17 09:59:56.245: INFO: Pod "pod-f6b6121c-7c28-49ee-9e56-2a7225f5c7ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047730883s
STEP: Saw pod success
Dec 17 09:59:56.245: INFO: Pod "pod-f6b6121c-7c28-49ee-9e56-2a7225f5c7ae" satisfied condition "Succeeded or Failed"
Dec 17 09:59:56.258: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-f6b6121c-7c28-49ee-9e56-2a7225f5c7ae container test-container: <nil>
STEP: delete the pod
Dec 17 09:59:56.309: INFO: Waiting for pod pod-f6b6121c-7c28-49ee-9e56-2a7225f5c7ae to disappear
Dec 17 09:59:56.316: INFO: Pod pod-f6b6121c-7c28-49ee-9e56-2a7225f5c7ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 09:59:56.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5168" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":132,"skipped":2445,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 09:59:56.354: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-3476
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Dec 17 09:59:56.618: INFO: Found 0 stateful pods, waiting for 3
Dec 17 10:00:06.625: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 10:00:06.625: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 10:00:06.625: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 17 10:00:06.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-3476 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 17 10:00:07.099: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 17 10:00:07.099: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 17 10:00:07.099: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 17 10:00:17.143: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 17 10:00:27.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-3476 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 17 10:00:27.466: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 17 10:00:27.466: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 17 10:00:27.466: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 17 10:00:47.497: INFO: Waiting for StatefulSet statefulset-3476/ss2 to complete update
Dec 17 10:00:47.497: INFO: Waiting for Pod statefulset-3476/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Dec 17 10:00:57.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-3476 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 17 10:00:57.793: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 17 10:00:57.793: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 17 10:00:57.793: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 17 10:01:07.851: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 17 10:01:17.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=statefulset-3476 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 17 10:01:18.195: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 17 10:01:18.195: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 17 10:01:18.195: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 17 10:01:28.231: INFO: Waiting for StatefulSet statefulset-3476/ss2 to complete update
Dec 17 10:01:28.231: INFO: Waiting for Pod statefulset-3476/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 17 10:01:28.231: INFO: Waiting for Pod statefulset-3476/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 17 10:01:28.231: INFO: Waiting for Pod statefulset-3476/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 17 10:01:38.245: INFO: Waiting for StatefulSet statefulset-3476/ss2 to complete update
Dec 17 10:01:38.245: INFO: Waiting for Pod statefulset-3476/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 17 10:01:38.245: INFO: Waiting for Pod statefulset-3476/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 17 10:01:48.244: INFO: Waiting for StatefulSet statefulset-3476/ss2 to complete update
Dec 17 10:01:48.244: INFO: Waiting for Pod statefulset-3476/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Dec 17 10:01:58.252: INFO: Deleting all statefulset in ns statefulset-3476
Dec 17 10:01:58.256: INFO: Scaling statefulset ss2 to 0
Dec 17 10:02:28.285: INFO: Waiting for statefulset status.replicas updated to 0
Dec 17 10:02:28.289: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:02:28.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3476" for this suite.

• [SLOW TEST:151.986 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":277,"completed":133,"skipped":2466,"failed":0}
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:02:28.341: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:02:28.497: INFO: Creating deployment "test-recreate-deployment"
Dec 17 10:02:28.505: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 17 10:02:28.529: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 17 10:02:30.540: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 17 10:02:30.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796148, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796148, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796148, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796148, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 10:02:32.553: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 17 10:02:32.571: INFO: Updating deployment test-recreate-deployment
Dec 17 10:02:32.571: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Dec 17 10:02:32.732: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9308 /apis/apps/v1/namespaces/deployment-9308/deployments/test-recreate-deployment b61becf5-a15b-4582-99b1-3fffed67e069 19597 2 2020-12-17 10:02:28 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-12-17 10:02:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-12-17 10:02:32 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002db1c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-12-17 10:02:32 +0000 UTC,LastTransitionTime:2020-12-17 10:02:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-d5667d9c7" is progressing.,LastUpdateTime:2020-12-17 10:02:32 +0000 UTC,LastTransitionTime:2020-12-17 10:02:28 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 17 10:02:32.737: INFO: New ReplicaSet "test-recreate-deployment-d5667d9c7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-d5667d9c7  deployment-9308 /apis/apps/v1/namespaces/deployment-9308/replicasets/test-recreate-deployment-d5667d9c7 c668a52f-1aee-484b-a34a-0ccfdbac76cc 19594 1 2020-12-17 10:02:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b61becf5-a15b-4582-99b1-3fffed67e069 0xc0051f43d0 0xc0051f43d1}] []  [{kube-controller-manager Update apps/v1 2020-12-17 10:02:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 54 49 98 101 99 102 53 45 97 49 53 98 45 52 53 56 50 45 57 57 98 49 45 51 102 102 102 101 100 54 55 101 48 54 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: d5667d9c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0051f4448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 17 10:02:32.737: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 17 10:02:32.738: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-74d98b5f7c  deployment-9308 /apis/apps/v1/namespaces/deployment-9308/replicasets/test-recreate-deployment-74d98b5f7c ac033ac9-99cf-453b-b24d-fd984ef291f7 19585 2 2020-12-17 10:02:28 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b61becf5-a15b-4582-99b1-3fffed67e069 0xc0051f42c7 0xc0051f42c8}] []  [{kube-controller-manager Update apps/v1 2020-12-17 10:02:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 54 49 98 101 99 102 53 45 97 49 53 98 45 52 53 56 50 45 57 57 98 49 45 51 102 102 102 101 100 54 55 101 48 54 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 74d98b5f7c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0051f4358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 17 10:02:32.743: INFO: Pod "test-recreate-deployment-d5667d9c7-8kfvq" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-d5667d9c7-8kfvq test-recreate-deployment-d5667d9c7- deployment-9308 /api/v1/namespaces/deployment-9308/pods/test-recreate-deployment-d5667d9c7-8kfvq 5440f5ee-64e3-4691-89b4-2f985bd21bf6 19598 0 2020-12-17 10:02:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-d5667d9c7 c668a52f-1aee-484b-a34a-0ccfdbac76cc 0xc0051f4940 0xc0051f4941}] []  [{kube-controller-manager Update v1 2020-12-17 10:02:32 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 54 54 56 97 53 50 102 45 49 97 101 101 45 52 56 52 98 45 97 51 52 97 45 48 99 99 102 100 98 97 99 55 54 99 99 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-12-17 10:02:32 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74tlr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74tlr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74tlr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 10:02:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 10:02:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 10:02:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 10:02:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:,StartTime:2020-12-17 10:02:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:02:32.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9308" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":134,"skipped":2466,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:02:32.759: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9313
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 17 10:02:32.933: INFO: Waiting up to 5m0s for pod "pod-a9dc990f-dcb9-40e9-aeba-295a79bde0f6" in namespace "emptydir-9313" to be "Succeeded or Failed"
Dec 17 10:02:32.944: INFO: Pod "pod-a9dc990f-dcb9-40e9-aeba-295a79bde0f6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.320676ms
Dec 17 10:02:34.950: INFO: Pod "pod-a9dc990f-dcb9-40e9-aeba-295a79bde0f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016986431s
Dec 17 10:02:36.956: INFO: Pod "pod-a9dc990f-dcb9-40e9-aeba-295a79bde0f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023053609s
STEP: Saw pod success
Dec 17 10:02:36.956: INFO: Pod "pod-a9dc990f-dcb9-40e9-aeba-295a79bde0f6" satisfied condition "Succeeded or Failed"
Dec 17 10:02:36.965: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-1 pod pod-a9dc990f-dcb9-40e9-aeba-295a79bde0f6 container test-container: <nil>
STEP: delete the pod
Dec 17 10:02:37.059: INFO: Waiting for pod pod-a9dc990f-dcb9-40e9-aeba-295a79bde0f6 to disappear
Dec 17 10:02:37.062: INFO: Pod pod-a9dc990f-dcb9-40e9-aeba-295a79bde0f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:02:37.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9313" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":135,"skipped":2467,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:02:37.086: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Dec 17 10:02:37.306: INFO: Waiting up to 5m0s for pod "downward-api-5bab1d1d-1da7-4ca3-8edb-3da373cf823e" in namespace "downward-api-7225" to be "Succeeded or Failed"
Dec 17 10:02:37.320: INFO: Pod "downward-api-5bab1d1d-1da7-4ca3-8edb-3da373cf823e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.715176ms
Dec 17 10:02:39.326: INFO: Pod "downward-api-5bab1d1d-1da7-4ca3-8edb-3da373cf823e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020059029s
Dec 17 10:02:41.333: INFO: Pod "downward-api-5bab1d1d-1da7-4ca3-8edb-3da373cf823e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026615459s
STEP: Saw pod success
Dec 17 10:02:41.333: INFO: Pod "downward-api-5bab1d1d-1da7-4ca3-8edb-3da373cf823e" satisfied condition "Succeeded or Failed"
Dec 17 10:02:41.338: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-1 pod downward-api-5bab1d1d-1da7-4ca3-8edb-3da373cf823e container dapi-container: <nil>
STEP: delete the pod
Dec 17 10:02:41.372: INFO: Waiting for pod downward-api-5bab1d1d-1da7-4ca3-8edb-3da373cf823e to disappear
Dec 17 10:02:41.375: INFO: Pod downward-api-5bab1d1d-1da7-4ca3-8edb-3da373cf823e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:02:41.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7225" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":277,"completed":136,"skipped":2469,"failed":0}

------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:02:41.393: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7357
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:02:41.573: INFO: Waiting up to 5m0s for pod "busybox-user-65534-36e17524-ad15-4996-a98e-5950fbe366fd" in namespace "security-context-test-7357" to be "Succeeded or Failed"
Dec 17 10:02:41.585: INFO: Pod "busybox-user-65534-36e17524-ad15-4996-a98e-5950fbe366fd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.492497ms
Dec 17 10:02:43.592: INFO: Pod "busybox-user-65534-36e17524-ad15-4996-a98e-5950fbe366fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018859967s
Dec 17 10:02:45.599: INFO: Pod "busybox-user-65534-36e17524-ad15-4996-a98e-5950fbe366fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025660402s
Dec 17 10:02:45.599: INFO: Pod "busybox-user-65534-36e17524-ad15-4996-a98e-5950fbe366fd" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:02:45.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7357" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":137,"skipped":2469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:02:45.615: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-721
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 17 10:02:48.824: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:02:48.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-721" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":277,"completed":138,"skipped":2531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:02:48.861: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9495
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 10:02:49.026: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82c44b8d-9cdb-425d-9fa1-7f96e9aa0593" in namespace "downward-api-9495" to be "Succeeded or Failed"
Dec 17 10:02:49.031: INFO: Pod "downwardapi-volume-82c44b8d-9cdb-425d-9fa1-7f96e9aa0593": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295113ms
Dec 17 10:02:51.038: INFO: Pod "downwardapi-volume-82c44b8d-9cdb-425d-9fa1-7f96e9aa0593": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01216645s
Dec 17 10:02:53.053: INFO: Pod "downwardapi-volume-82c44b8d-9cdb-425d-9fa1-7f96e9aa0593": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026980819s
STEP: Saw pod success
Dec 17 10:02:53.053: INFO: Pod "downwardapi-volume-82c44b8d-9cdb-425d-9fa1-7f96e9aa0593" satisfied condition "Succeeded or Failed"
Dec 17 10:02:53.059: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-82c44b8d-9cdb-425d-9fa1-7f96e9aa0593 container client-container: <nil>
STEP: delete the pod
Dec 17 10:02:53.104: INFO: Waiting for pod downwardapi-volume-82c44b8d-9cdb-425d-9fa1-7f96e9aa0593 to disappear
Dec 17 10:02:53.115: INFO: Pod downwardapi-volume-82c44b8d-9cdb-425d-9fa1-7f96e9aa0593 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:02:53.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9495" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":277,"completed":139,"skipped":2564,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:02:53.130: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 10:02:53.796: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796173, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796173, loc:(*time.Location)(0x7b675e0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-779fdc84d9\""}}, CollisionCount:(*int32)(nil)}
Dec 17 10:02:55.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796173, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796173, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796173, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796173, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 10:02:58.827: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:02:58.833: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3842-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:02:59.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3283" for this suite.
STEP: Destroying namespace "webhook-3283-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.984 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":277,"completed":140,"skipped":2566,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:03:00.114: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Dec 17 10:03:00.272: INFO: namespace kubectl-244
Dec 17 10:03:00.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-244'
Dec 17 10:03:00.659: INFO: stderr: ""
Dec 17 10:03:00.659: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Dec 17 10:03:01.666: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 10:03:01.666: INFO: Found 0 / 1
Dec 17 10:03:02.665: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 10:03:02.665: INFO: Found 0 / 1
Dec 17 10:03:03.666: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 10:03:03.666: INFO: Found 1 / 1
Dec 17 10:03:03.666: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 17 10:03:03.671: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 10:03:03.671: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 17 10:03:03.671: INFO: wait on agnhost-master startup in kubectl-244 
Dec 17 10:03:03.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 logs agnhost-master-9h7d8 agnhost-master --namespace=kubectl-244'
Dec 17 10:03:03.805: INFO: stderr: ""
Dec 17 10:03:03.805: INFO: stdout: "Paused\n"
STEP: exposing RC
Dec 17 10:03:03.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-244'
Dec 17 10:03:03.943: INFO: stderr: ""
Dec 17 10:03:03.943: INFO: stdout: "service/rm2 exposed\n"
Dec 17 10:03:03.959: INFO: Service rm2 in namespace kubectl-244 found.
STEP: exposing service
Dec 17 10:03:05.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-244'
Dec 17 10:03:06.093: INFO: stderr: ""
Dec 17 10:03:06.093: INFO: stdout: "service/rm3 exposed\n"
Dec 17 10:03:06.107: INFO: Service rm3 in namespace kubectl-244 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:03:08.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-244" for this suite.

• [SLOW TEST:8.038 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1119
    should create services for rc  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":277,"completed":141,"skipped":2567,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:03:08.153: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-projected-all-test-volume-34172d10-90b6-498e-8885-dcd41c71b977
STEP: Creating secret with name secret-projected-all-test-volume-04f0c47b-a7e3-4915-83d0-dbe1b8c228d1
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 17 10:03:08.393: INFO: Waiting up to 5m0s for pod "projected-volume-e106ab28-b9c4-4845-ad2a-de14d21ec4f8" in namespace "projected-8259" to be "Succeeded or Failed"
Dec 17 10:03:08.402: INFO: Pod "projected-volume-e106ab28-b9c4-4845-ad2a-de14d21ec4f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.589679ms
Dec 17 10:03:10.409: INFO: Pod "projected-volume-e106ab28-b9c4-4845-ad2a-de14d21ec4f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015369021s
Dec 17 10:03:12.414: INFO: Pod "projected-volume-e106ab28-b9c4-4845-ad2a-de14d21ec4f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020374672s
STEP: Saw pod success
Dec 17 10:03:12.414: INFO: Pod "projected-volume-e106ab28-b9c4-4845-ad2a-de14d21ec4f8" satisfied condition "Succeeded or Failed"
Dec 17 10:03:12.418: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-1 pod projected-volume-e106ab28-b9c4-4845-ad2a-de14d21ec4f8 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 17 10:03:12.488: INFO: Waiting for pod projected-volume-e106ab28-b9c4-4845-ad2a-de14d21ec4f8 to disappear
Dec 17 10:03:12.492: INFO: Pod projected-volume-e106ab28-b9c4-4845-ad2a-de14d21ec4f8 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:03:12.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8259" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":277,"completed":142,"skipped":2579,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:03:12.503: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4536
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-689653a8-4ccd-4bfc-9248-37f68b3663d6
STEP: Creating a pod to test consume secrets
Dec 17 10:03:12.682: INFO: Waiting up to 5m0s for pod "pod-secrets-f913407f-ae95-4299-a8ac-c326a715ca37" in namespace "secrets-4536" to be "Succeeded or Failed"
Dec 17 10:03:12.690: INFO: Pod "pod-secrets-f913407f-ae95-4299-a8ac-c326a715ca37": Phase="Pending", Reason="", readiness=false. Elapsed: 8.691579ms
Dec 17 10:03:14.698: INFO: Pod "pod-secrets-f913407f-ae95-4299-a8ac-c326a715ca37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016245335s
Dec 17 10:03:16.704: INFO: Pod "pod-secrets-f913407f-ae95-4299-a8ac-c326a715ca37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022436194s
STEP: Saw pod success
Dec 17 10:03:16.704: INFO: Pod "pod-secrets-f913407f-ae95-4299-a8ac-c326a715ca37" satisfied condition "Succeeded or Failed"
Dec 17 10:03:16.710: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-secrets-f913407f-ae95-4299-a8ac-c326a715ca37 container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 10:03:16.741: INFO: Waiting for pod pod-secrets-f913407f-ae95-4299-a8ac-c326a715ca37 to disappear
Dec 17 10:03:16.745: INFO: Pod pod-secrets-f913407f-ae95-4299-a8ac-c326a715ca37 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:03:16.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4536" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":143,"skipped":2582,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:03:16.759: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 17 10:03:16.935: INFO: Waiting up to 5m0s for pod "pod-1451106f-def7-417a-8745-0923b9dd8271" in namespace "emptydir-990" to be "Succeeded or Failed"
Dec 17 10:03:16.945: INFO: Pod "pod-1451106f-def7-417a-8745-0923b9dd8271": Phase="Pending", Reason="", readiness=false. Elapsed: 9.336714ms
Dec 17 10:03:18.951: INFO: Pod "pod-1451106f-def7-417a-8745-0923b9dd8271": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015859278s
Dec 17 10:03:20.957: INFO: Pod "pod-1451106f-def7-417a-8745-0923b9dd8271": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021758114s
Dec 17 10:03:22.964: INFO: Pod "pod-1451106f-def7-417a-8745-0923b9dd8271": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028884252s
STEP: Saw pod success
Dec 17 10:03:22.964: INFO: Pod "pod-1451106f-def7-417a-8745-0923b9dd8271" satisfied condition "Succeeded or Failed"
Dec 17 10:03:22.969: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-1451106f-def7-417a-8745-0923b9dd8271 container test-container: <nil>
STEP: delete the pod
Dec 17 10:03:23.004: INFO: Waiting for pod pod-1451106f-def7-417a-8745-0923b9dd8271 to disappear
Dec 17 10:03:23.009: INFO: Pod pod-1451106f-def7-417a-8745-0923b9dd8271 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:03:23.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-990" for this suite.

• [SLOW TEST:6.266 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":144,"skipped":2592,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:03:23.025: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-901
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:03:23.227: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 17 10:03:23.258: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:23.286: INFO: Number of nodes with available pods: 0
Dec 17 10:03:23.286: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 10:03:24.294: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:24.298: INFO: Number of nodes with available pods: 0
Dec 17 10:03:24.298: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 10:03:25.293: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:25.298: INFO: Number of nodes with available pods: 0
Dec 17 10:03:25.298: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 10:03:26.294: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:26.299: INFO: Number of nodes with available pods: 2
Dec 17 10:03:26.299: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 17 10:03:26.342: INFO: Wrong image for pod: daemon-set-h9tmx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:26.342: INFO: Wrong image for pod: daemon-set-rnjbx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:26.352: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:27.358: INFO: Wrong image for pod: daemon-set-h9tmx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:27.358: INFO: Wrong image for pod: daemon-set-rnjbx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:27.363: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:28.361: INFO: Wrong image for pod: daemon-set-h9tmx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:28.361: INFO: Wrong image for pod: daemon-set-rnjbx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:28.366: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:29.360: INFO: Wrong image for pod: daemon-set-h9tmx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:29.360: INFO: Pod daemon-set-h9tmx is not available
Dec 17 10:03:29.360: INFO: Wrong image for pod: daemon-set-rnjbx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:29.365: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:30.359: INFO: Pod daemon-set-mm9tc is not available
Dec 17 10:03:30.359: INFO: Wrong image for pod: daemon-set-rnjbx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:30.365: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:31.360: INFO: Pod daemon-set-mm9tc is not available
Dec 17 10:03:31.360: INFO: Wrong image for pod: daemon-set-rnjbx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:31.365: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:32.359: INFO: Pod daemon-set-mm9tc is not available
Dec 17 10:03:32.359: INFO: Wrong image for pod: daemon-set-rnjbx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:32.363: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:33.359: INFO: Wrong image for pod: daemon-set-rnjbx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:33.364: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:34.359: INFO: Wrong image for pod: daemon-set-rnjbx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:34.368: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:35.357: INFO: Wrong image for pod: daemon-set-rnjbx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Dec 17 10:03:35.358: INFO: Pod daemon-set-rnjbx is not available
Dec 17 10:03:35.363: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:36.360: INFO: Pod daemon-set-fxzn8 is not available
Dec 17 10:03:36.366: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 17 10:03:36.371: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:36.377: INFO: Number of nodes with available pods: 1
Dec 17 10:03:36.377: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 10:03:37.384: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:37.390: INFO: Number of nodes with available pods: 1
Dec 17 10:03:37.390: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 10:03:38.383: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:38.390: INFO: Number of nodes with available pods: 1
Dec 17 10:03:38.390: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 10:03:39.383: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:03:39.388: INFO: Number of nodes with available pods: 2
Dec 17 10:03:39.388: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-901, will wait for the garbage collector to delete the pods
Dec 17 10:03:39.482: INFO: Deleting DaemonSet.extensions daemon-set took: 11.700071ms
Dec 17 10:03:39.883: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.347121ms
Dec 17 10:03:53.287: INFO: Number of nodes with available pods: 0
Dec 17 10:03:53.287: INFO: Number of running nodes: 0, number of available pods: 0
Dec 17 10:03:53.291: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-901/daemonsets","resourceVersion":"20415"},"items":null}

Dec 17 10:03:53.295: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-901/pods","resourceVersion":"20415"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:03:53.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-901" for this suite.

• [SLOW TEST:30.298 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":277,"completed":145,"skipped":2602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:03:53.324: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 10:03:53.493: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5eb94e78-a3f9-45a4-8d31-3f2448fb21d6" in namespace "projected-9053" to be "Succeeded or Failed"
Dec 17 10:03:53.502: INFO: Pod "downwardapi-volume-5eb94e78-a3f9-45a4-8d31-3f2448fb21d6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.892045ms
Dec 17 10:03:55.508: INFO: Pod "downwardapi-volume-5eb94e78-a3f9-45a4-8d31-3f2448fb21d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014975546s
Dec 17 10:03:57.513: INFO: Pod "downwardapi-volume-5eb94e78-a3f9-45a4-8d31-3f2448fb21d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020062054s
STEP: Saw pod success
Dec 17 10:03:57.513: INFO: Pod "downwardapi-volume-5eb94e78-a3f9-45a4-8d31-3f2448fb21d6" satisfied condition "Succeeded or Failed"
Dec 17 10:03:57.518: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-5eb94e78-a3f9-45a4-8d31-3f2448fb21d6 container client-container: <nil>
STEP: delete the pod
Dec 17 10:03:57.555: INFO: Waiting for pod downwardapi-volume-5eb94e78-a3f9-45a4-8d31-3f2448fb21d6 to disappear
Dec 17 10:03:57.560: INFO: Pod downwardapi-volume-5eb94e78-a3f9-45a4-8d31-3f2448fb21d6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:03:57.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9053" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":146,"skipped":2641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:03:57.574: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Dec 17 10:04:02.302: INFO: Successfully updated pod "labelsupdateb9415c93-2f6d-444b-92d0-a39ec0370298"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:04:04.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9167" for this suite.

• [SLOW TEST:6.798 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":147,"skipped":2667,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:04:04.372: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7243
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 10:04:04.603: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b1ea7d3-349e-4857-907f-7f48a63af14c" in namespace "downward-api-7243" to be "Succeeded or Failed"
Dec 17 10:04:04.611: INFO: Pod "downwardapi-volume-7b1ea7d3-349e-4857-907f-7f48a63af14c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.337923ms
Dec 17 10:04:06.618: INFO: Pod "downwardapi-volume-7b1ea7d3-349e-4857-907f-7f48a63af14c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014520483s
Dec 17 10:04:08.624: INFO: Pod "downwardapi-volume-7b1ea7d3-349e-4857-907f-7f48a63af14c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020588358s
STEP: Saw pod success
Dec 17 10:04:08.624: INFO: Pod "downwardapi-volume-7b1ea7d3-349e-4857-907f-7f48a63af14c" satisfied condition "Succeeded or Failed"
Dec 17 10:04:08.631: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-1 pod downwardapi-volume-7b1ea7d3-349e-4857-907f-7f48a63af14c container client-container: <nil>
STEP: delete the pod
Dec 17 10:04:08.679: INFO: Waiting for pod downwardapi-volume-7b1ea7d3-349e-4857-907f-7f48a63af14c to disappear
Dec 17 10:04:08.683: INFO: Pod downwardapi-volume-7b1ea7d3-349e-4857-907f-7f48a63af14c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:04:08.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7243" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":148,"skipped":2682,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:04:08.693: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4623
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating all guestbook components
Dec 17 10:04:08.841: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Dec 17 10:04:08.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-4623'
Dec 17 10:04:09.172: INFO: stderr: ""
Dec 17 10:04:09.172: INFO: stdout: "service/agnhost-slave created\n"
Dec 17 10:04:09.172: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Dec 17 10:04:09.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-4623'
Dec 17 10:04:09.509: INFO: stderr: ""
Dec 17 10:04:09.509: INFO: stdout: "service/agnhost-master created\n"
Dec 17 10:04:09.509: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 17 10:04:09.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-4623'
Dec 17 10:04:09.773: INFO: stderr: ""
Dec 17 10:04:09.773: INFO: stdout: "service/frontend created\n"
Dec 17 10:04:09.774: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 17 10:04:09.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-4623'
Dec 17 10:04:09.987: INFO: stderr: ""
Dec 17 10:04:09.987: INFO: stdout: "deployment.apps/frontend created\n"
Dec 17 10:04:09.987: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 17 10:04:09.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-4623'
Dec 17 10:04:10.205: INFO: stderr: ""
Dec 17 10:04:10.205: INFO: stdout: "deployment.apps/agnhost-master created\n"
Dec 17 10:04:10.205: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 17 10:04:10.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-4623'
Dec 17 10:04:10.421: INFO: stderr: ""
Dec 17 10:04:10.421: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Dec 17 10:04:10.421: INFO: Waiting for all frontend pods to be Running.
Dec 17 10:04:15.472: INFO: Waiting for frontend to serve content.
Dec 17 10:04:15.490: INFO: Trying to add a new entry to the guestbook.
Dec 17 10:04:15.503: INFO: Verifying that added entry can be retrieved.
Dec 17 10:04:15.515: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Dec 17 10:04:20.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 delete --grace-period=0 --force -f - --namespace=kubectl-4623'
Dec 17 10:04:20.713: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 10:04:20.713: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 17 10:04:20.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 delete --grace-period=0 --force -f - --namespace=kubectl-4623'
Dec 17 10:04:20.855: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 10:04:20.855: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 17 10:04:20.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 delete --grace-period=0 --force -f - --namespace=kubectl-4623'
Dec 17 10:04:21.013: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 10:04:21.013: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 17 10:04:21.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 delete --grace-period=0 --force -f - --namespace=kubectl-4623'
Dec 17 10:04:21.124: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 10:04:21.124: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 17 10:04:21.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 delete --grace-period=0 --force -f - --namespace=kubectl-4623'
Dec 17 10:04:21.247: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 10:04:21.247: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 17 10:04:21.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 delete --grace-period=0 --force -f - --namespace=kubectl-4623'
Dec 17 10:04:21.371: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 10:04:21.371: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:04:21.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4623" for this suite.

• [SLOW TEST:12.696 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:310
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":277,"completed":149,"skipped":2705,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:04:21.390: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1800
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:04:25.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1800" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":277,"completed":150,"skipped":2728,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:04:25.620: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2042
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 10:04:26.299: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 10:04:28.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796266, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796266, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796266, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796266, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 10:04:31.339: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:04:41.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2042" for this suite.
STEP: Destroying namespace "webhook-2042-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.030 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":277,"completed":151,"skipped":2745,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:04:41.650: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:04:41.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 version'
Dec 17 10:04:41.913: INFO: stderr: ""
Dec 17 10:04:41.913: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.12\", GitCommit:\"7cd5e9086de8ae25d6a1514d0c87bac67ca4a481\", GitTreeState:\"clean\", BuildDate:\"2020-11-12T09:18:55Z\", GoVersion:\"go1.13.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.12\", GitCommit:\"7cd5e9086de8ae25d6a1514d0c87bac67ca4a481\", GitTreeState:\"clean\", BuildDate:\"2020-11-12T09:11:15Z\", GoVersion:\"go1.13.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:04:41.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2000" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":277,"completed":152,"skipped":2747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:04:41.926: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-554
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override arguments
Dec 17 10:04:42.088: INFO: Waiting up to 5m0s for pod "client-containers-0a30cb99-40f8-453b-a1b6-590684710b6d" in namespace "containers-554" to be "Succeeded or Failed"
Dec 17 10:04:42.096: INFO: Pod "client-containers-0a30cb99-40f8-453b-a1b6-590684710b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005336ms
Dec 17 10:04:44.104: INFO: Pod "client-containers-0a30cb99-40f8-453b-a1b6-590684710b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015415066s
Dec 17 10:04:46.110: INFO: Pod "client-containers-0a30cb99-40f8-453b-a1b6-590684710b6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021505807s
STEP: Saw pod success
Dec 17 10:04:46.110: INFO: Pod "client-containers-0a30cb99-40f8-453b-a1b6-590684710b6d" satisfied condition "Succeeded or Failed"
Dec 17 10:04:46.118: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod client-containers-0a30cb99-40f8-453b-a1b6-590684710b6d container test-container: <nil>
STEP: delete the pod
Dec 17 10:04:46.164: INFO: Waiting for pod client-containers-0a30cb99-40f8-453b-a1b6-590684710b6d to disappear
Dec 17 10:04:46.170: INFO: Pod client-containers-0a30cb99-40f8-453b-a1b6-590684710b6d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:04:46.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-554" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":277,"completed":153,"skipped":2769,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:04:46.195: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 17 10:04:54.456: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 17 10:04:54.462: INFO: Pod pod-with-poststart-http-hook still exists
Dec 17 10:04:56.462: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 17 10:04:56.468: INFO: Pod pod-with-poststart-http-hook still exists
Dec 17 10:04:58.462: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 17 10:04:58.469: INFO: Pod pod-with-poststart-http-hook still exists
Dec 17 10:05:00.462: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 17 10:05:00.474: INFO: Pod pod-with-poststart-http-hook still exists
Dec 17 10:05:02.462: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 17 10:05:02.468: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:05:02.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5666" for this suite.

• [SLOW TEST:16.287 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":277,"completed":154,"skipped":2839,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:05:02.483: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
Dec 17 10:05:03.238: INFO: created pod pod-service-account-defaultsa
Dec 17 10:05:03.238: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 17 10:05:03.273: INFO: created pod pod-service-account-mountsa
Dec 17 10:05:03.273: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 17 10:05:03.298: INFO: created pod pod-service-account-nomountsa
Dec 17 10:05:03.298: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 17 10:05:03.323: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 17 10:05:03.323: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 17 10:05:03.342: INFO: created pod pod-service-account-mountsa-mountspec
Dec 17 10:05:03.342: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 17 10:05:03.377: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 17 10:05:03.377: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 17 10:05:03.396: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 17 10:05:03.396: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 17 10:05:03.416: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 17 10:05:03.416: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 17 10:05:03.443: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 17 10:05:03.443: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:05:03.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5810" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":277,"completed":155,"skipped":2849,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:05:03.479: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4136
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 10:05:04.293: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 10:05:06.309: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796304, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796304, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796304, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796304, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 10:05:09.342: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:05:09.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4136" for this suite.
STEP: Destroying namespace "webhook-4136-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.045 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":277,"completed":156,"skipped":2860,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:05:09.525: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Dec 17 10:05:09.687: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 17 10:05:09.709: INFO: Waiting for terminating namespaces to be deleted...
Dec 17 10:05:09.714: INFO: 
Logging pods the kubelet thinks is on node ck8s-conftest-118-workload-cluster-worker-0 before test
Dec 17 10:05:09.726: INFO: sample-webhook-deployment-779fdc84d9-nhjt6 from webhook-4136 started at 2020-12-17 10:05:04 +0000 UTC (1 container statuses recorded)
Dec 17 10:05:09.726: INFO: 	Container sample-webhook ready: true, restart count 0
Dec 17 10:05:09.726: INFO: kube-proxy-87hvk from kube-system started at 2020-12-17 09:10:11 +0000 UTC (1 container statuses recorded)
Dec 17 10:05:09.726: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 10:05:09.726: INFO: sonobuoy from sonobuoy started at 2020-12-17 09:17:49 +0000 UTC (1 container statuses recorded)
Dec 17 10:05:09.726: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 17 10:05:09.726: INFO: pod-handle-http-request from container-lifecycle-hook-5666 started at 2020-12-17 10:04:46 +0000 UTC (1 container statuses recorded)
Dec 17 10:05:09.726: INFO: 	Container pod-handle-http-request ready: true, restart count 0
Dec 17 10:05:09.726: INFO: calico-node-hv857 from kube-system started at 2020-12-17 09:10:11 +0000 UTC (1 container statuses recorded)
Dec 17 10:05:09.726: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 10:05:09.726: INFO: sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-hdzhs from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 10:05:09.726: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 10:05:09.726: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 10:05:09.726: INFO: 
Logging pods the kubelet thinks is on node ck8s-conftest-118-workload-cluster-worker-1 before test
Dec 17 10:05:09.739: INFO: sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-nn8jm from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 10:05:09.740: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 10:05:09.740: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 10:05:09.740: INFO: calico-node-dfdx8 from kube-system started at 2020-12-17 09:10:13 +0000 UTC (1 container statuses recorded)
Dec 17 10:05:09.740: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 10:05:09.740: INFO: kube-proxy-xk6gr from kube-system started at 2020-12-17 09:10:13 +0000 UTC (1 container statuses recorded)
Dec 17 10:05:09.740: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 10:05:09.740: INFO: sonobuoy-e2e-job-c7b0b40574f24e1c from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 10:05:09.740: INFO: 	Container e2e ready: true, restart count 0
Dec 17 10:05:09.740: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-0808f231-b840-4859-9cf2-0485e594d298 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-0808f231-b840-4859-9cf2-0485e594d298 off the node ck8s-conftest-118-workload-cluster-worker-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0808f231-b840-4859-9cf2-0485e594d298
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:05:17.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-779" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:8.381 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":277,"completed":157,"skipped":2867,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:05:17.906: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7960
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name secret-emptykey-test-7860df9b-28e4-41a1-a3f7-9649102a033d
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:05:18.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7960" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":277,"completed":158,"skipped":2890,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:05:18.081: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2375
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Dec 17 10:05:18.252: INFO: Waiting up to 5m0s for pod "downward-api-8237a091-decc-4990-bd42-a0de6b7f21ae" in namespace "downward-api-2375" to be "Succeeded or Failed"
Dec 17 10:05:18.256: INFO: Pod "downward-api-8237a091-decc-4990-bd42-a0de6b7f21ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.593189ms
Dec 17 10:05:20.263: INFO: Pod "downward-api-8237a091-decc-4990-bd42-a0de6b7f21ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011324538s
Dec 17 10:05:22.269: INFO: Pod "downward-api-8237a091-decc-4990-bd42-a0de6b7f21ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017596739s
STEP: Saw pod success
Dec 17 10:05:22.270: INFO: Pod "downward-api-8237a091-decc-4990-bd42-a0de6b7f21ae" satisfied condition "Succeeded or Failed"
Dec 17 10:05:22.276: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downward-api-8237a091-decc-4990-bd42-a0de6b7f21ae container dapi-container: <nil>
STEP: delete the pod
Dec 17 10:05:22.308: INFO: Waiting for pod downward-api-8237a091-decc-4990-bd42-a0de6b7f21ae to disappear
Dec 17 10:05:22.326: INFO: Pod downward-api-8237a091-decc-4990-bd42-a0de6b7f21ae no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:05:22.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2375" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":277,"completed":159,"skipped":2910,"failed":0}

------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:05:22.357: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5717
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-b2e55bc5-f968-4f12-988f-e2c02ae0efdc
STEP: Creating a pod to test consume secrets
Dec 17 10:05:22.541: INFO: Waiting up to 5m0s for pod "pod-secrets-6aca4572-a54e-43e7-a7ea-a49cf07ef219" in namespace "secrets-5717" to be "Succeeded or Failed"
Dec 17 10:05:22.549: INFO: Pod "pod-secrets-6aca4572-a54e-43e7-a7ea-a49cf07ef219": Phase="Pending", Reason="", readiness=false. Elapsed: 7.658584ms
Dec 17 10:05:24.555: INFO: Pod "pod-secrets-6aca4572-a54e-43e7-a7ea-a49cf07ef219": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013873797s
Dec 17 10:05:26.561: INFO: Pod "pod-secrets-6aca4572-a54e-43e7-a7ea-a49cf07ef219": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020045204s
STEP: Saw pod success
Dec 17 10:05:26.561: INFO: Pod "pod-secrets-6aca4572-a54e-43e7-a7ea-a49cf07ef219" satisfied condition "Succeeded or Failed"
Dec 17 10:05:26.566: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-secrets-6aca4572-a54e-43e7-a7ea-a49cf07ef219 container secret-env-test: <nil>
STEP: delete the pod
Dec 17 10:05:26.604: INFO: Waiting for pod pod-secrets-6aca4572-a54e-43e7-a7ea-a49cf07ef219 to disappear
Dec 17 10:05:26.610: INFO: Pod pod-secrets-6aca4572-a54e-43e7-a7ea-a49cf07ef219 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:05:26.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5717" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":277,"completed":160,"skipped":2910,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:05:26.623: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4246
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-ccb4472e-cbfe-4c86-8f95-d146cd285c7f
STEP: Creating a pod to test consume configMaps
Dec 17 10:05:26.813: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c5d13e46-157e-40d0-9f7a-1be6afe84fad" in namespace "projected-4246" to be "Succeeded or Failed"
Dec 17 10:05:26.839: INFO: Pod "pod-projected-configmaps-c5d13e46-157e-40d0-9f7a-1be6afe84fad": Phase="Pending", Reason="", readiness=false. Elapsed: 26.015616ms
Dec 17 10:05:28.845: INFO: Pod "pod-projected-configmaps-c5d13e46-157e-40d0-9f7a-1be6afe84fad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032336926s
Dec 17 10:05:30.853: INFO: Pod "pod-projected-configmaps-c5d13e46-157e-40d0-9f7a-1be6afe84fad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03947785s
STEP: Saw pod success
Dec 17 10:05:30.853: INFO: Pod "pod-projected-configmaps-c5d13e46-157e-40d0-9f7a-1be6afe84fad" satisfied condition "Succeeded or Failed"
Dec 17 10:05:30.858: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-projected-configmaps-c5d13e46-157e-40d0-9f7a-1be6afe84fad container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 10:05:30.891: INFO: Waiting for pod pod-projected-configmaps-c5d13e46-157e-40d0-9f7a-1be6afe84fad to disappear
Dec 17 10:05:30.895: INFO: Pod pod-projected-configmaps-c5d13e46-157e-40d0-9f7a-1be6afe84fad no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:05:30.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4246" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":161,"skipped":2928,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:05:30.913: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2056
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Dec 17 10:05:31.078: INFO: Waiting up to 5m0s for pod "downward-api-db908626-ae5c-4d0d-b0ef-1d153f08c4ff" in namespace "downward-api-2056" to be "Succeeded or Failed"
Dec 17 10:05:31.084: INFO: Pod "downward-api-db908626-ae5c-4d0d-b0ef-1d153f08c4ff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029454ms
Dec 17 10:05:33.090: INFO: Pod "downward-api-db908626-ae5c-4d0d-b0ef-1d153f08c4ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012561178s
Dec 17 10:05:35.097: INFO: Pod "downward-api-db908626-ae5c-4d0d-b0ef-1d153f08c4ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018786238s
STEP: Saw pod success
Dec 17 10:05:35.097: INFO: Pod "downward-api-db908626-ae5c-4d0d-b0ef-1d153f08c4ff" satisfied condition "Succeeded or Failed"
Dec 17 10:05:35.101: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downward-api-db908626-ae5c-4d0d-b0ef-1d153f08c4ff container dapi-container: <nil>
STEP: delete the pod
Dec 17 10:05:35.143: INFO: Waiting for pod downward-api-db908626-ae5c-4d0d-b0ef-1d153f08c4ff to disappear
Dec 17 10:05:35.157: INFO: Pod downward-api-db908626-ae5c-4d0d-b0ef-1d153f08c4ff no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:05:35.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2056" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":277,"completed":162,"skipped":2948,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:05:35.175: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6208
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:05:35.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6208" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":277,"completed":163,"skipped":2961,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:05:35.448: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:157
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:05:35.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3350" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":277,"completed":164,"skipped":2968,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:05:35.661: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1034
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-5dfc4d68-55b6-406d-828b-bbb44bdf3acd in namespace container-probe-1034
Dec 17 10:05:39.852: INFO: Started pod busybox-5dfc4d68-55b6-406d-828b-bbb44bdf3acd in namespace container-probe-1034
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 10:05:39.858: INFO: Initial restart count of pod busybox-5dfc4d68-55b6-406d-828b-bbb44bdf3acd is 0
Dec 17 10:06:26.050: INFO: Restart count of pod container-probe-1034/busybox-5dfc4d68-55b6-406d-828b-bbb44bdf3acd is now 1 (46.192149996s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:06:26.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1034" for this suite.

• [SLOW TEST:50.436 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":165,"skipped":2981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:06:26.099: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6162
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 10:06:26.313: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee281323-bd13-4cda-b4ed-386b76f0a042" in namespace "projected-6162" to be "Succeeded or Failed"
Dec 17 10:06:26.331: INFO: Pod "downwardapi-volume-ee281323-bd13-4cda-b4ed-386b76f0a042": Phase="Pending", Reason="", readiness=false. Elapsed: 17.840771ms
Dec 17 10:06:28.340: INFO: Pod "downwardapi-volume-ee281323-bd13-4cda-b4ed-386b76f0a042": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026915516s
Dec 17 10:06:30.346: INFO: Pod "downwardapi-volume-ee281323-bd13-4cda-b4ed-386b76f0a042": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033447403s
STEP: Saw pod success
Dec 17 10:06:30.346: INFO: Pod "downwardapi-volume-ee281323-bd13-4cda-b4ed-386b76f0a042" satisfied condition "Succeeded or Failed"
Dec 17 10:06:30.352: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-ee281323-bd13-4cda-b4ed-386b76f0a042 container client-container: <nil>
STEP: delete the pod
Dec 17 10:06:30.388: INFO: Waiting for pod downwardapi-volume-ee281323-bd13-4cda-b4ed-386b76f0a042 to disappear
Dec 17 10:06:30.395: INFO: Pod downwardapi-volume-ee281323-bd13-4cda-b4ed-386b76f0a042 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:06:30.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6162" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":166,"skipped":3015,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:06:30.411: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7166
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:06:30.566: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 17 10:06:32.628: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:06:33.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7166" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":277,"completed":167,"skipped":3019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:06:33.674: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7929
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 17 10:06:33.850: INFO: Waiting up to 5m0s for pod "pod-ae2a9ed2-380b-4fd9-8f5d-bcb8280de85d" in namespace "emptydir-7929" to be "Succeeded or Failed"
Dec 17 10:06:33.855: INFO: Pod "pod-ae2a9ed2-380b-4fd9-8f5d-bcb8280de85d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.433542ms
Dec 17 10:06:35.861: INFO: Pod "pod-ae2a9ed2-380b-4fd9-8f5d-bcb8280de85d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010979748s
Dec 17 10:06:37.866: INFO: Pod "pod-ae2a9ed2-380b-4fd9-8f5d-bcb8280de85d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016857816s
STEP: Saw pod success
Dec 17 10:06:37.866: INFO: Pod "pod-ae2a9ed2-380b-4fd9-8f5d-bcb8280de85d" satisfied condition "Succeeded or Failed"
Dec 17 10:06:37.871: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-ae2a9ed2-380b-4fd9-8f5d-bcb8280de85d container test-container: <nil>
STEP: delete the pod
Dec 17 10:06:37.909: INFO: Waiting for pod pod-ae2a9ed2-380b-4fd9-8f5d-bcb8280de85d to disappear
Dec 17 10:06:37.914: INFO: Pod pod-ae2a9ed2-380b-4fd9-8f5d-bcb8280de85d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:06:37.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7929" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":168,"skipped":3050,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:06:37.932: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7027
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 17 10:06:38.122: INFO: Waiting up to 5m0s for pod "pod-db166ef1-f838-4a39-9162-66c2d1b80778" in namespace "emptydir-7027" to be "Succeeded or Failed"
Dec 17 10:06:38.131: INFO: Pod "pod-db166ef1-f838-4a39-9162-66c2d1b80778": Phase="Pending", Reason="", readiness=false. Elapsed: 8.751196ms
Dec 17 10:06:40.136: INFO: Pod "pod-db166ef1-f838-4a39-9162-66c2d1b80778": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01363666s
Dec 17 10:06:42.141: INFO: Pod "pod-db166ef1-f838-4a39-9162-66c2d1b80778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018567005s
STEP: Saw pod success
Dec 17 10:06:42.141: INFO: Pod "pod-db166ef1-f838-4a39-9162-66c2d1b80778" satisfied condition "Succeeded or Failed"
Dec 17 10:06:42.145: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-db166ef1-f838-4a39-9162-66c2d1b80778 container test-container: <nil>
STEP: delete the pod
Dec 17 10:06:42.176: INFO: Waiting for pod pod-db166ef1-f838-4a39-9162-66c2d1b80778 to disappear
Dec 17 10:06:42.180: INFO: Pod pod-db166ef1-f838-4a39-9162-66c2d1b80778 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:06:42.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7027" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":169,"skipped":3058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:06:42.197: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:06:53.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4538" for this suite.

• [SLOW TEST:11.253 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":277,"completed":170,"skipped":3113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:06:53.453: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test env composition
Dec 17 10:06:53.671: INFO: Waiting up to 5m0s for pod "var-expansion-b83ff3cf-5508-4282-b051-4b9423c0dac2" in namespace "var-expansion-7401" to be "Succeeded or Failed"
Dec 17 10:06:53.676: INFO: Pod "var-expansion-b83ff3cf-5508-4282-b051-4b9423c0dac2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.835801ms
Dec 17 10:06:55.685: INFO: Pod "var-expansion-b83ff3cf-5508-4282-b051-4b9423c0dac2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01341667s
Dec 17 10:06:57.692: INFO: Pod "var-expansion-b83ff3cf-5508-4282-b051-4b9423c0dac2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020329356s
STEP: Saw pod success
Dec 17 10:06:57.692: INFO: Pod "var-expansion-b83ff3cf-5508-4282-b051-4b9423c0dac2" satisfied condition "Succeeded or Failed"
Dec 17 10:06:57.696: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod var-expansion-b83ff3cf-5508-4282-b051-4b9423c0dac2 container dapi-container: <nil>
STEP: delete the pod
Dec 17 10:06:57.736: INFO: Waiting for pod var-expansion-b83ff3cf-5508-4282-b051-4b9423c0dac2 to disappear
Dec 17 10:06:57.741: INFO: Pod var-expansion-b83ff3cf-5508-4282-b051-4b9423c0dac2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:06:57.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7401" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":277,"completed":171,"skipped":3155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:06:57.757: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8049
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name projected-secret-test-3643c857-e9b2-47b8-b900-01a0783d6e8c
STEP: Creating a pod to test consume secrets
Dec 17 10:06:57.931: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7fc6e40d-e4fe-412d-befc-74cba1efa858" in namespace "projected-8049" to be "Succeeded or Failed"
Dec 17 10:06:57.951: INFO: Pod "pod-projected-secrets-7fc6e40d-e4fe-412d-befc-74cba1efa858": Phase="Pending", Reason="", readiness=false. Elapsed: 19.353251ms
Dec 17 10:06:59.957: INFO: Pod "pod-projected-secrets-7fc6e40d-e4fe-412d-befc-74cba1efa858": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02518024s
Dec 17 10:07:01.963: INFO: Pod "pod-projected-secrets-7fc6e40d-e4fe-412d-befc-74cba1efa858": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031619135s
STEP: Saw pod success
Dec 17 10:07:01.963: INFO: Pod "pod-projected-secrets-7fc6e40d-e4fe-412d-befc-74cba1efa858" satisfied condition "Succeeded or Failed"
Dec 17 10:07:01.968: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-projected-secrets-7fc6e40d-e4fe-412d-befc-74cba1efa858 container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 10:07:02.007: INFO: Waiting for pod pod-projected-secrets-7fc6e40d-e4fe-412d-befc-74cba1efa858 to disappear
Dec 17 10:07:02.012: INFO: Pod pod-projected-secrets-7fc6e40d-e4fe-412d-befc-74cba1efa858 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:07:02.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8049" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":172,"skipped":3184,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:07:02.033: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7515
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-793a2cf8-163c-4a88-a234-148139b6a4b7
STEP: Creating secret with name s-test-opt-upd-60de4523-c232-4ae5-b1d8-a9c62c7b354a
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-793a2cf8-163c-4a88-a234-148139b6a4b7
STEP: Updating secret s-test-opt-upd-60de4523-c232-4ae5-b1d8-a9c62c7b354a
STEP: Creating secret with name s-test-opt-create-03776f3c-89a7-4ae0-9244-c251658750aa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:07:10.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7515" for this suite.

• [SLOW TEST:8.422 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":173,"skipped":3189,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:07:10.456: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl replace
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 17 10:07:10.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5122'
Dec 17 10:07:10.973: INFO: stderr: ""
Dec 17 10:07:10.973: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 17 10:07:16.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pod e2e-test-httpd-pod --namespace=kubectl-5122 -o json'
Dec 17 10:07:16.133: INFO: stderr: ""
Dec 17 10:07:16.133: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.137.32/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.137.32/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-12-17T10:07:10Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-17T10:07:10Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-17T10:07:12Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"192.168.137.32\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-12-17T10:07:13Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5122\",\n        \"resourceVersion\": \"22326\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5122/pods/e2e-test-httpd-pod\",\n        \"uid\": \"6171f34f-e170-43c8-9519-f29440c87d90\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5c5b6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ck8s-conftest-118-workload-cluster-worker-1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5c5b6\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5c5b6\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-17T10:07:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-17T10:07:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-17T10:07:13Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-12-17T10:07:10Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://44cc5a859d1b755037f35d917427b975c9400ccd30becea775acee6fa352fe8a\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-12-17T10:07:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.0.10.72\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.137.32\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.137.32\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-12-17T10:07:10Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 17 10:07:16.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 replace -f - --namespace=kubectl-5122'
Dec 17 10:07:16.394: INFO: stderr: ""
Dec 17 10:07:16.394: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Dec 17 10:07:16.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 delete pods e2e-test-httpd-pod --namespace=kubectl-5122'
Dec 17 10:07:23.215: INFO: stderr: ""
Dec 17 10:07:23.215: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:07:23.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5122" for this suite.

• [SLOW TEST:12.778 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":277,"completed":174,"skipped":3196,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:07:23.234: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6366
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:07:23.391: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:07:24.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6366" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":277,"completed":175,"skipped":3213,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:07:24.086: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:07:32.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9317" for this suite.

• [SLOW TEST:8.203 seconds]
[sig-apps] Job
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":277,"completed":176,"skipped":3221,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:07:32.289: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6706
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-14424c64-08f9-4e82-86e9-21e31ef9c994
STEP: Creating a pod to test consume configMaps
Dec 17 10:07:32.515: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-171d164e-b763-470c-b9d6-b9c4ae91d6de" in namespace "projected-6706" to be "Succeeded or Failed"
Dec 17 10:07:32.522: INFO: Pod "pod-projected-configmaps-171d164e-b763-470c-b9d6-b9c4ae91d6de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.436967ms
Dec 17 10:07:34.559: INFO: Pod "pod-projected-configmaps-171d164e-b763-470c-b9d6-b9c4ae91d6de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043685377s
STEP: Saw pod success
Dec 17 10:07:34.559: INFO: Pod "pod-projected-configmaps-171d164e-b763-470c-b9d6-b9c4ae91d6de" satisfied condition "Succeeded or Failed"
Dec 17 10:07:34.565: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-projected-configmaps-171d164e-b763-470c-b9d6-b9c4ae91d6de container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 10:07:34.610: INFO: Waiting for pod pod-projected-configmaps-171d164e-b763-470c-b9d6-b9c4ae91d6de to disappear
Dec 17 10:07:34.616: INFO: Pod pod-projected-configmaps-171d164e-b763-470c-b9d6-b9c4ae91d6de no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:07:34.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6706" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":177,"skipped":3241,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:07:34.636: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3442 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3442;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3442 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3442;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3442.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3442.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3442.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3442.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3442.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3442.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3442.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3442.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3442.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3442.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3442.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 204.139.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.139.204_udp@PTR;check="$$(dig +tcp +noall +answer +search 204.139.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.139.204_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3442 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3442;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3442 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3442;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3442.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3442.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3442.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3442.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3442.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3442.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3442.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3442.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3442.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3442.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3442.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3442.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 204.139.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.139.204_udp@PTR;check="$$(dig +tcp +noall +answer +search 204.139.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.139.204_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 10:07:38.946: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:38.952: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:38.957: INFO: Unable to read wheezy_udp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:38.963: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:38.970: INFO: Unable to read wheezy_udp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:38.976: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:38.982: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:38.987: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:39.045: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:39.050: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:39.057: INFO: Unable to read jessie_udp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:39.064: INFO: Unable to read jessie_tcp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:39.070: INFO: Unable to read jessie_udp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:39.076: INFO: Unable to read jessie_tcp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:39.081: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:39.088: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:39.125: INFO: Lookups using dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3442 wheezy_tcp@dns-test-service.dns-3442 wheezy_udp@dns-test-service.dns-3442.svc wheezy_tcp@dns-test-service.dns-3442.svc wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3442 jessie_tcp@dns-test-service.dns-3442 jessie_udp@dns-test-service.dns-3442.svc jessie_tcp@dns-test-service.dns-3442.svc jessie_udp@_http._tcp.dns-test-service.dns-3442.svc jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc]

Dec 17 10:07:44.135: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.141: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.147: INFO: Unable to read wheezy_udp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.152: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.158: INFO: Unable to read wheezy_udp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.166: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.170: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.176: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.220: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.227: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.234: INFO: Unable to read jessie_udp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.241: INFO: Unable to read jessie_tcp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.247: INFO: Unable to read jessie_udp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.253: INFO: Unable to read jessie_tcp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.259: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.265: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:44.304: INFO: Lookups using dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3442 wheezy_tcp@dns-test-service.dns-3442 wheezy_udp@dns-test-service.dns-3442.svc wheezy_tcp@dns-test-service.dns-3442.svc wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3442 jessie_tcp@dns-test-service.dns-3442 jessie_udp@dns-test-service.dns-3442.svc jessie_tcp@dns-test-service.dns-3442.svc jessie_udp@_http._tcp.dns-test-service.dns-3442.svc jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc]

Dec 17 10:07:49.133: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.139: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.145: INFO: Unable to read wheezy_udp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.151: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.156: INFO: Unable to read wheezy_udp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.162: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.171: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.182: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.224: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.231: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.237: INFO: Unable to read jessie_udp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.244: INFO: Unable to read jessie_tcp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.251: INFO: Unable to read jessie_udp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.258: INFO: Unable to read jessie_tcp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.275: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.281: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:49.318: INFO: Lookups using dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3442 wheezy_tcp@dns-test-service.dns-3442 wheezy_udp@dns-test-service.dns-3442.svc wheezy_tcp@dns-test-service.dns-3442.svc wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3442 jessie_tcp@dns-test-service.dns-3442 jessie_udp@dns-test-service.dns-3442.svc jessie_tcp@dns-test-service.dns-3442.svc jessie_udp@_http._tcp.dns-test-service.dns-3442.svc jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc]

Dec 17 10:07:54.132: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.139: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.145: INFO: Unable to read wheezy_udp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.151: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.165: INFO: Unable to read wheezy_udp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.172: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.178: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.184: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.227: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.234: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.241: INFO: Unable to read jessie_udp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.247: INFO: Unable to read jessie_tcp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.265: INFO: Unable to read jessie_udp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.270: INFO: Unable to read jessie_tcp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.275: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.280: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:54.309: INFO: Lookups using dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3442 wheezy_tcp@dns-test-service.dns-3442 wheezy_udp@dns-test-service.dns-3442.svc wheezy_tcp@dns-test-service.dns-3442.svc wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3442 jessie_tcp@dns-test-service.dns-3442 jessie_udp@dns-test-service.dns-3442.svc jessie_tcp@dns-test-service.dns-3442.svc jessie_udp@_http._tcp.dns-test-service.dns-3442.svc jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc]

Dec 17 10:07:59.138: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.146: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.152: INFO: Unable to read wheezy_udp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.157: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.163: INFO: Unable to read wheezy_udp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.167: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.174: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.179: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.219: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.224: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.229: INFO: Unable to read jessie_udp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.234: INFO: Unable to read jessie_tcp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.239: INFO: Unable to read jessie_udp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.245: INFO: Unable to read jessie_tcp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.251: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.257: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:07:59.305: INFO: Lookups using dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3442 wheezy_tcp@dns-test-service.dns-3442 wheezy_udp@dns-test-service.dns-3442.svc wheezy_tcp@dns-test-service.dns-3442.svc wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3442 jessie_tcp@dns-test-service.dns-3442 jessie_udp@dns-test-service.dns-3442.svc jessie_tcp@dns-test-service.dns-3442.svc jessie_udp@_http._tcp.dns-test-service.dns-3442.svc jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc]

Dec 17 10:08:04.132: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.140: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.146: INFO: Unable to read wheezy_udp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.151: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.156: INFO: Unable to read wheezy_udp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.160: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.165: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.170: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.214: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.220: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.225: INFO: Unable to read jessie_udp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.230: INFO: Unable to read jessie_tcp@dns-test-service.dns-3442 from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.234: INFO: Unable to read jessie_udp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.239: INFO: Unable to read jessie_tcp@dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.244: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.250: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc from pod dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469: the server could not find the requested resource (get pods dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469)
Dec 17 10:08:04.287: INFO: Lookups using dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3442 wheezy_tcp@dns-test-service.dns-3442 wheezy_udp@dns-test-service.dns-3442.svc wheezy_tcp@dns-test-service.dns-3442.svc wheezy_udp@_http._tcp.dns-test-service.dns-3442.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3442.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3442 jessie_tcp@dns-test-service.dns-3442 jessie_udp@dns-test-service.dns-3442.svc jessie_tcp@dns-test-service.dns-3442.svc jessie_udp@_http._tcp.dns-test-service.dns-3442.svc jessie_tcp@_http._tcp.dns-test-service.dns-3442.svc]

Dec 17 10:08:09.297: INFO: DNS probes using dns-3442/dns-test-58b6148d-6dfc-45cb-8a1e-e1cf86c75469 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:08:09.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3442" for this suite.

• [SLOW TEST:34.811 seconds]
[sig-network] DNS
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":277,"completed":178,"skipped":3248,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:08:09.448: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5320
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating secret secrets-5320/secret-test-95e6660c-5104-45a0-8f64-54fcb50a8141
STEP: Creating a pod to test consume secrets
Dec 17 10:08:09.621: INFO: Waiting up to 5m0s for pod "pod-configmaps-d91dee71-8785-4985-bd69-1d6616170c78" in namespace "secrets-5320" to be "Succeeded or Failed"
Dec 17 10:08:09.626: INFO: Pod "pod-configmaps-d91dee71-8785-4985-bd69-1d6616170c78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.900046ms
Dec 17 10:08:11.634: INFO: Pod "pod-configmaps-d91dee71-8785-4985-bd69-1d6616170c78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012463161s
Dec 17 10:08:13.642: INFO: Pod "pod-configmaps-d91dee71-8785-4985-bd69-1d6616170c78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020496753s
STEP: Saw pod success
Dec 17 10:08:13.642: INFO: Pod "pod-configmaps-d91dee71-8785-4985-bd69-1d6616170c78" satisfied condition "Succeeded or Failed"
Dec 17 10:08:13.647: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-configmaps-d91dee71-8785-4985-bd69-1d6616170c78 container env-test: <nil>
STEP: delete the pod
Dec 17 10:08:13.681: INFO: Waiting for pod pod-configmaps-d91dee71-8785-4985-bd69-1d6616170c78 to disappear
Dec 17 10:08:13.685: INFO: Pod pod-configmaps-d91dee71-8785-4985-bd69-1d6616170c78 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:08:13.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5320" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":179,"skipped":3261,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:08:13.696: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9728
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 17 10:08:13.864: INFO: Waiting up to 5m0s for pod "pod-60bbaec4-aa98-4a3b-a79e-45b0dcf7fb15" in namespace "emptydir-9728" to be "Succeeded or Failed"
Dec 17 10:08:13.873: INFO: Pod "pod-60bbaec4-aa98-4a3b-a79e-45b0dcf7fb15": Phase="Pending", Reason="", readiness=false. Elapsed: 9.211926ms
Dec 17 10:08:15.879: INFO: Pod "pod-60bbaec4-aa98-4a3b-a79e-45b0dcf7fb15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014926187s
Dec 17 10:08:17.886: INFO: Pod "pod-60bbaec4-aa98-4a3b-a79e-45b0dcf7fb15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02152433s
STEP: Saw pod success
Dec 17 10:08:17.886: INFO: Pod "pod-60bbaec4-aa98-4a3b-a79e-45b0dcf7fb15" satisfied condition "Succeeded or Failed"
Dec 17 10:08:17.892: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-60bbaec4-aa98-4a3b-a79e-45b0dcf7fb15 container test-container: <nil>
STEP: delete the pod
Dec 17 10:08:17.929: INFO: Waiting for pod pod-60bbaec4-aa98-4a3b-a79e-45b0dcf7fb15 to disappear
Dec 17 10:08:17.935: INFO: Pod pod-60bbaec4-aa98-4a3b-a79e-45b0dcf7fb15 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:08:17.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9728" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":180,"skipped":3264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:08:17.959: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:08:18.156: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 17 10:08:18.173: INFO: Number of nodes with available pods: 0
Dec 17 10:08:18.173: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 17 10:08:18.223: INFO: Number of nodes with available pods: 0
Dec 17 10:08:18.223: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:19.231: INFO: Number of nodes with available pods: 0
Dec 17 10:08:19.231: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:20.231: INFO: Number of nodes with available pods: 0
Dec 17 10:08:20.231: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:21.230: INFO: Number of nodes with available pods: 1
Dec 17 10:08:21.230: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 17 10:08:21.260: INFO: Number of nodes with available pods: 1
Dec 17 10:08:21.260: INFO: Number of running nodes: 0, number of available pods: 1
Dec 17 10:08:22.266: INFO: Number of nodes with available pods: 0
Dec 17 10:08:22.266: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 17 10:08:22.296: INFO: Number of nodes with available pods: 0
Dec 17 10:08:22.296: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:23.302: INFO: Number of nodes with available pods: 0
Dec 17 10:08:23.302: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:24.302: INFO: Number of nodes with available pods: 0
Dec 17 10:08:24.302: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:25.303: INFO: Number of nodes with available pods: 0
Dec 17 10:08:25.303: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:26.302: INFO: Number of nodes with available pods: 0
Dec 17 10:08:26.302: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:27.302: INFO: Number of nodes with available pods: 0
Dec 17 10:08:27.302: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:28.301: INFO: Number of nodes with available pods: 0
Dec 17 10:08:28.301: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:29.301: INFO: Number of nodes with available pods: 0
Dec 17 10:08:29.301: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:30.302: INFO: Number of nodes with available pods: 0
Dec 17 10:08:30.302: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:31.304: INFO: Number of nodes with available pods: 0
Dec 17 10:08:31.304: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:32.301: INFO: Number of nodes with available pods: 0
Dec 17 10:08:32.301: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:33.303: INFO: Number of nodes with available pods: 0
Dec 17 10:08:33.303: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:34.302: INFO: Number of nodes with available pods: 0
Dec 17 10:08:34.302: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:35.308: INFO: Number of nodes with available pods: 0
Dec 17 10:08:35.308: INFO: Node ck8s-conftest-118-workload-cluster-worker-1 is running more than one daemon pod
Dec 17 10:08:36.301: INFO: Number of nodes with available pods: 1
Dec 17 10:08:36.301: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8826, will wait for the garbage collector to delete the pods
Dec 17 10:08:36.379: INFO: Deleting DaemonSet.extensions daemon-set took: 16.125671ms
Dec 17 10:08:36.780: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.542678ms
Dec 17 10:08:39.486: INFO: Number of nodes with available pods: 0
Dec 17 10:08:39.486: INFO: Number of running nodes: 0, number of available pods: 0
Dec 17 10:08:39.491: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8826/daemonsets","resourceVersion":"22995"},"items":null}

Dec 17 10:08:39.495: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8826/pods","resourceVersion":"22995"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:08:39.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8826" for this suite.

• [SLOW TEST:21.576 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":277,"completed":181,"skipped":3295,"failed":0}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:08:39.536: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Dec 17 10:08:39.741: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:08:43.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4889" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":277,"completed":182,"skipped":3301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:08:43.105: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9609
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-258006ee-2139-4498-83ed-02bb1cde28f9
STEP: Creating secret with name s-test-opt-upd-38583b68-a77a-4a35-83f0-fd0126ac4eeb
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-258006ee-2139-4498-83ed-02bb1cde28f9
STEP: Updating secret s-test-opt-upd-38583b68-a77a-4a35-83f0-fd0126ac4eeb
STEP: Creating secret with name s-test-opt-create-62929b23-bb3e-4897-beae-a45a2fea8958
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:08:51.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9609" for this suite.

• [SLOW TEST:8.360 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":183,"skipped":3336,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:08:51.466: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Dec 17 10:08:56.226: INFO: Successfully updated pod "annotationupdateb864b825-0cb4-4327-9908-c0949be90885"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:08:58.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4325" for this suite.

• [SLOW TEST:6.797 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":184,"skipped":3354,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:08:58.263: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Dec 17 10:08:58.422: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 17 10:08:58.445: INFO: Waiting for terminating namespaces to be deleted...
Dec 17 10:08:58.449: INFO: 
Logging pods the kubelet thinks is on node ck8s-conftest-118-workload-cluster-worker-0 before test
Dec 17 10:08:58.461: INFO: kube-proxy-87hvk from kube-system started at 2020-12-17 09:10:11 +0000 UTC (1 container statuses recorded)
Dec 17 10:08:58.461: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 10:08:58.461: INFO: sonobuoy from sonobuoy started at 2020-12-17 09:17:49 +0000 UTC (1 container statuses recorded)
Dec 17 10:08:58.461: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 17 10:08:58.461: INFO: calico-node-hv857 from kube-system started at 2020-12-17 09:10:11 +0000 UTC (1 container statuses recorded)
Dec 17 10:08:58.461: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 10:08:58.461: INFO: pod-secrets-247d7c80-6652-4558-87e3-72fbbffdf257 from secrets-9609 started at 2020-12-17 10:08:43 +0000 UTC (3 container statuses recorded)
Dec 17 10:08:58.461: INFO: 	Container creates-volume-test ready: false, restart count 0
Dec 17 10:08:58.461: INFO: 	Container dels-volume-test ready: false, restart count 0
Dec 17 10:08:58.461: INFO: 	Container upds-volume-test ready: false, restart count 0
Dec 17 10:08:58.461: INFO: sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-hdzhs from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 10:08:58.461: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 10:08:58.461: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 10:08:58.461: INFO: 
Logging pods the kubelet thinks is on node ck8s-conftest-118-workload-cluster-worker-1 before test
Dec 17 10:08:58.471: INFO: kube-proxy-xk6gr from kube-system started at 2020-12-17 09:10:13 +0000 UTC (1 container statuses recorded)
Dec 17 10:08:58.471: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 10:08:58.471: INFO: sonobuoy-e2e-job-c7b0b40574f24e1c from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 10:08:58.471: INFO: 	Container e2e ready: true, restart count 0
Dec 17 10:08:58.471: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 10:08:58.471: INFO: sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-nn8jm from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 10:08:58.471: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 10:08:58.471: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 10:08:58.471: INFO: annotationupdateb864b825-0cb4-4327-9908-c0949be90885 from projected-4325 started at 2020-12-17 10:08:51 +0000 UTC (1 container statuses recorded)
Dec 17 10:08:58.471: INFO: 	Container client-container ready: true, restart count 0
Dec 17 10:08:58.471: INFO: calico-node-dfdx8 from kube-system started at 2020-12-17 09:10:13 +0000 UTC (1 container statuses recorded)
Dec 17 10:08:58.471: INFO: 	Container calico-node ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4fe376ff-b53d-4899-acea-5f7663b6f7ac 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-4fe376ff-b53d-4899-acea-5f7663b6f7ac off the node ck8s-conftest-118-workload-cluster-worker-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4fe376ff-b53d-4899-acea-5f7663b6f7ac
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:09:14.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3031" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:16.417 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":277,"completed":185,"skipped":3359,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:09:14.680: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-71
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating pod
Dec 17 10:09:18.911: INFO: Pod pod-hostip-b98b5f8e-eb3f-449a-a360-9282d1e4ac11 has hostIP: 172.0.10.72
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:09:18.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-71" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":277,"completed":186,"skipped":3368,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:09:18.923: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-4577
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 17 10:09:25.156: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4577 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:09:25.156: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:09:25.328: INFO: Exec stderr: ""
Dec 17 10:09:25.328: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4577 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:09:25.328: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:09:25.516: INFO: Exec stderr: ""
Dec 17 10:09:25.516: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4577 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:09:25.516: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:09:25.689: INFO: Exec stderr: ""
Dec 17 10:09:25.690: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4577 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:09:25.690: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:09:25.877: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 17 10:09:25.877: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4577 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:09:25.877: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:09:26.057: INFO: Exec stderr: ""
Dec 17 10:09:26.057: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4577 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:09:26.057: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:09:26.219: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 17 10:09:26.219: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4577 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:09:26.219: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:09:26.378: INFO: Exec stderr: ""
Dec 17 10:09:26.378: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4577 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:09:26.378: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:09:26.543: INFO: Exec stderr: ""
Dec 17 10:09:26.543: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4577 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:09:26.543: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:09:26.699: INFO: Exec stderr: ""
Dec 17 10:09:26.699: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4577 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:09:26.699: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:09:26.866: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:09:26.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4577" for this suite.

• [SLOW TEST:7.962 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":187,"skipped":3376,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:09:26.885: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2436
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2436
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2436
STEP: creating replication controller externalsvc in namespace services-2436
I1217 10:09:27.175586      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-2436, replica count: 2
I1217 10:09:30.232301      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 17 10:09:30.262: INFO: Creating new exec pod
Dec 17 10:09:34.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=services-2436 execpod2cjv4 -- /bin/sh -x -c nslookup clusterip-service'
Dec 17 10:09:34.641: INFO: stderr: "+ nslookup clusterip-service\n"
Dec 17 10:09:34.642: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-2436.svc.cluster.local\tcanonical name = externalsvc.services-2436.svc.cluster.local.\nName:\texternalsvc.services-2436.svc.cluster.local\nAddress: 10.107.69.195\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2436, will wait for the garbage collector to delete the pods
Dec 17 10:09:34.744: INFO: Deleting ReplicationController externalsvc took: 46.164818ms
Dec 17 10:09:35.145: INFO: Terminating ReplicationController externalsvc pods took: 400.518995ms
Dec 17 10:09:39.800: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:09:39.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2436" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:12.962 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":277,"completed":188,"skipped":3383,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:09:39.848: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override command
Dec 17 10:09:40.017: INFO: Waiting up to 5m0s for pod "client-containers-b897d05f-781a-490d-a363-caaa404aaa94" in namespace "containers-9795" to be "Succeeded or Failed"
Dec 17 10:09:40.023: INFO: Pod "client-containers-b897d05f-781a-490d-a363-caaa404aaa94": Phase="Pending", Reason="", readiness=false. Elapsed: 6.410475ms
Dec 17 10:09:42.029: INFO: Pod "client-containers-b897d05f-781a-490d-a363-caaa404aaa94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011866998s
Dec 17 10:09:44.045: INFO: Pod "client-containers-b897d05f-781a-490d-a363-caaa404aaa94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027690661s
STEP: Saw pod success
Dec 17 10:09:44.045: INFO: Pod "client-containers-b897d05f-781a-490d-a363-caaa404aaa94" satisfied condition "Succeeded or Failed"
Dec 17 10:09:44.050: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod client-containers-b897d05f-781a-490d-a363-caaa404aaa94 container test-container: <nil>
STEP: delete the pod
Dec 17 10:09:44.095: INFO: Waiting for pod client-containers-b897d05f-781a-490d-a363-caaa404aaa94 to disappear
Dec 17 10:09:44.099: INFO: Pod client-containers-b897d05f-781a-490d-a363-caaa404aaa94 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:09:44.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9795" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":277,"completed":189,"skipped":3388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:09:44.117: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 10:09:44.286: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8e0dffa7-7a08-4213-964e-e74b1353f21d" in namespace "projected-5228" to be "Succeeded or Failed"
Dec 17 10:09:44.294: INFO: Pod "downwardapi-volume-8e0dffa7-7a08-4213-964e-e74b1353f21d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.367304ms
Dec 17 10:09:46.302: INFO: Pod "downwardapi-volume-8e0dffa7-7a08-4213-964e-e74b1353f21d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015990178s
Dec 17 10:09:48.308: INFO: Pod "downwardapi-volume-8e0dffa7-7a08-4213-964e-e74b1353f21d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022287388s
STEP: Saw pod success
Dec 17 10:09:48.308: INFO: Pod "downwardapi-volume-8e0dffa7-7a08-4213-964e-e74b1353f21d" satisfied condition "Succeeded or Failed"
Dec 17 10:09:48.313: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-8e0dffa7-7a08-4213-964e-e74b1353f21d container client-container: <nil>
STEP: delete the pod
Dec 17 10:09:48.345: INFO: Waiting for pod downwardapi-volume-8e0dffa7-7a08-4213-964e-e74b1353f21d to disappear
Dec 17 10:09:48.349: INFO: Pod downwardapi-volume-8e0dffa7-7a08-4213-964e-e74b1353f21d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:09:48.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5228" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":190,"skipped":3414,"failed":0}

------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:09:48.363: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:09:48.535: INFO: (0) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.797913ms)
Dec 17 10:09:48.541: INFO: (1) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.674733ms)
Dec 17 10:09:48.547: INFO: (2) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.81404ms)
Dec 17 10:09:48.554: INFO: (3) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.435625ms)
Dec 17 10:09:48.560: INFO: (4) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.04516ms)
Dec 17 10:09:48.566: INFO: (5) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.966692ms)
Dec 17 10:09:48.572: INFO: (6) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.103171ms)
Dec 17 10:09:48.578: INFO: (7) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.874014ms)
Dec 17 10:09:48.585: INFO: (8) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.970842ms)
Dec 17 10:09:48.591: INFO: (9) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.135124ms)
Dec 17 10:09:48.596: INFO: (10) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.12524ms)
Dec 17 10:09:48.603: INFO: (11) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.267235ms)
Dec 17 10:09:48.610: INFO: (12) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.605282ms)
Dec 17 10:09:48.617: INFO: (13) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.46951ms)
Dec 17 10:09:48.623: INFO: (14) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.391084ms)
Dec 17 10:09:48.630: INFO: (15) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.316203ms)
Dec 17 10:09:48.635: INFO: (16) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.223128ms)
Dec 17 10:09:48.641: INFO: (17) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.037122ms)
Dec 17 10:09:48.648: INFO: (18) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.601447ms)
Dec 17 10:09:48.653: INFO: (19) /api/v1/nodes/ck8s-conftest-118-workload-cluster-worker-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.306064ms)
[AfterEach] version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:09:48.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6196" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":277,"completed":191,"skipped":3414,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:09:48.679: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 17 10:09:48.853: INFO: Waiting up to 5m0s for pod "pod-6d0251ba-d268-4d1b-9418-f8b24578beff" in namespace "emptydir-511" to be "Succeeded or Failed"
Dec 17 10:09:48.860: INFO: Pod "pod-6d0251ba-d268-4d1b-9418-f8b24578beff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.619562ms
Dec 17 10:09:50.869: INFO: Pod "pod-6d0251ba-d268-4d1b-9418-f8b24578beff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01592646s
Dec 17 10:09:52.881: INFO: Pod "pod-6d0251ba-d268-4d1b-9418-f8b24578beff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027764414s
STEP: Saw pod success
Dec 17 10:09:52.881: INFO: Pod "pod-6d0251ba-d268-4d1b-9418-f8b24578beff" satisfied condition "Succeeded or Failed"
Dec 17 10:09:52.886: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-6d0251ba-d268-4d1b-9418-f8b24578beff container test-container: <nil>
STEP: delete the pod
Dec 17 10:09:52.912: INFO: Waiting for pod pod-6d0251ba-d268-4d1b-9418-f8b24578beff to disappear
Dec 17 10:09:52.917: INFO: Pod pod-6d0251ba-d268-4d1b-9418-f8b24578beff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:09:52.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-511" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":192,"skipped":3441,"failed":0}

------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:09:52.931: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3434
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-3434/configmap-test-c798c697-8416-42af-80a8-f0363eff3705
STEP: Creating a pod to test consume configMaps
Dec 17 10:09:53.102: INFO: Waiting up to 5m0s for pod "pod-configmaps-c951fcee-78f4-4057-a925-f349d2ac92ea" in namespace "configmap-3434" to be "Succeeded or Failed"
Dec 17 10:09:53.107: INFO: Pod "pod-configmaps-c951fcee-78f4-4057-a925-f349d2ac92ea": Phase="Pending", Reason="", readiness=false. Elapsed: 5.03619ms
Dec 17 10:09:55.113: INFO: Pod "pod-configmaps-c951fcee-78f4-4057-a925-f349d2ac92ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011370884s
Dec 17 10:09:57.119: INFO: Pod "pod-configmaps-c951fcee-78f4-4057-a925-f349d2ac92ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017273181s
STEP: Saw pod success
Dec 17 10:09:57.119: INFO: Pod "pod-configmaps-c951fcee-78f4-4057-a925-f349d2ac92ea" satisfied condition "Succeeded or Failed"
Dec 17 10:09:57.124: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-configmaps-c951fcee-78f4-4057-a925-f349d2ac92ea container env-test: <nil>
STEP: delete the pod
Dec 17 10:09:57.162: INFO: Waiting for pod pod-configmaps-c951fcee-78f4-4057-a925-f349d2ac92ea to disappear
Dec 17 10:09:57.166: INFO: Pod pod-configmaps-c951fcee-78f4-4057-a925-f349d2ac92ea no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:09:57.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3434" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":193,"skipped":3441,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:09:57.383: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1116
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-555
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7915
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:10:03.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1116" for this suite.
STEP: Destroying namespace "nsdeletetest-555" for this suite.
Dec 17 10:10:03.989: INFO: Namespace nsdeletetest-555 was already deleted
STEP: Destroying namespace "nsdeletetest-7915" for this suite.

• [SLOW TEST:6.614 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":277,"completed":194,"skipped":3518,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:10:03.997: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2847
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 10:10:04.171: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aefe6c5d-5d6a-413d-90d6-dba4f945b962" in namespace "projected-2847" to be "Succeeded or Failed"
Dec 17 10:10:04.186: INFO: Pod "downwardapi-volume-aefe6c5d-5d6a-413d-90d6-dba4f945b962": Phase="Pending", Reason="", readiness=false. Elapsed: 14.212297ms
Dec 17 10:10:06.201: INFO: Pod "downwardapi-volume-aefe6c5d-5d6a-413d-90d6-dba4f945b962": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029755618s
Dec 17 10:10:08.207: INFO: Pod "downwardapi-volume-aefe6c5d-5d6a-413d-90d6-dba4f945b962": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035553925s
STEP: Saw pod success
Dec 17 10:10:08.207: INFO: Pod "downwardapi-volume-aefe6c5d-5d6a-413d-90d6-dba4f945b962" satisfied condition "Succeeded or Failed"
Dec 17 10:10:08.213: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-1 pod downwardapi-volume-aefe6c5d-5d6a-413d-90d6-dba4f945b962 container client-container: <nil>
STEP: delete the pod
Dec 17 10:10:08.250: INFO: Waiting for pod downwardapi-volume-aefe6c5d-5d6a-413d-90d6-dba4f945b962 to disappear
Dec 17 10:10:08.255: INFO: Pod downwardapi-volume-aefe6c5d-5d6a-413d-90d6-dba4f945b962 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:10:08.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2847" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":195,"skipped":3527,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:10:08.270: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3056
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-f245da41-cdf6-446c-a047-ce0a1d064f2f
STEP: Creating configMap with name cm-test-opt-upd-e629c98e-e539-48dd-bf06-0bbdc47403e5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f245da41-cdf6-446c-a047-ce0a1d064f2f
STEP: Updating configmap cm-test-opt-upd-e629c98e-e539-48dd-bf06-0bbdc47403e5
STEP: Creating configMap with name cm-test-opt-create-c80d7a16-55b6-472d-90da-3f68751e3902
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:10:16.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3056" for this suite.

• [SLOW TEST:8.337 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":196,"skipped":3528,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:10:16.608: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-89bc12a1-9a21-4903-bfad-a54dbf9a5562
STEP: Creating a pod to test consume configMaps
Dec 17 10:10:16.800: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6e74997b-11bd-44fa-84a6-2cf7536c38d4" in namespace "projected-1967" to be "Succeeded or Failed"
Dec 17 10:10:16.809: INFO: Pod "pod-projected-configmaps-6e74997b-11bd-44fa-84a6-2cf7536c38d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.732753ms
Dec 17 10:10:18.814: INFO: Pod "pod-projected-configmaps-6e74997b-11bd-44fa-84a6-2cf7536c38d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013931098s
Dec 17 10:10:20.819: INFO: Pod "pod-projected-configmaps-6e74997b-11bd-44fa-84a6-2cf7536c38d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018516129s
STEP: Saw pod success
Dec 17 10:10:20.819: INFO: Pod "pod-projected-configmaps-6e74997b-11bd-44fa-84a6-2cf7536c38d4" satisfied condition "Succeeded or Failed"
Dec 17 10:10:20.823: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-projected-configmaps-6e74997b-11bd-44fa-84a6-2cf7536c38d4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 10:10:20.853: INFO: Waiting for pod pod-projected-configmaps-6e74997b-11bd-44fa-84a6-2cf7536c38d4 to disappear
Dec 17 10:10:20.857: INFO: Pod pod-projected-configmaps-6e74997b-11bd-44fa-84a6-2cf7536c38d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:10:20.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1967" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":197,"skipped":3537,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:10:20.872: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3542
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:10:21.024: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 17 10:10:24.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-3542 create -f -'
Dec 17 10:10:25.196: INFO: stderr: ""
Dec 17 10:10:25.196: INFO: stdout: "e2e-test-crd-publish-openapi-1624-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 17 10:10:25.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-3542 delete e2e-test-crd-publish-openapi-1624-crds test-cr'
Dec 17 10:10:25.322: INFO: stderr: ""
Dec 17 10:10:25.322: INFO: stdout: "e2e-test-crd-publish-openapi-1624-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 17 10:10:25.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-3542 apply -f -'
Dec 17 10:10:25.551: INFO: stderr: ""
Dec 17 10:10:25.551: INFO: stdout: "e2e-test-crd-publish-openapi-1624-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 17 10:10:25.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-3542 delete e2e-test-crd-publish-openapi-1624-crds test-cr'
Dec 17 10:10:25.659: INFO: stderr: ""
Dec 17 10:10:25.659: INFO: stdout: "e2e-test-crd-publish-openapi-1624-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 17 10:10:25.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 explain e2e-test-crd-publish-openapi-1624-crds'
Dec 17 10:10:25.872: INFO: stderr: ""
Dec 17 10:10:25.872: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1624-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:10:29.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3542" for this suite.

• [SLOW TEST:8.729 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":277,"completed":198,"skipped":3544,"failed":0}
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:10:29.600: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7154.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7154.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 10:10:33.858: INFO: DNS probes using dns-7154/dns-test-0abff8c2-a7b6-4f8c-b4d2-dd7bef547bcd succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:10:33.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7154" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":277,"completed":199,"skipped":3544,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:10:33.905: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-789
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 17 10:10:34.986: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Dec 17 10:10:37.010: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796634, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796634, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796635, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796634, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 10:10:40.058: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:10:40.062: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:10:41.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-789" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.779 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":277,"completed":200,"skipped":3544,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:10:41.684: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6518
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7314
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:10:57.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6522" for this suite.
STEP: Destroying namespace "nsdeletetest-6518" for this suite.
Dec 17 10:10:57.631: INFO: Namespace nsdeletetest-6518 was already deleted
STEP: Destroying namespace "nsdeletetest-7314" for this suite.

• [SLOW TEST:15.958 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":277,"completed":201,"skipped":3559,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:10:57.642: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8705
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 17 10:10:57.864: INFO: Waiting up to 5m0s for pod "pod-9ad9701e-b3f3-441c-91aa-616a7eebc0e6" in namespace "emptydir-8705" to be "Succeeded or Failed"
Dec 17 10:10:57.877: INFO: Pod "pod-9ad9701e-b3f3-441c-91aa-616a7eebc0e6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.268878ms
Dec 17 10:10:59.883: INFO: Pod "pod-9ad9701e-b3f3-441c-91aa-616a7eebc0e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018818766s
Dec 17 10:11:01.890: INFO: Pod "pod-9ad9701e-b3f3-441c-91aa-616a7eebc0e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025720016s
STEP: Saw pod success
Dec 17 10:11:01.890: INFO: Pod "pod-9ad9701e-b3f3-441c-91aa-616a7eebc0e6" satisfied condition "Succeeded or Failed"
Dec 17 10:11:01.894: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-9ad9701e-b3f3-441c-91aa-616a7eebc0e6 container test-container: <nil>
STEP: delete the pod
Dec 17 10:11:01.924: INFO: Waiting for pod pod-9ad9701e-b3f3-441c-91aa-616a7eebc0e6 to disappear
Dec 17 10:11:01.928: INFO: Pod pod-9ad9701e-b3f3-441c-91aa-616a7eebc0e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:11:01.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8705" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":202,"skipped":3575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:11:01.942: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 17 10:11:02.111: INFO: Waiting up to 5m0s for pod "pod-18f135da-e5bd-4f10-89b8-dce5f4684597" in namespace "emptydir-9917" to be "Succeeded or Failed"
Dec 17 10:11:02.126: INFO: Pod "pod-18f135da-e5bd-4f10-89b8-dce5f4684597": Phase="Pending", Reason="", readiness=false. Elapsed: 15.595732ms
Dec 17 10:11:04.133: INFO: Pod "pod-18f135da-e5bd-4f10-89b8-dce5f4684597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022506662s
Dec 17 10:11:06.140: INFO: Pod "pod-18f135da-e5bd-4f10-89b8-dce5f4684597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029411598s
STEP: Saw pod success
Dec 17 10:11:06.140: INFO: Pod "pod-18f135da-e5bd-4f10-89b8-dce5f4684597" satisfied condition "Succeeded or Failed"
Dec 17 10:11:06.144: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-18f135da-e5bd-4f10-89b8-dce5f4684597 container test-container: <nil>
STEP: delete the pod
Dec 17 10:11:06.176: INFO: Waiting for pod pod-18f135da-e5bd-4f10-89b8-dce5f4684597 to disappear
Dec 17 10:11:06.180: INFO: Pod pod-18f135da-e5bd-4f10-89b8-dce5f4684597 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:11:06.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9917" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":203,"skipped":3646,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:11:06.194: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3902
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 10:11:07.065: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796666, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796666, loc:(*time.Location)(0x7b675e0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-779fdc84d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796666, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796666, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 17 10:11:09.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796666, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796666, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796667, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796666, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 10:11:12.089: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:11:12.094: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:11:13.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3902" for this suite.
STEP: Destroying namespace "webhook-3902-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.394 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":277,"completed":204,"skipped":3664,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:11:13.589: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6523
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 17 10:11:13.758: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:11:21.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6523" for this suite.

• [SLOW TEST:8.032 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":277,"completed":205,"skipped":3672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:11:21.621: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 17 10:11:29.886: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 10:11:29.892: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 10:11:31.893: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 10:11:31.901: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 17 10:11:33.893: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 17 10:11:33.898: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:11:33.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7412" for this suite.

• [SLOW TEST:12.294 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":277,"completed":206,"skipped":3694,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:11:33.916: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9420
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap that has name configmap-test-emptyKey-90897c1a-7739-4336-b4f1-57ae65e86737
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:11:34.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9420" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":277,"completed":207,"skipped":3698,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:11:34.080: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-5272/configmap-test-b83f976f-78ee-40c3-b4d7-51da02601835
STEP: Creating a pod to test consume configMaps
Dec 17 10:11:34.244: INFO: Waiting up to 5m0s for pod "pod-configmaps-c7eea3ef-3027-44b0-bd51-3af777adfa2b" in namespace "configmap-5272" to be "Succeeded or Failed"
Dec 17 10:11:34.249: INFO: Pod "pod-configmaps-c7eea3ef-3027-44b0-bd51-3af777adfa2b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378672ms
Dec 17 10:11:36.254: INFO: Pod "pod-configmaps-c7eea3ef-3027-44b0-bd51-3af777adfa2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009868577s
Dec 17 10:11:38.260: INFO: Pod "pod-configmaps-c7eea3ef-3027-44b0-bd51-3af777adfa2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016120381s
STEP: Saw pod success
Dec 17 10:11:38.260: INFO: Pod "pod-configmaps-c7eea3ef-3027-44b0-bd51-3af777adfa2b" satisfied condition "Succeeded or Failed"
Dec 17 10:11:38.264: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-1 pod pod-configmaps-c7eea3ef-3027-44b0-bd51-3af777adfa2b container env-test: <nil>
STEP: delete the pod
Dec 17 10:11:38.298: INFO: Waiting for pod pod-configmaps-c7eea3ef-3027-44b0-bd51-3af777adfa2b to disappear
Dec 17 10:11:38.310: INFO: Pod pod-configmaps-c7eea3ef-3027-44b0-bd51-3af777adfa2b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:11:38.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5272" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":277,"completed":208,"skipped":3710,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:11:38.324: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8809
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 17 10:11:38.489: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:11:42.252: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:11:57.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8809" for this suite.

• [SLOW TEST:18.992 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":277,"completed":209,"skipped":3742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:11:57.317: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-232
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Dec 17 10:11:57.492: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Dec 17 10:11:57.518: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 17 10:11:57.518: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Dec 17 10:11:57.547: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 17 10:11:57.547: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Dec 17 10:11:57.574: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec 17 10:11:57.574: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Dec 17 10:12:04.639: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:12:04.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-232" for this suite.

• [SLOW TEST:7.368 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":277,"completed":210,"skipped":3776,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:12:04.686: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5162
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:12:04.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5162" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":277,"completed":211,"skipped":3796,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:12:04.948: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5551.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5551.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 10:12:09.154: INFO: DNS probes using dns-test-bb3c5986-eddc-46d3-b85b-8630fb1134d5 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5551.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5551.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 10:12:13.238: INFO: File wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local from pod  dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 17 10:12:13.247: INFO: File jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local from pod  dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 17 10:12:13.247: INFO: Lookups using dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec failed for: [wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local]

Dec 17 10:12:18.256: INFO: File wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local from pod  dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 17 10:12:18.262: INFO: File jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local from pod  dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 17 10:12:18.262: INFO: Lookups using dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec failed for: [wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local]

Dec 17 10:12:23.255: INFO: File wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local from pod  dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 17 10:12:23.270: INFO: File jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local from pod  dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 17 10:12:23.270: INFO: Lookups using dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec failed for: [wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local]

Dec 17 10:12:28.254: INFO: File wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local from pod  dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec contains '' instead of 'bar.example.com.'
Dec 17 10:12:28.261: INFO: File jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local from pod  dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 17 10:12:28.261: INFO: Lookups using dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec failed for: [wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local]

Dec 17 10:12:33.256: INFO: File wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local from pod  dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 17 10:12:33.276: INFO: File jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local from pod  dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 17 10:12:33.276: INFO: Lookups using dns-5551/dns-test-5824a682-441f-4ae6-8424-75f329cec6ec failed for: [wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local]

Dec 17 10:12:38.262: INFO: DNS probes using dns-test-5824a682-441f-4ae6-8424-75f329cec6ec succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5551.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5551.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5551.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5551.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 10:12:42.371: INFO: DNS probes using dns-test-c0150e44-74e9-470e-8842-e21b66ffa18f succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:12:42.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5551" for this suite.

• [SLOW TEST:37.536 seconds]
[sig-network] DNS
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":277,"completed":212,"skipped":3797,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:12:42.484: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 10:12:43.088: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 10:12:45.101: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796763, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796763, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796763, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743796763, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 10:12:48.132: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:12:48.139: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8820-crds.webhook.example.com via the AdmissionRegistration API
Dec 17 10:12:48.839: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:12:49.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7450" for this suite.
STEP: Destroying namespace "webhook-7450-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.262 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":277,"completed":213,"skipped":3797,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:12:49.748: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-8a02851d-aa65-4473-9ee9-0695d96946ea
STEP: Creating a pod to test consume configMaps
Dec 17 10:12:49.938: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5d99fff-fc20-43d9-8bf8-4e189f2d34fb" in namespace "projected-3651" to be "Succeeded or Failed"
Dec 17 10:12:49.956: INFO: Pod "pod-projected-configmaps-a5d99fff-fc20-43d9-8bf8-4e189f2d34fb": Phase="Pending", Reason="", readiness=false. Elapsed: 17.739593ms
Dec 17 10:12:51.962: INFO: Pod "pod-projected-configmaps-a5d99fff-fc20-43d9-8bf8-4e189f2d34fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023685246s
Dec 17 10:12:53.968: INFO: Pod "pod-projected-configmaps-a5d99fff-fc20-43d9-8bf8-4e189f2d34fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029647831s
STEP: Saw pod success
Dec 17 10:12:53.968: INFO: Pod "pod-projected-configmaps-a5d99fff-fc20-43d9-8bf8-4e189f2d34fb" satisfied condition "Succeeded or Failed"
Dec 17 10:12:53.972: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-1 pod pod-projected-configmaps-a5d99fff-fc20-43d9-8bf8-4e189f2d34fb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 10:12:54.008: INFO: Waiting for pod pod-projected-configmaps-a5d99fff-fc20-43d9-8bf8-4e189f2d34fb to disappear
Dec 17 10:12:54.012: INFO: Pod pod-projected-configmaps-a5d99fff-fc20-43d9-8bf8-4e189f2d34fb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:12:54.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3651" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":214,"skipped":3868,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:12:54.026: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5480
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 10:12:54.204: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5cfea2fe-9843-481c-a9c4-d8c410550205" in namespace "downward-api-5480" to be "Succeeded or Failed"
Dec 17 10:12:54.210: INFO: Pod "downwardapi-volume-5cfea2fe-9843-481c-a9c4-d8c410550205": Phase="Pending", Reason="", readiness=false. Elapsed: 6.567263ms
Dec 17 10:12:56.218: INFO: Pod "downwardapi-volume-5cfea2fe-9843-481c-a9c4-d8c410550205": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014189913s
Dec 17 10:12:58.223: INFO: Pod "downwardapi-volume-5cfea2fe-9843-481c-a9c4-d8c410550205": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019050439s
STEP: Saw pod success
Dec 17 10:12:58.223: INFO: Pod "downwardapi-volume-5cfea2fe-9843-481c-a9c4-d8c410550205" satisfied condition "Succeeded or Failed"
Dec 17 10:12:58.227: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-5cfea2fe-9843-481c-a9c4-d8c410550205 container client-container: <nil>
STEP: delete the pod
Dec 17 10:12:58.260: INFO: Waiting for pod downwardapi-volume-5cfea2fe-9843-481c-a9c4-d8c410550205 to disappear
Dec 17 10:12:58.264: INFO: Pod downwardapi-volume-5cfea2fe-9843-481c-a9c4-d8c410550205 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:12:58.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5480" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":215,"skipped":3884,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:12:58.278: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-739
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-622w
STEP: Creating a pod to test atomic-volume-subpath
Dec 17 10:12:58.468: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-622w" in namespace "subpath-739" to be "Succeeded or Failed"
Dec 17 10:12:58.475: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Pending", Reason="", readiness=false. Elapsed: 7.142405ms
Dec 17 10:13:00.481: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013209578s
Dec 17 10:13:02.487: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Running", Reason="", readiness=true. Elapsed: 4.01900131s
Dec 17 10:13:04.494: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Running", Reason="", readiness=true. Elapsed: 6.026097231s
Dec 17 10:13:06.500: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Running", Reason="", readiness=true. Elapsed: 8.032356112s
Dec 17 10:13:08.507: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Running", Reason="", readiness=true. Elapsed: 10.038754567s
Dec 17 10:13:10.513: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Running", Reason="", readiness=true. Elapsed: 12.044852646s
Dec 17 10:13:12.518: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Running", Reason="", readiness=true. Elapsed: 14.049798151s
Dec 17 10:13:14.530: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Running", Reason="", readiness=true. Elapsed: 16.061738722s
Dec 17 10:13:16.547: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Running", Reason="", readiness=true. Elapsed: 18.078985227s
Dec 17 10:13:18.552: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Running", Reason="", readiness=true. Elapsed: 20.084306127s
Dec 17 10:13:20.560: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Running", Reason="", readiness=true. Elapsed: 22.092093071s
Dec 17 10:13:22.566: INFO: Pod "pod-subpath-test-configmap-622w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.098008132s
STEP: Saw pod success
Dec 17 10:13:22.566: INFO: Pod "pod-subpath-test-configmap-622w" satisfied condition "Succeeded or Failed"
Dec 17 10:13:22.571: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-subpath-test-configmap-622w container test-container-subpath-configmap-622w: <nil>
STEP: delete the pod
Dec 17 10:13:22.619: INFO: Waiting for pod pod-subpath-test-configmap-622w to disappear
Dec 17 10:13:22.623: INFO: Pod pod-subpath-test-configmap-622w no longer exists
STEP: Deleting pod pod-subpath-test-configmap-622w
Dec 17 10:13:22.623: INFO: Deleting pod "pod-subpath-test-configmap-622w" in namespace "subpath-739"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:13:22.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-739" for this suite.

• [SLOW TEST:24.365 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":277,"completed":216,"skipped":3895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:13:22.644: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3106
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-f1178230-77c4-4896-9fa4-727a3114a29f
STEP: Creating a pod to test consume configMaps
Dec 17 10:13:22.835: INFO: Waiting up to 5m0s for pod "pod-configmaps-b9f7e32c-443e-4fed-84a9-d65673ff417f" in namespace "configmap-3106" to be "Succeeded or Failed"
Dec 17 10:13:22.855: INFO: Pod "pod-configmaps-b9f7e32c-443e-4fed-84a9-d65673ff417f": Phase="Pending", Reason="", readiness=false. Elapsed: 19.18209ms
Dec 17 10:13:24.866: INFO: Pod "pod-configmaps-b9f7e32c-443e-4fed-84a9-d65673ff417f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030790982s
Dec 17 10:13:26.874: INFO: Pod "pod-configmaps-b9f7e32c-443e-4fed-84a9-d65673ff417f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038881689s
STEP: Saw pod success
Dec 17 10:13:26.874: INFO: Pod "pod-configmaps-b9f7e32c-443e-4fed-84a9-d65673ff417f" satisfied condition "Succeeded or Failed"
Dec 17 10:13:26.879: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-configmaps-b9f7e32c-443e-4fed-84a9-d65673ff417f container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 10:13:26.923: INFO: Waiting for pod pod-configmaps-b9f7e32c-443e-4fed-84a9-d65673ff417f to disappear
Dec 17 10:13:26.927: INFO: Pod pod-configmaps-b9f7e32c-443e-4fed-84a9-d65673ff417f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:13:26.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3106" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":217,"skipped":3918,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:13:26.947: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1828
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 17 10:13:31.689: INFO: Successfully updated pod "adopt-release-4d8mg"
STEP: Checking that the Job readopts the Pod
Dec 17 10:13:31.689: INFO: Waiting up to 15m0s for pod "adopt-release-4d8mg" in namespace "job-1828" to be "adopted"
Dec 17 10:13:31.715: INFO: Pod "adopt-release-4d8mg": Phase="Running", Reason="", readiness=true. Elapsed: 25.589927ms
Dec 17 10:13:33.722: INFO: Pod "adopt-release-4d8mg": Phase="Running", Reason="", readiness=true. Elapsed: 2.032862661s
Dec 17 10:13:33.722: INFO: Pod "adopt-release-4d8mg" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 17 10:13:34.243: INFO: Successfully updated pod "adopt-release-4d8mg"
STEP: Checking that the Job releases the Pod
Dec 17 10:13:34.243: INFO: Waiting up to 15m0s for pod "adopt-release-4d8mg" in namespace "job-1828" to be "released"
Dec 17 10:13:34.262: INFO: Pod "adopt-release-4d8mg": Phase="Running", Reason="", readiness=true. Elapsed: 15.746261ms
Dec 17 10:13:34.262: INFO: Pod "adopt-release-4d8mg" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:13:34.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1828" for this suite.

• [SLOW TEST:7.357 seconds]
[sig-apps] Job
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":277,"completed":218,"skipped":3954,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:13:34.305: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2509
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating api versions
Dec 17 10:13:34.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 api-versions'
Dec 17 10:13:34.573: INFO: stderr: ""
Dec 17 10:13:34.573: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:13:34.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2509" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":277,"completed":219,"skipped":3960,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:13:34.591: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod test-webserver-b87deec5-0ceb-418f-8f30-a127dd3b94fa in namespace container-probe-8585
Dec 17 10:13:38.784: INFO: Started pod test-webserver-b87deec5-0ceb-418f-8f30-a127dd3b94fa in namespace container-probe-8585
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 10:13:38.791: INFO: Initial restart count of pod test-webserver-b87deec5-0ceb-418f-8f30-a127dd3b94fa is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:17:39.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8585" for this suite.

• [SLOW TEST:245.076 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":220,"skipped":3967,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:17:39.669: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-768
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 17 10:17:39.864: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-768 /api/v1/namespaces/watch-768/configmaps/e2e-watch-test-watch-closed 91a433c7-a042-41a1-8900-ba52348111b2 26151 0 2020-12-17 10:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-17 10:17:39 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 10:17:39.865: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-768 /api/v1/namespaces/watch-768/configmaps/e2e-watch-test-watch-closed 91a433c7-a042-41a1-8900-ba52348111b2 26152 0 2020-12-17 10:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-17 10:17:39 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 17 10:17:39.887: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-768 /api/v1/namespaces/watch-768/configmaps/e2e-watch-test-watch-closed 91a433c7-a042-41a1-8900-ba52348111b2 26153 0 2020-12-17 10:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-17 10:17:39 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 17 10:17:39.887: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-768 /api/v1/namespaces/watch-768/configmaps/e2e-watch-test-watch-closed 91a433c7-a042-41a1-8900-ba52348111b2 26154 0 2020-12-17 10:17:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-12-17 10:17:39 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:17:39.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-768" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":277,"completed":221,"skipped":4006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:17:39.901: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-d61ff5cd-b22c-4931-a455-8c742bb1e851
STEP: Creating a pod to test consume configMaps
Dec 17 10:17:40.085: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-57b0321e-3154-478c-a27d-36d01fbf003c" in namespace "projected-4077" to be "Succeeded or Failed"
Dec 17 10:17:40.095: INFO: Pod "pod-projected-configmaps-57b0321e-3154-478c-a27d-36d01fbf003c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.541048ms
Dec 17 10:17:42.102: INFO: Pod "pod-projected-configmaps-57b0321e-3154-478c-a27d-36d01fbf003c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017381635s
Dec 17 10:17:44.109: INFO: Pod "pod-projected-configmaps-57b0321e-3154-478c-a27d-36d01fbf003c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023840317s
STEP: Saw pod success
Dec 17 10:17:44.109: INFO: Pod "pod-projected-configmaps-57b0321e-3154-478c-a27d-36d01fbf003c" satisfied condition "Succeeded or Failed"
Dec 17 10:17:44.114: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-projected-configmaps-57b0321e-3154-478c-a27d-36d01fbf003c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 10:17:44.165: INFO: Waiting for pod pod-projected-configmaps-57b0321e-3154-478c-a27d-36d01fbf003c to disappear
Dec 17 10:17:44.170: INFO: Pod pod-projected-configmaps-57b0321e-3154-478c-a27d-36d01fbf003c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:17:44.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4077" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":222,"skipped":4038,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:17:44.187: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6223
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 10:17:44.363: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d74a155-cde1-4a74-a52d-0b57f973e2dc" in namespace "projected-6223" to be "Succeeded or Failed"
Dec 17 10:17:44.368: INFO: Pod "downwardapi-volume-9d74a155-cde1-4a74-a52d-0b57f973e2dc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.017036ms
Dec 17 10:17:46.375: INFO: Pod "downwardapi-volume-9d74a155-cde1-4a74-a52d-0b57f973e2dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011862883s
Dec 17 10:17:48.382: INFO: Pod "downwardapi-volume-9d74a155-cde1-4a74-a52d-0b57f973e2dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018965089s
STEP: Saw pod success
Dec 17 10:17:48.382: INFO: Pod "downwardapi-volume-9d74a155-cde1-4a74-a52d-0b57f973e2dc" satisfied condition "Succeeded or Failed"
Dec 17 10:17:48.387: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-9d74a155-cde1-4a74-a52d-0b57f973e2dc container client-container: <nil>
STEP: delete the pod
Dec 17 10:17:48.417: INFO: Waiting for pod downwardapi-volume-9d74a155-cde1-4a74-a52d-0b57f973e2dc to disappear
Dec 17 10:17:48.422: INFO: Pod downwardapi-volume-9d74a155-cde1-4a74-a52d-0b57f973e2dc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:17:48.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6223" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":277,"completed":223,"skipped":4062,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:17:48.441: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5462
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-ec9cc509-98a0-423a-bec8-14690af6974e
STEP: Creating a pod to test consume secrets
Dec 17 10:17:48.632: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4600ee5a-7150-47d6-addb-94ef0f342bd8" in namespace "projected-5462" to be "Succeeded or Failed"
Dec 17 10:17:48.651: INFO: Pod "pod-projected-secrets-4600ee5a-7150-47d6-addb-94ef0f342bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.367741ms
Dec 17 10:17:50.657: INFO: Pod "pod-projected-secrets-4600ee5a-7150-47d6-addb-94ef0f342bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025279869s
Dec 17 10:17:52.664: INFO: Pod "pod-projected-secrets-4600ee5a-7150-47d6-addb-94ef0f342bd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031858633s
STEP: Saw pod success
Dec 17 10:17:52.664: INFO: Pod "pod-projected-secrets-4600ee5a-7150-47d6-addb-94ef0f342bd8" satisfied condition "Succeeded or Failed"
Dec 17 10:17:52.669: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-projected-secrets-4600ee5a-7150-47d6-addb-94ef0f342bd8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 17 10:17:52.704: INFO: Waiting for pod pod-projected-secrets-4600ee5a-7150-47d6-addb-94ef0f342bd8 to disappear
Dec 17 10:17:52.708: INFO: Pod pod-projected-secrets-4600ee5a-7150-47d6-addb-94ef0f342bd8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:17:52.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5462" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":224,"skipped":4071,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:17:52.725: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-262
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Dec 17 10:17:52.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-262'
Dec 17 10:17:53.240: INFO: stderr: ""
Dec 17 10:17:53.240: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Dec 17 10:17:54.246: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 10:17:54.247: INFO: Found 0 / 1
Dec 17 10:17:55.245: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 10:17:55.245: INFO: Found 0 / 1
Dec 17 10:17:56.246: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 10:17:56.246: INFO: Found 1 / 1
Dec 17 10:17:56.246: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 17 10:17:56.252: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 10:17:56.252: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 17 10:17:56.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 patch pod agnhost-master-5z6wg --namespace=kubectl-262 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 17 10:17:56.380: INFO: stderr: ""
Dec 17 10:17:56.380: INFO: stdout: "pod/agnhost-master-5z6wg patched\n"
STEP: checking annotations
Dec 17 10:17:56.385: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 17 10:17:56.385: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:17:56.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-262" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":277,"completed":225,"skipped":4078,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:17:56.397: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8049
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Dec 17 10:17:56.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 create -f - --namespace=kubectl-8049'
Dec 17 10:17:56.878: INFO: stderr: ""
Dec 17 10:17:56.878: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 17 10:17:56.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8049'
Dec 17 10:17:57.009: INFO: stderr: ""
Dec 17 10:17:57.009: INFO: stdout: "update-demo-nautilus-2k9s5 update-demo-nautilus-8bnp7 "
Dec 17 10:17:57.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-2k9s5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8049'
Dec 17 10:17:57.123: INFO: stderr: ""
Dec 17 10:17:57.123: INFO: stdout: ""
Dec 17 10:17:57.123: INFO: update-demo-nautilus-2k9s5 is created but not running
Dec 17 10:18:02.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8049'
Dec 17 10:18:02.255: INFO: stderr: ""
Dec 17 10:18:02.255: INFO: stdout: "update-demo-nautilus-2k9s5 update-demo-nautilus-8bnp7 "
Dec 17 10:18:02.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-2k9s5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8049'
Dec 17 10:18:02.368: INFO: stderr: ""
Dec 17 10:18:02.368: INFO: stdout: "true"
Dec 17 10:18:02.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-2k9s5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8049'
Dec 17 10:18:02.477: INFO: stderr: ""
Dec 17 10:18:02.477: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 10:18:02.477: INFO: validating pod update-demo-nautilus-2k9s5
Dec 17 10:18:02.485: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 10:18:02.485: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 10:18:02.485: INFO: update-demo-nautilus-2k9s5 is verified up and running
Dec 17 10:18:02.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-8bnp7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8049'
Dec 17 10:18:02.592: INFO: stderr: ""
Dec 17 10:18:02.592: INFO: stdout: "true"
Dec 17 10:18:02.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods update-demo-nautilus-8bnp7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8049'
Dec 17 10:18:02.703: INFO: stderr: ""
Dec 17 10:18:02.703: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 17 10:18:02.703: INFO: validating pod update-demo-nautilus-8bnp7
Dec 17 10:18:02.711: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 17 10:18:02.711: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 17 10:18:02.711: INFO: update-demo-nautilus-8bnp7 is verified up and running
STEP: using delete to clean up resources
Dec 17 10:18:02.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 delete --grace-period=0 --force -f - --namespace=kubectl-8049'
Dec 17 10:18:02.847: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 17 10:18:02.847: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 17 10:18:02.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8049'
Dec 17 10:18:02.963: INFO: stderr: "No resources found in kubectl-8049 namespace.\n"
Dec 17 10:18:02.963: INFO: stdout: ""
Dec 17 10:18:02.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -l name=update-demo --namespace=kubectl-8049 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 17 10:18:03.086: INFO: stderr: ""
Dec 17 10:18:03.086: INFO: stdout: "update-demo-nautilus-2k9s5\nupdate-demo-nautilus-8bnp7\n"
Dec 17 10:18:03.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8049'
Dec 17 10:18:03.699: INFO: stderr: "No resources found in kubectl-8049 namespace.\n"
Dec 17 10:18:03.699: INFO: stdout: ""
Dec 17 10:18:03.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 get pods -l name=update-demo --namespace=kubectl-8049 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 17 10:18:03.803: INFO: stderr: ""
Dec 17 10:18:03.803: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:18:03.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8049" for this suite.

• [SLOW TEST:7.422 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":277,"completed":226,"skipped":4079,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:18:03.820: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7661
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-6a9faf63-bbb5-4918-9125-5cc0715c91cc
STEP: Creating configMap with name cm-test-opt-upd-84f66c03-ca77-4be3-a13e-a3039b1be012
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6a9faf63-bbb5-4918-9125-5cc0715c91cc
STEP: Updating configmap cm-test-opt-upd-84f66c03-ca77-4be3-a13e-a3039b1be012
STEP: Creating configMap with name cm-test-opt-create-3df452a7-c3e8-4aa0-bcd8-b39338563317
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:18:12.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7661" for this suite.

• [SLOW TEST:8.364 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":227,"skipped":4081,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:18:12.185: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-d8c42dc7-6ca1-47af-ae59-53bef20d648c in namespace container-probe-5598
Dec 17 10:18:16.374: INFO: Started pod liveness-d8c42dc7-6ca1-47af-ae59-53bef20d648c in namespace container-probe-5598
STEP: checking the pod's current state and verifying that restartCount is present
Dec 17 10:18:16.379: INFO: Initial restart count of pod liveness-d8c42dc7-6ca1-47af-ae59-53bef20d648c is 0
Dec 17 10:18:26.415: INFO: Restart count of pod container-probe-5598/liveness-d8c42dc7-6ca1-47af-ae59-53bef20d648c is now 1 (10.035781523s elapsed)
Dec 17 10:18:46.486: INFO: Restart count of pod container-probe-5598/liveness-d8c42dc7-6ca1-47af-ae59-53bef20d648c is now 2 (30.107074428s elapsed)
Dec 17 10:19:06.556: INFO: Restart count of pod container-probe-5598/liveness-d8c42dc7-6ca1-47af-ae59-53bef20d648c is now 3 (50.177736195s elapsed)
Dec 17 10:19:26.621: INFO: Restart count of pod container-probe-5598/liveness-d8c42dc7-6ca1-47af-ae59-53bef20d648c is now 4 (1m10.242204655s elapsed)
Dec 17 10:20:26.856: INFO: Restart count of pod container-probe-5598/liveness-d8c42dc7-6ca1-47af-ae59-53bef20d648c is now 5 (2m10.477702519s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:20:26.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5598" for this suite.

• [SLOW TEST:134.739 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":277,"completed":228,"skipped":4089,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:20:26.924: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 17 10:20:30.172: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:20:30.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-769" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":229,"skipped":4092,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:20:30.212: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 10:20:30.988: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Dec 17 10:20:33.009: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797230, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797230, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797231, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797230, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 10:20:36.033: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:20:36.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6694" for this suite.
STEP: Destroying namespace "webhook-6694-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.049 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":277,"completed":230,"skipped":4103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:20:36.263: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7213
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 10:20:37.052: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 10:20:39.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797237, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797237, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797237, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797236, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 10:20:42.120: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:20:42.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7213" for this suite.
STEP: Destroying namespace "webhook-7213-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.199 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":277,"completed":231,"skipped":4138,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:20:42.462: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-1690
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 17 10:20:42.667: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 17 10:20:42.714: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 17 10:20:44.719: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 17 10:20:46.721: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 10:20:48.720: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 10:20:50.720: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 10:20:52.720: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 10:20:54.719: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 10:20:56.719: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 10:20:58.722: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 10:21:00.720: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 10:21:02.720: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 17 10:21:04.727: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 17 10:21:04.739: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Dec 17 10:21:08.776: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.108:8080/dial?request=hostname&protocol=http&host=192.168.192.118&port=8080&tries=1'] Namespace:pod-network-test-1690 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:21:08.776: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:21:08.965: INFO: Waiting for responses: map[]
Dec 17 10:21:08.971: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.192.108:8080/dial?request=hostname&protocol=http&host=192.168.137.51&port=8080&tries=1'] Namespace:pod-network-test-1690 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:21:08.971: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:21:09.158: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:21:09.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1690" for this suite.

• [SLOW TEST:26.714 seconds]
[sig-network] Networking
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":277,"completed":232,"skipped":4144,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:21:09.177: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-48
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:21:25.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-48" for this suite.

• [SLOW TEST:16.262 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":277,"completed":233,"skipped":4147,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:21:25.439: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1977
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 17 10:21:35.770: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:21:35.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1217 10:21:35.770429      24 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1977" for this suite.

• [SLOW TEST:10.343 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":277,"completed":234,"skipped":4166,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:21:35.782: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:21:52.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3932" for this suite.

• [SLOW TEST:17.223 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":277,"completed":235,"skipped":4180,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:21:53.006: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 10:21:54.313: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 10:21:56.334: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797314, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797314, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797314, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797314, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 10:21:59.371: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:22:11.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6972" for this suite.
STEP: Destroying namespace "webhook-6972-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.657 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":277,"completed":236,"skipped":4191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:22:11.665: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6220
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 10:22:12.977: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 10:22:14.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797332, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797332, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797332, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797332, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 10:22:18.056: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:22:18.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6220" for this suite.
STEP: Destroying namespace "webhook-6220-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.606 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":277,"completed":237,"skipped":4221,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:22:18.274: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1867
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 17 10:22:18.506: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 17 10:22:23.512: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:22:23.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1867" for this suite.

• [SLOW TEST:5.314 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":277,"completed":238,"skipped":4269,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:22:23.589: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Dec 17 10:22:23.851: INFO: Waiting up to 5m0s for pod "downward-api-6fafd8f2-46ec-43b0-b86e-94d6432435d2" in namespace "downward-api-1827" to be "Succeeded or Failed"
Dec 17 10:22:23.861: INFO: Pod "downward-api-6fafd8f2-46ec-43b0-b86e-94d6432435d2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.41709ms
Dec 17 10:22:25.871: INFO: Pod "downward-api-6fafd8f2-46ec-43b0-b86e-94d6432435d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020648409s
STEP: Saw pod success
Dec 17 10:22:25.871: INFO: Pod "downward-api-6fafd8f2-46ec-43b0-b86e-94d6432435d2" satisfied condition "Succeeded or Failed"
Dec 17 10:22:25.876: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downward-api-6fafd8f2-46ec-43b0-b86e-94d6432435d2 container dapi-container: <nil>
STEP: delete the pod
Dec 17 10:22:25.930: INFO: Waiting for pod downward-api-6fafd8f2-46ec-43b0-b86e-94d6432435d2 to disappear
Dec 17 10:22:25.935: INFO: Pod downward-api-6fafd8f2-46ec-43b0-b86e-94d6432435d2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:22:25.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1827" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":277,"completed":239,"skipped":4277,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:22:25.951: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8441
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-b8f419ed-4864-40fc-989b-477edd161087
STEP: Creating a pod to test consume secrets
Dec 17 10:22:26.119: INFO: Waiting up to 5m0s for pod "pod-secrets-37ec55de-0aba-4451-8f3e-fcf76559544a" in namespace "secrets-8441" to be "Succeeded or Failed"
Dec 17 10:22:26.137: INFO: Pod "pod-secrets-37ec55de-0aba-4451-8f3e-fcf76559544a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.274621ms
Dec 17 10:22:28.142: INFO: Pod "pod-secrets-37ec55de-0aba-4451-8f3e-fcf76559544a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022806445s
Dec 17 10:22:30.149: INFO: Pod "pod-secrets-37ec55de-0aba-4451-8f3e-fcf76559544a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029668502s
STEP: Saw pod success
Dec 17 10:22:30.149: INFO: Pod "pod-secrets-37ec55de-0aba-4451-8f3e-fcf76559544a" satisfied condition "Succeeded or Failed"
Dec 17 10:22:30.155: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-secrets-37ec55de-0aba-4451-8f3e-fcf76559544a container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 10:22:30.198: INFO: Waiting for pod pod-secrets-37ec55de-0aba-4451-8f3e-fcf76559544a to disappear
Dec 17 10:22:30.202: INFO: Pod pod-secrets-37ec55de-0aba-4451-8f3e-fcf76559544a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:22:30.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8441" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":240,"skipped":4277,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:22:30.220: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2205
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 17 10:22:30.386: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:22:34.082: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:22:48.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2205" for this suite.

• [SLOW TEST:18.108 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":277,"completed":241,"skipped":4277,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:22:48.328: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 17 10:22:53.061: INFO: Successfully updated pod "pod-update-activedeadlineseconds-423578cc-da4a-4ef9-b66b-fbb9eb09f726"
Dec 17 10:22:53.062: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-423578cc-da4a-4ef9-b66b-fbb9eb09f726" in namespace "pods-2447" to be "terminated due to deadline exceeded"
Dec 17 10:22:53.068: INFO: Pod "pod-update-activedeadlineseconds-423578cc-da4a-4ef9-b66b-fbb9eb09f726": Phase="Running", Reason="", readiness=true. Elapsed: 6.552359ms
Dec 17 10:22:55.074: INFO: Pod "pod-update-activedeadlineseconds-423578cc-da4a-4ef9-b66b-fbb9eb09f726": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.012547119s
Dec 17 10:22:55.074: INFO: Pod "pod-update-activedeadlineseconds-423578cc-da4a-4ef9-b66b-fbb9eb09f726" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:22:55.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2447" for this suite.

• [SLOW TEST:6.768 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":277,"completed":242,"skipped":4279,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:22:55.097: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7968
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-74b6493b-e47c-4809-9c0d-9f1c73c70934
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:22:59.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7968" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":243,"skipped":4300,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:22:59.362: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec 17 10:23:09.597: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:23:09.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1217 10:23:09.597784      24 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1322" for this suite.

• [SLOW TEST:10.247 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":277,"completed":244,"skipped":4315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:23:09.609: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-993
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Dec 17 10:23:14.363: INFO: Successfully updated pod "annotationupdate0ca60878-28b5-4f0f-a50f-e47b98f7aa68"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:23:16.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-993" for this suite.

• [SLOW TEST:6.799 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":245,"skipped":4338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:23:16.409: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 10:23:16.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e48e4e5-5a85-468e-8f73-b6db92fce7db" in namespace "projected-9994" to be "Succeeded or Failed"
Dec 17 10:23:16.595: INFO: Pod "downwardapi-volume-0e48e4e5-5a85-468e-8f73-b6db92fce7db": Phase="Pending", Reason="", readiness=false. Elapsed: 6.818524ms
Dec 17 10:23:18.603: INFO: Pod "downwardapi-volume-0e48e4e5-5a85-468e-8f73-b6db92fce7db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014926114s
Dec 17 10:23:20.612: INFO: Pod "downwardapi-volume-0e48e4e5-5a85-468e-8f73-b6db92fce7db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023578745s
STEP: Saw pod success
Dec 17 10:23:20.612: INFO: Pod "downwardapi-volume-0e48e4e5-5a85-468e-8f73-b6db92fce7db" satisfied condition "Succeeded or Failed"
Dec 17 10:23:20.616: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-0e48e4e5-5a85-468e-8f73-b6db92fce7db container client-container: <nil>
STEP: delete the pod
Dec 17 10:23:20.646: INFO: Waiting for pod downwardapi-volume-0e48e4e5-5a85-468e-8f73-b6db92fce7db to disappear
Dec 17 10:23:20.651: INFO: Pod downwardapi-volume-0e48e4e5-5a85-468e-8f73-b6db92fce7db no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:23:20.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9994" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":246,"skipped":4363,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:23:20.666: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1686
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 17 10:23:20.833: INFO: Waiting up to 5m0s for pod "pod-99968652-5cd1-48f8-bfcb-4759465b4d22" in namespace "emptydir-1686" to be "Succeeded or Failed"
Dec 17 10:23:20.839: INFO: Pod "pod-99968652-5cd1-48f8-bfcb-4759465b4d22": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346987ms
Dec 17 10:23:22.844: INFO: Pod "pod-99968652-5cd1-48f8-bfcb-4759465b4d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011821785s
Dec 17 10:23:24.856: INFO: Pod "pod-99968652-5cd1-48f8-bfcb-4759465b4d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022964312s
STEP: Saw pod success
Dec 17 10:23:24.856: INFO: Pod "pod-99968652-5cd1-48f8-bfcb-4759465b4d22" satisfied condition "Succeeded or Failed"
Dec 17 10:23:24.859: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-99968652-5cd1-48f8-bfcb-4759465b4d22 container test-container: <nil>
STEP: delete the pod
Dec 17 10:23:24.889: INFO: Waiting for pod pod-99968652-5cd1-48f8-bfcb-4759465b4d22 to disappear
Dec 17 10:23:24.891: INFO: Pod pod-99968652-5cd1-48f8-bfcb-4759465b4d22 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:23:24.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1686" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":247,"skipped":4371,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:23:24.903: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5355
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:23:25.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5355" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":277,"completed":248,"skipped":4373,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:23:25.080: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2149
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 17 10:23:33.391: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 17 10:23:33.397: INFO: Pod pod-with-prestop-http-hook still exists
Dec 17 10:23:35.397: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 17 10:23:35.404: INFO: Pod pod-with-prestop-http-hook still exists
Dec 17 10:23:37.397: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 17 10:23:37.403: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:23:37.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2149" for this suite.

• [SLOW TEST:12.351 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":277,"completed":249,"skipped":4381,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:23:37.432: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Dec 17 10:23:37.592: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 17 10:23:37.611: INFO: Waiting for terminating namespaces to be deleted...
Dec 17 10:23:37.615: INFO: 
Logging pods the kubelet thinks is on node ck8s-conftest-118-workload-cluster-worker-0 before test
Dec 17 10:23:37.625: INFO: sonobuoy from sonobuoy started at 2020-12-17 09:17:49 +0000 UTC (1 container statuses recorded)
Dec 17 10:23:37.625: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 17 10:23:37.625: INFO: kube-proxy-87hvk from kube-system started at 2020-12-17 09:10:11 +0000 UTC (1 container statuses recorded)
Dec 17 10:23:37.625: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 10:23:37.625: INFO: calico-node-hv857 from kube-system started at 2020-12-17 09:10:11 +0000 UTC (1 container statuses recorded)
Dec 17 10:23:37.625: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 10:23:37.625: INFO: pod-handle-http-request from container-lifecycle-hook-2149 started at 2020-12-17 10:23:25 +0000 UTC (1 container statuses recorded)
Dec 17 10:23:37.625: INFO: 	Container pod-handle-http-request ready: true, restart count 0
Dec 17 10:23:37.625: INFO: sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-hdzhs from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 10:23:37.625: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 17 10:23:37.625: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 17 10:23:37.625: INFO: 
Logging pods the kubelet thinks is on node ck8s-conftest-118-workload-cluster-worker-1 before test
Dec 17 10:23:37.633: INFO: calico-node-dfdx8 from kube-system started at 2020-12-17 09:10:13 +0000 UTC (1 container statuses recorded)
Dec 17 10:23:37.633: INFO: 	Container calico-node ready: true, restart count 0
Dec 17 10:23:37.633: INFO: kube-proxy-xk6gr from kube-system started at 2020-12-17 09:10:13 +0000 UTC (1 container statuses recorded)
Dec 17 10:23:37.633: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 17 10:23:37.633: INFO: sonobuoy-e2e-job-c7b0b40574f24e1c from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 10:23:37.633: INFO: 	Container e2e ready: true, restart count 0
Dec 17 10:23:37.633: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 17 10:23:37.633: INFO: sonobuoy-systemd-logs-daemon-set-1e6ad749c6644ce8-nn8jm from sonobuoy started at 2020-12-17 09:17:56 +0000 UTC (2 container statuses recorded)
Dec 17 10:23:37.633: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 17 10:23:37.633: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c890f236-c53d-4703-9f4c-9010749ce0a4 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-c890f236-c53d-4703-9f4c-9010749ce0a4 off the node ck8s-conftest-118-workload-cluster-worker-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c890f236-c53d-4703-9f4c-9010749ce0a4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:28:45.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8749" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:308.388 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":277,"completed":250,"skipped":4396,"failed":0}
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:28:45.822: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's args
Dec 17 10:28:46.003: INFO: Waiting up to 5m0s for pod "var-expansion-a7ce45ef-c61a-4a4c-a86f-8dabb1aa2700" in namespace "var-expansion-8800" to be "Succeeded or Failed"
Dec 17 10:28:46.016: INFO: Pod "var-expansion-a7ce45ef-c61a-4a4c-a86f-8dabb1aa2700": Phase="Pending", Reason="", readiness=false. Elapsed: 12.66901ms
Dec 17 10:28:48.022: INFO: Pod "var-expansion-a7ce45ef-c61a-4a4c-a86f-8dabb1aa2700": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01869127s
Dec 17 10:28:50.029: INFO: Pod "var-expansion-a7ce45ef-c61a-4a4c-a86f-8dabb1aa2700": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025304549s
STEP: Saw pod success
Dec 17 10:28:50.029: INFO: Pod "var-expansion-a7ce45ef-c61a-4a4c-a86f-8dabb1aa2700" satisfied condition "Succeeded or Failed"
Dec 17 10:28:50.035: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod var-expansion-a7ce45ef-c61a-4a4c-a86f-8dabb1aa2700 container dapi-container: <nil>
STEP: delete the pod
Dec 17 10:28:50.094: INFO: Waiting for pod var-expansion-a7ce45ef-c61a-4a4c-a86f-8dabb1aa2700 to disappear
Dec 17 10:28:50.100: INFO: Pod var-expansion-a7ce45ef-c61a-4a4c-a86f-8dabb1aa2700 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:28:50.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8800" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":277,"completed":251,"skipped":4396,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:28:50.114: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-3077061e-8163-4a06-9dfa-065b89ad4911
STEP: Creating a pod to test consume configMaps
Dec 17 10:28:50.312: INFO: Waiting up to 5m0s for pod "pod-configmaps-479ef737-b351-4d7a-9d88-5a40a8c59a7d" in namespace "configmap-7632" to be "Succeeded or Failed"
Dec 17 10:28:50.321: INFO: Pod "pod-configmaps-479ef737-b351-4d7a-9d88-5a40a8c59a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.780646ms
Dec 17 10:28:52.327: INFO: Pod "pod-configmaps-479ef737-b351-4d7a-9d88-5a40a8c59a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014409189s
Dec 17 10:28:54.336: INFO: Pod "pod-configmaps-479ef737-b351-4d7a-9d88-5a40a8c59a7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023238385s
STEP: Saw pod success
Dec 17 10:28:54.336: INFO: Pod "pod-configmaps-479ef737-b351-4d7a-9d88-5a40a8c59a7d" satisfied condition "Succeeded or Failed"
Dec 17 10:28:54.341: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-configmaps-479ef737-b351-4d7a-9d88-5a40a8c59a7d container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 10:28:54.372: INFO: Waiting for pod pod-configmaps-479ef737-b351-4d7a-9d88-5a40a8c59a7d to disappear
Dec 17 10:28:54.376: INFO: Pod pod-configmaps-479ef737-b351-4d7a-9d88-5a40a8c59a7d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:28:54.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7632" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":252,"skipped":4411,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:28:54.392: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-256
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-256, will wait for the garbage collector to delete the pods
Dec 17 10:28:58.622: INFO: Deleting Job.batch foo took: 10.88508ms
Dec 17 10:28:59.022: INFO: Terminating Job.batch foo pods took: 400.414957ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:29:41.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-256" for this suite.

• [SLOW TEST:47.253 seconds]
[sig-apps] Job
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":277,"completed":253,"skipped":4415,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:29:41.645: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8154
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8154
I1217 10:29:41.855811      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8154, replica count: 2
Dec 17 10:29:44.906: INFO: Creating new exec pod
I1217 10:29:44.906511      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 17 10:29:49.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=services-8154 execpodrtqs5 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 17 10:29:50.374: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 17 10:29:50.374: INFO: stdout: ""
Dec 17 10:29:50.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=services-8154 execpodrtqs5 -- /bin/sh -x -c nc -zv -t -w 2 10.108.130.106 80'
Dec 17 10:29:50.649: INFO: stderr: "+ nc -zv -t -w 2 10.108.130.106 80\nConnection to 10.108.130.106 80 port [tcp/http] succeeded!\n"
Dec 17 10:29:50.649: INFO: stdout: ""
Dec 17 10:29:50.649: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:29:50.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8154" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:9.090 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":277,"completed":254,"skipped":4415,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:29:50.735: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7626
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 17 10:29:55.003: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7626 PodName:pod-sharedvolume-1875f8df-1f6a-4c66-9960-e6179e40b5a7 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:29:55.003: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:29:55.169: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:29:55.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7626" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":277,"completed":255,"skipped":4421,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:29:55.185: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Dec 17 10:29:55.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-097c5970-d154-4bb3-8598-6e2d3d21edf1" in namespace "downward-api-8078" to be "Succeeded or Failed"
Dec 17 10:29:55.368: INFO: Pod "downwardapi-volume-097c5970-d154-4bb3-8598-6e2d3d21edf1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.494017ms
Dec 17 10:29:57.376: INFO: Pod "downwardapi-volume-097c5970-d154-4bb3-8598-6e2d3d21edf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021460799s
Dec 17 10:29:59.384: INFO: Pod "downwardapi-volume-097c5970-d154-4bb3-8598-6e2d3d21edf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028791246s
STEP: Saw pod success
Dec 17 10:29:59.384: INFO: Pod "downwardapi-volume-097c5970-d154-4bb3-8598-6e2d3d21edf1" satisfied condition "Succeeded or Failed"
Dec 17 10:29:59.390: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod downwardapi-volume-097c5970-d154-4bb3-8598-6e2d3d21edf1 container client-container: <nil>
STEP: delete the pod
Dec 17 10:29:59.426: INFO: Waiting for pod downwardapi-volume-097c5970-d154-4bb3-8598-6e2d3d21edf1 to disappear
Dec 17 10:29:59.431: INFO: Pod downwardapi-volume-097c5970-d154-4bb3-8598-6e2d3d21edf1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:29:59.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8078" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":256,"skipped":4440,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:29:59.451: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting the proxy server
Dec 17 10:29:59.603: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-588444857 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:29:59.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2411" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":277,"completed":257,"skipped":4459,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:29:59.720: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-405825c5-52bd-400b-9151-80cc14ff7a70
STEP: Creating a pod to test consume configMaps
Dec 17 10:29:59.895: INFO: Waiting up to 5m0s for pod "pod-configmaps-4c41277b-ae43-4aa2-b72a-596fdcb1875a" in namespace "configmap-6653" to be "Succeeded or Failed"
Dec 17 10:29:59.904: INFO: Pod "pod-configmaps-4c41277b-ae43-4aa2-b72a-596fdcb1875a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.340766ms
Dec 17 10:30:01.910: INFO: Pod "pod-configmaps-4c41277b-ae43-4aa2-b72a-596fdcb1875a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01532399s
Dec 17 10:30:03.916: INFO: Pod "pod-configmaps-4c41277b-ae43-4aa2-b72a-596fdcb1875a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021792113s
STEP: Saw pod success
Dec 17 10:30:03.916: INFO: Pod "pod-configmaps-4c41277b-ae43-4aa2-b72a-596fdcb1875a" satisfied condition "Succeeded or Failed"
Dec 17 10:30:03.920: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-configmaps-4c41277b-ae43-4aa2-b72a-596fdcb1875a container configmap-volume-test: <nil>
STEP: delete the pod
Dec 17 10:30:03.958: INFO: Waiting for pod pod-configmaps-4c41277b-ae43-4aa2-b72a-596fdcb1875a to disappear
Dec 17 10:30:03.962: INFO: Pod pod-configmaps-4c41277b-ae43-4aa2-b72a-596fdcb1875a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:30:03.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6653" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":258,"skipped":4459,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:30:03.974: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6055
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service endpoint-test2 in namespace services-6055
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6055 to expose endpoints map[]
Dec 17 10:30:04.172: INFO: Get endpoints failed (8.326538ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 17 10:30:05.178: INFO: successfully validated that service endpoint-test2 in namespace services-6055 exposes endpoints map[] (1.013950623s elapsed)
STEP: Creating pod pod1 in namespace services-6055
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6055 to expose endpoints map[pod1:[80]]
Dec 17 10:30:08.250: INFO: successfully validated that service endpoint-test2 in namespace services-6055 exposes endpoints map[pod1:[80]] (3.053768278s elapsed)
STEP: Creating pod pod2 in namespace services-6055
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6055 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 17 10:30:11.332: INFO: successfully validated that service endpoint-test2 in namespace services-6055 exposes endpoints map[pod1:[80] pod2:[80]] (3.070939604s elapsed)
STEP: Deleting pod pod1 in namespace services-6055
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6055 to expose endpoints map[pod2:[80]]
Dec 17 10:30:11.361: INFO: successfully validated that service endpoint-test2 in namespace services-6055 exposes endpoints map[pod2:[80]] (14.190457ms elapsed)
STEP: Deleting pod pod2 in namespace services-6055
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6055 to expose endpoints map[]
Dec 17 10:30:11.391: INFO: successfully validated that service endpoint-test2 in namespace services-6055 exposes endpoints map[] (12.084581ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:30:11.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6055" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:7.477 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":277,"completed":259,"skipped":4465,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:30:11.455: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-79
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:30:11.616: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 17 10:30:11.631: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 17 10:30:16.637: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 17 10:30:16.637: INFO: Creating deployment "test-rolling-update-deployment"
Dec 17 10:30:16.646: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 17 10:30:16.670: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Dec 17 10:30:18.681: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 17 10:30:18.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797816, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797816, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797816, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743797816, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-59d5cb45c7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 17 10:30:20.692: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Dec 17 10:30:20.705: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-79 /apis/apps/v1/namespaces/deployment-79/deployments/test-rolling-update-deployment 31301e67-5a34-4eba-8807-b6434a6ac2d7 30153 1 2020-12-17 10:30:16 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-12-17 10:30:16 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-12-17 10:30:18 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0025b54a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-12-17 10:30:16 +0000 UTC,LastTransitionTime:2020-12-17 10:30:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-59d5cb45c7" has successfully progressed.,LastUpdateTime:2020-12-17 10:30:18 +0000 UTC,LastTransitionTime:2020-12-17 10:30:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 17 10:30:20.709: INFO: New ReplicaSet "test-rolling-update-deployment-59d5cb45c7" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7  deployment-79 /apis/apps/v1/namespaces/deployment-79/replicasets/test-rolling-update-deployment-59d5cb45c7 80582791-ad93-4d43-b8c5-43b802039158 30143 1 2020-12-17 10:30:16 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 31301e67-5a34-4eba-8807-b6434a6ac2d7 0xc0025b5ab7 0xc0025b5ab8}] []  [{kube-controller-manager Update apps/v1 2020-12-17 10:30:18 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 49 51 48 49 101 54 55 45 53 97 51 52 45 52 101 98 97 45 56 56 48 55 45 98 54 52 51 52 97 54 97 99 50 100 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 59d5cb45c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0025b5b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 17 10:30:20.709: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 17 10:30:20.709: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-79 /apis/apps/v1/namespaces/deployment-79/replicasets/test-rolling-update-controller 0d9d7b60-a56d-4413-bf60-d4f4fba071f2 30152 2 2020-12-17 10:30:11 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 31301e67-5a34-4eba-8807-b6434a6ac2d7 0xc0025b5997 0xc0025b5998}] []  [{e2e.test Update apps/v1 2020-12-17 10:30:11 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-12-17 10:30:18 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 49 51 48 49 101 54 55 45 53 97 51 52 45 52 101 98 97 45 56 56 48 55 45 98 54 52 51 52 97 54 97 99 50 100 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0025b5a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 17 10:30:20.714: INFO: Pod "test-rolling-update-deployment-59d5cb45c7-7p2ks" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7-7p2ks test-rolling-update-deployment-59d5cb45c7- deployment-79 /api/v1/namespaces/deployment-79/pods/test-rolling-update-deployment-59d5cb45c7-7p2ks 85055687-6ce9-4a93-8955-ea0eceb3f810 30142 0 2020-12-17 10:30:16 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[cni.projectcalico.org/podIP:192.168.192.86/32 cni.projectcalico.org/podIPs:192.168.192.86/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-59d5cb45c7 80582791-ad93-4d43-b8c5-43b802039158 0xc002096047 0xc002096048}] []  [{kube-controller-manager Update v1 2020-12-17 10:30:16 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 56 48 53 56 50 55 57 49 45 97 100 57 51 45 52 100 52 51 45 98 56 99 53 45 52 51 98 56 48 50 48 51 57 49 53 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-12-17 10:30:17 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 44 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 115 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-12-17 10:30:18 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 57 50 46 49 54 56 46 49 57 50 46 56 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tq5fn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tq5fn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tq5fn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck8s-conftest-118-workload-cluster-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 10:30:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 10:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 10:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-12-17 10:30:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.0.10.205,PodIP:192.168.192.86,StartTime:2020-12-17 10:30:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-12-17 10:30:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://424ce2698db88272fb2305fec1c44807b36da59ed0a4019f762efe484576fdb1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.192.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:30:20.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-79" for this suite.

• [SLOW TEST:9.275 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":260,"skipped":4507,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:30:20.732: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1275
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Dec 17 10:30:20.907: INFO: Created pod &Pod{ObjectMeta:{dns-1275  dns-1275 /api/v1/namespaces/dns-1275/pods/dns-1275 309da91c-a85d-4d85-bffb-78395600b417 30168 0 2020-12-17 10:30:20 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2020-12-17 10:30:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 67 111 110 102 105 103 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 115 101 114 118 101 114 115 34 58 123 125 44 34 102 58 115 101 97 114 99 104 101 115 34 58 123 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bhfr7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bhfr7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bhfr7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 17 10:30:20.917: INFO: The status of Pod dns-1275 is Pending, waiting for it to be Running (with Ready = true)
Dec 17 10:30:22.923: INFO: The status of Pod dns-1275 is Pending, waiting for it to be Running (with Ready = true)
Dec 17 10:30:24.924: INFO: The status of Pod dns-1275 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Dec 17 10:30:24.924: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1275 PodName:dns-1275 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:30:24.924: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Verifying customized DNS server is configured on pod...
Dec 17 10:30:25.096: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1275 PodName:dns-1275 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 17 10:30:25.096: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
Dec 17 10:30:25.279: INFO: Deleting pod dns-1275...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:30:25.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1275" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":277,"completed":261,"skipped":4519,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:30:25.339: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 17 10:30:25.511: INFO: Waiting up to 5m0s for pod "pod-ffa7619a-bec0-46a7-9365-648293f97d57" in namespace "emptydir-3493" to be "Succeeded or Failed"
Dec 17 10:30:25.520: INFO: Pod "pod-ffa7619a-bec0-46a7-9365-648293f97d57": Phase="Pending", Reason="", readiness=false. Elapsed: 8.645022ms
Dec 17 10:30:27.527: INFO: Pod "pod-ffa7619a-bec0-46a7-9365-648293f97d57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015238556s
Dec 17 10:30:29.534: INFO: Pod "pod-ffa7619a-bec0-46a7-9365-648293f97d57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02220935s
STEP: Saw pod success
Dec 17 10:30:29.534: INFO: Pod "pod-ffa7619a-bec0-46a7-9365-648293f97d57" satisfied condition "Succeeded or Failed"
Dec 17 10:30:29.539: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-1 pod pod-ffa7619a-bec0-46a7-9365-648293f97d57 container test-container: <nil>
STEP: delete the pod
Dec 17 10:30:29.587: INFO: Waiting for pod pod-ffa7619a-bec0-46a7-9365-648293f97d57 to disappear
Dec 17 10:30:29.591: INFO: Pod pod-ffa7619a-bec0-46a7-9365-648293f97d57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:30:29.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3493" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":262,"skipped":4537,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:30:29.606: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2356
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 17 10:30:34.294: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2356 pod-service-account-e350c314-1896-4802-8d07-176511ba08f2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 17 10:30:34.595: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2356 pod-service-account-e350c314-1896-4802-8d07-176511ba08f2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 17 10:30:34.893: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2356 pod-service-account-e350c314-1896-4802-8d07-176511ba08f2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:30:35.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2356" for this suite.

• [SLOW TEST:5.610 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":277,"completed":263,"skipped":4544,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:30:35.216: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5205
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1418
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 17 10:30:35.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5205'
Dec 17 10:30:35.491: INFO: stderr: ""
Dec 17 10:30:35.491: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1423
Dec 17 10:30:35.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 delete pods e2e-test-httpd-pod --namespace=kubectl-5205'
Dec 17 10:30:51.593: INFO: stderr: ""
Dec 17 10:30:51.593: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:30:51.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5205" for this suite.

• [SLOW TEST:16.387 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":277,"completed":264,"skipped":4545,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:30:51.603: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8281
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8281.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8281.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8281.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8281.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8281.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8281.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 17 10:30:55.860: INFO: DNS probes using dns-8281/dns-test-eeb8cf84-7e0d-4b2d-84c5-5d58c1aff041 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:30:55.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8281" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":277,"completed":265,"skipped":4550,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:30:55.950: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-999
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:30:56.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-999" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":277,"completed":266,"skipped":4600,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:30:56.217: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8818
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with configMap that has name projected-configmap-test-upd-39591853-6390-44ef-97ba-1582988f0b1c
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-39591853-6390-44ef-97ba-1582988f0b1c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:32:06.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8818" for this suite.

• [SLOW TEST:70.789 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":267,"skipped":4615,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:32:07.006: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3676
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-271ea9cf-04f2-411d-a0f9-a7e277b97b3c
STEP: Creating a pod to test consume secrets
Dec 17 10:32:07.182: INFO: Waiting up to 5m0s for pod "pod-secrets-8331fb82-bae9-496c-8f52-4fe05e0bf231" in namespace "secrets-3676" to be "Succeeded or Failed"
Dec 17 10:32:07.202: INFO: Pod "pod-secrets-8331fb82-bae9-496c-8f52-4fe05e0bf231": Phase="Pending", Reason="", readiness=false. Elapsed: 19.663979ms
Dec 17 10:32:09.209: INFO: Pod "pod-secrets-8331fb82-bae9-496c-8f52-4fe05e0bf231": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02681705s
Dec 17 10:32:11.215: INFO: Pod "pod-secrets-8331fb82-bae9-496c-8f52-4fe05e0bf231": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032457048s
STEP: Saw pod success
Dec 17 10:32:11.215: INFO: Pod "pod-secrets-8331fb82-bae9-496c-8f52-4fe05e0bf231" satisfied condition "Succeeded or Failed"
Dec 17 10:32:11.219: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-secrets-8331fb82-bae9-496c-8f52-4fe05e0bf231 container secret-volume-test: <nil>
STEP: delete the pod
Dec 17 10:32:11.257: INFO: Waiting for pod pod-secrets-8331fb82-bae9-496c-8f52-4fe05e0bf231 to disappear
Dec 17 10:32:11.269: INFO: Pod pod-secrets-8331fb82-bae9-496c-8f52-4fe05e0bf231 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:32:11.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3676" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":268,"skipped":4616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:32:11.288: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1768
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service nodeport-test with type=NodePort in namespace services-1768
STEP: creating replication controller nodeport-test in namespace services-1768
I1217 10:32:11.525378      24 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-1768, replica count: 2
I1217 10:32:14.576401      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 17 10:32:14.576: INFO: Creating new exec pod
Dec 17 10:32:19.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=services-1768 execpod4mn7l -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec 17 10:32:19.943: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 17 10:32:19.944: INFO: stdout: ""
Dec 17 10:32:19.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=services-1768 execpod4mn7l -- /bin/sh -x -c nc -zv -t -w 2 10.108.41.134 80'
Dec 17 10:32:20.233: INFO: stderr: "+ nc -zv -t -w 2 10.108.41.134 80\nConnection to 10.108.41.134 80 port [tcp/http] succeeded!\n"
Dec 17 10:32:20.233: INFO: stdout: ""
Dec 17 10:32:20.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=services-1768 execpod4mn7l -- /bin/sh -x -c nc -zv -t -w 2 172.0.10.205 31718'
Dec 17 10:32:20.528: INFO: stderr: "+ nc -zv -t -w 2 172.0.10.205 31718\nConnection to 172.0.10.205 31718 port [tcp/31718] succeeded!\n"
Dec 17 10:32:20.528: INFO: stdout: ""
Dec 17 10:32:20.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 exec --namespace=services-1768 execpod4mn7l -- /bin/sh -x -c nc -zv -t -w 2 172.0.10.72 31718'
Dec 17 10:32:20.845: INFO: stderr: "+ nc -zv -t -w 2 172.0.10.72 31718\nConnection to 172.0.10.72 31718 port [tcp/31718] succeeded!\n"
Dec 17 10:32:20.845: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:32:20.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1768" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

• [SLOW TEST:9.570 seconds]
[sig-network] Services
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":277,"completed":269,"skipped":4665,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:32:20.859: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5785
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:32:21.010: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 17 10:32:24.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-5785 create -f -'
Dec 17 10:32:25.123: INFO: stderr: ""
Dec 17 10:32:25.123: INFO: stdout: "e2e-test-crd-publish-openapi-132-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 17 10:32:25.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-5785 delete e2e-test-crd-publish-openapi-132-crds test-foo'
Dec 17 10:32:25.248: INFO: stderr: ""
Dec 17 10:32:25.248: INFO: stdout: "e2e-test-crd-publish-openapi-132-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 17 10:32:25.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-5785 apply -f -'
Dec 17 10:32:25.783: INFO: stderr: ""
Dec 17 10:32:25.783: INFO: stdout: "e2e-test-crd-publish-openapi-132-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 17 10:32:25.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-5785 delete e2e-test-crd-publish-openapi-132-crds test-foo'
Dec 17 10:32:25.909: INFO: stderr: ""
Dec 17 10:32:25.909: INFO: stdout: "e2e-test-crd-publish-openapi-132-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 17 10:32:25.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-5785 create -f -'
Dec 17 10:32:26.103: INFO: rc: 1
Dec 17 10:32:26.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-5785 apply -f -'
Dec 17 10:32:26.319: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 17 10:32:26.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-5785 create -f -'
Dec 17 10:32:26.525: INFO: rc: 1
Dec 17 10:32:26.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 --namespace=crd-publish-openapi-5785 apply -f -'
Dec 17 10:32:26.725: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 17 10:32:26.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 explain e2e-test-crd-publish-openapi-132-crds'
Dec 17 10:32:26.937: INFO: stderr: ""
Dec 17 10:32:26.937: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-132-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 17 10:32:26.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 explain e2e-test-crd-publish-openapi-132-crds.metadata'
Dec 17 10:32:27.153: INFO: stderr: ""
Dec 17 10:32:27.153: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-132-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 17 10:32:27.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 explain e2e-test-crd-publish-openapi-132-crds.spec'
Dec 17 10:32:27.385: INFO: stderr: ""
Dec 17 10:32:27.385: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-132-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 17 10:32:27.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 explain e2e-test-crd-publish-openapi-132-crds.spec.bars'
Dec 17 10:32:27.605: INFO: stderr: ""
Dec 17 10:32:27.605: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-132-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 17 10:32:27.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-588444857 explain e2e-test-crd-publish-openapi-132-crds.spec.bars2'
Dec 17 10:32:27.821: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:32:31.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5785" for this suite.

• [SLOW TEST:10.680 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":277,"completed":270,"skipped":4668,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:32:31.539: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5956
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-59cda042-0498-426e-937d-6eed7fb85d4d-2938
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:32:32.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5956" for this suite.
STEP: Destroying namespace "nspatchtest-59cda042-0498-426e-937d-6eed7fb85d4d-2938" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":277,"completed":271,"skipped":4677,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:32:32.039: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:32:32.229: INFO: Create a RollingUpdate DaemonSet
Dec 17 10:32:32.242: INFO: Check that daemon pods launch on every node of the cluster
Dec 17 10:32:32.250: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:32:32.255: INFO: Number of nodes with available pods: 0
Dec 17 10:32:32.255: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 10:32:33.278: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:32:33.285: INFO: Number of nodes with available pods: 0
Dec 17 10:32:33.285: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 10:32:34.263: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:32:34.269: INFO: Number of nodes with available pods: 0
Dec 17 10:32:34.269: INFO: Node ck8s-conftest-118-workload-cluster-worker-0 is running more than one daemon pod
Dec 17 10:32:35.262: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:32:35.271: INFO: Number of nodes with available pods: 2
Dec 17 10:32:35.271: INFO: Number of running nodes: 2, number of available pods: 2
Dec 17 10:32:35.271: INFO: Update the DaemonSet to trigger a rollout
Dec 17 10:32:35.283: INFO: Updating DaemonSet daemon-set
Dec 17 10:32:39.312: INFO: Roll back the DaemonSet before rollout is complete
Dec 17 10:32:39.321: INFO: Updating DaemonSet daemon-set
Dec 17 10:32:39.322: INFO: Make sure DaemonSet rollback is complete
Dec 17 10:32:39.326: INFO: Wrong image for pod: daemon-set-8l2zg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 17 10:32:39.326: INFO: Pod daemon-set-8l2zg is not available
Dec 17 10:32:39.334: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:32:40.344: INFO: Wrong image for pod: daemon-set-8l2zg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 17 10:32:40.344: INFO: Pod daemon-set-8l2zg is not available
Dec 17 10:32:40.349: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:32:41.340: INFO: Wrong image for pod: daemon-set-8l2zg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 17 10:32:41.340: INFO: Pod daemon-set-8l2zg is not available
Dec 17 10:32:41.346: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec 17 10:32:42.342: INFO: Pod daemon-set-m6xrk is not available
Dec 17 10:32:42.349: INFO: DaemonSet pods can't tolerate node ck8s-conftest-118-workload-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8125, will wait for the garbage collector to delete the pods
Dec 17 10:32:42.424: INFO: Deleting DaemonSet.extensions daemon-set took: 11.163229ms
Dec 17 10:32:42.524: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.369116ms
Dec 17 10:32:53.229: INFO: Number of nodes with available pods: 0
Dec 17 10:32:53.229: INFO: Number of running nodes: 0, number of available pods: 0
Dec 17 10:32:53.232: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8125/daemonsets","resourceVersion":"31088"},"items":null}

Dec 17 10:32:53.235: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8125/pods","resourceVersion":"31088"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:32:53.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8125" for this suite.

• [SLOW TEST:21.218 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":277,"completed":272,"skipped":4685,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:32:53.258: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-downwardapi-js79
STEP: Creating a pod to test atomic-volume-subpath
Dec 17 10:32:53.424: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-js79" in namespace "subpath-7046" to be "Succeeded or Failed"
Dec 17 10:32:53.428: INFO: Pod "pod-subpath-test-downwardapi-js79": Phase="Pending", Reason="", readiness=false. Elapsed: 3.66714ms
Dec 17 10:32:55.434: INFO: Pod "pod-subpath-test-downwardapi-js79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009684222s
Dec 17 10:32:57.441: INFO: Pod "pod-subpath-test-downwardapi-js79": Phase="Running", Reason="", readiness=true. Elapsed: 4.016848912s
Dec 17 10:32:59.448: INFO: Pod "pod-subpath-test-downwardapi-js79": Phase="Running", Reason="", readiness=true. Elapsed: 6.024157318s
Dec 17 10:33:01.453: INFO: Pod "pod-subpath-test-downwardapi-js79": Phase="Running", Reason="", readiness=true. Elapsed: 8.029201897s
Dec 17 10:33:03.458: INFO: Pod "pod-subpath-test-downwardapi-js79": Phase="Running", Reason="", readiness=true. Elapsed: 10.033661674s
Dec 17 10:33:05.464: INFO: Pod "pod-subpath-test-downwardapi-js79": Phase="Running", Reason="", readiness=true. Elapsed: 12.039959164s
Dec 17 10:33:07.471: INFO: Pod "pod-subpath-test-downwardapi-js79": Phase="Running", Reason="", readiness=true. Elapsed: 14.046827589s
Dec 17 10:33:09.477: INFO: Pod "pod-subpath-test-downwardapi-js79": Phase="Running", Reason="", readiness=true. Elapsed: 16.053161409s
Dec 17 10:33:11.484: INFO: Pod "pod-subpath-test-downwardapi-js79": Phase="Running", Reason="", readiness=true. Elapsed: 18.059944452s
Dec 17 10:33:13.490: INFO: Pod "pod-subpath-test-downwardapi-js79": Phase="Running", Reason="", readiness=true. Elapsed: 20.066130526s
Dec 17 10:33:15.495: INFO: Pod "pod-subpath-test-downwardapi-js79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.070822419s
STEP: Saw pod success
Dec 17 10:33:15.495: INFO: Pod "pod-subpath-test-downwardapi-js79" satisfied condition "Succeeded or Failed"
Dec 17 10:33:15.500: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-subpath-test-downwardapi-js79 container test-container-subpath-downwardapi-js79: <nil>
STEP: delete the pod
Dec 17 10:33:15.534: INFO: Waiting for pod pod-subpath-test-downwardapi-js79 to disappear
Dec 17 10:33:15.538: INFO: Pod pod-subpath-test-downwardapi-js79 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-js79
Dec 17 10:33:15.538: INFO: Deleting pod "pod-subpath-test-downwardapi-js79" in namespace "subpath-7046"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:33:15.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7046" for this suite.

• [SLOW TEST:22.298 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":277,"completed":273,"skipped":4685,"failed":0}
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:33:15.556: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6200
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Dec 17 10:33:15.728: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-dc79c656-8b19-45a2-933f-e039bb319688" in namespace "security-context-test-6200" to be "Succeeded or Failed"
Dec 17 10:33:15.737: INFO: Pod "busybox-privileged-false-dc79c656-8b19-45a2-933f-e039bb319688": Phase="Pending", Reason="", readiness=false. Elapsed: 9.000228ms
Dec 17 10:33:17.743: INFO: Pod "busybox-privileged-false-dc79c656-8b19-45a2-933f-e039bb319688": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015347367s
Dec 17 10:33:19.750: INFO: Pod "busybox-privileged-false-dc79c656-8b19-45a2-933f-e039bb319688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021699985s
Dec 17 10:33:19.750: INFO: Pod "busybox-privileged-false-dc79c656-8b19-45a2-933f-e039bb319688" satisfied condition "Succeeded or Failed"
Dec 17 10:33:19.761: INFO: Got logs for pod "busybox-privileged-false-dc79c656-8b19-45a2-933f-e039bb319688": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:33:19.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6200" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":274,"skipped":4690,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:33:19.779: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5568
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 17 10:33:20.455: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 17 10:33:22.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743798000, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743798000, loc:(*time.Location)(0x7b675e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63743798000, loc:(*time.Location)(0x7b675e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63743798000, loc:(*time.Location)(0x7b675e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 17 10:33:25.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:33:25.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5568" for this suite.
STEP: Destroying namespace "webhook-5568-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.041 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":277,"completed":275,"skipped":4697,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:33:25.820: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2530
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 17 10:33:26.007: INFO: Waiting up to 5m0s for pod "pod-8f8d06c4-6772-438c-b121-c033e2a643e7" in namespace "emptydir-2530" to be "Succeeded or Failed"
Dec 17 10:33:26.042: INFO: Pod "pod-8f8d06c4-6772-438c-b121-c033e2a643e7": Phase="Pending", Reason="", readiness=false. Elapsed: 34.436845ms
Dec 17 10:33:28.052: INFO: Pod "pod-8f8d06c4-6772-438c-b121-c033e2a643e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044999793s
Dec 17 10:33:30.058: INFO: Pod "pod-8f8d06c4-6772-438c-b121-c033e2a643e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051124469s
STEP: Saw pod success
Dec 17 10:33:30.058: INFO: Pod "pod-8f8d06c4-6772-438c-b121-c033e2a643e7" satisfied condition "Succeeded or Failed"
Dec 17 10:33:30.062: INFO: Trying to get logs from node ck8s-conftest-118-workload-cluster-worker-0 pod pod-8f8d06c4-6772-438c-b121-c033e2a643e7 container test-container: <nil>
STEP: delete the pod
Dec 17 10:33:30.098: INFO: Waiting for pod pod-8f8d06c4-6772-438c-b121-c033e2a643e7 to disappear
Dec 17 10:33:30.103: INFO: Pod pod-8f8d06c4-6772-438c-b121-c033e2a643e7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:33:30.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2530" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":276,"skipped":4708,"failed":0}

------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Dec 17 10:33:30.116: INFO: >>> kubeConfig: /tmp/kubeconfig-588444857
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8283
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-kvfrd in namespace proxy-8283
I1217 10:33:30.291635      24 runners.go:190] Created replication controller with name: proxy-service-kvfrd, namespace: proxy-8283, replica count: 1
I1217 10:33:31.342643      24 runners.go:190] proxy-service-kvfrd Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1217 10:33:32.343268      24 runners.go:190] proxy-service-kvfrd Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1217 10:33:33.343942      24 runners.go:190] proxy-service-kvfrd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1217 10:33:34.344356      24 runners.go:190] proxy-service-kvfrd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1217 10:33:35.344709      24 runners.go:190] proxy-service-kvfrd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1217 10:33:36.345314      24 runners.go:190] proxy-service-kvfrd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1217 10:33:37.345649      24 runners.go:190] proxy-service-kvfrd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1217 10:33:38.346135      24 runners.go:190] proxy-service-kvfrd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1217 10:33:39.346571      24 runners.go:190] proxy-service-kvfrd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1217 10:33:40.346994      24 runners.go:190] proxy-service-kvfrd Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1217 10:33:41.347434      24 runners.go:190] proxy-service-kvfrd Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 17 10:33:41.352: INFO: setup took 11.083165179s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 17 10:33:41.377: INFO: (0) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 25.074269ms)
Dec 17 10:33:41.378: INFO: (0) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 25.330777ms)
Dec 17 10:33:41.378: INFO: (0) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 25.838253ms)
Dec 17 10:33:41.379: INFO: (0) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 26.571965ms)
Dec 17 10:33:41.379: INFO: (0) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 26.704482ms)
Dec 17 10:33:41.379: INFO: (0) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 26.641193ms)
Dec 17 10:33:41.379: INFO: (0) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 26.901462ms)
Dec 17 10:33:41.379: INFO: (0) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 26.356676ms)
Dec 17 10:33:41.379: INFO: (0) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 26.636597ms)
Dec 17 10:33:41.379: INFO: (0) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 26.954469ms)
Dec 17 10:33:41.383: INFO: (0) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 31.015453ms)
Dec 17 10:33:41.383: INFO: (0) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 31.273579ms)
Dec 17 10:33:41.384: INFO: (0) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 31.422639ms)
Dec 17 10:33:41.384: INFO: (0) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 31.575275ms)
Dec 17 10:33:41.384: INFO: (0) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 32.186779ms)
Dec 17 10:33:41.387: INFO: (0) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 34.278634ms)
Dec 17 10:33:41.412: INFO: (1) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 24.94565ms)
Dec 17 10:33:41.412: INFO: (1) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 24.493302ms)
Dec 17 10:33:41.413: INFO: (1) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 25.573986ms)
Dec 17 10:33:41.413: INFO: (1) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 25.679793ms)
Dec 17 10:33:41.413: INFO: (1) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 25.722977ms)
Dec 17 10:33:41.415: INFO: (1) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 28.019426ms)
Dec 17 10:33:41.417: INFO: (1) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 29.819195ms)
Dec 17 10:33:41.419: INFO: (1) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 31.397987ms)
Dec 17 10:33:41.419: INFO: (1) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 31.341748ms)
Dec 17 10:33:41.420: INFO: (1) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 33.125939ms)
Dec 17 10:33:41.420: INFO: (1) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 32.942902ms)
Dec 17 10:33:41.421: INFO: (1) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 33.314002ms)
Dec 17 10:33:41.421: INFO: (1) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 33.616645ms)
Dec 17 10:33:41.421: INFO: (1) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 33.839577ms)
Dec 17 10:33:41.421: INFO: (1) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 33.935579ms)
Dec 17 10:33:41.421: INFO: (1) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 33.974744ms)
Dec 17 10:33:41.443: INFO: (2) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 21.870539ms)
Dec 17 10:33:41.443: INFO: (2) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 20.928114ms)
Dec 17 10:33:41.444: INFO: (2) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 21.907746ms)
Dec 17 10:33:41.444: INFO: (2) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 22.42562ms)
Dec 17 10:33:41.444: INFO: (2) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 22.566305ms)
Dec 17 10:33:41.444: INFO: (2) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 22.519043ms)
Dec 17 10:33:41.445: INFO: (2) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 22.78144ms)
Dec 17 10:33:41.444: INFO: (2) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 22.843504ms)
Dec 17 10:33:41.445: INFO: (2) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 22.61238ms)
Dec 17 10:33:41.445: INFO: (2) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 22.89229ms)
Dec 17 10:33:41.445: INFO: (2) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 22.073914ms)
Dec 17 10:33:41.445: INFO: (2) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 22.326666ms)
Dec 17 10:33:41.445: INFO: (2) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 22.243815ms)
Dec 17 10:33:41.445: INFO: (2) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 22.086438ms)
Dec 17 10:33:41.445: INFO: (2) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 22.559333ms)
Dec 17 10:33:41.446: INFO: (2) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 23.710042ms)
Dec 17 10:33:41.460: INFO: (3) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 13.827248ms)
Dec 17 10:33:41.460: INFO: (3) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 14.015854ms)
Dec 17 10:33:41.460: INFO: (3) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 14.374308ms)
Dec 17 10:33:41.460: INFO: (3) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 14.273115ms)
Dec 17 10:33:41.460: INFO: (3) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 14.119318ms)
Dec 17 10:33:41.460: INFO: (3) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 14.04725ms)
Dec 17 10:33:41.462: INFO: (3) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 15.991623ms)
Dec 17 10:33:41.462: INFO: (3) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 15.808057ms)
Dec 17 10:33:41.463: INFO: (3) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 16.447217ms)
Dec 17 10:33:41.466: INFO: (3) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 19.206828ms)
Dec 17 10:33:41.466: INFO: (3) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 19.67444ms)
Dec 17 10:33:41.466: INFO: (3) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 19.735358ms)
Dec 17 10:33:41.466: INFO: (3) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 19.76456ms)
Dec 17 10:33:41.469: INFO: (3) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 22.598891ms)
Dec 17 10:33:41.469: INFO: (3) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 22.543914ms)
Dec 17 10:33:41.469: INFO: (3) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 22.527362ms)
Dec 17 10:33:41.484: INFO: (4) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 14.225641ms)
Dec 17 10:33:41.488: INFO: (4) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 17.802793ms)
Dec 17 10:33:41.488: INFO: (4) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 18.336026ms)
Dec 17 10:33:41.488: INFO: (4) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 18.302377ms)
Dec 17 10:33:41.488: INFO: (4) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 18.268904ms)
Dec 17 10:33:41.488: INFO: (4) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 18.509183ms)
Dec 17 10:33:41.488: INFO: (4) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 18.171663ms)
Dec 17 10:33:41.488: INFO: (4) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 18.824756ms)
Dec 17 10:33:41.488: INFO: (4) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 18.592044ms)
Dec 17 10:33:41.489: INFO: (4) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 18.607567ms)
Dec 17 10:33:41.489: INFO: (4) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 19.567102ms)
Dec 17 10:33:41.489: INFO: (4) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 20.027098ms)
Dec 17 10:33:41.489: INFO: (4) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 19.589486ms)
Dec 17 10:33:41.489: INFO: (4) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 19.695424ms)
Dec 17 10:33:41.489: INFO: (4) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 19.505855ms)
Dec 17 10:33:41.490: INFO: (4) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 19.859513ms)
Dec 17 10:33:41.504: INFO: (5) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 14.358106ms)
Dec 17 10:33:41.505: INFO: (5) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 14.136176ms)
Dec 17 10:33:41.505: INFO: (5) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 14.690744ms)
Dec 17 10:33:41.505: INFO: (5) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 13.919314ms)
Dec 17 10:33:41.505: INFO: (5) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 14.190222ms)
Dec 17 10:33:41.508: INFO: (5) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 17.549329ms)
Dec 17 10:33:41.509: INFO: (5) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 19.062571ms)
Dec 17 10:33:41.509: INFO: (5) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 18.472943ms)
Dec 17 10:33:41.509: INFO: (5) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 18.318834ms)
Dec 17 10:33:41.511: INFO: (5) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 21.396485ms)
Dec 17 10:33:41.514: INFO: (5) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 23.03929ms)
Dec 17 10:33:41.514: INFO: (5) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 23.565103ms)
Dec 17 10:33:41.515: INFO: (5) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 24.122384ms)
Dec 17 10:33:41.517: INFO: (5) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 27.049149ms)
Dec 17 10:33:41.517: INFO: (5) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 27.318534ms)
Dec 17 10:33:41.517: INFO: (5) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 27.2095ms)
Dec 17 10:33:41.527: INFO: (6) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 8.792692ms)
Dec 17 10:33:41.527: INFO: (6) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 8.920069ms)
Dec 17 10:33:41.532: INFO: (6) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 14.383481ms)
Dec 17 10:33:41.535: INFO: (6) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 17.055436ms)
Dec 17 10:33:41.535: INFO: (6) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 17.420341ms)
Dec 17 10:33:41.536: INFO: (6) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 18.379907ms)
Dec 17 10:33:41.536: INFO: (6) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 18.385329ms)
Dec 17 10:33:41.537: INFO: (6) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 18.683601ms)
Dec 17 10:33:41.537: INFO: (6) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 19.259768ms)
Dec 17 10:33:41.537: INFO: (6) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 19.076759ms)
Dec 17 10:33:41.537: INFO: (6) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 19.408345ms)
Dec 17 10:33:41.537: INFO: (6) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 19.104882ms)
Dec 17 10:33:41.537: INFO: (6) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 19.167113ms)
Dec 17 10:33:41.538: INFO: (6) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 19.686462ms)
Dec 17 10:33:41.538: INFO: (6) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 19.708784ms)
Dec 17 10:33:41.539: INFO: (6) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 21.583923ms)
Dec 17 10:33:41.550: INFO: (7) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 10.236679ms)
Dec 17 10:33:41.551: INFO: (7) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 11.735771ms)
Dec 17 10:33:41.552: INFO: (7) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 11.92917ms)
Dec 17 10:33:41.582: INFO: (7) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 41.679411ms)
Dec 17 10:33:41.583: INFO: (7) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 42.62204ms)
Dec 17 10:33:41.583: INFO: (7) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 42.444498ms)
Dec 17 10:33:41.586: INFO: (7) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 46.633451ms)
Dec 17 10:33:41.587: INFO: (7) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 46.427766ms)
Dec 17 10:33:41.587: INFO: (7) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 46.485819ms)
Dec 17 10:33:41.587: INFO: (7) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 46.694876ms)
Dec 17 10:33:41.602: INFO: (7) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 62.215858ms)
Dec 17 10:33:41.603: INFO: (7) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 63.134422ms)
Dec 17 10:33:41.603: INFO: (7) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 63.066024ms)
Dec 17 10:33:41.603: INFO: (7) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 62.946666ms)
Dec 17 10:33:41.603: INFO: (7) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 62.899723ms)
Dec 17 10:33:41.603: INFO: (7) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 63.373149ms)
Dec 17 10:33:41.670: INFO: (8) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 66.65927ms)
Dec 17 10:33:41.681: INFO: (8) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 76.965565ms)
Dec 17 10:33:41.681: INFO: (8) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 76.946248ms)
Dec 17 10:33:41.682: INFO: (8) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 77.748875ms)
Dec 17 10:33:41.682: INFO: (8) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 78.088744ms)
Dec 17 10:33:41.682: INFO: (8) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 76.921931ms)
Dec 17 10:33:41.683: INFO: (8) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 78.95248ms)
Dec 17 10:33:41.683: INFO: (8) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 79.277231ms)
Dec 17 10:33:41.684: INFO: (8) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 79.53418ms)
Dec 17 10:33:41.684: INFO: (8) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 79.813363ms)
Dec 17 10:33:41.697: INFO: (8) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 93.954339ms)
Dec 17 10:33:41.697: INFO: (8) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 93.659686ms)
Dec 17 10:33:41.699: INFO: (8) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 95.678023ms)
Dec 17 10:33:41.700: INFO: (8) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 96.344554ms)
Dec 17 10:33:41.700: INFO: (8) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 96.666785ms)
Dec 17 10:33:41.701: INFO: (8) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 97.323651ms)
Dec 17 10:33:41.726: INFO: (9) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 25.274252ms)
Dec 17 10:33:41.727: INFO: (9) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 25.956349ms)
Dec 17 10:33:41.728: INFO: (9) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 27.206568ms)
Dec 17 10:33:41.729: INFO: (9) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 27.791477ms)
Dec 17 10:33:41.731: INFO: (9) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 29.923676ms)
Dec 17 10:33:41.731: INFO: (9) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 30.141576ms)
Dec 17 10:33:41.731: INFO: (9) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 30.246383ms)
Dec 17 10:33:41.731: INFO: (9) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 29.905352ms)
Dec 17 10:33:41.733: INFO: (9) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 31.401283ms)
Dec 17 10:33:41.733: INFO: (9) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 31.893436ms)
Dec 17 10:33:41.734: INFO: (9) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 32.60545ms)
Dec 17 10:33:41.747: INFO: (9) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 45.536757ms)
Dec 17 10:33:41.747: INFO: (9) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 46.132742ms)
Dec 17 10:33:41.747: INFO: (9) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 46.377939ms)
Dec 17 10:33:41.747: INFO: (9) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 45.915947ms)
Dec 17 10:33:41.748: INFO: (9) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 46.826685ms)
Dec 17 10:33:41.797: INFO: (10) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 49.150525ms)
Dec 17 10:33:41.798: INFO: (10) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 49.536836ms)
Dec 17 10:33:41.801: INFO: (10) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 52.60218ms)
Dec 17 10:33:41.801: INFO: (10) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 52.241195ms)
Dec 17 10:33:41.802: INFO: (10) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 52.625744ms)
Dec 17 10:33:41.807: INFO: (10) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 57.719395ms)
Dec 17 10:33:41.809: INFO: (10) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 60.105071ms)
Dec 17 10:33:41.816: INFO: (10) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 67.149242ms)
Dec 17 10:33:41.816: INFO: (10) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 66.915609ms)
Dec 17 10:33:41.816: INFO: (10) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 66.831804ms)
Dec 17 10:33:41.816: INFO: (10) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 67.317627ms)
Dec 17 10:33:41.816: INFO: (10) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 67.736657ms)
Dec 17 10:33:41.816: INFO: (10) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 67.199556ms)
Dec 17 10:33:41.817: INFO: (10) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 68.552812ms)
Dec 17 10:33:41.817: INFO: (10) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 68.502257ms)
Dec 17 10:33:41.817: INFO: (10) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 68.381709ms)
Dec 17 10:33:41.833: INFO: (11) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 14.914101ms)
Dec 17 10:33:41.834: INFO: (11) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 16.824809ms)
Dec 17 10:33:41.836: INFO: (11) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 16.90206ms)
Dec 17 10:33:41.836: INFO: (11) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 18.258872ms)
Dec 17 10:33:41.838: INFO: (11) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 20.296294ms)
Dec 17 10:33:41.838: INFO: (11) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 20.448007ms)
Dec 17 10:33:41.838: INFO: (11) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 19.411585ms)
Dec 17 10:33:41.838: INFO: (11) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 20.323905ms)
Dec 17 10:33:41.840: INFO: (11) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 22.310188ms)
Dec 17 10:33:41.840: INFO: (11) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 20.73238ms)
Dec 17 10:33:41.840: INFO: (11) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 20.867435ms)
Dec 17 10:33:41.840: INFO: (11) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 22.337893ms)
Dec 17 10:33:41.841: INFO: (11) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 22.821646ms)
Dec 17 10:33:41.846: INFO: (11) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 27.597128ms)
Dec 17 10:33:41.846: INFO: (11) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 27.70303ms)
Dec 17 10:33:41.846: INFO: (11) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 27.943708ms)
Dec 17 10:33:41.859: INFO: (12) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 12.00595ms)
Dec 17 10:33:41.859: INFO: (12) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 11.644645ms)
Dec 17 10:33:41.859: INFO: (12) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 11.84708ms)
Dec 17 10:33:41.859: INFO: (12) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 11.583217ms)
Dec 17 10:33:41.859: INFO: (12) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 11.794539ms)
Dec 17 10:33:41.861: INFO: (12) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 14.347499ms)
Dec 17 10:33:41.862: INFO: (12) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 15.042987ms)
Dec 17 10:33:41.864: INFO: (12) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 17.077617ms)
Dec 17 10:33:41.865: INFO: (12) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 17.567048ms)
Dec 17 10:33:41.865: INFO: (12) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 17.799589ms)
Dec 17 10:33:41.865: INFO: (12) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 17.916706ms)
Dec 17 10:33:41.865: INFO: (12) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 17.997035ms)
Dec 17 10:33:41.865: INFO: (12) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 17.789116ms)
Dec 17 10:33:41.865: INFO: (12) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 18.728861ms)
Dec 17 10:33:41.865: INFO: (12) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 18.6656ms)
Dec 17 10:33:41.867: INFO: (12) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 20.148516ms)
Dec 17 10:33:41.880: INFO: (13) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 12.031293ms)
Dec 17 10:33:41.880: INFO: (13) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 13.155035ms)
Dec 17 10:33:41.885: INFO: (13) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 17.480674ms)
Dec 17 10:33:41.885: INFO: (13) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 17.211844ms)
Dec 17 10:33:41.885: INFO: (13) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 17.363498ms)
Dec 17 10:33:41.887: INFO: (13) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 19.162009ms)
Dec 17 10:33:41.888: INFO: (13) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 20.106563ms)
Dec 17 10:33:41.888: INFO: (13) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 19.939054ms)
Dec 17 10:33:41.888: INFO: (13) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 20.196263ms)
Dec 17 10:33:41.888: INFO: (13) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 20.731604ms)
Dec 17 10:33:41.893: INFO: (13) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 25.446643ms)
Dec 17 10:33:41.893: INFO: (13) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 25.612398ms)
Dec 17 10:33:41.894: INFO: (13) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 26.62889ms)
Dec 17 10:33:41.895: INFO: (13) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 27.051453ms)
Dec 17 10:33:41.895: INFO: (13) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 27.178202ms)
Dec 17 10:33:41.895: INFO: (13) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 27.485331ms)
Dec 17 10:33:41.905: INFO: (14) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 9.260834ms)
Dec 17 10:33:41.905: INFO: (14) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 9.643006ms)
Dec 17 10:33:41.906: INFO: (14) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 10.066562ms)
Dec 17 10:33:41.906: INFO: (14) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 10.541064ms)
Dec 17 10:33:41.910: INFO: (14) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 14.475157ms)
Dec 17 10:33:41.914: INFO: (14) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 18.478014ms)
Dec 17 10:33:41.914: INFO: (14) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 19.135966ms)
Dec 17 10:33:41.914: INFO: (14) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 18.992927ms)
Dec 17 10:33:41.914: INFO: (14) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 18.902245ms)
Dec 17 10:33:41.915: INFO: (14) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 19.592655ms)
Dec 17 10:33:41.915: INFO: (14) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 19.517086ms)
Dec 17 10:33:41.919: INFO: (14) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 23.679347ms)
Dec 17 10:33:41.920: INFO: (14) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 24.234146ms)
Dec 17 10:33:41.925: INFO: (14) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 29.323548ms)
Dec 17 10:33:41.925: INFO: (14) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 29.479401ms)
Dec 17 10:33:41.925: INFO: (14) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 29.785321ms)
Dec 17 10:33:41.939: INFO: (15) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 12.644716ms)
Dec 17 10:33:41.939: INFO: (15) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 12.712323ms)
Dec 17 10:33:41.939: INFO: (15) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 13.610033ms)
Dec 17 10:33:41.939: INFO: (15) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 13.430343ms)
Dec 17 10:33:41.939: INFO: (15) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 13.471413ms)
Dec 17 10:33:41.940: INFO: (15) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 13.856495ms)
Dec 17 10:33:41.944: INFO: (15) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 18.807774ms)
Dec 17 10:33:41.946: INFO: (15) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 20.835258ms)
Dec 17 10:33:41.947: INFO: (15) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 21.173689ms)
Dec 17 10:33:41.949: INFO: (15) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 22.766739ms)
Dec 17 10:33:41.949: INFO: (15) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 22.732296ms)
Dec 17 10:33:41.949: INFO: (15) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 23.530427ms)
Dec 17 10:33:41.949: INFO: (15) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 23.709298ms)
Dec 17 10:33:41.950: INFO: (15) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 24.527232ms)
Dec 17 10:33:41.950: INFO: (15) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 24.501211ms)
Dec 17 10:33:41.950: INFO: (15) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 24.310032ms)
Dec 17 10:33:41.968: INFO: (16) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 16.933955ms)
Dec 17 10:33:41.968: INFO: (16) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 17.365338ms)
Dec 17 10:33:41.968: INFO: (16) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 17.545796ms)
Dec 17 10:33:41.968: INFO: (16) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 17.659248ms)
Dec 17 10:33:41.974: INFO: (16) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 23.339094ms)
Dec 17 10:33:41.974: INFO: (16) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 23.837794ms)
Dec 17 10:33:41.975: INFO: (16) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 24.269151ms)
Dec 17 10:33:41.975: INFO: (16) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 23.838431ms)
Dec 17 10:33:41.975: INFO: (16) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 23.893517ms)
Dec 17 10:33:41.975: INFO: (16) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 24.624304ms)
Dec 17 10:33:41.975: INFO: (16) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 24.70073ms)
Dec 17 10:33:41.975: INFO: (16) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 24.657972ms)
Dec 17 10:33:41.975: INFO: (16) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 24.936914ms)
Dec 17 10:33:41.976: INFO: (16) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 25.104113ms)
Dec 17 10:33:41.976: INFO: (16) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 25.619214ms)
Dec 17 10:33:41.978: INFO: (16) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 27.854089ms)
Dec 17 10:33:41.989: INFO: (17) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 10.847019ms)
Dec 17 10:33:41.990: INFO: (17) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 11.169146ms)
Dec 17 10:33:41.990: INFO: (17) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 10.654635ms)
Dec 17 10:33:41.990: INFO: (17) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 11.474031ms)
Dec 17 10:33:41.992: INFO: (17) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 12.492463ms)
Dec 17 10:33:41.992: INFO: (17) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 12.631727ms)
Dec 17 10:33:41.995: INFO: (17) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 16.056039ms)
Dec 17 10:33:41.995: INFO: (17) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 16.094743ms)
Dec 17 10:33:41.995: INFO: (17) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 16.383569ms)
Dec 17 10:33:41.996: INFO: (17) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 17.359448ms)
Dec 17 10:33:41.997: INFO: (17) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 18.052438ms)
Dec 17 10:33:41.997: INFO: (17) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 17.593001ms)
Dec 17 10:33:41.997: INFO: (17) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 18.811463ms)
Dec 17 10:33:41.998: INFO: (17) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 18.683427ms)
Dec 17 10:33:41.999: INFO: (17) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 19.45666ms)
Dec 17 10:33:42.006: INFO: (17) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 26.414586ms)
Dec 17 10:33:42.018: INFO: (18) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 12.677944ms)
Dec 17 10:33:42.019: INFO: (18) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 12.660532ms)
Dec 17 10:33:42.019: INFO: (18) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 12.827435ms)
Dec 17 10:33:42.019: INFO: (18) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 12.687445ms)
Dec 17 10:33:42.019: INFO: (18) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 13.324315ms)
Dec 17 10:33:42.020: INFO: (18) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 14.080988ms)
Dec 17 10:33:42.020: INFO: (18) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 13.940976ms)
Dec 17 10:33:42.020: INFO: (18) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 14.10945ms)
Dec 17 10:33:42.020: INFO: (18) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 14.293802ms)
Dec 17 10:33:42.020: INFO: (18) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 14.618521ms)
Dec 17 10:33:42.027: INFO: (18) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 21.464261ms)
Dec 17 10:33:42.028: INFO: (18) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 21.435335ms)
Dec 17 10:33:42.028: INFO: (18) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 22.12151ms)
Dec 17 10:33:42.028: INFO: (18) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 22.111697ms)
Dec 17 10:33:42.029: INFO: (18) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 22.609489ms)
Dec 17 10:33:42.038: INFO: (18) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 32.343539ms)
Dec 17 10:33:42.052: INFO: (19) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname2/proxy/: tls qux (200; 12.667907ms)
Dec 17 10:33:42.052: INFO: (19) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname2/proxy/: bar (200; 13.332369ms)
Dec 17 10:33:42.052: INFO: (19) /api/v1/namespaces/proxy-8283/services/https:proxy-service-kvfrd:tlsportname1/proxy/: tls baz (200; 13.25452ms)
Dec 17 10:33:42.052: INFO: (19) /api/v1/namespaces/proxy-8283/services/proxy-service-kvfrd:portname1/proxy/: foo (200; 13.695576ms)
Dec 17 10:33:42.053: INFO: (19) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname1/proxy/: foo (200; 13.766986ms)
Dec 17 10:33:42.053: INFO: (19) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:1080/proxy/rewriteme">... (200; 13.839241ms)
Dec 17 10:33:42.054: INFO: (19) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:1080/proxy/rewriteme">test<... (200; 15.014635ms)
Dec 17 10:33:42.054: INFO: (19) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:162/proxy/: bar (200; 15.804179ms)
Dec 17 10:33:42.054: INFO: (19) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m:160/proxy/: foo (200; 15.759914ms)
Dec 17 10:33:42.055: INFO: (19) /api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/proxy-service-kvfrd-6827m/proxy/rewriteme">test</a> (200; 15.740419ms)
Dec 17 10:33:42.059: INFO: (19) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/: <a href="/api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:443/proxy/tlsrewritem... (200; 20.166012ms)
Dec 17 10:33:42.060: INFO: (19) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:460/proxy/: tls baz (200; 21.752604ms)
Dec 17 10:33:42.061: INFO: (19) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:160/proxy/: foo (200; 21.547991ms)
Dec 17 10:33:42.062: INFO: (19) /api/v1/namespaces/proxy-8283/pods/http:proxy-service-kvfrd-6827m:162/proxy/: bar (200; 23.470959ms)
Dec 17 10:33:42.063: INFO: (19) /api/v1/namespaces/proxy-8283/pods/https:proxy-service-kvfrd-6827m:462/proxy/: tls qux (200; 23.706794ms)
Dec 17 10:33:42.064: INFO: (19) /api/v1/namespaces/proxy-8283/services/http:proxy-service-kvfrd:portname2/proxy/: bar (200; 24.80602ms)
STEP: deleting ReplicationController proxy-service-kvfrd in namespace proxy-8283, will wait for the garbage collector to delete the pods
Dec 17 10:33:42.130: INFO: Deleting ReplicationController proxy-service-kvfrd took: 10.525051ms
Dec 17 10:33:42.230: INFO: Terminating ReplicationController proxy-service-kvfrd pods took: 100.35487ms
[AfterEach] version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Dec 17 10:33:51.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8283" for this suite.

• [SLOW TEST:21.536 seconds]
[sig-network] Proxy
/workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.18.12-rc.1.2+66318483a9efd9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":277,"completed":277,"skipped":4708,"failed":0}
SSSSSSSSDec 17 10:33:51.653: INFO: Running AfterSuite actions on all nodes
Dec 17 10:33:51.653: INFO: Running AfterSuite actions on node 1
Dec 17 10:33:51.653: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":277,"completed":277,"skipped":4716,"failed":0}

Ran 277 of 4993 Specs in 4531.482 seconds
SUCCESS! -- 277 Passed | 0 Failed | 0 Pending | 4716 Skipped
PASS

Ginkgo ran 1 suite in 1h15m33.46447996s
Test Suite Passed
