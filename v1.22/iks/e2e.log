I0302 19:21:30.148173      21 e2e.go:129] Starting e2e run "2ade9e29-0f64-4727-8ad9-030358750b63" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1646248890 - Will randomize all specs
Will run 346 of 6434 specs

Mar  2 19:21:31.739: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
E0302 19:21:31.740386      21 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar  2 19:21:31.742: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  2 19:21:31.832: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  2 19:21:31.933: INFO: 26 / 26 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  2 19:21:31.933: INFO: expected 13 pod replicas in namespace 'kube-system', 13 are Running and Ready.
Mar  2 19:21:31.933: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  2 19:21:31.965: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar  2 19:21:31.965: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-vpc-block-csi-node' (0 seconds elapsed)
Mar  2 19:21:31.965: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Mar  2 19:21:31.965: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Mar  2 19:21:31.965: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Mar  2 19:21:31.965: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Mar  2 19:21:31.965: INFO: e2e test version: v1.22.7
Mar  2 19:21:31.971: INFO: kube-apiserver version: v1.22.7+IKS
Mar  2 19:21:31.971: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:21:31.985: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:21:31.985: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename daemonsets
W0302 19:21:32.136248      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Mar  2 19:21:32.136: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Mar  2 19:21:32.164: INFO: PSP annotation exists on dry run pod: "ibm-privileged-psp"; assuming PodSecurityPolicy is enabled
W0302 19:21:32.176221      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
W0302 19:21:32.190066      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Mar  2 19:21:32.215: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 19:21:32.486: INFO: Number of nodes with available pods: 0
Mar  2 19:21:32.486: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:33.543: INFO: Number of nodes with available pods: 0
Mar  2 19:21:33.543: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:34.518: INFO: Number of nodes with available pods: 0
Mar  2 19:21:34.518: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:35.521: INFO: Number of nodes with available pods: 0
Mar  2 19:21:35.521: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:36.543: INFO: Number of nodes with available pods: 0
Mar  2 19:21:36.543: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:37.525: INFO: Number of nodes with available pods: 0
Mar  2 19:21:37.525: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:38.530: INFO: Number of nodes with available pods: 0
Mar  2 19:21:38.530: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:39.527: INFO: Number of nodes with available pods: 0
Mar  2 19:21:39.528: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:40.523: INFO: Number of nodes with available pods: 0
Mar  2 19:21:40.523: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:41.518: INFO: Number of nodes with available pods: 0
Mar  2 19:21:41.519: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:42.542: INFO: Number of nodes with available pods: 0
Mar  2 19:21:42.542: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:43.533: INFO: Number of nodes with available pods: 0
Mar  2 19:21:43.533: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:44.524: INFO: Number of nodes with available pods: 0
Mar  2 19:21:44.524: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:21:45.528: INFO: Number of nodes with available pods: 2
Mar  2 19:21:45.528: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 19:21:46.520: INFO: Number of nodes with available pods: 2
Mar  2 19:21:46.520: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 19:21:47.526: INFO: Number of nodes with available pods: 2
Mar  2 19:21:47.526: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 19:21:48.525: INFO: Number of nodes with available pods: 3
Mar  2 19:21:48.525: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  2 19:21:48.624: INFO: Number of nodes with available pods: 2
Mar  2 19:21:48.624: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 19:21:49.693: INFO: Number of nodes with available pods: 2
Mar  2 19:21:49.693: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 19:21:50.658: INFO: Number of nodes with available pods: 3
Mar  2 19:21:50.658: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7867, will wait for the garbage collector to delete the pods
Mar  2 19:21:50.763: INFO: Deleting DaemonSet.extensions daemon-set took: 21.458456ms
Mar  2 19:21:50.863: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.290846ms
Mar  2 19:21:53.688: INFO: Number of nodes with available pods: 0
Mar  2 19:21:53.688: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 19:21:53.702: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17349"},"items":null}

Mar  2 19:21:53.727: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17349"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:21:53.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7867" for this suite.

• [SLOW TEST:21.845 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":1,"skipped":49,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:21:53.831: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:21:54.103: INFO: Creating simple deployment test-new-deployment
Mar  2 19:21:54.221: INFO: deployment "test-new-deployment" doesn't have the required revision set
Mar  2 19:21:56.261: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781845714, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781845714, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781845714, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781845714, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-847dcfb7fb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar  2 19:21:58.405: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2381  d87068f1-dd11-4eee-99bf-8cb5dfd1d501 17390 3 2022-03-02 19:21:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-03-02 19:21:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 19:21:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037dce08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-02 19:21:56 +0000 UTC,LastTransitionTime:2022-03-02 19:21:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2022-03-02 19:21:56 +0000 UTC,LastTransitionTime:2022-03-02 19:21:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 19:21:58.434: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-2381  61a4ff35-fa2d-4808-8055-346e9b789c59 17389 2 2022-03-02 19:21:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d87068f1-dd11-4eee-99bf-8cb5dfd1d501 0xc00395d6f7 0xc00395d6f8}] []  [{kube-controller-manager Update apps/v1 2022-03-02 19:21:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d87068f1-dd11-4eee-99bf-8cb5dfd1d501\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 19:21:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00395d788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 19:21:58.460: INFO: Pod "test-new-deployment-847dcfb7fb-d4wjd" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-d4wjd test-new-deployment-847dcfb7fb- deployment-2381  7f822734-859d-46e8-95d1-c40453c5019e 17380 0 2022-03-02 19:21:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:5b83d6adc5901e9cc013653b6abe9189f4c8f07882e0ade14bec04c306942ceb cni.projectcalico.org/podIP:172.17.100.155/32 cni.projectcalico.org/podIPs:172.17.100.155/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 61a4ff35-fa2d-4808-8055-346e9b789c59 0xc0037dd207 0xc0037dd208}] []  [{kube-controller-manager Update v1 2022-03-02 19:21:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61a4ff35-fa2d-4808-8055-346e9b789c59\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 19:21:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 19:21:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.100.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vsdfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vsdfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:21:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:21:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:21:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:21:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.4,PodIP:172.17.100.155,StartTime:2022-03-02 19:21:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 19:21:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://53beef21d992e744d11dbd2df723752c3266ad48a6d54600a61f0d3986d70a9e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.100.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:21:58.461: INFO: Pod "test-new-deployment-847dcfb7fb-dzxnr" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-dzxnr test-new-deployment-847dcfb7fb- deployment-2381  14dbac78-72cd-465a-8b4e-64684f05b770 17395 0 2022-03-02 19:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 61a4ff35-fa2d-4808-8055-346e9b789c59 0xc0037dd410 0xc0037dd411}] []  [{kube-controller-manager Update v1 2022-03-02 19:21:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61a4ff35-fa2d-4808-8055-346e9b789c59\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5p58w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5p58w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:21:58.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2381" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":2,"skipped":65,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:21:58.506: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:21:58.773: INFO: The status of Pod busybox-host-aliases1b07f1f5-1bf2-43de-8710-2c8cdcd2b31c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:22:00.797: INFO: The status of Pod busybox-host-aliases1b07f1f5-1bf2-43de-8710-2c8cdcd2b31c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:22:02.794: INFO: The status of Pod busybox-host-aliases1b07f1f5-1bf2-43de-8710-2c8cdcd2b31c is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:22:04.797: INFO: The status of Pod busybox-host-aliases1b07f1f5-1bf2-43de-8710-2c8cdcd2b31c is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:22:04.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3596" for this suite.

• [SLOW TEST:6.420 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox Pod with hostAliases
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":3,"skipped":108,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:22:04.926: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3040
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:22:16.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3040" for this suite.

• [SLOW TEST:11.711 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":4,"skipped":108,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:22:16.637: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:22:16.965: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar  2 19:22:16.994: INFO: Number of nodes with available pods: 0
Mar  2 19:22:16.994: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar  2 19:22:17.057: INFO: Number of nodes with available pods: 0
Mar  2 19:22:17.057: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 19:22:18.074: INFO: Number of nodes with available pods: 0
Mar  2 19:22:18.074: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 19:22:19.076: INFO: Number of nodes with available pods: 1
Mar  2 19:22:19.077: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  2 19:22:19.152: INFO: Number of nodes with available pods: 1
Mar  2 19:22:19.152: INFO: Number of running nodes: 0, number of available pods: 1
Mar  2 19:22:20.171: INFO: Number of nodes with available pods: 0
Mar  2 19:22:20.171: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  2 19:22:20.209: INFO: Number of nodes with available pods: 0
Mar  2 19:22:20.209: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 19:22:21.225: INFO: Number of nodes with available pods: 0
Mar  2 19:22:21.225: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 19:22:22.228: INFO: Number of nodes with available pods: 0
Mar  2 19:22:22.228: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 19:22:23.226: INFO: Number of nodes with available pods: 0
Mar  2 19:22:23.226: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 19:22:24.229: INFO: Number of nodes with available pods: 1
Mar  2 19:22:24.229: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7064, will wait for the garbage collector to delete the pods
Mar  2 19:22:24.336: INFO: Deleting DaemonSet.extensions daemon-set took: 22.040507ms
Mar  2 19:22:24.437: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.078054ms
Mar  2 19:22:26.757: INFO: Number of nodes with available pods: 0
Mar  2 19:22:26.757: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 19:22:26.768: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17657"},"items":null}

Mar  2 19:22:26.780: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17657"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:22:26.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7064" for this suite.

• [SLOW TEST:10.294 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":5,"skipped":113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:22:26.933: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2571
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Mar  2 19:22:27.209: INFO: Waiting up to 5m0s for pod "test-pod-069951c7-9624-4cd1-9dea-ce0f83872785" in namespace "svcaccounts-2571" to be "Succeeded or Failed"
Mar  2 19:22:27.219: INFO: Pod "test-pod-069951c7-9624-4cd1-9dea-ce0f83872785": Phase="Pending", Reason="", readiness=false. Elapsed: 9.939551ms
Mar  2 19:22:29.241: INFO: Pod "test-pod-069951c7-9624-4cd1-9dea-ce0f83872785": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031707072s
Mar  2 19:22:31.265: INFO: Pod "test-pod-069951c7-9624-4cd1-9dea-ce0f83872785": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05558457s
Mar  2 19:22:33.288: INFO: Pod "test-pod-069951c7-9624-4cd1-9dea-ce0f83872785": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078614007s
Mar  2 19:22:35.301: INFO: Pod "test-pod-069951c7-9624-4cd1-9dea-ce0f83872785": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09158841s
Mar  2 19:22:37.337: INFO: Pod "test-pod-069951c7-9624-4cd1-9dea-ce0f83872785": Phase="Pending", Reason="", readiness=false. Elapsed: 10.127523673s
Mar  2 19:22:39.369: INFO: Pod "test-pod-069951c7-9624-4cd1-9dea-ce0f83872785": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.159183872s
STEP: Saw pod success
Mar  2 19:22:39.369: INFO: Pod "test-pod-069951c7-9624-4cd1-9dea-ce0f83872785" satisfied condition "Succeeded or Failed"
Mar  2 19:22:39.380: INFO: Trying to get logs from node 10.245.0.5 pod test-pod-069951c7-9624-4cd1-9dea-ce0f83872785 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 19:22:39.547: INFO: Waiting for pod test-pod-069951c7-9624-4cd1-9dea-ce0f83872785 to disappear
Mar  2 19:22:39.559: INFO: Pod test-pod-069951c7-9624-4cd1-9dea-ce0f83872785 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:22:39.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2571" for this suite.

• [SLOW TEST:12.681 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":6,"skipped":150,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:22:39.615: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0302 19:22:50.109094      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 19:22:50.109: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:22:50.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-753" for this suite.

• [SLOW TEST:10.572 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":7,"skipped":159,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:22:50.187: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar  2 19:22:50.497: INFO: Waiting up to 5m0s for pod "downward-api-723d6e05-1118-47eb-b6d8-e0e4792378e2" in namespace "downward-api-4498" to be "Succeeded or Failed"
Mar  2 19:22:50.507: INFO: Pod "downward-api-723d6e05-1118-47eb-b6d8-e0e4792378e2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.751821ms
Mar  2 19:22:52.527: INFO: Pod "downward-api-723d6e05-1118-47eb-b6d8-e0e4792378e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030140629s
Mar  2 19:22:54.552: INFO: Pod "downward-api-723d6e05-1118-47eb-b6d8-e0e4792378e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055524416s
STEP: Saw pod success
Mar  2 19:22:54.552: INFO: Pod "downward-api-723d6e05-1118-47eb-b6d8-e0e4792378e2" satisfied condition "Succeeded or Failed"
Mar  2 19:22:54.563: INFO: Trying to get logs from node 10.245.0.4 pod downward-api-723d6e05-1118-47eb-b6d8-e0e4792378e2 container dapi-container: <nil>
STEP: delete the pod
Mar  2 19:22:54.635: INFO: Waiting for pod downward-api-723d6e05-1118-47eb-b6d8-e0e4792378e2 to disappear
Mar  2 19:22:54.679: INFO: Pod downward-api-723d6e05-1118-47eb-b6d8-e0e4792378e2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:22:54.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4498" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":8,"skipped":164,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:22:54.766: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7077
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:22:55.104: INFO: created pod
Mar  2 19:22:55.104: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7077" to be "Succeeded or Failed"
Mar  2 19:22:55.116: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.925733ms
Mar  2 19:22:57.132: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027609322s
Mar  2 19:22:59.157: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052594016s
Mar  2 19:23:01.180: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076310129s
Mar  2 19:23:03.201: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096620382s
Mar  2 19:23:05.226: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.122279522s
Mar  2 19:23:07.245: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.140701906s
STEP: Saw pod success
Mar  2 19:23:07.245: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar  2 19:23:37.246: INFO: polling logs
Mar  2 19:23:37.270: INFO: Pod logs: 
2022/03/02 19:23:04 OK: Got token
2022/03/02 19:23:04 validating with in-cluster discovery
2022/03/02 19:23:04 OK: got issuer https://kubernetes.default.svc
2022/03/02 19:23:04 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-7077:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1646249575, NotBefore:1646248975, IssuedAt:1646248975, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7077", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"180b3464-62f5-4b6e-a6b6-8782819f6900"}}}
2022/03/02 19:23:04 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
2022/03/02 19:23:04 OK: Validated signature on JWT
2022/03/02 19:23:04 OK: Got valid claims from token!
2022/03/02 19:23:04 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-7077:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1646249575, NotBefore:1646248975, IssuedAt:1646248975, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7077", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"180b3464-62f5-4b6e-a6b6-8782819f6900"}}}

Mar  2 19:23:37.270: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:23:37.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7077" for this suite.

• [SLOW TEST:42.631 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":9,"skipped":174,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:23:37.396: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9649
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 19:23:37.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc34c5ad-345a-4785-9b2d-c62faaedea1d" in namespace "projected-9649" to be "Succeeded or Failed"
Mar  2 19:23:37.834: INFO: Pod "downwardapi-volume-dc34c5ad-345a-4785-9b2d-c62faaedea1d": Phase="Pending", Reason="", readiness=false. Elapsed: 42.41ms
Mar  2 19:23:39.863: INFO: Pod "downwardapi-volume-dc34c5ad-345a-4785-9b2d-c62faaedea1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.071567951s
STEP: Saw pod success
Mar  2 19:23:39.863: INFO: Pod "downwardapi-volume-dc34c5ad-345a-4785-9b2d-c62faaedea1d" satisfied condition "Succeeded or Failed"
Mar  2 19:23:39.873: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-dc34c5ad-345a-4785-9b2d-c62faaedea1d container client-container: <nil>
STEP: delete the pod
Mar  2 19:23:39.937: INFO: Waiting for pod downwardapi-volume-dc34c5ad-345a-4785-9b2d-c62faaedea1d to disappear
Mar  2 19:23:39.975: INFO: Pod downwardapi-volume-dc34c5ad-345a-4785-9b2d-c62faaedea1d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:23:39.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9649" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":176,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:23:40.023: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-377
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Mar  2 19:23:40.291: INFO: Waiting up to 5m0s for pod "client-containers-a330bf96-2522-41af-b097-a55b78d53429" in namespace "containers-377" to be "Succeeded or Failed"
Mar  2 19:23:40.309: INFO: Pod "client-containers-a330bf96-2522-41af-b097-a55b78d53429": Phase="Pending", Reason="", readiness=false. Elapsed: 18.153179ms
Mar  2 19:23:42.331: INFO: Pod "client-containers-a330bf96-2522-41af-b097-a55b78d53429": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039351123s
Mar  2 19:23:44.369: INFO: Pod "client-containers-a330bf96-2522-41af-b097-a55b78d53429": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077598492s
STEP: Saw pod success
Mar  2 19:23:44.369: INFO: Pod "client-containers-a330bf96-2522-41af-b097-a55b78d53429" satisfied condition "Succeeded or Failed"
Mar  2 19:23:44.381: INFO: Trying to get logs from node 10.245.0.4 pod client-containers-a330bf96-2522-41af-b097-a55b78d53429 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 19:23:44.443: INFO: Waiting for pod client-containers-a330bf96-2522-41af-b097-a55b78d53429 to disappear
Mar  2 19:23:44.455: INFO: Pod client-containers-a330bf96-2522-41af-b097-a55b78d53429 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:23:44.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-377" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":11,"skipped":219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:23:44.502: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3586
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 19:23:44.768: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2126976e-a782-484d-be1c-2ced720b9cb1" in namespace "projected-3586" to be "Succeeded or Failed"
Mar  2 19:23:44.781: INFO: Pod "downwardapi-volume-2126976e-a782-484d-be1c-2ced720b9cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.825533ms
Mar  2 19:23:46.796: INFO: Pod "downwardapi-volume-2126976e-a782-484d-be1c-2ced720b9cb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028035893s
STEP: Saw pod success
Mar  2 19:23:46.796: INFO: Pod "downwardapi-volume-2126976e-a782-484d-be1c-2ced720b9cb1" satisfied condition "Succeeded or Failed"
Mar  2 19:23:46.808: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-2126976e-a782-484d-be1c-2ced720b9cb1 container client-container: <nil>
STEP: delete the pod
Mar  2 19:23:46.868: INFO: Waiting for pod downwardapi-volume-2126976e-a782-484d-be1c-2ced720b9cb1 to disappear
Mar  2 19:23:46.913: INFO: Pod downwardapi-volume-2126976e-a782-484d-be1c-2ced720b9cb1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:23:46.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3586" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":12,"skipped":311,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:23:46.965: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:23:58.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-188" for this suite.

• [SLOW TEST:11.518 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":13,"skipped":394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:23:58.484: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-e78b45f8-8a26-42d5-9a48-2ef30447cc33
STEP: Creating a pod to test consume secrets
Mar  2 19:23:58.789: INFO: Waiting up to 5m0s for pod "pod-secrets-3b76358b-9a40-43ca-b078-c37021bd63bc" in namespace "secrets-2401" to be "Succeeded or Failed"
Mar  2 19:23:58.801: INFO: Pod "pod-secrets-3b76358b-9a40-43ca-b078-c37021bd63bc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.067754ms
Mar  2 19:24:00.825: INFO: Pod "pod-secrets-3b76358b-9a40-43ca-b078-c37021bd63bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0358307s
STEP: Saw pod success
Mar  2 19:24:00.825: INFO: Pod "pod-secrets-3b76358b-9a40-43ca-b078-c37021bd63bc" satisfied condition "Succeeded or Failed"
Mar  2 19:24:00.836: INFO: Trying to get logs from node 10.245.0.4 pod pod-secrets-3b76358b-9a40-43ca-b078-c37021bd63bc container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 19:24:00.895: INFO: Waiting for pod pod-secrets-3b76358b-9a40-43ca-b078-c37021bd63bc to disappear
Mar  2 19:24:00.906: INFO: Pod pod-secrets-3b76358b-9a40-43ca-b078-c37021bd63bc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:24:00.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2401" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":14,"skipped":418,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:24:00.957: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:24:01.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-317" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":15,"skipped":426,"failed":0}
S
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:24:01.393: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6099
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-e7c38650-4eb3-43c6-b37d-b10558017409
STEP: Creating a pod to test consume secrets
Mar  2 19:24:01.677: INFO: Waiting up to 5m0s for pod "pod-secrets-428bbce9-8548-43aa-b122-c67956dd58ec" in namespace "secrets-6099" to be "Succeeded or Failed"
Mar  2 19:24:01.689: INFO: Pod "pod-secrets-428bbce9-8548-43aa-b122-c67956dd58ec": Phase="Pending", Reason="", readiness=false. Elapsed: 12.344656ms
Mar  2 19:24:03.714: INFO: Pod "pod-secrets-428bbce9-8548-43aa-b122-c67956dd58ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036889656s
Mar  2 19:24:05.769: INFO: Pod "pod-secrets-428bbce9-8548-43aa-b122-c67956dd58ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.091894299s
STEP: Saw pod success
Mar  2 19:24:05.769: INFO: Pod "pod-secrets-428bbce9-8548-43aa-b122-c67956dd58ec" satisfied condition "Succeeded or Failed"
Mar  2 19:24:05.782: INFO: Trying to get logs from node 10.245.0.4 pod pod-secrets-428bbce9-8548-43aa-b122-c67956dd58ec container secret-env-test: <nil>
STEP: delete the pod
Mar  2 19:24:05.843: INFO: Waiting for pod pod-secrets-428bbce9-8548-43aa-b122-c67956dd58ec to disappear
Mar  2 19:24:05.856: INFO: Pod pod-secrets-428bbce9-8548-43aa-b122-c67956dd58ec no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:24:05.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6099" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":16,"skipped":427,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:24:05.902: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6261
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Mar  2 19:24:06.257: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:24:08.280: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:24:10.281: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Mar  2 19:24:10.357: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:24:12.376: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  2 19:24:12.387: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6261 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:24:12.387: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:24:12.563: INFO: Exec stderr: ""
Mar  2 19:24:12.563: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6261 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:24:12.563: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:24:12.727: INFO: Exec stderr: ""
Mar  2 19:24:12.727: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6261 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:24:12.727: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:24:12.953: INFO: Exec stderr: ""
Mar  2 19:24:12.953: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6261 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:24:12.953: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:24:13.122: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  2 19:24:13.122: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6261 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:24:13.122: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:24:13.337: INFO: Exec stderr: ""
Mar  2 19:24:13.337: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6261 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:24:13.337: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:24:13.509: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  2 19:24:13.509: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6261 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:24:13.509: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:24:13.683: INFO: Exec stderr: ""
Mar  2 19:24:13.683: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6261 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:24:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:24:13.859: INFO: Exec stderr: ""
Mar  2 19:24:13.859: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6261 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:24:13.859: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:24:14.012: INFO: Exec stderr: ""
Mar  2 19:24:14.013: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6261 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:24:14.013: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:24:14.149: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:24:14.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6261" for this suite.

• [SLOW TEST:8.310 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":17,"skipped":445,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:24:14.213: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9985
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 19:24:14.604: INFO: Number of nodes with available pods: 0
Mar  2 19:24:14.604: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:24:15.640: INFO: Number of nodes with available pods: 0
Mar  2 19:24:15.640: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:24:16.642: INFO: Number of nodes with available pods: 0
Mar  2 19:24:16.642: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 19:24:17.645: INFO: Number of nodes with available pods: 3
Mar  2 19:24:17.645: INFO: Number of running nodes: 3, number of available pods: 3
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Mar  2 19:24:17.751: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18371"},"items":null}

Mar  2 19:24:17.774: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18371"},"items":[{"metadata":{"name":"daemon-set-5tnvj","generateName":"daemon-set-","namespace":"daemonsets-9985","uid":"dc23ebf9-bfd2-4d3e-9386-e3edc5edd95e","resourceVersion":"18367","creationTimestamp":"2022-03-02T19:24:14Z","labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"20db3a6dcc07b4327d6b2f7d5c95d875bba853b81def6e33d8163ecf555b44f9","cni.projectcalico.org/podIP":"172.17.74.18/32","cni.projectcalico.org/podIPs":"172.17.74.18/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"da2a94c5-8ffc-47cf-9a58-9d53c9cab3af","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-03-02T19:24:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da2a94c5-8ffc-47cf-9a58-9d53c9cab3af\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-03-02T19:24:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-03-02T19:24:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.74.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5r4rp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5r4rp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.245.0.6","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.245.0.6"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-02T19:24:14Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-02T19:24:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-02T19:24:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-02T19:24:14Z"}],"hostIP":"10.245.0.6","podIP":"172.17.74.18","podIPs":[{"ip":"172.17.74.18"}],"startTime":"2022-03-02T19:24:14Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-03-02T19:24:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"containerd://da02cf85fc9d393953e2e25e1599c7b744df01aed09251e347ba734dcc6cc906","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mrm46","generateName":"daemon-set-","namespace":"daemonsets-9985","uid":"1ebd1913-bdff-4517-934c-079f3b8cf510","resourceVersion":"18363","creationTimestamp":"2022-03-02T19:24:14Z","labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"402ebbbdcd006843641497c7094584a81e19203ef87d68bebaa9fbfa4de63384","cni.projectcalico.org/podIP":"172.17.125.235/32","cni.projectcalico.org/podIPs":"172.17.125.235/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"da2a94c5-8ffc-47cf-9a58-9d53c9cab3af","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-03-02T19:24:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da2a94c5-8ffc-47cf-9a58-9d53c9cab3af\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-03-02T19:24:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-03-02T19:24:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.125.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-c825t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-c825t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.245.0.5","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.245.0.5"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-02T19:24:14Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-02T19:24:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-02T19:24:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-02T19:24:14Z"}],"hostIP":"10.245.0.5","podIP":"172.17.125.235","podIPs":[{"ip":"172.17.125.235"}],"startTime":"2022-03-02T19:24:14Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-03-02T19:24:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"containerd://d8ba3db38166188698f69fdf53334d7dc76cb28b0b20e0c7ffda816558efe1bb","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-r9r2h","generateName":"daemon-set-","namespace":"daemonsets-9985","uid":"482a39ee-8ce9-435a-a6e8-687cbdda94d4","resourceVersion":"18365","creationTimestamp":"2022-03-02T19:24:14Z","labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"da45b997b0392e84c07b8d5948a203e9e30608ecfd82a8ecaa18ae90f1c68945","cni.projectcalico.org/podIP":"172.17.100.166/32","cni.projectcalico.org/podIPs":"172.17.100.166/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"da2a94c5-8ffc-47cf-9a58-9d53c9cab3af","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-03-02T19:24:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da2a94c5-8ffc-47cf-9a58-9d53c9cab3af\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-03-02T19:24:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-03-02T19:24:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.100.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rrsds","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rrsds","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.245.0.4","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.245.0.4"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-02T19:24:14Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-02T19:24:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-02T19:24:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-03-02T19:24:14Z"}],"hostIP":"10.245.0.4","podIP":"172.17.100.166","podIPs":[{"ip":"172.17.100.166"}],"startTime":"2022-03-02T19:24:14Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-03-02T19:24:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"containerd://4ab00e2ab1a13b407a7cb798b55e168368cce889c20a7ed1c1ed9153fac65b46","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:24:17.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9985" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":18,"skipped":456,"failed":0}
S
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:24:17.879: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-5763
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Mar  2 19:24:20.198: INFO: pods: 0 < 3
Mar  2 19:24:22.225: INFO: running pods: 0 < 3
Mar  2 19:24:24.212: INFO: running pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:24:30.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5763" for this suite.

• [SLOW TEST:12.839 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":19,"skipped":457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:24:30.718: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7818
STEP: creating service affinity-clusterip-transition in namespace services-7818
STEP: creating replication controller affinity-clusterip-transition in namespace services-7818
I0302 19:24:31.017239      21 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-7818, replica count: 3
I0302 19:24:34.081358      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 19:24:37.081521      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 19:24:40.082145      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 19:24:43.082776      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 19:24:43.113: INFO: Creating new exec pod
Mar  2 19:24:46.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-7818 exec execpod-affinitywql49 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Mar  2 19:24:46.623: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar  2 19:24:46.623: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 19:24:46.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-7818 exec execpod-affinitywql49 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.12.147 80'
Mar  2 19:24:46.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.12.147 80\nConnection to 172.21.12.147 80 port [tcp/http] succeeded!\n"
Mar  2 19:24:46.871: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 19:24:46.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-7818 exec execpod-affinitywql49 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.12.147:80/ ; done'
Mar  2 19:24:47.219: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n"
Mar  2 19:24:47.219: INFO: stdout: "\naffinity-clusterip-transition-sxzbb\naffinity-clusterip-transition-tnz87\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-tnz87\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-sxzbb\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-sxzbb\naffinity-clusterip-transition-tnz87\naffinity-clusterip-transition-sxzbb\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-tnz87\naffinity-clusterip-transition-tnz87\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff"
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-sxzbb
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-tnz87
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-tnz87
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-sxzbb
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-sxzbb
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-tnz87
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-sxzbb
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-tnz87
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-tnz87
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:24:47.219: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:24:47.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-7818 exec execpod-affinitywql49 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.12.147:80/ ; done'
Mar  2 19:24:47.545: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n"
Mar  2 19:24:47.546: INFO: stdout: "\naffinity-clusterip-transition-tnz87\naffinity-clusterip-transition-tnz87\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-sxzbb\naffinity-clusterip-transition-sxzbb\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-sxzbb\naffinity-clusterip-transition-sxzbb\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-tnz87\naffinity-clusterip-transition-tnz87\naffinity-clusterip-transition-tnz87\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-tnz87\naffinity-clusterip-transition-tnz87\naffinity-clusterip-transition-sxzbb"
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-tnz87
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-tnz87
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-sxzbb
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-sxzbb
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-sxzbb
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-sxzbb
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-tnz87
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-tnz87
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-tnz87
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-tnz87
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-tnz87
Mar  2 19:24:47.546: INFO: Received response from host: affinity-clusterip-transition-sxzbb
Mar  2 19:25:17.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-7818 exec execpod-affinitywql49 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.12.147:80/ ; done'
Mar  2 19:25:17.847: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.12.147:80/\n"
Mar  2 19:25:17.847: INFO: stdout: "\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff\naffinity-clusterip-transition-fs9ff"
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Received response from host: affinity-clusterip-transition-fs9ff
Mar  2 19:25:17.848: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7818, will wait for the garbage collector to delete the pods
Mar  2 19:25:17.977: INFO: Deleting ReplicationController affinity-clusterip-transition took: 20.925006ms
Mar  2 19:25:18.078: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.668407ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:25:20.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7818" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:49.793 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":20,"skipped":482,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:25:20.511: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6755
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-523420a8-9eee-4a45-a34e-56dc2e0945cb
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:25:24.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6755" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":498,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:25:24.951: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-2056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:25:25.567: INFO: Checking APIGroup: apiregistration.k8s.io
Mar  2 19:25:25.576: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar  2 19:25:25.576: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar  2 19:25:25.576: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar  2 19:25:25.576: INFO: Checking APIGroup: apps
Mar  2 19:25:25.596: INFO: PreferredVersion.GroupVersion: apps/v1
Mar  2 19:25:25.596: INFO: Versions found [{apps/v1 v1}]
Mar  2 19:25:25.596: INFO: apps/v1 matches apps/v1
Mar  2 19:25:25.596: INFO: Checking APIGroup: events.k8s.io
Mar  2 19:25:25.600: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar  2 19:25:25.600: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Mar  2 19:25:25.600: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar  2 19:25:25.600: INFO: Checking APIGroup: authentication.k8s.io
Mar  2 19:25:25.605: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar  2 19:25:25.605: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar  2 19:25:25.605: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar  2 19:25:25.605: INFO: Checking APIGroup: authorization.k8s.io
Mar  2 19:25:25.610: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar  2 19:25:25.610: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar  2 19:25:25.610: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar  2 19:25:25.610: INFO: Checking APIGroup: autoscaling
Mar  2 19:25:25.615: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Mar  2 19:25:25.615: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Mar  2 19:25:25.615: INFO: autoscaling/v1 matches autoscaling/v1
Mar  2 19:25:25.615: INFO: Checking APIGroup: batch
Mar  2 19:25:25.619: INFO: PreferredVersion.GroupVersion: batch/v1
Mar  2 19:25:25.619: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Mar  2 19:25:25.619: INFO: batch/v1 matches batch/v1
Mar  2 19:25:25.619: INFO: Checking APIGroup: certificates.k8s.io
Mar  2 19:25:25.623: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar  2 19:25:25.623: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar  2 19:25:25.623: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar  2 19:25:25.623: INFO: Checking APIGroup: networking.k8s.io
Mar  2 19:25:25.628: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar  2 19:25:25.628: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar  2 19:25:25.628: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar  2 19:25:25.628: INFO: Checking APIGroup: policy
Mar  2 19:25:25.632: INFO: PreferredVersion.GroupVersion: policy/v1
Mar  2 19:25:25.632: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Mar  2 19:25:25.632: INFO: policy/v1 matches policy/v1
Mar  2 19:25:25.632: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar  2 19:25:25.637: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar  2 19:25:25.637: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar  2 19:25:25.637: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar  2 19:25:25.637: INFO: Checking APIGroup: storage.k8s.io
Mar  2 19:25:25.642: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar  2 19:25:25.642: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar  2 19:25:25.642: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar  2 19:25:25.642: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar  2 19:25:25.647: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar  2 19:25:25.647: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar  2 19:25:25.647: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar  2 19:25:25.647: INFO: Checking APIGroup: apiextensions.k8s.io
Mar  2 19:25:25.652: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar  2 19:25:25.652: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar  2 19:25:25.652: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar  2 19:25:25.652: INFO: Checking APIGroup: scheduling.k8s.io
Mar  2 19:25:25.658: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar  2 19:25:25.658: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar  2 19:25:25.658: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar  2 19:25:25.658: INFO: Checking APIGroup: coordination.k8s.io
Mar  2 19:25:25.663: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar  2 19:25:25.663: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar  2 19:25:25.663: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar  2 19:25:25.663: INFO: Checking APIGroup: node.k8s.io
Mar  2 19:25:25.669: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar  2 19:25:25.669: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Mar  2 19:25:25.669: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar  2 19:25:25.669: INFO: Checking APIGroup: discovery.k8s.io
Mar  2 19:25:25.675: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar  2 19:25:25.675: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Mar  2 19:25:25.675: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar  2 19:25:25.675: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar  2 19:25:25.681: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Mar  2 19:25:25.681: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar  2 19:25:25.681: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Mar  2 19:25:25.681: INFO: Checking APIGroup: crd.projectcalico.org
Mar  2 19:25:25.688: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar  2 19:25:25.688: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar  2 19:25:25.688: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar  2 19:25:25.688: INFO: Checking APIGroup: operators.coreos.com
Mar  2 19:25:25.693: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v1
Mar  2 19:25:25.693: INFO: Versions found [{operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Mar  2 19:25:25.693: INFO: operators.coreos.com/v1 matches operators.coreos.com/v1
Mar  2 19:25:25.693: INFO: Checking APIGroup: snapshot.storage.k8s.io
Mar  2 19:25:25.699: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Mar  2 19:25:25.699: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Mar  2 19:25:25.699: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Mar  2 19:25:25.699: INFO: Checking APIGroup: ibm.com
Mar  2 19:25:25.705: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Mar  2 19:25:25.705: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Mar  2 19:25:25.705: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Mar  2 19:25:25.705: INFO: Checking APIGroup: metrics.k8s.io
Mar  2 19:25:25.710: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar  2 19:25:25.710: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar  2 19:25:25.710: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:25:25.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2056" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":22,"skipped":530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:25:25.788: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7273
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:25:37.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7273" for this suite.

• [SLOW TEST:11.473 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":23,"skipped":571,"failed":0}
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:25:37.262: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9673
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:25:37.555: INFO: The status of Pod busybox-scheduling-b7e6d68e-94f3-40f2-aab0-f080e3d74282 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:25:39.578: INFO: The status of Pod busybox-scheduling-b7e6d68e-94f3-40f2-aab0-f080e3d74282 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:25:39.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9673" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":24,"skipped":573,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:25:39.671: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4340
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4340
I0302 19:25:39.977989      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4340, replica count: 2
I0302 19:25:43.028998      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 19:25:43.029: INFO: Creating new exec pod
Mar  2 19:25:46.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4340 exec execpod8qdv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 19:25:46.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 19:25:46.363: INFO: stdout: ""
Mar  2 19:25:47.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4340 exec execpod8qdv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 19:25:47.596: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 19:25:47.596: INFO: stdout: "externalname-service-6mmmq"
Mar  2 19:25:47.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4340 exec execpod8qdv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.1.33 80'
Mar  2 19:25:47.818: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.1.33 80\nConnection to 172.21.1.33 80 port [tcp/http] succeeded!\n"
Mar  2 19:25:47.818: INFO: stdout: ""
Mar  2 19:25:48.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4340 exec execpod8qdv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.1.33 80'
Mar  2 19:25:49.032: INFO: stderr: "+ nc -v -t -w 2 172.21.1.33 80\n+ echo hostName\nConnection to 172.21.1.33 80 port [tcp/http] succeeded!\n"
Mar  2 19:25:49.032: INFO: stdout: ""
Mar  2 19:25:49.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4340 exec execpod8qdv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.1.33 80'
Mar  2 19:25:50.063: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.1.33 80\nConnection to 172.21.1.33 80 port [tcp/http] succeeded!\n"
Mar  2 19:25:50.063: INFO: stdout: ""
Mar  2 19:25:50.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4340 exec execpod8qdv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.1.33 80'
Mar  2 19:25:51.088: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.1.33 80\nConnection to 172.21.1.33 80 port [tcp/http] succeeded!\n"
Mar  2 19:25:51.088: INFO: stdout: ""
Mar  2 19:25:51.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4340 exec execpod8qdv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.1.33 80'
Mar  2 19:25:52.094: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.1.33 80\nConnection to 172.21.1.33 80 port [tcp/http] succeeded!\n"
Mar  2 19:25:52.094: INFO: stdout: "externalname-service-vnbch"
Mar  2 19:25:52.094: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:25:52.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4340" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.518 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":25,"skipped":577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:25:52.190: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:25:56.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3610" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":26,"skipped":603,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:25:56.574: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-338
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 19:25:56.841: INFO: Waiting up to 5m0s for pod "downwardapi-volume-954dc39b-db8d-43a9-a7db-cf6cae0fc78d" in namespace "projected-338" to be "Succeeded or Failed"
Mar  2 19:25:56.853: INFO: Pod "downwardapi-volume-954dc39b-db8d-43a9-a7db-cf6cae0fc78d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039542ms
Mar  2 19:25:58.875: INFO: Pod "downwardapi-volume-954dc39b-db8d-43a9-a7db-cf6cae0fc78d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034361955s
Mar  2 19:26:00.899: INFO: Pod "downwardapi-volume-954dc39b-db8d-43a9-a7db-cf6cae0fc78d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058435179s
STEP: Saw pod success
Mar  2 19:26:00.899: INFO: Pod "downwardapi-volume-954dc39b-db8d-43a9-a7db-cf6cae0fc78d" satisfied condition "Succeeded or Failed"
Mar  2 19:26:00.911: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-954dc39b-db8d-43a9-a7db-cf6cae0fc78d container client-container: <nil>
STEP: delete the pod
Mar  2 19:26:00.978: INFO: Waiting for pod downwardapi-volume-954dc39b-db8d-43a9-a7db-cf6cae0fc78d to disappear
Mar  2 19:26:00.991: INFO: Pod downwardapi-volume-954dc39b-db8d-43a9-a7db-cf6cae0fc78d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:26:00.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-338" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":27,"skipped":608,"failed":0}
S
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:26:01.034: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7379
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Mar  2 19:26:01.281: INFO: created test-pod-1
Mar  2 19:26:01.306: INFO: created test-pod-2
Mar  2 19:26:01.328: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Mar  2 19:26:01.413: INFO: Pod quantity 3 is different from expected quantity 0
Mar  2 19:26:02.425: INFO: Pod quantity 3 is different from expected quantity 0
Mar  2 19:26:03.434: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:26:04.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7379" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":28,"skipped":609,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:26:04.470: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6552
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-ba712c0d-a90d-4811-8a7b-c7a43ce54ec2
STEP: Creating a pod to test consume configMaps
Mar  2 19:26:04.742: INFO: Waiting up to 5m0s for pod "pod-configmaps-57552e26-7fe6-410d-ac90-23dd6e100479" in namespace "configmap-6552" to be "Succeeded or Failed"
Mar  2 19:26:04.756: INFO: Pod "pod-configmaps-57552e26-7fe6-410d-ac90-23dd6e100479": Phase="Pending", Reason="", readiness=false. Elapsed: 13.782353ms
Mar  2 19:26:06.773: INFO: Pod "pod-configmaps-57552e26-7fe6-410d-ac90-23dd6e100479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030487801s
Mar  2 19:26:08.829: INFO: Pod "pod-configmaps-57552e26-7fe6-410d-ac90-23dd6e100479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.086529422s
STEP: Saw pod success
Mar  2 19:26:08.829: INFO: Pod "pod-configmaps-57552e26-7fe6-410d-ac90-23dd6e100479" satisfied condition "Succeeded or Failed"
Mar  2 19:26:08.841: INFO: Trying to get logs from node 10.245.0.4 pod pod-configmaps-57552e26-7fe6-410d-ac90-23dd6e100479 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 19:26:08.969: INFO: Waiting for pod pod-configmaps-57552e26-7fe6-410d-ac90-23dd6e100479 to disappear
Mar  2 19:26:08.981: INFO: Pod pod-configmaps-57552e26-7fe6-410d-ac90-23dd6e100479 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:26:08.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6552" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":29,"skipped":610,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:26:09.024: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2627
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-173a78eb-2e1d-49a1-80da-5d8c9119e8c7
STEP: Creating a pod to test consume secrets
Mar  2 19:26:09.294: INFO: Waiting up to 5m0s for pod "pod-secrets-b26c4014-eb16-4474-8431-1b944a79ef33" in namespace "secrets-2627" to be "Succeeded or Failed"
Mar  2 19:26:09.312: INFO: Pod "pod-secrets-b26c4014-eb16-4474-8431-1b944a79ef33": Phase="Pending", Reason="", readiness=false. Elapsed: 17.470055ms
Mar  2 19:26:11.325: INFO: Pod "pod-secrets-b26c4014-eb16-4474-8431-1b944a79ef33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030684387s
STEP: Saw pod success
Mar  2 19:26:11.325: INFO: Pod "pod-secrets-b26c4014-eb16-4474-8431-1b944a79ef33" satisfied condition "Succeeded or Failed"
Mar  2 19:26:11.338: INFO: Trying to get logs from node 10.245.0.4 pod pod-secrets-b26c4014-eb16-4474-8431-1b944a79ef33 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 19:26:11.401: INFO: Waiting for pod pod-secrets-b26c4014-eb16-4474-8431-1b944a79ef33 to disappear
Mar  2 19:26:11.412: INFO: Pod pod-secrets-b26c4014-eb16-4474-8431-1b944a79ef33 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:26:11.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2627" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":30,"skipped":627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:26:11.456: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-65b9c854-40b6-421f-89b8-ba9c3d3086cf
STEP: Creating a pod to test consume configMaps
Mar  2 19:26:11.877: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7450f04f-6fd3-45a4-8df3-e78daffe1690" in namespace "projected-2753" to be "Succeeded or Failed"
Mar  2 19:26:11.893: INFO: Pod "pod-projected-configmaps-7450f04f-6fd3-45a4-8df3-e78daffe1690": Phase="Pending", Reason="", readiness=false. Elapsed: 16.148161ms
Mar  2 19:26:13.914: INFO: Pod "pod-projected-configmaps-7450f04f-6fd3-45a4-8df3-e78daffe1690": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037489398s
Mar  2 19:26:15.936: INFO: Pod "pod-projected-configmaps-7450f04f-6fd3-45a4-8df3-e78daffe1690": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059543595s
STEP: Saw pod success
Mar  2 19:26:15.936: INFO: Pod "pod-projected-configmaps-7450f04f-6fd3-45a4-8df3-e78daffe1690" satisfied condition "Succeeded or Failed"
Mar  2 19:26:15.948: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-configmaps-7450f04f-6fd3-45a4-8df3-e78daffe1690 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 19:26:16.007: INFO: Waiting for pod pod-projected-configmaps-7450f04f-6fd3-45a4-8df3-e78daffe1690 to disappear
Mar  2 19:26:16.021: INFO: Pod pod-projected-configmaps-7450f04f-6fd3-45a4-8df3-e78daffe1690 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:26:16.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2753" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":31,"skipped":650,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:26:16.084: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4645
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar  2 19:26:16.332: INFO: Waiting up to 5m0s for pod "downward-api-bdb3d057-bba9-498e-9704-51ed2c0ac1e4" in namespace "downward-api-4645" to be "Succeeded or Failed"
Mar  2 19:26:16.343: INFO: Pod "downward-api-bdb3d057-bba9-498e-9704-51ed2c0ac1e4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.849909ms
Mar  2 19:26:18.367: INFO: Pod "downward-api-bdb3d057-bba9-498e-9704-51ed2c0ac1e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035250762s
STEP: Saw pod success
Mar  2 19:26:18.368: INFO: Pod "downward-api-bdb3d057-bba9-498e-9704-51ed2c0ac1e4" satisfied condition "Succeeded or Failed"
Mar  2 19:26:18.379: INFO: Trying to get logs from node 10.245.0.4 pod downward-api-bdb3d057-bba9-498e-9704-51ed2c0ac1e4 container dapi-container: <nil>
STEP: delete the pod
Mar  2 19:26:18.449: INFO: Waiting for pod downward-api-bdb3d057-bba9-498e-9704-51ed2c0ac1e4 to disappear
Mar  2 19:26:18.460: INFO: Pod downward-api-bdb3d057-bba9-498e-9704-51ed2c0ac1e4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:26:18.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4645" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":32,"skipped":667,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:26:18.503: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6008
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-0c33981f-0ff5-4912-8703-93545c8b7f43
STEP: Creating a pod to test consume secrets
Mar  2 19:26:18.772: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a438c8df-4d62-40e1-8b0a-7c92051eed10" in namespace "projected-6008" to be "Succeeded or Failed"
Mar  2 19:26:18.786: INFO: Pod "pod-projected-secrets-a438c8df-4d62-40e1-8b0a-7c92051eed10": Phase="Pending", Reason="", readiness=false. Elapsed: 13.211345ms
Mar  2 19:26:20.819: INFO: Pod "pod-projected-secrets-a438c8df-4d62-40e1-8b0a-7c92051eed10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047004369s
Mar  2 19:26:22.848: INFO: Pod "pod-projected-secrets-a438c8df-4d62-40e1-8b0a-7c92051eed10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075846938s
STEP: Saw pod success
Mar  2 19:26:22.848: INFO: Pod "pod-projected-secrets-a438c8df-4d62-40e1-8b0a-7c92051eed10" satisfied condition "Succeeded or Failed"
Mar  2 19:26:22.859: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-secrets-a438c8df-4d62-40e1-8b0a-7c92051eed10 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 19:26:22.953: INFO: Waiting for pod pod-projected-secrets-a438c8df-4d62-40e1-8b0a-7c92051eed10 to disappear
Mar  2 19:26:22.996: INFO: Pod pod-projected-secrets-a438c8df-4d62-40e1-8b0a-7c92051eed10 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:26:23.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6008" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":669,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:26:23.061: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-2906
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-2906
Mar  2 19:26:23.341: INFO: Found 0 stateful pods, waiting for 1
Mar  2 19:26:33.377: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Mar  2 19:26:33.471: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Mar  2 19:26:33.543: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Mar  2 19:26:33.550: INFO: Observed &StatefulSet event: ADDED
Mar  2 19:26:33.550: INFO: Found Statefulset ss in namespace statefulset-2906 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 19:26:33.550: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Mar  2 19:26:33.550: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  2 19:26:33.570: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Mar  2 19:26:33.576: INFO: Observed &StatefulSet event: ADDED
Mar  2 19:26:33.576: INFO: Observed Statefulset ss in namespace statefulset-2906 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 19:26:33.576: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Mar  2 19:26:33.576: INFO: Deleting all statefulset in ns statefulset-2906
Mar  2 19:26:33.589: INFO: Scaling statefulset ss to 0
Mar  2 19:26:43.660: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 19:26:43.706: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:26:43.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2906" for this suite.

• [SLOW TEST:20.738 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":34,"skipped":675,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:26:43.800: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4123
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-214fceb0-af33-42c6-bb73-c365c27d603a
STEP: Creating a pod to test consume secrets
Mar  2 19:26:44.056: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-36102488-011c-4f56-82c1-2d6bd2b1af80" in namespace "projected-4123" to be "Succeeded or Failed"
Mar  2 19:26:44.065: INFO: Pod "pod-projected-secrets-36102488-011c-4f56-82c1-2d6bd2b1af80": Phase="Pending", Reason="", readiness=false. Elapsed: 9.301925ms
Mar  2 19:26:46.087: INFO: Pod "pod-projected-secrets-36102488-011c-4f56-82c1-2d6bd2b1af80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031207683s
Mar  2 19:26:48.105: INFO: Pod "pod-projected-secrets-36102488-011c-4f56-82c1-2d6bd2b1af80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049288371s
STEP: Saw pod success
Mar  2 19:26:48.105: INFO: Pod "pod-projected-secrets-36102488-011c-4f56-82c1-2d6bd2b1af80" satisfied condition "Succeeded or Failed"
Mar  2 19:26:48.117: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-secrets-36102488-011c-4f56-82c1-2d6bd2b1af80 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 19:26:48.196: INFO: Waiting for pod pod-projected-secrets-36102488-011c-4f56-82c1-2d6bd2b1af80 to disappear
Mar  2 19:26:48.208: INFO: Pod pod-projected-secrets-36102488-011c-4f56-82c1-2d6bd2b1af80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:26:48.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4123" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":35,"skipped":733,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:26:48.256: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8564
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-dc852a07-2eba-447e-8999-18b270dc83c6
STEP: Creating secret with name secret-projected-all-test-volume-0aa861e8-249b-4a4f-8bc7-67ff5e4a9e42
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  2 19:26:48.584: INFO: Waiting up to 5m0s for pod "projected-volume-8eef040a-c5b3-4f82-8404-1265022efe5d" in namespace "projected-8564" to be "Succeeded or Failed"
Mar  2 19:26:48.621: INFO: Pod "projected-volume-8eef040a-c5b3-4f82-8404-1265022efe5d": Phase="Pending", Reason="", readiness=false. Elapsed: 36.623535ms
Mar  2 19:26:50.642: INFO: Pod "projected-volume-8eef040a-c5b3-4f82-8404-1265022efe5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.057315898s
STEP: Saw pod success
Mar  2 19:26:50.642: INFO: Pod "projected-volume-8eef040a-c5b3-4f82-8404-1265022efe5d" satisfied condition "Succeeded or Failed"
Mar  2 19:26:50.654: INFO: Trying to get logs from node 10.245.0.4 pod projected-volume-8eef040a-c5b3-4f82-8404-1265022efe5d container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  2 19:26:50.763: INFO: Waiting for pod projected-volume-8eef040a-c5b3-4f82-8404-1265022efe5d to disappear
Mar  2 19:26:50.774: INFO: Pod projected-volume-8eef040a-c5b3-4f82-8404-1265022efe5d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:26:50.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8564" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":36,"skipped":738,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:26:50.815: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1070
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  2 19:26:51.048: INFO: Waiting up to 5m0s for pod "pod-e29be26b-44ea-4dbf-828a-9bd061202c75" in namespace "emptydir-1070" to be "Succeeded or Failed"
Mar  2 19:26:51.060: INFO: Pod "pod-e29be26b-44ea-4dbf-828a-9bd061202c75": Phase="Pending", Reason="", readiness=false. Elapsed: 12.301642ms
Mar  2 19:26:53.122: INFO: Pod "pod-e29be26b-44ea-4dbf-828a-9bd061202c75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073540073s
Mar  2 19:26:55.150: INFO: Pod "pod-e29be26b-44ea-4dbf-828a-9bd061202c75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.101378865s
STEP: Saw pod success
Mar  2 19:26:55.150: INFO: Pod "pod-e29be26b-44ea-4dbf-828a-9bd061202c75" satisfied condition "Succeeded or Failed"
Mar  2 19:26:55.160: INFO: Trying to get logs from node 10.245.0.4 pod pod-e29be26b-44ea-4dbf-828a-9bd061202c75 container test-container: <nil>
STEP: delete the pod
Mar  2 19:26:55.221: INFO: Waiting for pod pod-e29be26b-44ea-4dbf-828a-9bd061202c75 to disappear
Mar  2 19:26:55.232: INFO: Pod pod-e29be26b-44ea-4dbf-828a-9bd061202c75 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:26:55.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1070" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":37,"skipped":755,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:26:55.272: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-7206
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:26:55.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7206" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":38,"skipped":775,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:26:55.555: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4941
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-83cb9e16-b56c-4cf8-b04e-4a4333cc7f92 in namespace container-probe-4941
Mar  2 19:26:59.909: INFO: Started pod liveness-83cb9e16-b56c-4cf8-b04e-4a4333cc7f92 in namespace container-probe-4941
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 19:26:59.921: INFO: Initial restart count of pod liveness-83cb9e16-b56c-4cf8-b04e-4a4333cc7f92 is 0
Mar  2 19:27:18.140: INFO: Restart count of pod container-probe-4941/liveness-83cb9e16-b56c-4cf8-b04e-4a4333cc7f92 is now 1 (18.219509742s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:27:18.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4941" for this suite.

• [SLOW TEST:22.674 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":39,"skipped":778,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:27:18.230: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3211
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Mar  2 19:27:18.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-3211 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar  2 19:27:18.573: INFO: stderr: ""
Mar  2 19:27:18.573: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar  2 19:27:23.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-3211 get pod e2e-test-httpd-pod -o json'
Mar  2 19:27:23.703: INFO: stderr: ""
Mar  2 19:27:23.703: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"4c36293de92ec481cd5ba796d4f24554d795fa07454dcb6b4741313c502830ea\",\n            \"cni.projectcalico.org/podIP\": \"172.17.100.186/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.17.100.186/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2022-03-02T19:27:18Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3211\",\n        \"resourceVersion\": \"19942\",\n        \"uid\": \"294e250c-cfd0-4675-b1d3-da00c7ab0950\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-cfx9m\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.245.0.4\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-cfx9m\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-02T19:27:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-02T19:27:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-02T19:27:20Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-02T19:27:18Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://aab108efac472a561b7fd4bfc50932bc3a6227fe1dad49c50c3eda437cdc18db\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-03-02T19:27:19Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.245.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.17.100.186\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.17.100.186\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-03-02T19:27:18Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  2 19:27:23.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-3211 replace -f -'
Mar  2 19:27:24.009: INFO: stderr: ""
Mar  2 19:27:24.009: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Mar  2 19:27:24.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-3211 delete pods e2e-test-httpd-pod'
Mar  2 19:27:26.371: INFO: stderr: ""
Mar  2 19:27:26.371: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:27:26.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3211" for this suite.

• [SLOW TEST:8.195 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1555
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":40,"skipped":781,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:27:26.426: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:27:26.653: INFO: Creating deployment "webserver-deployment"
Mar  2 19:27:26.688: INFO: Waiting for observed generation 1
Mar  2 19:27:28.722: INFO: Waiting for all required pods to come up
Mar  2 19:27:28.745: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  2 19:27:30.787: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  2 19:27:30.813: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  2 19:27:30.839: INFO: Updating deployment webserver-deployment
Mar  2 19:27:30.839: INFO: Waiting for observed generation 2
Mar  2 19:27:32.867: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  2 19:27:32.880: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  2 19:27:32.894: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 19:27:32.953: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  2 19:27:32.953: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  2 19:27:32.966: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 19:27:33.001: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  2 19:27:33.001: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  2 19:27:33.036: INFO: Updating deployment webserver-deployment
Mar  2 19:27:33.036: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  2 19:27:33.073: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  2 19:27:33.095: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar  2 19:27:33.161: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6112  b69ddbf0-f5b3-488d-baca-41f01ea26f73 20246 3 2022-03-02 19:27:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-03-02 19:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 19:27:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000b12148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2022-03-02 19:27:31 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-03-02 19:27:33 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  2 19:27:33.186: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-6112  05c041b8-aa2e-4863-8c81-a9fc225f4b70 20236 3 2022-03-02 19:27:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b69ddbf0-f5b3-488d-baca-41f01ea26f73 0xc004097157 0xc004097158}] []  [{kube-controller-manager Update apps/v1 2022-03-02 19:27:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b69ddbf0-f5b3-488d-baca-41f01ea26f73\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 19:27:30 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040971f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 19:27:33.186: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  2 19:27:33.186: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-6112  4be7a5f9-ab7d-4fda-83f0-ec003340755b 20232 3 2022-03-02 19:27:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b69ddbf0-f5b3-488d-baca-41f01ea26f73 0xc004097257 0xc004097258}] []  [{kube-controller-manager Update apps/v1 2022-03-02 19:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b69ddbf0-f5b3-488d-baca-41f01ea26f73\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 19:27:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040972e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  2 19:27:33.212: INFO: Pod "webserver-deployment-795d758f88-2cvbc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2cvbc webserver-deployment-795d758f88- deployment-6112  b623ae06-ee56-4446-b196-6c1f5b464ddd 20195 0 2022-03-02 19:27:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:3aa7b0531baec95b5e28b8f4d4abb1f98b2bfd2c172985ee4c4019198d783866 cni.projectcalico.org/podIP:172.17.125.243/32 cni.projectcalico.org/podIPs:172.17.125.243/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc000b12557 0xc000b12558}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-03-02 19:27:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-03-02 19:27:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b9hjw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b9hjw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.5,PodIP:,StartTime:2022-03-02 19:27:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.212: INFO: Pod "webserver-deployment-795d758f88-f7t4d" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-f7t4d webserver-deployment-795d758f88- deployment-6112  815ff122-4564-4b1b-88fa-c37e0ad42128 20284 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc000b127a0 0xc000b127a1}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbn9p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbn9p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.212: INFO: Pod "webserver-deployment-795d758f88-f9msd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-f9msd webserver-deployment-795d758f88- deployment-6112  6cfb9282-c1b1-423f-976f-f1bd730838ce 20273 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc000b12940 0xc000b12941}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wq8x6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wq8x6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.212: INFO: Pod "webserver-deployment-795d758f88-k6jv6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-k6jv6 webserver-deployment-795d758f88- deployment-6112  c0c8b5c2-8cbc-49a3-9d71-5215e118dbb6 20213 0 2022-03-02 19:27:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:397ff26cbffd391b5dec69888e4518be3b1745a18009615878ca161dcfebc022 cni.projectcalico.org/podIP:172.17.100.132/32 cni.projectcalico.org/podIPs:172.17.100.132/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc000b12b30 0xc000b12b31}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-03-02 19:27:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-03-02 19:27:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnz6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnz6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.4,PodIP:,StartTime:2022-03-02 19:27:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.213: INFO: Pod "webserver-deployment-795d758f88-k9hrt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-k9hrt webserver-deployment-795d758f88- deployment-6112  a2a8d357-d526-42c5-9895-32cb29ccd20d 20248 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc000b12dc0 0xc000b12dc1}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjz6r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjz6r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.214: INFO: Pod "webserver-deployment-795d758f88-lgj6c" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lgj6c webserver-deployment-795d758f88- deployment-6112  68d95c8a-5a0b-44fe-8f49-fc836487407f 20204 0 2022-03-02 19:27:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:06c3d3aa9a81b1ef6b0e3025e8e4f09c55222f31a6638604299ae9ebbe3cdecd cni.projectcalico.org/podIP:172.17.100.191/32 cni.projectcalico.org/podIPs:172.17.100.191/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc000b12fe0 0xc000b12fe1}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 19:27:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 19:27:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xqcn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xqcn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.4,PodIP:,StartTime:2022-03-02 19:27:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.214: INFO: Pod "webserver-deployment-795d758f88-lw5vh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lw5vh webserver-deployment-795d758f88- deployment-6112  137279c3-0fbf-4f65-a624-2a0a31605d45 20282 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc000b13310 0xc000b13311}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hplhj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hplhj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.214: INFO: Pod "webserver-deployment-795d758f88-pz7xb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pz7xb webserver-deployment-795d758f88- deployment-6112  fe95ea60-8726-4387-b4cc-4c9c0f7ae0d1 20288 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc000b134d0 0xc000b134d1}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79qhl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79qhl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.214: INFO: Pod "webserver-deployment-795d758f88-rrfk7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rrfk7 webserver-deployment-795d758f88- deployment-6112  ea00df83-4e25-43af-bb66-87fe3f4cd6b7 20209 0 2022-03-02 19:27:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:a70124a839e983dde67177af17188e01a7f8abd09fc216ce609b903941d03f31 cni.projectcalico.org/podIP:172.17.125.244/32 cni.projectcalico.org/podIPs:172.17.125.244/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc000b13697 0xc000b13698}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-03-02 19:27:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-03-02 19:27:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2278,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2278,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.5,PodIP:,StartTime:2022-03-02 19:27:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.214: INFO: Pod "webserver-deployment-795d758f88-sxr2b" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-sxr2b webserver-deployment-795d758f88- deployment-6112  0d5e7916-812b-4f34-bc14-d79111feba06 20203 0 2022-03-02 19:27:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:308b938f06189cf53368ee7f5cd282ba525ed9676507d0c2bb1cb47140227d08 cni.projectcalico.org/podIP:172.17.74.27/32 cni.projectcalico.org/podIPs:172.17.74.27/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc002046870 0xc002046871}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-03-02 19:27:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-03-02 19:27:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fftmm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fftmm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.6,PodIP:,StartTime:2022-03-02 19:27:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.215: INFO: Pod "webserver-deployment-795d758f88-w4qgr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-w4qgr webserver-deployment-795d758f88- deployment-6112  ed9e31db-c78e-4a98-b2dd-7112af63d937 20277 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc002046a70 0xc002046a71}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wvq9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wvq9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.215: INFO: Pod "webserver-deployment-795d758f88-ws7cs" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ws7cs webserver-deployment-795d758f88- deployment-6112  514063bb-acc5-4e09-a8fb-ef8db1da6f15 20285 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc002046be0 0xc002046be1}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pr6nh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pr6nh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.215: INFO: Pod "webserver-deployment-795d758f88-zdcjn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zdcjn webserver-deployment-795d758f88- deployment-6112  b0346889-b475-49ee-adfd-d8df89b771ce 20281 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 05c041b8-aa2e-4863-8c81-a9fc225f4b70 0xc002046d50 0xc002046d51}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05c041b8-aa2e-4863-8c81-a9fc225f4b70\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kt2kq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kt2kq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.215: INFO: Pod "webserver-deployment-847dcfb7fb-2m7sv" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2m7sv webserver-deployment-847dcfb7fb- deployment-6112  69e176e0-4d48-4f9f-80ae-84e858370cfb 20133 0 2022-03-02 19:27:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:87807009d0fd8fe62256c1b295157944793ddb599df7b0a9874e622594082fd4 cni.projectcalico.org/podIP:172.17.74.26/32 cni.projectcalico.org/podIPs:172.17.74.26/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc002046ee0 0xc002046ee1}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 19:27:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 19:27:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.74.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bt24j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bt24j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.6,PodIP:172.17.74.26,StartTime:2022-03-02 19:27:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 19:27:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://dd92b43c8d2608cae5369eb94725ebdf141ee2b6eee504c488a08183e4b8e197,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.74.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.215: INFO: Pod "webserver-deployment-847dcfb7fb-46zpw" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-46zpw webserver-deployment-847dcfb7fb- deployment-6112  65406cd9-f2df-45b5-95e7-8a27cdc6efdb 20088 0 2022-03-02 19:27:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:e46d833dc7013a74e7830f016cf7cdc11ab031f39d76b628cbe69017935175f7 cni.projectcalico.org/podIP:172.17.74.24/32 cni.projectcalico.org/podIPs:172.17.74.24/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc002047100 0xc002047101}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 19:27:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 19:27:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.74.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h49rn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h49rn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.6,PodIP:172.17.74.24,StartTime:2022-03-02 19:27:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 19:27:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://1ab91b914e1121556e643612243aee817b570cfa817ecbe0e120533161c8dc3d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.74.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.216: INFO: Pod "webserver-deployment-847dcfb7fb-57dpk" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-57dpk webserver-deployment-847dcfb7fb- deployment-6112  4ce2bfb9-2158-45ac-bcdb-5276d86bf0b4 20085 0 2022-03-02 19:27:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:c8d0768900fda2786ebeac1491af3b4cd9c2d9d4a692d1bb7a47543557dbb4e5 cni.projectcalico.org/podIP:172.17.100.187/32 cni.projectcalico.org/podIPs:172.17.100.187/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc002047310 0xc002047311}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 19:27:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 19:27:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.100.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jf2jn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jf2jn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.4,PodIP:172.17.100.187,StartTime:2022-03-02 19:27:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 19:27:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://5e7b075b1b47971cd306705dcdcc7141b1cbc52e5f9bd6b4e99873c47a0d8d99,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.100.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.218: INFO: Pod "webserver-deployment-847dcfb7fb-5jr7g" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-5jr7g webserver-deployment-847dcfb7fb- deployment-6112  1acb79d4-76c1-4571-8f23-bc1974aa2925 20122 0 2022-03-02 19:27:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:b54e048f911f6d1b3fc8282ba5f9ffcb724c23886d9495c452548612d70eda73 cni.projectcalico.org/podIP:172.17.100.188/32 cni.projectcalico.org/podIPs:172.17.100.188/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc002047510 0xc002047511}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 19:27:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 19:27:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.100.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqhcf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqhcf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.4,PodIP:172.17.100.188,StartTime:2022-03-02 19:27:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 19:27:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://a44d9b0f1d6d2e1dead3c34efc14cd6160c4d3df3039c00e9a8879c25c5e0f97,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.100.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.218: INFO: Pod "webserver-deployment-847dcfb7fb-778th" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-778th webserver-deployment-847dcfb7fb- deployment-6112  9593ba29-11fa-47fc-8339-b17fe22494b1 20287 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc002047720 0xc002047721}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzpqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzpqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.218: INFO: Pod "webserver-deployment-847dcfb7fb-9z4rc" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-9z4rc webserver-deployment-847dcfb7fb- deployment-6112  8aaf226a-61e9-4567-b2d0-6c6927bf815e 20274 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc002047880 0xc002047881}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tq6q9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tq6q9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.218: INFO: Pod "webserver-deployment-847dcfb7fb-cn7c4" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-cn7c4 webserver-deployment-847dcfb7fb- deployment-6112  f9e873c0-b320-42a6-b1ef-1da80a3177f1 20276 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc0020479e0 0xc0020479e1}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mwn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mwn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.218: INFO: Pod "webserver-deployment-847dcfb7fb-d67ch" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-d67ch webserver-deployment-847dcfb7fb- deployment-6112  69153517-5188-4aa6-b980-dae7de2210b1 20119 0 2022-03-02 19:27:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:2b378653fb298590e1258bdb9e29f1a2e1a272f43fc26705e2119718a870bade cni.projectcalico.org/podIP:172.17.125.241/32 cni.projectcalico.org/podIPs:172.17.125.241/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc002047b40 0xc002047b41}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 19:27:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 19:27:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.125.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pfmvk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pfmvk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.5,PodIP:172.17.125.241,StartTime:2022-03-02 19:27:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 19:27:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://71b6c6017f13478ea2b1c5f82ac3d388a29edc42e1f61cf0b5abe5cddabf1f3d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.125.241,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.218: INFO: Pod "webserver-deployment-847dcfb7fb-dfd8w" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-dfd8w webserver-deployment-847dcfb7fb- deployment-6112  be04bc08-ec82-41ad-ac07-a7d789f19170 20247 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc002047d40 0xc002047d41}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dgt4n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgt4n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.219: INFO: Pod "webserver-deployment-847dcfb7fb-g4c67" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-g4c67 webserver-deployment-847dcfb7fb- deployment-6112  ef8b1e59-2a2b-4277-be43-847f9f11e8a9 20283 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc002047ea0 0xc002047ea1}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvsnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvsnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.219: INFO: Pod "webserver-deployment-847dcfb7fb-hp62j" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hp62j webserver-deployment-847dcfb7fb- deployment-6112  6db8fe3a-be5f-47e5-92fa-e98fd52dd2ea 20114 0 2022-03-02 19:27:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:ae58276c045ca41040e7e7f9e7ac8e34b73b89534d635d2a68626fff33d28277 cni.projectcalico.org/podIP:172.17.125.240/32 cni.projectcalico.org/podIPs:172.17.125.240/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc000a66000 0xc000a66001}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 19:27:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 19:27:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.125.240\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ffpkw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ffpkw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.5,PodIP:172.17.125.240,StartTime:2022-03-02 19:27:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 19:27:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://5438636d816b05afd70a342bfa52c0ab5ea8a865fca49dd1ba1d02fde3813abe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.125.240,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.219: INFO: Pod "webserver-deployment-847dcfb7fb-hvbb5" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hvbb5 webserver-deployment-847dcfb7fb- deployment-6112  7b77623e-bc19-4177-8cee-68994d72868d 20278 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc000a66240 0xc000a66241}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m6ddh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m6ddh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.219: INFO: Pod "webserver-deployment-847dcfb7fb-lk2w4" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-lk2w4 webserver-deployment-847dcfb7fb- deployment-6112  829a32d9-7086-4c47-bea5-aadad0d7bfe6 20280 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc000a663e0 0xc000a663e1}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvxck,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvxck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.219: INFO: Pod "webserver-deployment-847dcfb7fb-mjdvj" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mjdvj webserver-deployment-847dcfb7fb- deployment-6112  3d4b374e-0910-449c-b19f-dadd9a32ecfd 20275 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc000a66540 0xc000a66541}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nwk75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nwk75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.221: INFO: Pod "webserver-deployment-847dcfb7fb-pgv49" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-pgv49 webserver-deployment-847dcfb7fb- deployment-6112  bd7ada43-af37-4be6-ab3f-b635e5a34408 20249 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc000a666a0 0xc000a666a1}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9whbc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9whbc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.221: INFO: Pod "webserver-deployment-847dcfb7fb-qs748" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-qs748 webserver-deployment-847dcfb7fb- deployment-6112  8b73a842-6fd9-4002-a1cc-ffa42a1ce880 20272 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc000a66810 0xc000a66811}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xbdxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xbdxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.4,PodIP:,StartTime:2022-03-02 19:27:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.221: INFO: Pod "webserver-deployment-847dcfb7fb-shg58" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-shg58 webserver-deployment-847dcfb7fb- deployment-6112  7cb8a85d-6932-4636-a14e-d369824490ff 20129 0 2022-03-02 19:27:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:9c255101795a54b3c7481f669b5d90ba27071860d0b542457403cf583868a658 cni.projectcalico.org/podIP:172.17.74.25/32 cni.projectcalico.org/podIPs:172.17.74.25/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc000a669f0 0xc000a669f1}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 19:27:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 19:27:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.74.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99hw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99hw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.6,PodIP:172.17.74.25,StartTime:2022-03-02 19:27:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 19:27:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://92f7450fc5e81cf54c632f0b01ed41feb68dfcf8fc41ceebbecf3572f577c583,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.74.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.221: INFO: Pod "webserver-deployment-847dcfb7fb-vdpkg" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-vdpkg webserver-deployment-847dcfb7fb- deployment-6112  fac36c42-6b88-47a4-bc59-2a1af7ab6418 20286 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc000a66c00 0xc000a66c01}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xdntz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xdntz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.222: INFO: Pod "webserver-deployment-847dcfb7fb-vx6s5" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-vx6s5 webserver-deployment-847dcfb7fb- deployment-6112  e3691835-c594-48a4-b99a-331799a0e1f1 20279 0 2022-03-02 19:27:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc000a66da0 0xc000a66da1}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nr4k8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nr4k8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:27:33.222: INFO: Pod "webserver-deployment-847dcfb7fb-zm9gb" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-zm9gb webserver-deployment-847dcfb7fb- deployment-6112  5dfa5735-40a3-45ed-921a-a4ebd371f044 20112 0 2022-03-02 19:27:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:b7c6075cb509e601278d1c219a2e2acb73ed10e7b0d191f6e22e8c7975fd861c cni.projectcalico.org/podIP:172.17.125.242/32 cni.projectcalico.org/podIPs:172.17.125.242/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 4be7a5f9-ab7d-4fda-83f0-ec003340755b 0xc000a66f20 0xc000a66f21}] []  [{kube-controller-manager Update v1 2022-03-02 19:27:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4be7a5f9-ab7d-4fda-83f0-ec003340755b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 19:27:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 19:27:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.125.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z9g5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z9g5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:27:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.5,PodIP:172.17.125.242,StartTime:2022-03-02 19:27:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 19:27:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://59c1c8631a6df4509afd88f230e92a69660d5705f9d8961b86efe63f522b9872,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.125.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:27:33.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6112" for this suite.

• [SLOW TEST:6.835 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":41,"skipped":799,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:27:33.262: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9269
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:27:33.500: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar  2 19:27:33.538: INFO: The status of Pod pod-logs-websocket-23a66cf8-ae1d-4c5a-ae34-80242536cb24 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:27:35.560: INFO: The status of Pod pod-logs-websocket-23a66cf8-ae1d-4c5a-ae34-80242536cb24 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:27:37.551: INFO: The status of Pod pod-logs-websocket-23a66cf8-ae1d-4c5a-ae34-80242536cb24 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:27:39.607: INFO: The status of Pod pod-logs-websocket-23a66cf8-ae1d-4c5a-ae34-80242536cb24 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:27:41.554: INFO: The status of Pod pod-logs-websocket-23a66cf8-ae1d-4c5a-ae34-80242536cb24 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:27:43.563: INFO: The status of Pod pod-logs-websocket-23a66cf8-ae1d-4c5a-ae34-80242536cb24 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:27:43.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9269" for this suite.

• [SLOW TEST:10.446 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":42,"skipped":862,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:27:43.708: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-8907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:32:44.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8907" for this suite.

• [SLOW TEST:300.526 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":43,"skipped":931,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:32:44.235: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4526
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:32:44.491: INFO: Creating ReplicaSet my-hostname-basic-b7f5a095-577a-42bc-acd9-2f97c0c274eb
Mar  2 19:32:44.536: INFO: Pod name my-hostname-basic-b7f5a095-577a-42bc-acd9-2f97c0c274eb: Found 0 pods out of 1
Mar  2 19:32:49.549: INFO: Pod name my-hostname-basic-b7f5a095-577a-42bc-acd9-2f97c0c274eb: Found 1 pods out of 1
Mar  2 19:32:49.549: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b7f5a095-577a-42bc-acd9-2f97c0c274eb" is running
Mar  2 19:32:49.587: INFO: Pod "my-hostname-basic-b7f5a095-577a-42bc-acd9-2f97c0c274eb-h44jh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 19:32:44 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 19:32:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 19:32:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 19:32:44 +0000 UTC Reason: Message:}])
Mar  2 19:32:49.588: INFO: Trying to dial the pod
Mar  2 19:32:54.654: INFO: Controller my-hostname-basic-b7f5a095-577a-42bc-acd9-2f97c0c274eb: Got expected result from replica 1 [my-hostname-basic-b7f5a095-577a-42bc-acd9-2f97c0c274eb-h44jh]: "my-hostname-basic-b7f5a095-577a-42bc-acd9-2f97c0c274eb-h44jh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:32:54.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4526" for this suite.

• [SLOW TEST:10.475 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":44,"skipped":937,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:32:54.709: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-880
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 19:32:55.645: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 19:32:57.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846375, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846375, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846375, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846375, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 19:33:00.778: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar  2 19:33:00.870: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:33:00.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-880" for this suite.
STEP: Destroying namespace "webhook-880-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.393 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":45,"skipped":948,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:33:01.103: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-121
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 19:33:01.867: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 19:33:03.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846381, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846381, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846381, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846381, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 19:33:06.956: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Mar  2 19:33:12.071: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:33:12.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-121" for this suite.
STEP: Destroying namespace "webhook-121-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.521 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":46,"skipped":957,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:33:12.624: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 19:33:12.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32f94b7b-12fd-4580-81ad-b1dd17de6218" in namespace "projected-6604" to be "Succeeded or Failed"
Mar  2 19:33:12.914: INFO: Pod "downwardapi-volume-32f94b7b-12fd-4580-81ad-b1dd17de6218": Phase="Pending", Reason="", readiness=false. Elapsed: 16.255277ms
Mar  2 19:33:14.936: INFO: Pod "downwardapi-volume-32f94b7b-12fd-4580-81ad-b1dd17de6218": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03852246s
Mar  2 19:33:16.950: INFO: Pod "downwardapi-volume-32f94b7b-12fd-4580-81ad-b1dd17de6218": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052036754s
STEP: Saw pod success
Mar  2 19:33:16.950: INFO: Pod "downwardapi-volume-32f94b7b-12fd-4580-81ad-b1dd17de6218" satisfied condition "Succeeded or Failed"
Mar  2 19:33:16.964: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-32f94b7b-12fd-4580-81ad-b1dd17de6218 container client-container: <nil>
STEP: delete the pod
Mar  2 19:33:17.100: INFO: Waiting for pod downwardapi-volume-32f94b7b-12fd-4580-81ad-b1dd17de6218 to disappear
Mar  2 19:33:17.112: INFO: Pod downwardapi-volume-32f94b7b-12fd-4580-81ad-b1dd17de6218 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:33:17.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6604" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":47,"skipped":976,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:33:17.157: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7893
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5000
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1834
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:33:34.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7893" for this suite.
STEP: Destroying namespace "nsdeletetest-5000" for this suite.
Mar  2 19:33:34.149: INFO: Namespace nsdeletetest-5000 was already deleted
STEP: Destroying namespace "nsdeletetest-1834" for this suite.

• [SLOW TEST:17.018 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":48,"skipped":992,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:33:34.176: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3262
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Mar  2 19:33:34.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-3262 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar  2 19:33:34.525: INFO: stderr: ""
Mar  2 19:33:34.525: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Mar  2 19:33:34.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-3262 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Mar  2 19:33:34.782: INFO: stderr: ""
Mar  2 19:33:34.782: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Mar  2 19:33:34.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-3262 delete pods e2e-test-httpd-pod'
Mar  2 19:33:37.216: INFO: stderr: ""
Mar  2 19:33:37.217: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:33:37.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3262" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":49,"skipped":1005,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:33:37.274: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Mar  2 19:33:37.507: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:33:42.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6807" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":50,"skipped":1020,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:33:42.274: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar  2 19:33:42.508: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 19:33:42.537: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 19:33:42.553: INFO: 
Logging pods the apiserver thinks is on node 10.245.0.4 before test
Mar  2 19:33:42.582: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-03-02 17:42:16 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.582: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Mar  2 19:33:42.582: INFO: pod-init-be929501-e97a-40bc-9dfb-e76455f3635b from init-container-6807 started at 2022-03-02 19:33:37 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.582: INFO: 	Container run1 ready: true, restart count 0
Mar  2 19:33:42.582: INFO: calico-node-6bmpc from kube-system started at 2022-03-02 17:40:12 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.582: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 19:33:42.582: INFO: calico-typha-7cdb864b94-fxx52 from kube-system started at 2022-03-02 17:41:09 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.582: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 19:33:42.582: INFO: coredns-b58d5f584-tvvw8 from kube-system started at 2022-03-02 17:49:06 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.582: INFO: 	Container coredns ready: true, restart count 0
Mar  2 19:33:42.582: INFO: ibm-master-proxy-static-10.245.0.4 from kube-system started at 2022-03-02 17:39:53 +0000 UTC (2 container statuses recorded)
Mar  2 19:33:42.582: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 19:33:42.582: INFO: 	Container pause ready: true, restart count 0
Mar  2 19:33:42.582: INFO: ibm-vpc-block-csi-node-m9dxv from kube-system started at 2022-03-02 17:40:12 +0000 UTC (4 container statuses recorded)
Mar  2 19:33:42.582: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Mar  2 19:33:42.582: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Mar  2 19:33:42.582: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 19:33:42.582: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 19:33:42.582: INFO: konnectivity-agent-cnbg9 from kube-system started at 2022-03-02 17:48:31 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.582: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  2 19:33:42.582: INFO: sonobuoy from sonobuoy started at 2022-03-02 19:20:48 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.582: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 19:33:42.582: INFO: sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-867x2 from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 19:33:42.582: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 19:33:42.582: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 19:33:42.582: INFO: 
Logging pods the apiserver thinks is on node 10.245.0.5 before test
Mar  2 19:33:42.606: INFO: catalog-operator-6c4b4d7c9-gnzj6 from ibm-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.606: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 19:33:42.606: INFO: olm-operator-785cdc5884-8vxvx from ibm-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.606: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 19:33:42.606: INFO: calico-kube-controllers-dcd8d986c-g4lwp from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.606: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 19:33:42.606: INFO: calico-node-7n2gc from kube-system started at 2022-03-02 17:38:33 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.606: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 19:33:42.606: INFO: calico-typha-7cdb864b94-t4bmt from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.607: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 19:33:42.607: INFO: coredns-autoscaler-689fb74d49-hqj9c from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.607: INFO: 	Container autoscaler ready: true, restart count 0
Mar  2 19:33:42.607: INFO: coredns-b58d5f584-m7b7m from kube-system started at 2022-03-02 17:49:06 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.607: INFO: 	Container coredns ready: true, restart count 0
Mar  2 19:33:42.607: INFO: dashboard-metrics-scraper-6747f89c97-4dq9w from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.607: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar  2 19:33:42.607: INFO: ibm-master-proxy-static-10.245.0.5 from kube-system started at 2022-03-02 17:38:18 +0000 UTC (2 container statuses recorded)
Mar  2 19:33:42.607: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 19:33:42.607: INFO: 	Container pause ready: true, restart count 0
Mar  2 19:33:42.607: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2022-03-02 17:38:51 +0000 UTC (6 container statuses recorded)
Mar  2 19:33:42.607: INFO: 	Container csi-attacher ready: true, restart count 0
Mar  2 19:33:42.607: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar  2 19:33:42.607: INFO: 	Container csi-resizer ready: true, restart count 0
Mar  2 19:33:42.607: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Mar  2 19:33:42.607: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 19:33:42.607: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 19:33:42.607: INFO: ibm-vpc-block-csi-node-n5lr5 from kube-system started at 2022-03-02 17:38:33 +0000 UTC (4 container statuses recorded)
Mar  2 19:33:42.607: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Mar  2 19:33:42.607: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Mar  2 19:33:42.607: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 19:33:42.607: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 19:33:42.607: INFO: konnectivity-agent-gkp9m from kube-system started at 2022-03-02 17:48:34 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.607: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  2 19:33:42.607: INFO: kubernetes-dashboard-54c47dd995-vkxsd from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.607: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  2 19:33:42.607: INFO: public-crc8fqf88s0ubnto7pbi70-alb1-6559fd4c4b-kzxmb from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.607: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  2 19:33:42.607: INFO: sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-4g544 from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 19:33:42.607: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 19:33:42.607: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 19:33:42.607: INFO: 
Logging pods the apiserver thinks is on node 10.245.0.6 before test
Mar  2 19:33:42.631: INFO: calico-node-tl6xm from kube-system started at 2022-03-02 17:38:42 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.631: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 19:33:42.631: INFO: calico-typha-7cdb864b94-znnrz from kube-system started at 2022-03-02 17:39:02 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.631: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 19:33:42.631: INFO: coredns-b58d5f584-sdfgx from kube-system started at 2022-03-02 17:49:06 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.631: INFO: 	Container coredns ready: true, restart count 0
Mar  2 19:33:42.631: INFO: ibm-master-proxy-static-10.245.0.6 from kube-system started at 2022-03-02 17:38:23 +0000 UTC (2 container statuses recorded)
Mar  2 19:33:42.631: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 19:33:42.631: INFO: 	Container pause ready: true, restart count 0
Mar  2 19:33:42.631: INFO: ibm-vpc-block-csi-node-nkxms from kube-system started at 2022-03-02 17:38:42 +0000 UTC (4 container statuses recorded)
Mar  2 19:33:42.631: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Mar  2 19:33:42.631: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Mar  2 19:33:42.631: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 19:33:42.631: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 19:33:42.631: INFO: konnectivity-agent-bnglr from kube-system started at 2022-03-02 17:48:28 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.631: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  2 19:33:42.631: INFO: metrics-server-7b97867cc5-fpb5z from kube-system started at 2022-03-02 18:23:06 +0000 UTC (3 container statuses recorded)
Mar  2 19:33:42.631: INFO: 	Container config-watcher ready: true, restart count 0
Mar  2 19:33:42.631: INFO: 	Container metrics-server ready: true, restart count 0
Mar  2 19:33:42.631: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar  2 19:33:42.631: INFO: public-crc8fqf88s0ubnto7pbi70-alb1-6559fd4c4b-lwmtq from kube-system started at 2022-03-02 17:39:02 +0000 UTC (1 container statuses recorded)
Mar  2 19:33:42.631: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  2 19:33:42.631: INFO: sonobuoy-e2e-job-6fc4581698214e95 from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 19:33:42.631: INFO: 	Container e2e ready: true, restart count 0
Mar  2 19:33:42.631: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 19:33:42.631: INFO: sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-f24tq from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 19:33:42.631: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 19:33:42.631: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-dcdd2cec-6afb-4329-adc8-45f4f9c78371 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.245.0.5 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-dcdd2cec-6afb-4329-adc8-45f4f9c78371 off the node 10.245.0.5
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dcdd2cec-6afb-4329-adc8-45f4f9c78371
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:38:50.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9735" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.760 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":51,"skipped":1036,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:38:51.036: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1895.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1895.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1895.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1895.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1895.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1895.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 19:39:09.562: INFO: DNS probes using dns-1895/dns-test-2a24add2-7f6d-446f-b440-db9e948277fc succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:39:09.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1895" for this suite.

• [SLOW TEST:18.677 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":52,"skipped":1051,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:39:09.714: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-5325
STEP: creating replication controller nodeport-test in namespace services-5325
I0302 19:39:10.067591      21 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-5325, replica count: 2
Mar  2 19:39:13.118: INFO: Creating new exec pod
I0302 19:39:13.118676      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 19:39:16.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5325 exec execpodzw5lv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar  2 19:39:16.746: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  2 19:39:16.746: INFO: stdout: "nodeport-test-dtjcz"
Mar  2 19:39:16.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5325 exec execpodzw5lv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.84.214 80'
Mar  2 19:39:16.961: INFO: stderr: "+ nc -v -t -w 2 172.21.84.214 80\n+ echo hostName\nConnection to 172.21.84.214 80 port [tcp/http] succeeded!\n"
Mar  2 19:39:16.961: INFO: stdout: ""
Mar  2 19:39:17.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5325 exec execpodzw5lv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.84.214 80'
Mar  2 19:39:18.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.84.214 80\nConnection to 172.21.84.214 80 port [tcp/http] succeeded!\n"
Mar  2 19:39:18.222: INFO: stdout: ""
Mar  2 19:39:18.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5325 exec execpodzw5lv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.84.214 80'
Mar  2 19:39:19.185: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.84.214 80\nConnection to 172.21.84.214 80 port [tcp/http] succeeded!\n"
Mar  2 19:39:19.185: INFO: stdout: ""
Mar  2 19:39:19.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5325 exec execpodzw5lv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.84.214 80'
Mar  2 19:39:20.271: INFO: stderr: "+ nc -v -t -w 2 172.21.84.214 80\nConnection to 172.21.84.214 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Mar  2 19:39:20.271: INFO: stdout: ""
Mar  2 19:39:20.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5325 exec execpodzw5lv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.84.214 80'
Mar  2 19:39:21.190: INFO: stderr: "+ nc -v -t -w 2 172.21.84.214 80\n+ echo hostName\nConnection to 172.21.84.214 80 port [tcp/http] succeeded!\n"
Mar  2 19:39:21.190: INFO: stdout: "nodeport-test-dtjcz"
Mar  2 19:39:21.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5325 exec execpodzw5lv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.245.0.5 30302'
Mar  2 19:39:21.463: INFO: stderr: "+ nc -v -t -w 2 10.245.0.5 30302\n+ echo hostName\nConnection to 10.245.0.5 30302 port [tcp/*] succeeded!\n"
Mar  2 19:39:21.463: INFO: stdout: "nodeport-test-dtjcz"
Mar  2 19:39:21.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5325 exec execpodzw5lv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.245.0.6 30302'
Mar  2 19:39:21.737: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.245.0.6 30302\nConnection to 10.245.0.6 30302 port [tcp/*] succeeded!\n"
Mar  2 19:39:21.737: INFO: stdout: "nodeport-test-tq2fw"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:39:21.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5325" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.089 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":53,"skipped":1071,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:39:21.803: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-49f52a9a-d4c5-4f45-a4a4-f6dda64d6cfd-9946
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:39:22.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2818" for this suite.
STEP: Destroying namespace "nspatchtest-49f52a9a-d4c5-4f45-a4a4-f6dda64d6cfd-9946" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":54,"skipped":1081,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:39:22.352: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-6661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:39:22.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6661" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":55,"skipped":1097,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:39:22.666: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Mar  2 19:39:22.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-774 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  2 19:39:23.063: INFO: stderr: ""
Mar  2 19:39:23.063: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Mar  2 19:39:23.063: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  2 19:39:23.063: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-774" to be "running and ready, or succeeded"
Mar  2 19:39:23.080: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.731884ms
Mar  2 19:39:25.109: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.046472268s
Mar  2 19:39:25.109: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  2 19:39:25.109: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar  2 19:39:25.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-774 logs logs-generator logs-generator'
Mar  2 19:39:25.347: INFO: stderr: ""
Mar  2 19:39:25.347: INFO: stdout: "I0302 19:39:24.421622       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/jfd 497\nI0302 19:39:24.621751       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/zrmc 538\nI0302 19:39:24.821678       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/bfr 368\nI0302 19:39:25.022097       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/ppbx 582\nI0302 19:39:25.222558       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/g8t 447\n"
STEP: limiting log lines
Mar  2 19:39:25.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-774 logs logs-generator logs-generator --tail=1'
Mar  2 19:39:25.454: INFO: stderr: ""
Mar  2 19:39:25.454: INFO: stdout: "I0302 19:39:25.421720       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/lks 537\n"
Mar  2 19:39:25.454: INFO: got output "I0302 19:39:25.421720       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/lks 537\n"
STEP: limiting log bytes
Mar  2 19:39:25.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-774 logs logs-generator logs-generator --limit-bytes=1'
Mar  2 19:39:25.544: INFO: stderr: ""
Mar  2 19:39:25.545: INFO: stdout: "I"
Mar  2 19:39:25.545: INFO: got output "I"
STEP: exposing timestamps
Mar  2 19:39:25.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-774 logs logs-generator logs-generator --tail=1 --timestamps'
Mar  2 19:39:25.725: INFO: stderr: ""
Mar  2 19:39:25.725: INFO: stdout: "2022-03-02T19:39:25.622138055Z I0302 19:39:25.622053       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/hsh 409\n"
Mar  2 19:39:25.725: INFO: got output "2022-03-02T19:39:25.622138055Z I0302 19:39:25.622053       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/hsh 409\n"
STEP: restricting to a time range
Mar  2 19:39:28.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-774 logs logs-generator logs-generator --since=1s'
Mar  2 19:39:28.339: INFO: stderr: ""
Mar  2 19:39:28.339: INFO: stdout: "I0302 19:39:27.422011       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/cw4 386\nI0302 19:39:27.622396       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/s8k 540\nI0302 19:39:27.833538       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/f2vn 297\nI0302 19:39:28.021931       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/bqbp 595\nI0302 19:39:28.222278       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/5wd 455\n"
Mar  2 19:39:28.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-774 logs logs-generator logs-generator --since=24h'
Mar  2 19:39:28.458: INFO: stderr: ""
Mar  2 19:39:28.458: INFO: stdout: "I0302 19:39:24.421622       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/jfd 497\nI0302 19:39:24.621751       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/zrmc 538\nI0302 19:39:24.821678       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/bfr 368\nI0302 19:39:25.022097       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/ppbx 582\nI0302 19:39:25.222558       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/g8t 447\nI0302 19:39:25.421720       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/lks 537\nI0302 19:39:25.622053       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/hsh 409\nI0302 19:39:25.822475       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/687 524\nI0302 19:39:26.021711       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/txb 449\nI0302 19:39:26.222055       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/zwn 252\nI0302 19:39:26.422366       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/6xzx 537\nI0302 19:39:26.622677       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/l9k 286\nI0302 19:39:26.821988       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/7bmf 259\nI0302 19:39:27.022383       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/mf4 510\nI0302 19:39:27.222698       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/f5j 496\nI0302 19:39:27.422011       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/cw4 386\nI0302 19:39:27.622396       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/s8k 540\nI0302 19:39:27.833538       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/f2vn 297\nI0302 19:39:28.021931       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/bqbp 595\nI0302 19:39:28.222278       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/5wd 455\nI0302 19:39:28.422660       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/kxfv 246\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Mar  2 19:39:28.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-774 delete pod logs-generator'
Mar  2 19:39:30.036: INFO: stderr: ""
Mar  2 19:39:30.036: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:39:30.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-774" for this suite.

• [SLOW TEST:7.437 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1393
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":56,"skipped":1097,"failed":0}
SSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:39:30.103: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-2902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar  2 19:39:30.337: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar  2 19:39:30.383: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  2 19:39:30.384: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar  2 19:39:30.434: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  2 19:39:30.434: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar  2 19:39:30.466: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar  2 19:39:30.466: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar  2 19:39:37.615: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:39:37.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-2902" for this suite.

• [SLOW TEST:7.584 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":57,"skipped":1101,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:39:37.687: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4288
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  2 19:39:37.948: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 19:40:38.085: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Mar  2 19:40:38.170: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar  2 19:40:38.188: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar  2 19:40:38.233: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar  2 19:40:38.250: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar  2 19:40:38.292: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar  2 19:40:38.305: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:40:54.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4288" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:77.149 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":58,"skipped":1114,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:40:54.837: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2434
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:40:55.054: INFO: Creating deployment "test-recreate-deployment"
Mar  2 19:40:55.067: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  2 19:40:55.090: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  2 19:40:57.136: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  2 19:40:57.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846855, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846855, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846855, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846855, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6cb8b65c46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 19:40:59.174: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  2 19:40:59.234: INFO: Updating deployment test-recreate-deployment
Mar  2 19:40:59.234: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar  2 19:40:59.482: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2434  a1bd1d27-1fd5-496c-a454-3ae4f2f94c7b 22696 2 2022-03-02 19:40:55 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-03-02 19:40:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 19:40:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f89a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-03-02 19:40:59 +0000 UTC,LastTransitionTime:2022-03-02 19:40:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2022-03-02 19:40:59 +0000 UTC,LastTransitionTime:2022-03-02 19:40:55 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 19:40:59.498: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-2434  382f4a4f-ba24-48e0-b1d4-f609818771ee 22695 1 2022-03-02 19:40:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment a1bd1d27-1fd5-496c-a454-3ae4f2f94c7b 0xc002959200 0xc002959201}] []  [{kube-controller-manager Update apps/v1 2022-03-02 19:40:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a1bd1d27-1fd5-496c-a454-3ae4f2f94c7b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 19:40:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002959298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 19:40:59.498: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  2 19:40:59.498: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-2434  3a0d0e4c-df6e-4a74-a7ae-d9a66a4bf0b5 22685 2 2022-03-02 19:40:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment a1bd1d27-1fd5-496c-a454-3ae4f2f94c7b 0xc0029590d7 0xc0029590d8}] []  [{kube-controller-manager Update apps/v1 2022-03-02 19:40:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a1bd1d27-1fd5-496c-a454-3ae4f2f94c7b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 19:40:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002959198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 19:40:59.510: INFO: Pod "test-recreate-deployment-85d47dcb4-8mks6" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-8mks6 test-recreate-deployment-85d47dcb4- deployment-2434  d9d01305-b7d1-45e3-8c45-bc42db3a6ef1 22697 0 2022-03-02 19:40:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 382f4a4f-ba24-48e0-b1d4-f609818771ee 0xc003f89df0 0xc003f89df1}] []  [{kube-controller-manager Update v1 2022-03-02 19:40:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"382f4a4f-ba24-48e0-b1d4-f609818771ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-03-02 19:40:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xck68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xck68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:40:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:40:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:40:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:40:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.4,PodIP:,StartTime:2022-03-02 19:40:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:40:59.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2434" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":59,"skipped":1123,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:40:59.553: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 19:41:00.571: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 19:41:03.666: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:41:14.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8526" for this suite.
STEP: Destroying namespace "webhook-8526-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.846 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":60,"skipped":1130,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:41:14.399: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6937
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:41:14.646: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  2 19:41:14.678: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 19:41:19.705: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 19:41:19.705: INFO: Creating deployment "test-rolling-update-deployment"
Mar  2 19:41:19.717: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  2 19:41:19.748: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  2 19:41:21.778: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  2 19:41:21.788: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846879, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846879, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846879, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846879, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-585b757574\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 19:41:23.808: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar  2 19:41:23.873: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6937  c568a52e-e0e4-4697-83f5-6377472dfdb8 22988 1 2022-03-02 19:41:19 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-03-02 19:41:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 19:41:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004644fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-02 19:41:19 +0000 UTC,LastTransitionTime:2022-03-02 19:41:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2022-03-02 19:41:22 +0000 UTC,LastTransitionTime:2022-03-02 19:41:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 19:41:23.887: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-6937  315372fe-768a-4628-aa72-6d68613eda19 22978 1 2022-03-02 19:41:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c568a52e-e0e4-4697-83f5-6377472dfdb8 0xc004645497 0xc004645498}] []  [{kube-controller-manager Update apps/v1 2022-03-02 19:41:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c568a52e-e0e4-4697-83f5-6377472dfdb8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 19:41:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004645548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 19:41:23.887: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  2 19:41:23.887: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6937  bcbefa52-aab0-4f05-ada0-55e3e97fb76a 22987 2 2022-03-02 19:41:14 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c568a52e-e0e4-4697-83f5-6377472dfdb8 0xc004645367 0xc004645368}] []  [{e2e.test Update apps/v1 2022-03-02 19:41:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 19:41:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c568a52e-e0e4-4697-83f5-6377472dfdb8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-03-02 19:41:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004645428 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 19:41:23.899: INFO: Pod "test-rolling-update-deployment-585b757574-4kqlw" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-4kqlw test-rolling-update-deployment-585b757574- deployment-6937  8fc3a863-ce1e-42f6-b64d-f37c9c5ee0d5 22977 0 2022-03-02 19:41:19 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/containerID:64619642a6cba401ec9e1bec9a157119495fc7db572c0a1ee4dd83ec91a8274b cni.projectcalico.org/podIP:172.17.125.252/32 cni.projectcalico.org/podIPs:172.17.125.252/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 315372fe-768a-4628-aa72-6d68613eda19 0xc0046459c7 0xc0046459c8}] []  [{kube-controller-manager Update v1 2022-03-02 19:41:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"315372fe-768a-4628-aa72-6d68613eda19\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 19:41:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 19:41:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.125.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxcfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxcfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:41:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:41:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:41:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 19:41:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.5,PodIP:172.17.125.252,StartTime:2022-03-02 19:41:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 19:41:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://14163692134af5b3d6b9d33a158b75642b0a6c0f3af794e5b1f3c7959ced02fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.125.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:41:23.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6937" for this suite.

• [SLOW TEST:9.546 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":61,"skipped":1145,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:41:23.946: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-2757
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2757 to expose endpoints map[]
Mar  2 19:41:24.261: INFO: successfully validated that service multi-endpoint-test in namespace services-2757 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2757
Mar  2 19:41:24.302: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:41:26.353: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:41:28.325: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2757 to expose endpoints map[pod1:[100]]
Mar  2 19:41:28.376: INFO: successfully validated that service multi-endpoint-test in namespace services-2757 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2757
Mar  2 19:41:28.412: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:41:30.436: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2757 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  2 19:41:30.496: INFO: successfully validated that service multi-endpoint-test in namespace services-2757 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Mar  2 19:41:30.496: INFO: Creating new exec pod
Mar  2 19:41:33.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2757 exec execpodhsh22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Mar  2 19:41:33.838: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar  2 19:41:33.838: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 19:41:33.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2757 exec execpodhsh22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.188.224 80'
Mar  2 19:41:34.103: INFO: stderr: "+ nc -v -t -w 2 172.21.188.224 80\n+ echo hostName\nConnection to 172.21.188.224 80 port [tcp/http] succeeded!\n"
Mar  2 19:41:34.103: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 19:41:34.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2757 exec execpodhsh22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Mar  2 19:41:34.333: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar  2 19:41:34.333: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 19:41:34.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2757 exec execpodhsh22 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.188.224 81'
Mar  2 19:41:34.591: INFO: stderr: "+ nc -v -t -w 2 172.21.188.224 81\n+ echo hostName\nConnection to 172.21.188.224 81 port [tcp/*] succeeded!\n"
Mar  2 19:41:34.591: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2757
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2757 to expose endpoints map[pod2:[101]]
Mar  2 19:41:34.706: INFO: successfully validated that service multi-endpoint-test in namespace services-2757 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2757
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2757 to expose endpoints map[]
Mar  2 19:41:34.800: INFO: successfully validated that service multi-endpoint-test in namespace services-2757 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:41:34.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2757" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.974 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":62,"skipped":1154,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:41:34.920: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Mar  2 19:41:35.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 create -f -'
Mar  2 19:41:35.411: INFO: stderr: ""
Mar  2 19:41:35.411: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 19:41:35.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 19:41:35.519: INFO: stderr: ""
Mar  2 19:41:35.519: INFO: stdout: "update-demo-nautilus-sk8r5 update-demo-nautilus-zj86g "
Mar  2 19:41:35.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 get pods update-demo-nautilus-sk8r5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 19:41:35.611: INFO: stderr: ""
Mar  2 19:41:35.611: INFO: stdout: ""
Mar  2 19:41:35.611: INFO: update-demo-nautilus-sk8r5 is created but not running
Mar  2 19:41:40.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 19:41:40.690: INFO: stderr: ""
Mar  2 19:41:40.690: INFO: stdout: "update-demo-nautilus-sk8r5 update-demo-nautilus-zj86g "
Mar  2 19:41:40.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 get pods update-demo-nautilus-sk8r5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 19:41:40.773: INFO: stderr: ""
Mar  2 19:41:40.773: INFO: stdout: ""
Mar  2 19:41:40.773: INFO: update-demo-nautilus-sk8r5 is created but not running
Mar  2 19:41:45.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 19:41:45.870: INFO: stderr: ""
Mar  2 19:41:45.871: INFO: stdout: "update-demo-nautilus-sk8r5 update-demo-nautilus-zj86g "
Mar  2 19:41:45.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 get pods update-demo-nautilus-sk8r5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 19:41:45.949: INFO: stderr: ""
Mar  2 19:41:45.949: INFO: stdout: "true"
Mar  2 19:41:45.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 get pods update-demo-nautilus-sk8r5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 19:41:46.015: INFO: stderr: ""
Mar  2 19:41:46.016: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar  2 19:41:46.016: INFO: validating pod update-demo-nautilus-sk8r5
Mar  2 19:41:46.065: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 19:41:46.066: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 19:41:46.066: INFO: update-demo-nautilus-sk8r5 is verified up and running
Mar  2 19:41:46.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 get pods update-demo-nautilus-zj86g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 19:41:46.145: INFO: stderr: ""
Mar  2 19:41:46.145: INFO: stdout: "true"
Mar  2 19:41:46.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 get pods update-demo-nautilus-zj86g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 19:41:46.210: INFO: stderr: ""
Mar  2 19:41:46.210: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar  2 19:41:46.210: INFO: validating pod update-demo-nautilus-zj86g
Mar  2 19:41:46.251: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 19:41:46.251: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 19:41:46.251: INFO: update-demo-nautilus-zj86g is verified up and running
STEP: using delete to clean up resources
Mar  2 19:41:46.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 delete --grace-period=0 --force -f -'
Mar  2 19:41:46.350: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 19:41:46.350: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 19:41:46.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 get rc,svc -l name=update-demo --no-headers'
Mar  2 19:41:46.448: INFO: stderr: "No resources found in kubectl-9341 namespace.\n"
Mar  2 19:41:46.448: INFO: stdout: ""
Mar  2 19:41:46.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9341 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 19:41:46.523: INFO: stderr: ""
Mar  2 19:41:46.523: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:41:46.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9341" for this suite.

• [SLOW TEST:11.677 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":63,"skipped":1161,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:41:46.597: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-ae938a5a-03c2-45c9-9c38-bb94182a975b
STEP: Creating a pod to test consume configMaps
Mar  2 19:41:46.956: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b1570d6-a036-4092-a8f6-36059650ef8a" in namespace "configmap-8816" to be "Succeeded or Failed"
Mar  2 19:41:46.969: INFO: Pod "pod-configmaps-9b1570d6-a036-4092-a8f6-36059650ef8a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.082533ms
Mar  2 19:41:48.996: INFO: Pod "pod-configmaps-9b1570d6-a036-4092-a8f6-36059650ef8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04005445s
Mar  2 19:41:51.046: INFO: Pod "pod-configmaps-9b1570d6-a036-4092-a8f6-36059650ef8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.090808028s
STEP: Saw pod success
Mar  2 19:41:51.046: INFO: Pod "pod-configmaps-9b1570d6-a036-4092-a8f6-36059650ef8a" satisfied condition "Succeeded or Failed"
Mar  2 19:41:51.091: INFO: Trying to get logs from node 10.245.0.5 pod pod-configmaps-9b1570d6-a036-4092-a8f6-36059650ef8a container agnhost-container: <nil>
STEP: delete the pod
Mar  2 19:41:51.249: INFO: Waiting for pod pod-configmaps-9b1570d6-a036-4092-a8f6-36059650ef8a to disappear
Mar  2 19:41:51.261: INFO: Pod pod-configmaps-9b1570d6-a036-4092-a8f6-36059650ef8a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:41:51.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8816" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":64,"skipped":1168,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:41:51.333: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4486
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:41:58.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4486" for this suite.

• [SLOW TEST:7.438 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":65,"skipped":1196,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:41:58.772: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-280
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Mar  2 19:41:59.063: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Mar  2 19:42:01.125: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Mar  2 19:42:03.179: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:42:05.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-280" for this suite.

• [SLOW TEST:6.474 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":66,"skipped":1233,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:42:05.246: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8491
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 19:42:05.934: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 19:42:09.024: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
Mar  2 19:42:14.155: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:42:14.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8491" for this suite.
STEP: Destroying namespace "webhook-8491-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.406 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":67,"skipped":1240,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:42:14.653: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-24
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Mar  2 19:42:14.881: INFO: PodSpec: initContainers in spec.initContainers
Mar  2 19:42:57.617: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-496723ba-4151-49ff-a6a8-ad5c6e6db3c4", GenerateName:"", Namespace:"init-container-24", SelfLink:"", UID:"67617744-1bd2-4663-823a-a2fa4d2d3a2b", ResourceVersion:"23533", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63781846934, loc:(*time.Location)(0xa0aaf60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"881691675"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"04efc62f3fe12f28620ea87a4136d0d053dbc4548ca8f62a3f60f4b31224fda3", "cni.projectcalico.org/podIP":"172.17.100.155/32", "cni.projectcalico.org/podIPs":"172.17.100.155/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004e24048), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e24060), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004e24078), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e24090), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004e240a8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e240c0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-bj2pf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0031ae000), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bj2pf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bj2pf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bj2pf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004d8a0f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.245.0.4", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003bf0000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004d8a180)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004d8a1a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004d8a1a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004d8a1ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004d86020), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846934, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846934, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846934, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846934, loc:(*time.Location)(0xa0aaf60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.245.0.4", PodIP:"172.17.100.155", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.17.100.155"}}, StartTime:(*v1.Time)(0xc004e240f0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003bf00e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003bf0150)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"containerd://99d6267b314b310746f876888cef0e50404efd11db18aa55bf5f4319d0b716ed", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0031ae100), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0031ae0c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc004d8a22f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:42:57.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-24" for this suite.

• [SLOW TEST:43.034 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":68,"skipped":1251,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:42:57.689: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:42:58.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6808" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":69,"skipped":1278,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:42:58.104: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Mar  2 19:42:58.494: INFO: Waiting up to 5m0s for pod "var-expansion-9d2f7056-4d61-46bf-bc88-a5a317e8a110" in namespace "var-expansion-2077" to be "Succeeded or Failed"
Mar  2 19:42:58.509: INFO: Pod "var-expansion-9d2f7056-4d61-46bf-bc88-a5a317e8a110": Phase="Pending", Reason="", readiness=false. Elapsed: 15.432942ms
Mar  2 19:43:00.531: INFO: Pod "var-expansion-9d2f7056-4d61-46bf-bc88-a5a317e8a110": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037085604s
Mar  2 19:43:02.549: INFO: Pod "var-expansion-9d2f7056-4d61-46bf-bc88-a5a317e8a110": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055400167s
STEP: Saw pod success
Mar  2 19:43:02.549: INFO: Pod "var-expansion-9d2f7056-4d61-46bf-bc88-a5a317e8a110" satisfied condition "Succeeded or Failed"
Mar  2 19:43:02.562: INFO: Trying to get logs from node 10.245.0.4 pod var-expansion-9d2f7056-4d61-46bf-bc88-a5a317e8a110 container dapi-container: <nil>
STEP: delete the pod
Mar  2 19:43:02.710: INFO: Waiting for pod var-expansion-9d2f7056-4d61-46bf-bc88-a5a317e8a110 to disappear
Mar  2 19:43:02.722: INFO: Pod var-expansion-9d2f7056-4d61-46bf-bc88-a5a317e8a110 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:43:02.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2077" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":70,"skipped":1299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:43:02.787: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6733
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 19:43:03.500: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 19:43:05.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846983, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846983, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846983, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781846983, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 19:43:08.595: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:43:08.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6733" for this suite.
STEP: Destroying namespace "webhook-6733-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.245 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":71,"skipped":1323,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:43:09.033: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-4663
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 19:43:09.720: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 19:43:09.754: INFO: waiting for watch events with expected annotations
Mar  2 19:43:09.755: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:43:09.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-4663" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":72,"skipped":1338,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:43:10.001: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6761
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:43:26.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6761" for this suite.

• [SLOW TEST:16.626 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":73,"skipped":1353,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:43:26.629: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1428
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Mar  2 19:43:26.918: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 19:43:31.966: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:43:32.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1428" for this suite.

• [SLOW TEST:5.598 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":74,"skipped":1380,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:43:32.228: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:43:45.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-606" for this suite.

• [SLOW TEST:13.518 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":75,"skipped":1405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:43:45.748: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-1984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar  2 19:43:45.986: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 19:44:46.164: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:44:46.190: INFO: Starting informer...
STEP: Starting pod...
Mar  2 19:44:46.284: INFO: Pod is running on 10.245.0.4. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar  2 19:44:46.359: INFO: Pod wasn't evicted. Proceeding
Mar  2 19:44:46.359: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar  2 19:46:01.401: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:46:01.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1984" for this suite.

• [SLOW TEST:135.719 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":76,"skipped":1430,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:46:01.467: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Mar  2 19:46:01.800: INFO: The status of Pod labelsupdatecdcbf70d-df5c-4822-b4b9-5022b58314b7 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:46:03.814: INFO: The status of Pod labelsupdatecdcbf70d-df5c-4822-b4b9-5022b58314b7 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:46:05.821: INFO: The status of Pod labelsupdatecdcbf70d-df5c-4822-b4b9-5022b58314b7 is Running (Ready = true)
Mar  2 19:46:06.520: INFO: Successfully updated pod "labelsupdatecdcbf70d-df5c-4822-b4b9-5022b58314b7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:46:08.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2018" for this suite.

• [SLOW TEST:7.169 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":77,"skipped":1448,"failed":0}
SSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:46:08.637: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-9104
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 19:46:08.970: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar  2 19:46:08.993: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 19:46:09.041: INFO: waiting for watch events with expected annotations
Mar  2 19:46:09.041: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:46:09.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9104" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":78,"skipped":1455,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:46:09.196: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4736.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4736.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4736.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4736.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4736.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4736.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 19:46:13.731: INFO: DNS probes using dns-4736/dns-test-361365dd-3e2f-4fac-b7e9-64f5e1d12330 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:46:13.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4736" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":79,"skipped":1472,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:46:13.831: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0302 19:46:15.282099      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 19:46:15.282: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:46:15.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9326" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":80,"skipped":1472,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:46:15.321: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5603
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0302 19:46:55.716741      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 19:46:55.716: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar  2 19:46:55.716: INFO: Deleting pod "simpletest.rc-5rcpc" in namespace "gc-5603"
Mar  2 19:46:55.756: INFO: Deleting pod "simpletest.rc-6mlpk" in namespace "gc-5603"
Mar  2 19:46:55.796: INFO: Deleting pod "simpletest.rc-9bhkz" in namespace "gc-5603"
Mar  2 19:46:55.836: INFO: Deleting pod "simpletest.rc-cckcf" in namespace "gc-5603"
Mar  2 19:46:55.901: INFO: Deleting pod "simpletest.rc-dzts9" in namespace "gc-5603"
Mar  2 19:46:56.003: INFO: Deleting pod "simpletest.rc-nmhft" in namespace "gc-5603"
Mar  2 19:46:56.038: INFO: Deleting pod "simpletest.rc-pfd5p" in namespace "gc-5603"
Mar  2 19:46:56.137: INFO: Deleting pod "simpletest.rc-v58vg" in namespace "gc-5603"
Mar  2 19:46:56.217: INFO: Deleting pod "simpletest.rc-v7wpw" in namespace "gc-5603"
Mar  2 19:46:56.259: INFO: Deleting pod "simpletest.rc-z7d66" in namespace "gc-5603"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:46:56.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5603" for this suite.

• [SLOW TEST:41.042 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":81,"skipped":1538,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:46:56.364: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4260
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0302 19:46:57.828164      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 19:46:57.828: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:46:57.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4260" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":82,"skipped":1545,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:46:57.875: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar  2 19:46:58.131: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  2 19:47:03.151: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:47:04.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7534" for this suite.

• [SLOW TEST:6.420 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":83,"skipped":1545,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:47:04.295: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Mar  2 19:47:04.553: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar  2 19:47:09.580: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Mar  2 19:47:09.596: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:47:09.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6491" for this suite.

• [SLOW TEST:5.430 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":84,"skipped":1566,"failed":0}
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:47:09.727: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:47:09.964: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar  2 19:47:10.002: INFO: The status of Pod pod-exec-websocket-a19c5742-c902-45d7-b478-ccbc1d785b3d is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:47:12.016: INFO: The status of Pod pod-exec-websocket-a19c5742-c902-45d7-b478-ccbc1d785b3d is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:47:14.027: INFO: The status of Pod pod-exec-websocket-a19c5742-c902-45d7-b478-ccbc1d785b3d is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:47:14.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6643" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":85,"skipped":1566,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:47:14.223: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4688
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-4688
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4688 to expose endpoints map[]
Mar  2 19:47:14.571: INFO: successfully validated that service endpoint-test2 in namespace services-4688 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4688
Mar  2 19:47:14.613: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:47:16.632: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4688 to expose endpoints map[pod1:[80]]
Mar  2 19:47:16.686: INFO: successfully validated that service endpoint-test2 in namespace services-4688 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Mar  2 19:47:16.686: INFO: Creating new exec pod
Mar  2 19:47:21.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4688 exec execpodkw825 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 19:47:21.998: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  2 19:47:21.998: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 19:47:21.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4688 exec execpodkw825 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.241.22 80'
Mar  2 19:47:22.205: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.241.22 80\nConnection to 172.21.241.22 80 port [tcp/http] succeeded!\n"
Mar  2 19:47:22.205: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-4688
Mar  2 19:47:22.241: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:47:24.261: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4688 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  2 19:47:24.380: INFO: successfully validated that service endpoint-test2 in namespace services-4688 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Mar  2 19:47:25.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4688 exec execpodkw825 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 19:47:25.618: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  2 19:47:25.618: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 19:47:25.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4688 exec execpodkw825 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.241.22 80'
Mar  2 19:47:25.839: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.241.22 80\nConnection to 172.21.241.22 80 port [tcp/http] succeeded!\n"
Mar  2 19:47:25.839: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4688
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4688 to expose endpoints map[pod2:[80]]
Mar  2 19:47:25.948: INFO: successfully validated that service endpoint-test2 in namespace services-4688 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Mar  2 19:47:26.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4688 exec execpodkw825 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 19:47:29.197: INFO: rc: 1
Mar  2 19:47:29.197: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4688 exec execpodkw825 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80:
Command stdout:

stderr:
+ nc -v -t -w 2 endpoint-test2 80
+ echo hostName
nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Mar  2 19:47:30.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4688 exec execpodkw825 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 19:47:30.459: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  2 19:47:30.459: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 19:47:30.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4688 exec execpodkw825 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.241.22 80'
Mar  2 19:47:30.681: INFO: stderr: "+ nc -v -t -w 2 172.21.241.22 80\n+ echo hostName\nConnection to 172.21.241.22 80 port [tcp/http] succeeded!\n"
Mar  2 19:47:30.681: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-4688
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4688 to expose endpoints map[]
Mar  2 19:47:30.768: INFO: successfully validated that service endpoint-test2 in namespace services-4688 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:47:30.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4688" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:16.636 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":86,"skipped":1587,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:47:30.862: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6347
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-3801ef5e-ba50-4e8e-8929-07a0152baaed
STEP: Creating a pod to test consume secrets
Mar  2 19:47:31.169: INFO: Waiting up to 5m0s for pod "pod-secrets-298788b0-b8ef-4511-9ed5-2c868b8931d3" in namespace "secrets-6347" to be "Succeeded or Failed"
Mar  2 19:47:31.210: INFO: Pod "pod-secrets-298788b0-b8ef-4511-9ed5-2c868b8931d3": Phase="Pending", Reason="", readiness=false. Elapsed: 41.262291ms
Mar  2 19:47:33.232: INFO: Pod "pod-secrets-298788b0-b8ef-4511-9ed5-2c868b8931d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062802668s
Mar  2 19:47:35.246: INFO: Pod "pod-secrets-298788b0-b8ef-4511-9ed5-2c868b8931d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077263361s
STEP: Saw pod success
Mar  2 19:47:35.246: INFO: Pod "pod-secrets-298788b0-b8ef-4511-9ed5-2c868b8931d3" satisfied condition "Succeeded or Failed"
Mar  2 19:47:35.257: INFO: Trying to get logs from node 10.245.0.4 pod pod-secrets-298788b0-b8ef-4511-9ed5-2c868b8931d3 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 19:47:35.309: INFO: Waiting for pod pod-secrets-298788b0-b8ef-4511-9ed5-2c868b8931d3 to disappear
Mar  2 19:47:35.321: INFO: Pod pod-secrets-298788b0-b8ef-4511-9ed5-2c868b8931d3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:47:35.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6347" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1610,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:47:35.369: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2051
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:47:35.709: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-c10587ed-6120-474d-9cbf-1ebdfe04484a" in namespace "security-context-test-2051" to be "Succeeded or Failed"
Mar  2 19:47:35.721: INFO: Pod "busybox-readonly-false-c10587ed-6120-474d-9cbf-1ebdfe04484a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.368781ms
Mar  2 19:47:37.742: INFO: Pod "busybox-readonly-false-c10587ed-6120-474d-9cbf-1ebdfe04484a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032758814s
Mar  2 19:47:39.764: INFO: Pod "busybox-readonly-false-c10587ed-6120-474d-9cbf-1ebdfe04484a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054718901s
Mar  2 19:47:39.764: INFO: Pod "busybox-readonly-false-c10587ed-6120-474d-9cbf-1ebdfe04484a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:47:39.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2051" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":88,"skipped":1659,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:47:39.806: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9664
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:47:40.043: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar  2 19:47:44.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 --namespace=crd-publish-openapi-9664 create -f -'
Mar  2 19:47:45.443: INFO: stderr: ""
Mar  2 19:47:45.443: INFO: stdout: "e2e-test-crd-publish-openapi-9909-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 19:47:45.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 --namespace=crd-publish-openapi-9664 delete e2e-test-crd-publish-openapi-9909-crds test-foo'
Mar  2 19:47:45.550: INFO: stderr: ""
Mar  2 19:47:45.550: INFO: stdout: "e2e-test-crd-publish-openapi-9909-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  2 19:47:45.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 --namespace=crd-publish-openapi-9664 apply -f -'
Mar  2 19:47:46.492: INFO: stderr: ""
Mar  2 19:47:46.492: INFO: stdout: "e2e-test-crd-publish-openapi-9909-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 19:47:46.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 --namespace=crd-publish-openapi-9664 delete e2e-test-crd-publish-openapi-9909-crds test-foo'
Mar  2 19:47:46.614: INFO: stderr: ""
Mar  2 19:47:46.614: INFO: stdout: "e2e-test-crd-publish-openapi-9909-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar  2 19:47:46.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 --namespace=crd-publish-openapi-9664 create -f -'
Mar  2 19:47:46.788: INFO: rc: 1
Mar  2 19:47:46.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 --namespace=crd-publish-openapi-9664 apply -f -'
Mar  2 19:47:46.955: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar  2 19:47:46.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 --namespace=crd-publish-openapi-9664 create -f -'
Mar  2 19:47:47.130: INFO: rc: 1
Mar  2 19:47:47.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 --namespace=crd-publish-openapi-9664 apply -f -'
Mar  2 19:47:47.301: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar  2 19:47:47.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 explain e2e-test-crd-publish-openapi-9909-crds'
Mar  2 19:47:47.479: INFO: stderr: ""
Mar  2 19:47:47.479: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9909-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar  2 19:47:47.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 explain e2e-test-crd-publish-openapi-9909-crds.metadata'
Mar  2 19:47:47.654: INFO: stderr: ""
Mar  2 19:47:47.654: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9909-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  2 19:47:47.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 explain e2e-test-crd-publish-openapi-9909-crds.spec'
Mar  2 19:47:47.813: INFO: stderr: ""
Mar  2 19:47:47.813: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9909-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  2 19:47:47.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 explain e2e-test-crd-publish-openapi-9909-crds.spec.bars'
Mar  2 19:47:47.988: INFO: stderr: ""
Mar  2 19:47:47.988: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9909-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar  2 19:47:47.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-9664 explain e2e-test-crd-publish-openapi-9909-crds.spec.bars2'
Mar  2 19:47:48.161: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:47:51.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9664" for this suite.

• [SLOW TEST:11.808 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":89,"skipped":1664,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:47:51.615: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  2 19:47:52.011: INFO: Waiting up to 5m0s for pod "pod-310fe948-abed-4121-99fd-f38ed6272404" in namespace "emptydir-2289" to be "Succeeded or Failed"
Mar  2 19:47:52.024: INFO: Pod "pod-310fe948-abed-4121-99fd-f38ed6272404": Phase="Pending", Reason="", readiness=false. Elapsed: 12.379201ms
Mar  2 19:47:54.055: INFO: Pod "pod-310fe948-abed-4121-99fd-f38ed6272404": Phase="Running", Reason="", readiness=true. Elapsed: 2.043344649s
Mar  2 19:47:56.081: INFO: Pod "pod-310fe948-abed-4121-99fd-f38ed6272404": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069472164s
STEP: Saw pod success
Mar  2 19:47:56.081: INFO: Pod "pod-310fe948-abed-4121-99fd-f38ed6272404" satisfied condition "Succeeded or Failed"
Mar  2 19:47:56.116: INFO: Trying to get logs from node 10.245.0.4 pod pod-310fe948-abed-4121-99fd-f38ed6272404 container test-container: <nil>
STEP: delete the pod
Mar  2 19:47:56.254: INFO: Waiting for pod pod-310fe948-abed-4121-99fd-f38ed6272404 to disappear
Mar  2 19:47:56.267: INFO: Pod pod-310fe948-abed-4121-99fd-f38ed6272404 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:47:56.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2289" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":90,"skipped":1676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:47:56.359: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Mar  2 19:47:56.656: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:47:58.675: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Mar  2 19:47:58.718: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:48:00.736: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:48:02.736: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  2 19:48:02.845: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 19:48:02.890: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 19:48:04.890: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 19:48:04.907: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:48:04.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3973" for this suite.

• [SLOW TEST:8.621 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":91,"skipped":1711,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:48:04.980: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0302 19:48:11.387631      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 19:48:11.387: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:48:11.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6430" for this suite.

• [SLOW TEST:6.448 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":92,"skipped":1725,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:48:11.429: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5920
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Mar  2 19:48:11.662: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar  2 19:48:11.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5920 create -f -'
Mar  2 19:48:12.387: INFO: stderr: ""
Mar  2 19:48:12.388: INFO: stdout: "service/agnhost-replica created\n"
Mar  2 19:48:12.388: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar  2 19:48:12.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5920 create -f -'
Mar  2 19:48:12.597: INFO: stderr: ""
Mar  2 19:48:12.597: INFO: stdout: "service/agnhost-primary created\n"
Mar  2 19:48:12.597: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  2 19:48:12.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5920 create -f -'
Mar  2 19:48:12.772: INFO: stderr: ""
Mar  2 19:48:12.772: INFO: stdout: "service/frontend created\n"
Mar  2 19:48:12.772: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar  2 19:48:12.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5920 create -f -'
Mar  2 19:48:13.119: INFO: stderr: ""
Mar  2 19:48:13.119: INFO: stdout: "deployment.apps/frontend created\n"
Mar  2 19:48:13.119: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 19:48:13.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5920 create -f -'
Mar  2 19:48:14.001: INFO: stderr: ""
Mar  2 19:48:14.001: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar  2 19:48:14.001: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 19:48:14.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5920 create -f -'
Mar  2 19:48:14.193: INFO: stderr: ""
Mar  2 19:48:14.193: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Mar  2 19:48:14.193: INFO: Waiting for all frontend pods to be Running.
Mar  2 19:48:19.246: INFO: Waiting for frontend to serve content.
Mar  2 19:48:19.305: INFO: Trying to add a new entry to the guestbook.
Mar  2 19:48:19.425: INFO: Verifying that added entry can be retrieved.
Mar  2 19:48:19.454: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Mar  2 19:48:24.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5920 delete --grace-period=0 --force -f -'
Mar  2 19:48:24.651: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 19:48:24.651: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 19:48:24.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5920 delete --grace-period=0 --force -f -'
Mar  2 19:48:24.766: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 19:48:24.766: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 19:48:24.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5920 delete --grace-period=0 --force -f -'
Mar  2 19:48:24.906: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 19:48:24.906: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 19:48:24.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5920 delete --grace-period=0 --force -f -'
Mar  2 19:48:24.988: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 19:48:24.988: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 19:48:24.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5920 delete --grace-period=0 --force -f -'
Mar  2 19:48:25.135: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 19:48:25.135: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 19:48:25.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5920 delete --grace-period=0 --force -f -'
Mar  2 19:48:25.245: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 19:48:25.245: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:48:25.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5920" for this suite.

• [SLOW TEST:13.874 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":93,"skipped":1731,"failed":0}
SS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:48:25.303: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4109
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:48:27.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4109" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":94,"skipped":1733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:48:27.704: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6352
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6352
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 19:48:27.941: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 19:48:28.090: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:48:30.108: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:48:32.115: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:48:34.111: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:48:36.110: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:48:38.108: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:48:40.104: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:48:42.109: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:48:44.149: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:48:46.107: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:48:48.105: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 19:48:48.163: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 19:48:48.188: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  2 19:48:52.312: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 19:48:52.312: INFO: Going to poll 172.17.100.186 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 19:48:52.324: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.17.100.186 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6352 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:48:52.324: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:48:53.513: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  2 19:48:53.513: INFO: Going to poll 172.17.125.214 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 19:48:53.527: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.17.125.214 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6352 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:48:53.527: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:48:54.700: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  2 19:48:54.700: INFO: Going to poll 172.17.74.42 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 19:48:54.720: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.17.74.42 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6352 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:48:54.720: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:48:55.852: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:48:55.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6352" for this suite.

• [SLOW TEST:28.199 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":95,"skipped":1760,"failed":0}
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:48:55.904: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4126
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-b5674c97-6b6f-4ada-a457-dd1060566c02 in namespace container-probe-4126
Mar  2 19:48:58.343: INFO: Started pod test-webserver-b5674c97-6b6f-4ada-a457-dd1060566c02 in namespace container-probe-4126
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 19:48:58.356: INFO: Initial restart count of pod test-webserver-b5674c97-6b6f-4ada-a457-dd1060566c02 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:52:58.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4126" for this suite.

• [SLOW TEST:243.097 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":96,"skipped":1760,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:52:59.001: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-09531301-fc73-47b4-9cfe-1450caf4276d
STEP: Creating a pod to test consume configMaps
Mar  2 19:52:59.270: INFO: Waiting up to 5m0s for pod "pod-configmaps-54137e8d-bd3e-4b0f-b269-bffae6154591" in namespace "configmap-8856" to be "Succeeded or Failed"
Mar  2 19:52:59.281: INFO: Pod "pod-configmaps-54137e8d-bd3e-4b0f-b269-bffae6154591": Phase="Pending", Reason="", readiness=false. Elapsed: 11.356026ms
Mar  2 19:53:01.303: INFO: Pod "pod-configmaps-54137e8d-bd3e-4b0f-b269-bffae6154591": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033462311s
STEP: Saw pod success
Mar  2 19:53:01.303: INFO: Pod "pod-configmaps-54137e8d-bd3e-4b0f-b269-bffae6154591" satisfied condition "Succeeded or Failed"
Mar  2 19:53:01.317: INFO: Trying to get logs from node 10.245.0.4 pod pod-configmaps-54137e8d-bd3e-4b0f-b269-bffae6154591 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 19:53:01.408: INFO: Waiting for pod pod-configmaps-54137e8d-bd3e-4b0f-b269-bffae6154591 to disappear
Mar  2 19:53:01.421: INFO: Pod pod-configmaps-54137e8d-bd3e-4b0f-b269-bffae6154591 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:53:01.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8856" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":97,"skipped":1773,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:53:01.458: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4948
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar  2 19:53:01.774: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4948  215d9458-16db-4270-bb0c-9ab7ae87496c 26592 0 2022-03-02 19:53:01 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2022-03-02 19:53:01 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbl6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbl6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 19:53:01.789: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:53:03.805: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:53:05.814: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar  2 19:53:05.814: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4948 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:53:05.814: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Verifying customized DNS server is configured on pod...
Mar  2 19:53:06.031: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4948 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:53:06.031: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:53:06.198: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:53:06.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4948" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":98,"skipped":1776,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:53:06.287: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-5734
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Mar  2 19:53:26.949: INFO: EndpointSlice for Service endpointslice-5734/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:53:36.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5734" for this suite.

• [SLOW TEST:30.744 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":99,"skipped":1792,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:53:37.031: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1720
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-ngpj
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 19:53:37.353: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ngpj" in namespace "subpath-1720" to be "Succeeded or Failed"
Mar  2 19:53:37.371: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Pending", Reason="", readiness=false. Elapsed: 17.759213ms
Mar  2 19:53:39.387: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Running", Reason="", readiness=true. Elapsed: 2.034316404s
Mar  2 19:53:41.405: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Running", Reason="", readiness=true. Elapsed: 4.052426619s
Mar  2 19:53:43.426: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Running", Reason="", readiness=true. Elapsed: 6.072554714s
Mar  2 19:53:45.444: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Running", Reason="", readiness=true. Elapsed: 8.090562358s
Mar  2 19:53:47.462: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Running", Reason="", readiness=true. Elapsed: 10.109187496s
Mar  2 19:53:49.481: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Running", Reason="", readiness=true. Elapsed: 12.127710019s
Mar  2 19:53:51.502: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Running", Reason="", readiness=true. Elapsed: 14.148482478s
Mar  2 19:53:53.523: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Running", Reason="", readiness=true. Elapsed: 16.170145264s
Mar  2 19:53:55.543: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Running", Reason="", readiness=true. Elapsed: 18.189676952s
Mar  2 19:53:57.564: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Running", Reason="", readiness=true. Elapsed: 20.210818069s
Mar  2 19:53:59.583: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Running", Reason="", readiness=true. Elapsed: 22.230086199s
Mar  2 19:54:01.617: INFO: Pod "pod-subpath-test-configmap-ngpj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.264255153s
STEP: Saw pod success
Mar  2 19:54:01.617: INFO: Pod "pod-subpath-test-configmap-ngpj" satisfied condition "Succeeded or Failed"
Mar  2 19:54:01.631: INFO: Trying to get logs from node 10.245.0.5 pod pod-subpath-test-configmap-ngpj container test-container-subpath-configmap-ngpj: <nil>
STEP: delete the pod
Mar  2 19:54:01.846: INFO: Waiting for pod pod-subpath-test-configmap-ngpj to disappear
Mar  2 19:54:01.872: INFO: Pod pod-subpath-test-configmap-ngpj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ngpj
Mar  2 19:54:01.872: INFO: Deleting pod "pod-subpath-test-configmap-ngpj" in namespace "subpath-1720"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:54:01.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1720" for this suite.

• [SLOW TEST:24.900 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":100,"skipped":1803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:54:01.932: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7682
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar  2 19:54:02.329: INFO: The status of Pod pod-update-8bb6ed2d-33d5-45e1-b925-1d2720c8da53 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:54:04.373: INFO: The status of Pod pod-update-8bb6ed2d-33d5-45e1-b925-1d2720c8da53 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  2 19:54:04.967: INFO: Successfully updated pod "pod-update-8bb6ed2d-33d5-45e1-b925-1d2720c8da53"
STEP: verifying the updated pod is in kubernetes
Mar  2 19:54:04.993: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:54:04.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7682" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":101,"skipped":1828,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:54:05.030: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7066
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8962
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2595
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:54:13.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7066" for this suite.
STEP: Destroying namespace "nsdeletetest-8962" for this suite.
Mar  2 19:54:13.227: INFO: Namespace nsdeletetest-8962 was already deleted
STEP: Destroying namespace "nsdeletetest-2595" for this suite.

• [SLOW TEST:8.217 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":102,"skipped":1836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:54:13.248: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2874
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2874 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2874;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2874 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2874;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2874.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2874.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2874.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2874.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2874.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2874.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2874.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2874.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2874.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2874.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2874.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 46.124.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.124.46_udp@PTR;check="$$(dig +tcp +noall +answer +search 46.124.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.124.46_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2874 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2874;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2874 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2874;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2874.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2874.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2874.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2874.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2874.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2874.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2874.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2874.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2874.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2874.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2874.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2874.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 46.124.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.124.46_udp@PTR;check="$$(dig +tcp +noall +answer +search 46.124.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.124.46_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 19:54:17.717: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:17.762: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:17.796: INFO: Unable to read wheezy_udp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:17.819: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:17.836: INFO: Unable to read wheezy_udp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:17.874: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:17.890: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:17.939: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:18.127: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:18.143: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:18.161: INFO: Unable to read jessie_udp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:18.189: INFO: Unable to read jessie_tcp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:18.215: INFO: Unable to read jessie_udp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:18.242: INFO: Unable to read jessie_tcp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:18.263: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:18.280: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:18.394: INFO: Lookups using dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2874 wheezy_tcp@dns-test-service.dns-2874 wheezy_udp@dns-test-service.dns-2874.svc wheezy_tcp@dns-test-service.dns-2874.svc wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2874 jessie_tcp@dns-test-service.dns-2874 jessie_udp@dns-test-service.dns-2874.svc jessie_tcp@dns-test-service.dns-2874.svc jessie_udp@_http._tcp.dns-test-service.dns-2874.svc jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc]

Mar  2 19:54:23.430: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.506: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.524: INFO: Unable to read wheezy_udp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.542: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.586: INFO: Unable to read wheezy_udp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.603: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.622: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.639: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.766: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.782: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.807: INFO: Unable to read jessie_udp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.827: INFO: Unable to read jessie_tcp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.850: INFO: Unable to read jessie_udp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.874: INFO: Unable to read jessie_tcp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.898: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:23.933: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:24.167: INFO: Lookups using dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2874 wheezy_tcp@dns-test-service.dns-2874 wheezy_udp@dns-test-service.dns-2874.svc wheezy_tcp@dns-test-service.dns-2874.svc wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2874 jessie_tcp@dns-test-service.dns-2874 jessie_udp@dns-test-service.dns-2874.svc jessie_tcp@dns-test-service.dns-2874.svc jessie_udp@_http._tcp.dns-test-service.dns-2874.svc jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc]

Mar  2 19:54:28.425: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.443: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.460: INFO: Unable to read wheezy_udp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.479: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.509: INFO: Unable to read wheezy_udp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.525: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.541: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.558: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.683: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.699: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.723: INFO: Unable to read jessie_udp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.740: INFO: Unable to read jessie_tcp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.760: INFO: Unable to read jessie_udp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.777: INFO: Unable to read jessie_tcp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.795: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.844: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:28.969: INFO: Lookups using dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2874 wheezy_tcp@dns-test-service.dns-2874 wheezy_udp@dns-test-service.dns-2874.svc wheezy_tcp@dns-test-service.dns-2874.svc wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2874 jessie_tcp@dns-test-service.dns-2874 jessie_udp@dns-test-service.dns-2874.svc jessie_tcp@dns-test-service.dns-2874.svc jessie_udp@_http._tcp.dns-test-service.dns-2874.svc jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc]

Mar  2 19:54:33.414: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:33.431: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:33.450: INFO: Unable to read wheezy_udp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:33.467: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:33.484: INFO: Unable to read wheezy_udp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:33.501: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:33.540: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:33.583: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:33.885: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:33.901: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:33.943: INFO: Unable to read jessie_udp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:34.000: INFO: Unable to read jessie_tcp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:34.018: INFO: Unable to read jessie_udp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:34.037: INFO: Unable to read jessie_tcp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:34.084: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:34.101: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:34.201: INFO: Lookups using dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2874 wheezy_tcp@dns-test-service.dns-2874 wheezy_udp@dns-test-service.dns-2874.svc wheezy_tcp@dns-test-service.dns-2874.svc wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2874 jessie_tcp@dns-test-service.dns-2874 jessie_udp@dns-test-service.dns-2874.svc jessie_tcp@dns-test-service.dns-2874.svc jessie_udp@_http._tcp.dns-test-service.dns-2874.svc jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc]

Mar  2 19:54:38.414: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.433: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.450: INFO: Unable to read wheezy_udp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.472: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.489: INFO: Unable to read wheezy_udp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.505: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.522: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.538: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.661: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.682: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.701: INFO: Unable to read jessie_udp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.739: INFO: Unable to read jessie_tcp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.757: INFO: Unable to read jessie_udp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.774: INFO: Unable to read jessie_tcp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.791: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.808: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:38.913: INFO: Lookups using dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2874 wheezy_tcp@dns-test-service.dns-2874 wheezy_udp@dns-test-service.dns-2874.svc wheezy_tcp@dns-test-service.dns-2874.svc wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2874 jessie_tcp@dns-test-service.dns-2874 jessie_udp@dns-test-service.dns-2874.svc jessie_tcp@dns-test-service.dns-2874.svc jessie_udp@_http._tcp.dns-test-service.dns-2874.svc jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc]

Mar  2 19:54:43.423: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.465: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.481: INFO: Unable to read wheezy_udp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.499: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.517: INFO: Unable to read wheezy_udp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.546: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.563: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.601: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.738: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.754: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.770: INFO: Unable to read jessie_udp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.786: INFO: Unable to read jessie_tcp@dns-test-service.dns-2874 from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.803: INFO: Unable to read jessie_udp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.819: INFO: Unable to read jessie_tcp@dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.849: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.867: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc from pod dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe: the server could not find the requested resource (get pods dns-test-9d235d5e-8097-4240-8562-93cc08437fbe)
Mar  2 19:54:43.972: INFO: Lookups using dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2874 wheezy_tcp@dns-test-service.dns-2874 wheezy_udp@dns-test-service.dns-2874.svc wheezy_tcp@dns-test-service.dns-2874.svc wheezy_udp@_http._tcp.dns-test-service.dns-2874.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2874.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2874 jessie_tcp@dns-test-service.dns-2874 jessie_udp@dns-test-service.dns-2874.svc jessie_tcp@dns-test-service.dns-2874.svc jessie_udp@_http._tcp.dns-test-service.dns-2874.svc jessie_tcp@_http._tcp.dns-test-service.dns-2874.svc]

Mar  2 19:54:49.090: INFO: DNS probes using dns-2874/dns-test-9d235d5e-8097-4240-8562-93cc08437fbe succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:54:49.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2874" for this suite.

• [SLOW TEST:36.088 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":103,"skipped":1880,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:54:49.336: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9070
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9070
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9070
STEP: creating replication controller externalsvc in namespace services-9070
I0302 19:54:49.747054      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9070, replica count: 2
I0302 19:54:52.798293      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar  2 19:54:52.867: INFO: Creating new exec pod
Mar  2 19:54:54.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-9070 exec execpodqskm6 -- /bin/sh -x -c nslookup clusterip-service.services-9070.svc.cluster.local'
Mar  2 19:54:55.322: INFO: stderr: "+ nslookup clusterip-service.services-9070.svc.cluster.local\n"
Mar  2 19:54:55.322: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-9070.svc.cluster.local\tcanonical name = externalsvc.services-9070.svc.cluster.local.\nName:\texternalsvc.services-9070.svc.cluster.local\nAddress: 172.21.199.42\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9070, will wait for the garbage collector to delete the pods
Mar  2 19:54:55.405: INFO: Deleting ReplicationController externalsvc took: 19.011174ms
Mar  2 19:54:55.505: INFO: Terminating ReplicationController externalsvc pods took: 100.287298ms
Mar  2 19:54:58.060: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:54:58.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9070" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.792 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":104,"skipped":1908,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:54:58.128: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5313
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:54:58.476: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b836ff79-b47d-4b04-bf49-1238bbc80f5e", Controller:(*bool)(0xc002047ede), BlockOwnerDeletion:(*bool)(0xc002047edf)}}
Mar  2 19:54:58.494: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"cee3d491-0018-4d57-b21b-537c7c6e33b8", Controller:(*bool)(0xc000b1214e), BlockOwnerDeletion:(*bool)(0xc000b1214f)}}
Mar  2 19:54:58.540: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8c1bcab1-1235-49d4-bcb2-7746138131cf", Controller:(*bool)(0xc000b123be), BlockOwnerDeletion:(*bool)(0xc000b123bf)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:55:03.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5313" for this suite.

• [SLOW TEST:5.580 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":105,"skipped":1912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:55:03.711: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-965
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Mar  2 19:55:04.068: INFO: namespace kubectl-965
Mar  2 19:55:04.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-965 create -f -'
Mar  2 19:55:04.330: INFO: stderr: ""
Mar  2 19:55:04.330: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar  2 19:55:05.377: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 19:55:05.377: INFO: Found 0 / 1
Mar  2 19:55:06.359: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 19:55:06.359: INFO: Found 0 / 1
Mar  2 19:55:07.346: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 19:55:07.346: INFO: Found 1 / 1
Mar  2 19:55:07.346: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 19:55:07.359: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 19:55:07.359: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 19:55:07.359: INFO: wait on agnhost-primary startup in kubectl-965 
Mar  2 19:55:07.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-965 logs agnhost-primary-z5q2v agnhost-primary'
Mar  2 19:55:07.543: INFO: stderr: ""
Mar  2 19:55:07.543: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar  2 19:55:07.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-965 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar  2 19:55:07.758: INFO: stderr: ""
Mar  2 19:55:07.758: INFO: stdout: "service/rm2 exposed\n"
Mar  2 19:55:07.806: INFO: Service rm2 in namespace kubectl-965 found.
STEP: exposing service
Mar  2 19:55:09.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-965 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar  2 19:55:09.953: INFO: stderr: ""
Mar  2 19:55:09.953: INFO: stdout: "service/rm3 exposed\n"
Mar  2 19:55:09.965: INFO: Service rm3 in namespace kubectl-965 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:55:11.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-965" for this suite.

• [SLOW TEST:8.335 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1233
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":106,"skipped":1972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:55:12.046: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-6564
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:55:14.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6564" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":107,"skipped":2003,"failed":0}
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:55:14.501: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2746
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:55:14.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2746" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":108,"skipped":2004,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:55:14.889: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4211
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:55:15.128: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  2 19:55:19.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-4211 --namespace=crd-publish-openapi-4211 create -f -'
Mar  2 19:55:20.645: INFO: stderr: ""
Mar  2 19:55:20.645: INFO: stdout: "e2e-test-crd-publish-openapi-2500-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 19:55:20.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-4211 --namespace=crd-publish-openapi-4211 delete e2e-test-crd-publish-openapi-2500-crds test-cr'
Mar  2 19:55:20.755: INFO: stderr: ""
Mar  2 19:55:20.755: INFO: stdout: "e2e-test-crd-publish-openapi-2500-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  2 19:55:20.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-4211 --namespace=crd-publish-openapi-4211 apply -f -'
Mar  2 19:55:21.603: INFO: stderr: ""
Mar  2 19:55:21.603: INFO: stdout: "e2e-test-crd-publish-openapi-2500-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 19:55:21.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-4211 --namespace=crd-publish-openapi-4211 delete e2e-test-crd-publish-openapi-2500-crds test-cr'
Mar  2 19:55:21.704: INFO: stderr: ""
Mar  2 19:55:21.704: INFO: stdout: "e2e-test-crd-publish-openapi-2500-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar  2 19:55:21.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-4211 explain e2e-test-crd-publish-openapi-2500-crds'
Mar  2 19:55:21.886: INFO: stderr: ""
Mar  2 19:55:21.886: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2500-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:55:26.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4211" for this suite.

• [SLOW TEST:12.054 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":109,"skipped":2012,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:55:26.944: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-a111cd83-0e97-4ebf-a1ff-bb30211800db
STEP: Creating a pod to test consume secrets
Mar  2 19:55:27.238: INFO: Waiting up to 5m0s for pod "pod-secrets-25f1f676-e5ff-4ba6-8b98-491ea5e488ba" in namespace "secrets-531" to be "Succeeded or Failed"
Mar  2 19:55:27.253: INFO: Pod "pod-secrets-25f1f676-e5ff-4ba6-8b98-491ea5e488ba": Phase="Pending", Reason="", readiness=false. Elapsed: 14.856941ms
Mar  2 19:55:29.270: INFO: Pod "pod-secrets-25f1f676-e5ff-4ba6-8b98-491ea5e488ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032194516s
Mar  2 19:55:31.286: INFO: Pod "pod-secrets-25f1f676-e5ff-4ba6-8b98-491ea5e488ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047869628s
STEP: Saw pod success
Mar  2 19:55:31.286: INFO: Pod "pod-secrets-25f1f676-e5ff-4ba6-8b98-491ea5e488ba" satisfied condition "Succeeded or Failed"
Mar  2 19:55:31.299: INFO: Trying to get logs from node 10.245.0.4 pod pod-secrets-25f1f676-e5ff-4ba6-8b98-491ea5e488ba container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 19:55:31.383: INFO: Waiting for pod pod-secrets-25f1f676-e5ff-4ba6-8b98-491ea5e488ba to disappear
Mar  2 19:55:31.395: INFO: Pod pod-secrets-25f1f676-e5ff-4ba6-8b98-491ea5e488ba no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:55:31.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-531" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":110,"skipped":2040,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:55:31.430: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 19:55:32.102: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 19:55:35.183: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:55:35.197: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9660-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:55:38.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-962" for this suite.
STEP: Destroying namespace "webhook-962-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.759 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":111,"skipped":2052,"failed":0}
S
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:55:38.190: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-423
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-423
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-423
STEP: Deleting pre-stop pod
Mar  2 19:55:49.803: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:55:49.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-423" for this suite.

• [SLOW TEST:11.727 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":112,"skipped":2053,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:55:49.917: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6092
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-6092/configmap-test-76231893-aa9a-40ea-a6a5-e6542c65f9b7
STEP: Creating a pod to test consume configMaps
Mar  2 19:55:50.255: INFO: Waiting up to 5m0s for pod "pod-configmaps-33c823d9-4c75-403d-9b04-aece8e162d00" in namespace "configmap-6092" to be "Succeeded or Failed"
Mar  2 19:55:50.286: INFO: Pod "pod-configmaps-33c823d9-4c75-403d-9b04-aece8e162d00": Phase="Pending", Reason="", readiness=false. Elapsed: 31.745053ms
Mar  2 19:55:52.302: INFO: Pod "pod-configmaps-33c823d9-4c75-403d-9b04-aece8e162d00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047263887s
STEP: Saw pod success
Mar  2 19:55:52.302: INFO: Pod "pod-configmaps-33c823d9-4c75-403d-9b04-aece8e162d00" satisfied condition "Succeeded or Failed"
Mar  2 19:55:52.314: INFO: Trying to get logs from node 10.245.0.5 pod pod-configmaps-33c823d9-4c75-403d-9b04-aece8e162d00 container env-test: <nil>
STEP: delete the pod
Mar  2 19:55:52.405: INFO: Waiting for pod pod-configmaps-33c823d9-4c75-403d-9b04-aece8e162d00 to disappear
Mar  2 19:55:52.417: INFO: Pod pod-configmaps-33c823d9-4c75-403d-9b04-aece8e162d00 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:55:52.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6092" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":2060,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:55:52.454: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9730
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 19:55:52.723: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d2fe9e7-a085-413d-89f4-01051b453fbc" in namespace "downward-api-9730" to be "Succeeded or Failed"
Mar  2 19:55:52.738: INFO: Pod "downwardapi-volume-5d2fe9e7-a085-413d-89f4-01051b453fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.812435ms
Mar  2 19:55:54.755: INFO: Pod "downwardapi-volume-5d2fe9e7-a085-413d-89f4-01051b453fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032043134s
Mar  2 19:55:56.771: INFO: Pod "downwardapi-volume-5d2fe9e7-a085-413d-89f4-01051b453fbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047244477s
STEP: Saw pod success
Mar  2 19:55:56.771: INFO: Pod "downwardapi-volume-5d2fe9e7-a085-413d-89f4-01051b453fbc" satisfied condition "Succeeded or Failed"
Mar  2 19:55:56.783: INFO: Trying to get logs from node 10.245.0.5 pod downwardapi-volume-5d2fe9e7-a085-413d-89f4-01051b453fbc container client-container: <nil>
STEP: delete the pod
Mar  2 19:55:56.851: INFO: Waiting for pod downwardapi-volume-5d2fe9e7-a085-413d-89f4-01051b453fbc to disappear
Mar  2 19:55:56.861: INFO: Pod downwardapi-volume-5d2fe9e7-a085-413d-89f4-01051b453fbc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:55:56.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9730" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":114,"skipped":2079,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:55:56.893: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8627
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Mar  2 19:55:57.170: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:55:59.187: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:56:01.206: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:56:02.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8627" for this suite.

• [SLOW TEST:5.453 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":115,"skipped":2102,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:56:02.346: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3555
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  2 19:56:02.696: INFO: Waiting up to 5m0s for pod "pod-18e8ee73-6ce6-4528-b8b5-e086d7fedfc6" in namespace "emptydir-3555" to be "Succeeded or Failed"
Mar  2 19:56:02.710: INFO: Pod "pod-18e8ee73-6ce6-4528-b8b5-e086d7fedfc6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.267368ms
Mar  2 19:56:04.727: INFO: Pod "pod-18e8ee73-6ce6-4528-b8b5-e086d7fedfc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03077534s
Mar  2 19:56:06.743: INFO: Pod "pod-18e8ee73-6ce6-4528-b8b5-e086d7fedfc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046366186s
STEP: Saw pod success
Mar  2 19:56:06.743: INFO: Pod "pod-18e8ee73-6ce6-4528-b8b5-e086d7fedfc6" satisfied condition "Succeeded or Failed"
Mar  2 19:56:06.758: INFO: Trying to get logs from node 10.245.0.5 pod pod-18e8ee73-6ce6-4528-b8b5-e086d7fedfc6 container test-container: <nil>
STEP: delete the pod
Mar  2 19:56:06.813: INFO: Waiting for pod pod-18e8ee73-6ce6-4528-b8b5-e086d7fedfc6 to disappear
Mar  2 19:56:06.823: INFO: Pod pod-18e8ee73-6ce6-4528-b8b5-e086d7fedfc6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:56:06.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3555" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":116,"skipped":2107,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:56:06.860: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2243
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2243.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2243.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2243.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2243.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2243.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2243.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2243.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2243.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2243.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2243.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2243.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 105.42.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.42.105_udp@PTR;check="$$(dig +tcp +noall +answer +search 105.42.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.42.105_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2243.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2243.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2243.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2243.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2243.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2243.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2243.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2243.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2243.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2243.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2243.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 105.42.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.42.105_udp@PTR;check="$$(dig +tcp +noall +answer +search 105.42.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.42.105_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 19:56:25.311: INFO: Unable to read wheezy_udp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:25.357: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:25.389: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:25.409: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:25.630: INFO: Unable to read jessie_udp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:25.646: INFO: Unable to read jessie_tcp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:25.665: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:25.699: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:25.848: INFO: Lookups using dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f failed for: [wheezy_udp@dns-test-service.dns-2243.svc.cluster.local wheezy_tcp@dns-test-service.dns-2243.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local jessie_udp@dns-test-service.dns-2243.svc.cluster.local jessie_tcp@dns-test-service.dns-2243.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local]

Mar  2 19:56:30.867: INFO: Unable to read wheezy_udp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:30.883: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:30.898: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:30.918: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:31.066: INFO: Unable to read jessie_udp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:31.081: INFO: Unable to read jessie_tcp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:31.101: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:31.147: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:31.261: INFO: Lookups using dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f failed for: [wheezy_udp@dns-test-service.dns-2243.svc.cluster.local wheezy_tcp@dns-test-service.dns-2243.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local jessie_udp@dns-test-service.dns-2243.svc.cluster.local jessie_tcp@dns-test-service.dns-2243.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local]

Mar  2 19:56:35.870: INFO: Unable to read wheezy_udp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:35.885: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:35.901: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:35.917: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:36.021: INFO: Unable to read jessie_udp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:36.036: INFO: Unable to read jessie_tcp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:36.051: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:36.066: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:36.161: INFO: Lookups using dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f failed for: [wheezy_udp@dns-test-service.dns-2243.svc.cluster.local wheezy_tcp@dns-test-service.dns-2243.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local jessie_udp@dns-test-service.dns-2243.svc.cluster.local jessie_tcp@dns-test-service.dns-2243.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local]

Mar  2 19:56:40.886: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:41.051: INFO: Unable to read jessie_tcp@dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:41.083: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local from pod dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f: the server could not find the requested resource (get pods dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f)
Mar  2 19:56:41.172: INFO: Lookups using dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f failed for: [wheezy_tcp@dns-test-service.dns-2243.svc.cluster.local jessie_tcp@dns-test-service.dns-2243.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2243.svc.cluster.local]

Mar  2 19:56:46.211: INFO: DNS probes using dns-2243/dns-test-0ed4bcfc-5dca-4a6f-91d6-7af9abaf4b4f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:56:46.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2243" for this suite.

• [SLOW TEST:39.557 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":117,"skipped":2118,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:56:46.418: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Mar  2 19:56:46.733: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 19:56:51.753: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Mar  2 19:56:51.767: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Mar  2 19:56:51.803: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Mar  2 19:56:51.807: INFO: Observed &ReplicaSet event: ADDED
Mar  2 19:56:51.807: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 19:56:51.807: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 19:56:51.808: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 19:56:51.808: INFO: Found replicaset test-rs in namespace replicaset-8990 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 19:56:51.808: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Mar  2 19:56:51.808: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  2 19:56:51.829: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Mar  2 19:56:51.838: INFO: Observed &ReplicaSet event: ADDED
Mar  2 19:56:51.839: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 19:56:51.839: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 19:56:51.839: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 19:56:51.839: INFO: Observed replicaset test-rs in namespace replicaset-8990 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 19:56:51.839: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 19:56:51.839: INFO: Found replicaset test-rs in namespace replicaset-8990 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar  2 19:56:51.839: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:56:51.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8990" for this suite.

• [SLOW TEST:5.474 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":118,"skipped":2124,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:56:51.892: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3876
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-3876
STEP: creating service affinity-nodeport-transition in namespace services-3876
STEP: creating replication controller affinity-nodeport-transition in namespace services-3876
I0302 19:56:52.186368      21 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-3876, replica count: 3
I0302 19:56:55.238271      21 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 19:56:55.264: INFO: Creating new exec pod
Mar  2 19:56:58.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-3876 exec execpod-affinityqvcmj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Mar  2 19:56:58.599: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar  2 19:56:58.599: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 19:56:58.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-3876 exec execpod-affinityqvcmj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.63.115 80'
Mar  2 19:56:58.809: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.63.115 80\nConnection to 172.21.63.115 80 port [tcp/http] succeeded!\n"
Mar  2 19:56:58.809: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 19:56:58.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-3876 exec execpod-affinityqvcmj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.245.0.4 30314'
Mar  2 19:56:59.019: INFO: stderr: "+ nc -v -t -w 2 10.245.0.4 30314\n+ echo hostName\nConnection to 10.245.0.4 30314 port [tcp/*] succeeded!\n"
Mar  2 19:56:59.019: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 19:56:59.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-3876 exec execpod-affinityqvcmj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.245.0.5 30314'
Mar  2 19:56:59.254: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.245.0.5 30314\nConnection to 10.245.0.5 30314 port [tcp/*] succeeded!\n"
Mar  2 19:56:59.254: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 19:56:59.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-3876 exec execpod-affinityqvcmj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.245.0.4:30314/ ; done'
Mar  2 19:56:59.565: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n"
Mar  2 19:56:59.565: INFO: stdout: "\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n"
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:56:59.565: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:57:29.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-3876 exec execpod-affinityqvcmj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.245.0.4:30314/ ; done'
Mar  2 19:57:29.884: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n"
Mar  2 19:57:29.884: INFO: stdout: "\naffinity-nodeport-transition-576vc\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-576vc\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-vpk4n\naffinity-nodeport-transition-576vc\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-576vc\naffinity-nodeport-transition-576vc\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-576vc\naffinity-nodeport-transition-576vc\naffinity-nodeport-transition-vpk4n"
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-576vc
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-576vc
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-576vc
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-576vc
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-576vc
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-576vc
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-576vc
Mar  2 19:57:29.884: INFO: Received response from host: affinity-nodeport-transition-vpk4n
Mar  2 19:57:29.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-3876 exec execpod-affinityqvcmj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.245.0.4:30314/ ; done'
Mar  2 19:57:30.234: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:30314/\n"
Mar  2 19:57:30.234: INFO: stdout: "\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl\naffinity-nodeport-transition-n79nl"
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Received response from host: affinity-nodeport-transition-n79nl
Mar  2 19:57:30.234: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3876, will wait for the garbage collector to delete the pods
Mar  2 19:57:30.358: INFO: Deleting ReplicationController affinity-nodeport-transition took: 21.580104ms
Mar  2 19:57:30.459: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.904579ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:57:33.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3876" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:41.212 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":119,"skipped":2137,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:57:33.105: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5061
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:57:33.367: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-68d76a99-2fb6-4a9e-a08c-c209c51b77e0" in namespace "security-context-test-5061" to be "Succeeded or Failed"
Mar  2 19:57:33.380: INFO: Pod "busybox-privileged-false-68d76a99-2fb6-4a9e-a08c-c209c51b77e0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.582258ms
Mar  2 19:57:35.396: INFO: Pod "busybox-privileged-false-68d76a99-2fb6-4a9e-a08c-c209c51b77e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029079673s
Mar  2 19:57:35.396: INFO: Pod "busybox-privileged-false-68d76a99-2fb6-4a9e-a08c-c209c51b77e0" satisfied condition "Succeeded or Failed"
Mar  2 19:57:35.457: INFO: Got logs for pod "busybox-privileged-false-68d76a99-2fb6-4a9e-a08c-c209c51b77e0": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:57:35.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5061" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":120,"skipped":2146,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:57:35.520: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  2 19:57:35.790: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-508  7f629122-5d3a-440a-b52c-7e6c34610323 28325 0 2022-03-02 19:57:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 19:57:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 19:57:35.790: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-508  7f629122-5d3a-440a-b52c-7e6c34610323 28325 0 2022-03-02 19:57:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 19:57:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  2 19:57:45.816: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-508  7f629122-5d3a-440a-b52c-7e6c34610323 28384 0 2022-03-02 19:57:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 19:57:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 19:57:45.816: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-508  7f629122-5d3a-440a-b52c-7e6c34610323 28384 0 2022-03-02 19:57:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 19:57:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  2 19:57:55.843: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-508  7f629122-5d3a-440a-b52c-7e6c34610323 28398 0 2022-03-02 19:57:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 19:57:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 19:57:55.843: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-508  7f629122-5d3a-440a-b52c-7e6c34610323 28398 0 2022-03-02 19:57:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 19:57:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  2 19:58:05.865: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-508  7f629122-5d3a-440a-b52c-7e6c34610323 28413 0 2022-03-02 19:57:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 19:57:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 19:58:05.865: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-508  7f629122-5d3a-440a-b52c-7e6c34610323 28413 0 2022-03-02 19:57:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-02 19:57:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  2 19:58:15.892: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-508  ddabb79a-8216-4f53-87a2-677e1f42944f 28427 0 2022-03-02 19:58:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-02 19:58:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 19:58:15.892: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-508  ddabb79a-8216-4f53-87a2-677e1f42944f 28427 0 2022-03-02 19:58:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-02 19:58:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  2 19:58:25.914: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-508  ddabb79a-8216-4f53-87a2-677e1f42944f 28441 0 2022-03-02 19:58:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-02 19:58:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 19:58:25.914: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-508  ddabb79a-8216-4f53-87a2-677e1f42944f 28441 0 2022-03-02 19:58:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-02 19:58:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:58:35.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-508" for this suite.

• [SLOW TEST:60.565 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":121,"skipped":2160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:58:36.088: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  2 19:58:36.781: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  2 19:58:38.845: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781847916, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781847916, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781847916, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781847916, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-697cdbd8f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 19:58:41.885: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:58:41.921: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:58:44.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4717" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.801 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":122,"skipped":2234,"failed":0}
S
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:58:44.889: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9379
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Mar  2 19:58:45.288: INFO: created test-event-1
Mar  2 19:58:45.301: INFO: created test-event-2
Mar  2 19:58:45.312: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Mar  2 19:58:45.323: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Mar  2 19:58:45.420: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:58:45.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9379" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":123,"skipped":2235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:58:45.463: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-7175
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  2 19:58:46.216: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 19:58:49.304: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 19:58:49.319: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:58:52.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7175" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.710 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":124,"skipped":2277,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:58:53.173: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar  2 19:58:53.417: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 19:58:53.439: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 19:58:53.455: INFO: 
Logging pods the apiserver thinks is on node 10.245.0.4 before test
Mar  2 19:58:53.514: INFO: calico-node-6bmpc from kube-system started at 2022-03-02 17:40:12 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.514: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 19:58:53.514: INFO: calico-typha-7cdb864b94-9tjhg from kube-system started at 2022-03-02 19:44:47 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.514: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 19:58:53.514: INFO: coredns-b58d5f584-5xnk4 from kube-system started at 2022-03-02 19:44:46 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.514: INFO: 	Container coredns ready: true, restart count 0
Mar  2 19:58:53.514: INFO: ibm-master-proxy-static-10.245.0.4 from kube-system started at 2022-03-02 17:39:53 +0000 UTC (2 container statuses recorded)
Mar  2 19:58:53.514: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 19:58:53.514: INFO: 	Container pause ready: true, restart count 0
Mar  2 19:58:53.514: INFO: ibm-vpc-block-csi-node-m9dxv from kube-system started at 2022-03-02 17:40:12 +0000 UTC (4 container statuses recorded)
Mar  2 19:58:53.514: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Mar  2 19:58:53.514: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Mar  2 19:58:53.514: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 19:58:53.514: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 19:58:53.514: INFO: konnectivity-agent-cnbg9 from kube-system started at 2022-03-02 17:48:31 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.514: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  2 19:58:53.514: INFO: sonobuoy from sonobuoy started at 2022-03-02 19:20:48 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.514: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 19:58:53.514: INFO: sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-867x2 from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 19:58:53.514: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 19:58:53.514: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 19:58:53.514: INFO: 
Logging pods the apiserver thinks is on node 10.245.0.5 before test
Mar  2 19:58:53.540: INFO: catalog-operator-6c4b4d7c9-gnzj6 from ibm-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 19:58:53.540: INFO: olm-operator-785cdc5884-8vxvx from ibm-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 19:58:53.540: INFO: calico-kube-controllers-dcd8d986c-g4lwp from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 19:58:53.540: INFO: calico-node-7n2gc from kube-system started at 2022-03-02 17:38:33 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 19:58:53.540: INFO: calico-typha-7cdb864b94-t4bmt from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 19:58:53.540: INFO: coredns-autoscaler-689fb74d49-hqj9c from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container autoscaler ready: true, restart count 0
Mar  2 19:58:53.540: INFO: coredns-b58d5f584-m7b7m from kube-system started at 2022-03-02 17:49:06 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container coredns ready: true, restart count 0
Mar  2 19:58:53.540: INFO: dashboard-metrics-scraper-6747f89c97-4dq9w from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar  2 19:58:53.540: INFO: ibm-master-proxy-static-10.245.0.5 from kube-system started at 2022-03-02 17:38:18 +0000 UTC (2 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 19:58:53.540: INFO: 	Container pause ready: true, restart count 0
Mar  2 19:58:53.540: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2022-03-02 17:38:51 +0000 UTC (6 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container csi-attacher ready: true, restart count 0
Mar  2 19:58:53.540: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar  2 19:58:53.540: INFO: 	Container csi-resizer ready: true, restart count 0
Mar  2 19:58:53.540: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Mar  2 19:58:53.540: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 19:58:53.540: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 19:58:53.540: INFO: ibm-vpc-block-csi-node-n5lr5 from kube-system started at 2022-03-02 17:38:33 +0000 UTC (4 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Mar  2 19:58:53.540: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Mar  2 19:58:53.540: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 19:58:53.540: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 19:58:53.540: INFO: konnectivity-agent-gkp9m from kube-system started at 2022-03-02 17:48:34 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  2 19:58:53.540: INFO: kubernetes-dashboard-54c47dd995-vkxsd from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  2 19:58:53.540: INFO: public-crc8fqf88s0ubnto7pbi70-alb1-6559fd4c4b-kzxmb from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  2 19:58:53.540: INFO: sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-4g544 from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 19:58:53.540: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 19:58:53.540: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 19:58:53.540: INFO: 
Logging pods the apiserver thinks is on node 10.245.0.6 before test
Mar  2 19:58:53.573: INFO: calico-node-tl6xm from kube-system started at 2022-03-02 17:38:42 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.573: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 19:58:53.573: INFO: calico-typha-7cdb864b94-znnrz from kube-system started at 2022-03-02 17:39:02 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.573: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 19:58:53.573: INFO: coredns-b58d5f584-sdfgx from kube-system started at 2022-03-02 17:49:06 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.573: INFO: 	Container coredns ready: true, restart count 0
Mar  2 19:58:53.573: INFO: ibm-master-proxy-static-10.245.0.6 from kube-system started at 2022-03-02 17:38:23 +0000 UTC (2 container statuses recorded)
Mar  2 19:58:53.573: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 19:58:53.573: INFO: 	Container pause ready: true, restart count 0
Mar  2 19:58:53.573: INFO: ibm-vpc-block-csi-node-nkxms from kube-system started at 2022-03-02 17:38:42 +0000 UTC (4 container statuses recorded)
Mar  2 19:58:53.573: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Mar  2 19:58:53.573: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Mar  2 19:58:53.573: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 19:58:53.573: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 19:58:53.573: INFO: konnectivity-agent-bnglr from kube-system started at 2022-03-02 17:48:28 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.573: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  2 19:58:53.573: INFO: metrics-server-7b97867cc5-fpb5z from kube-system started at 2022-03-02 18:23:06 +0000 UTC (3 container statuses recorded)
Mar  2 19:58:53.573: INFO: 	Container config-watcher ready: true, restart count 0
Mar  2 19:58:53.573: INFO: 	Container metrics-server ready: true, restart count 0
Mar  2 19:58:53.573: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar  2 19:58:53.573: INFO: public-crc8fqf88s0ubnto7pbi70-alb1-6559fd4c4b-lwmtq from kube-system started at 2022-03-02 17:39:02 +0000 UTC (1 container statuses recorded)
Mar  2 19:58:53.573: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  2 19:58:53.573: INFO: sonobuoy-e2e-job-6fc4581698214e95 from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 19:58:53.573: INFO: 	Container e2e ready: true, restart count 0
Mar  2 19:58:53.573: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 19:58:53.573: INFO: sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-f24tq from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 19:58:53.573: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 19:58:53.573: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-dd3e8f23-05fb-4bc8-bb34-7819b1c6fa5d 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-dd3e8f23-05fb-4bc8-bb34-7819b1c6fa5d off the node 10.245.0.4
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dd3e8f23-05fb-4bc8-bb34-7819b1c6fa5d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:58:59.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6267" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.736 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":125,"skipped":2283,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:58:59.910: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-7522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Mar  2 19:59:00.255: INFO: Waiting up to 5m0s for pod "security-context-5b8d1974-eaca-404f-890e-c3be8c4e2ef7" in namespace "security-context-7522" to be "Succeeded or Failed"
Mar  2 19:59:00.292: INFO: Pod "security-context-5b8d1974-eaca-404f-890e-c3be8c4e2ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 37.917301ms
Mar  2 19:59:02.309: INFO: Pod "security-context-5b8d1974-eaca-404f-890e-c3be8c4e2ef7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054524016s
STEP: Saw pod success
Mar  2 19:59:02.309: INFO: Pod "security-context-5b8d1974-eaca-404f-890e-c3be8c4e2ef7" satisfied condition "Succeeded or Failed"
Mar  2 19:59:02.332: INFO: Trying to get logs from node 10.245.0.4 pod security-context-5b8d1974-eaca-404f-890e-c3be8c4e2ef7 container test-container: <nil>
STEP: delete the pod
Mar  2 19:59:02.431: INFO: Waiting for pod security-context-5b8d1974-eaca-404f-890e-c3be8c4e2ef7 to disappear
Mar  2 19:59:02.443: INFO: Pod security-context-5b8d1974-eaca-404f-890e-c3be8c4e2ef7 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:59:02.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7522" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":126,"skipped":2300,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:59:02.484: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3888
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Mar  2 19:59:02.790: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:59:04.809: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:59:06.804: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Mar  2 19:59:06.847: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:59:08.866: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Mar  2 19:59:08.929: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 19:59:08.944: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 19:59:10.944: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 19:59:10.965: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 19:59:12.944: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 19:59:12.960: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:59:12.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3888" for this suite.

• [SLOW TEST:10.548 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":127,"skipped":2301,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:59:13.033: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6335
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6335
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 19:59:13.291: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 19:59:13.377: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 19:59:15.394: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:59:17.393: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:59:19.395: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:59:21.393: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:59:23.395: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:59:25.395: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:59:27.393: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:59:29.399: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:59:31.398: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 19:59:33.413: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 19:59:33.439: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 19:59:33.463: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  2 19:59:35.534: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 19:59:35.534: INFO: Breadth first check of 172.17.100.149 on host 10.245.0.4...
Mar  2 19:59:35.547: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.100.154:9080/dial?request=hostname&protocol=http&host=172.17.100.149&port=8083&tries=1'] Namespace:pod-network-test-6335 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:59:35.547: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:59:35.789: INFO: Waiting for responses: map[]
Mar  2 19:59:35.789: INFO: reached 172.17.100.149 after 0/1 tries
Mar  2 19:59:35.789: INFO: Breadth first check of 172.17.125.225 on host 10.245.0.5...
Mar  2 19:59:35.801: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.100.154:9080/dial?request=hostname&protocol=http&host=172.17.125.225&port=8083&tries=1'] Namespace:pod-network-test-6335 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:59:35.801: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:59:35.967: INFO: Waiting for responses: map[]
Mar  2 19:59:35.967: INFO: reached 172.17.125.225 after 0/1 tries
Mar  2 19:59:35.967: INFO: Breadth first check of 172.17.74.44 on host 10.245.0.6...
Mar  2 19:59:35.980: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.100.154:9080/dial?request=hostname&protocol=http&host=172.17.74.44&port=8083&tries=1'] Namespace:pod-network-test-6335 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 19:59:35.980: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 19:59:36.126: INFO: Waiting for responses: map[]
Mar  2 19:59:36.126: INFO: reached 172.17.74.44 after 0/1 tries
Mar  2 19:59:36.126: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 19:59:36.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6335" for this suite.

• [SLOW TEST:23.131 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":128,"skipped":2304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 19:59:36.164: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-2945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar  2 19:59:36.443: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 20:00:36.498: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:00:36.511: INFO: Starting informer...
STEP: Starting pods...
Mar  2 20:00:36.770: INFO: Pod1 is running on 10.245.0.4. Tainting Node
Mar  2 20:00:39.033: INFO: Pod2 is running on 10.245.0.4. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar  2 20:00:45.427: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar  2 20:01:05.512: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:01:05.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2945" for this suite.

• [SLOW TEST:89.444 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":129,"skipped":2358,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:01:05.609: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  2 20:01:05.943: INFO: Waiting up to 5m0s for pod "pod-7b63ea6a-ed95-4781-8b8d-54744042f6ac" in namespace "emptydir-5363" to be "Succeeded or Failed"
Mar  2 20:01:05.954: INFO: Pod "pod-7b63ea6a-ed95-4781-8b8d-54744042f6ac": Phase="Pending", Reason="", readiness=false. Elapsed: 11.378561ms
Mar  2 20:01:07.968: INFO: Pod "pod-7b63ea6a-ed95-4781-8b8d-54744042f6ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024900203s
STEP: Saw pod success
Mar  2 20:01:07.968: INFO: Pod "pod-7b63ea6a-ed95-4781-8b8d-54744042f6ac" satisfied condition "Succeeded or Failed"
Mar  2 20:01:08.006: INFO: Trying to get logs from node 10.245.0.4 pod pod-7b63ea6a-ed95-4781-8b8d-54744042f6ac container test-container: <nil>
STEP: delete the pod
Mar  2 20:01:08.155: INFO: Waiting for pod pod-7b63ea6a-ed95-4781-8b8d-54744042f6ac to disappear
Mar  2 20:01:08.167: INFO: Pod pod-7b63ea6a-ed95-4781-8b8d-54744042f6ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:01:08.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5363" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":130,"skipped":2363,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:01:08.200: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-7900
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:07:00.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7900" for this suite.

• [SLOW TEST:352.420 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":131,"skipped":2372,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:07:00.620: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4476
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-b2d386cf-7f45-48f1-96cf-970f5e3f3f65
STEP: Creating a pod to test consume secrets
Mar  2 20:07:00.928: INFO: Waiting up to 5m0s for pod "pod-secrets-11e48851-0010-45c4-a2d6-e57f32ecd40c" in namespace "secrets-4476" to be "Succeeded or Failed"
Mar  2 20:07:00.942: INFO: Pod "pod-secrets-11e48851-0010-45c4-a2d6-e57f32ecd40c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.681237ms
Mar  2 20:07:02.961: INFO: Pod "pod-secrets-11e48851-0010-45c4-a2d6-e57f32ecd40c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032921679s
Mar  2 20:07:05.007: INFO: Pod "pod-secrets-11e48851-0010-45c4-a2d6-e57f32ecd40c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079248231s
STEP: Saw pod success
Mar  2 20:07:05.007: INFO: Pod "pod-secrets-11e48851-0010-45c4-a2d6-e57f32ecd40c" satisfied condition "Succeeded or Failed"
Mar  2 20:07:05.029: INFO: Trying to get logs from node 10.245.0.4 pod pod-secrets-11e48851-0010-45c4-a2d6-e57f32ecd40c container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 20:07:05.153: INFO: Waiting for pod pod-secrets-11e48851-0010-45c4-a2d6-e57f32ecd40c to disappear
Mar  2 20:07:05.164: INFO: Pod pod-secrets-11e48851-0010-45c4-a2d6-e57f32ecd40c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:07:05.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4476" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":132,"skipped":2396,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:07:05.250: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7979
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-7979
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 20:07:05.517: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 20:07:05.617: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:07:07.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:07:09.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:07:11.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:07:13.632: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:07:15.633: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:07:17.658: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:07:19.634: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:07:21.669: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:07:23.632: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:07:25.667: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 20:07:25.692: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 20:07:25.714: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  2 20:07:29.777: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 20:07:29.777: INFO: Breadth first check of 172.17.100.131 on host 10.245.0.4...
Mar  2 20:07:29.788: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.100.160:9080/dial?request=hostname&protocol=udp&host=172.17.100.131&port=8081&tries=1'] Namespace:pod-network-test-7979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 20:07:29.788: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 20:07:29.980: INFO: Waiting for responses: map[]
Mar  2 20:07:29.981: INFO: reached 172.17.100.131 after 0/1 tries
Mar  2 20:07:29.981: INFO: Breadth first check of 172.17.125.228 on host 10.245.0.5...
Mar  2 20:07:30.000: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.100.160:9080/dial?request=hostname&protocol=udp&host=172.17.125.228&port=8081&tries=1'] Namespace:pod-network-test-7979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 20:07:30.000: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 20:07:30.143: INFO: Waiting for responses: map[]
Mar  2 20:07:30.143: INFO: reached 172.17.125.228 after 0/1 tries
Mar  2 20:07:30.143: INFO: Breadth first check of 172.17.74.45 on host 10.245.0.6...
Mar  2 20:07:30.156: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.100.160:9080/dial?request=hostname&protocol=udp&host=172.17.74.45&port=8081&tries=1'] Namespace:pod-network-test-7979 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 20:07:30.156: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 20:07:30.321: INFO: Waiting for responses: map[]
Mar  2 20:07:30.321: INFO: reached 172.17.74.45 after 0/1 tries
Mar  2 20:07:30.321: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:07:30.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7979" for this suite.

• [SLOW TEST:25.106 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":133,"skipped":2399,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:07:30.356: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5399
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:07:30.633: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  2 20:07:35.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-5399 --namespace=crd-publish-openapi-5399 create -f -'
Mar  2 20:07:36.816: INFO: stderr: ""
Mar  2 20:07:36.816: INFO: stdout: "e2e-test-crd-publish-openapi-7045-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 20:07:36.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-5399 --namespace=crd-publish-openapi-5399 delete e2e-test-crd-publish-openapi-7045-crds test-cr'
Mar  2 20:07:36.916: INFO: stderr: ""
Mar  2 20:07:36.916: INFO: stdout: "e2e-test-crd-publish-openapi-7045-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  2 20:07:36.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-5399 --namespace=crd-publish-openapi-5399 apply -f -'
Mar  2 20:07:37.750: INFO: stderr: ""
Mar  2 20:07:37.750: INFO: stdout: "e2e-test-crd-publish-openapi-7045-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 20:07:37.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-5399 --namespace=crd-publish-openapi-5399 delete e2e-test-crd-publish-openapi-7045-crds test-cr'
Mar  2 20:07:37.845: INFO: stderr: ""
Mar  2 20:07:37.845: INFO: stdout: "e2e-test-crd-publish-openapi-7045-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  2 20:07:37.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-5399 explain e2e-test-crd-publish-openapi-7045-crds'
Mar  2 20:07:38.088: INFO: stderr: ""
Mar  2 20:07:38.088: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7045-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:07:42.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5399" for this suite.

• [SLOW TEST:12.461 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":134,"skipped":2400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:07:42.818: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7262
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Mar  2 20:07:43.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 create -f -'
Mar  2 20:07:43.872: INFO: stderr: ""
Mar  2 20:07:43.872: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 20:07:43.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 20:07:43.942: INFO: stderr: ""
Mar  2 20:07:43.942: INFO: stdout: "update-demo-nautilus-2n6j9 update-demo-nautilus-8bwfp "
Mar  2 20:07:43.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-2n6j9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 20:07:44.010: INFO: stderr: ""
Mar  2 20:07:44.011: INFO: stdout: ""
Mar  2 20:07:44.011: INFO: update-demo-nautilus-2n6j9 is created but not running
Mar  2 20:07:49.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 20:07:49.083: INFO: stderr: ""
Mar  2 20:07:49.083: INFO: stdout: "update-demo-nautilus-2n6j9 update-demo-nautilus-8bwfp "
Mar  2 20:07:49.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-2n6j9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 20:07:49.162: INFO: stderr: ""
Mar  2 20:07:49.162: INFO: stdout: ""
Mar  2 20:07:49.162: INFO: update-demo-nautilus-2n6j9 is created but not running
Mar  2 20:07:54.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 20:07:54.242: INFO: stderr: ""
Mar  2 20:07:54.242: INFO: stdout: "update-demo-nautilus-2n6j9 update-demo-nautilus-8bwfp "
Mar  2 20:07:54.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-2n6j9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 20:07:54.308: INFO: stderr: ""
Mar  2 20:07:54.308: INFO: stdout: "true"
Mar  2 20:07:54.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-2n6j9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 20:07:55.415: INFO: stderr: ""
Mar  2 20:07:55.415: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar  2 20:07:55.415: INFO: validating pod update-demo-nautilus-2n6j9
Mar  2 20:07:55.481: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 20:07:55.481: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 20:07:55.481: INFO: update-demo-nautilus-2n6j9 is verified up and running
Mar  2 20:07:55.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-8bwfp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 20:07:55.590: INFO: stderr: ""
Mar  2 20:07:55.590: INFO: stdout: "true"
Mar  2 20:07:55.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-8bwfp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 20:07:55.715: INFO: stderr: ""
Mar  2 20:07:55.715: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar  2 20:07:55.715: INFO: validating pod update-demo-nautilus-8bwfp
Mar  2 20:07:55.778: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 20:07:55.778: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 20:07:55.778: INFO: update-demo-nautilus-8bwfp is verified up and running
STEP: scaling down the replication controller
Mar  2 20:07:55.780: INFO: scanned /root for discovery docs: <nil>
Mar  2 20:07:55.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar  2 20:07:56.916: INFO: stderr: ""
Mar  2 20:07:56.917: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 20:07:56.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 20:07:56.991: INFO: stderr: ""
Mar  2 20:07:56.991: INFO: stdout: "update-demo-nautilus-2n6j9 update-demo-nautilus-8bwfp "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  2 20:08:01.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 20:08:02.067: INFO: stderr: ""
Mar  2 20:08:02.067: INFO: stdout: "update-demo-nautilus-8bwfp "
Mar  2 20:08:02.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-8bwfp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 20:08:02.144: INFO: stderr: ""
Mar  2 20:08:02.144: INFO: stdout: "true"
Mar  2 20:08:02.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-8bwfp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 20:08:02.214: INFO: stderr: ""
Mar  2 20:08:02.214: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar  2 20:08:02.214: INFO: validating pod update-demo-nautilus-8bwfp
Mar  2 20:08:02.230: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 20:08:02.230: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 20:08:02.230: INFO: update-demo-nautilus-8bwfp is verified up and running
STEP: scaling up the replication controller
Mar  2 20:08:02.232: INFO: scanned /root for discovery docs: <nil>
Mar  2 20:08:02.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar  2 20:08:03.391: INFO: stderr: ""
Mar  2 20:08:03.391: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 20:08:03.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 20:08:03.468: INFO: stderr: ""
Mar  2 20:08:03.468: INFO: stdout: "update-demo-nautilus-8bwfp update-demo-nautilus-s7g8w "
Mar  2 20:08:03.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-8bwfp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 20:08:03.529: INFO: stderr: ""
Mar  2 20:08:03.529: INFO: stdout: "true"
Mar  2 20:08:03.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-8bwfp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 20:08:03.603: INFO: stderr: ""
Mar  2 20:08:03.603: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar  2 20:08:03.603: INFO: validating pod update-demo-nautilus-8bwfp
Mar  2 20:08:03.619: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 20:08:03.619: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 20:08:03.619: INFO: update-demo-nautilus-8bwfp is verified up and running
Mar  2 20:08:03.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-s7g8w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 20:08:03.687: INFO: stderr: ""
Mar  2 20:08:03.687: INFO: stdout: ""
Mar  2 20:08:03.687: INFO: update-demo-nautilus-s7g8w is created but not running
Mar  2 20:08:08.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 20:08:08.767: INFO: stderr: ""
Mar  2 20:08:08.767: INFO: stdout: "update-demo-nautilus-8bwfp update-demo-nautilus-s7g8w "
Mar  2 20:08:08.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-8bwfp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 20:08:08.840: INFO: stderr: ""
Mar  2 20:08:08.840: INFO: stdout: "true"
Mar  2 20:08:08.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-8bwfp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 20:08:08.916: INFO: stderr: ""
Mar  2 20:08:08.916: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar  2 20:08:08.916: INFO: validating pod update-demo-nautilus-8bwfp
Mar  2 20:08:08.933: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 20:08:08.933: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 20:08:08.933: INFO: update-demo-nautilus-8bwfp is verified up and running
Mar  2 20:08:08.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-s7g8w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 20:08:08.999: INFO: stderr: ""
Mar  2 20:08:08.999: INFO: stdout: "true"
Mar  2 20:08:08.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods update-demo-nautilus-s7g8w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 20:08:09.082: INFO: stderr: ""
Mar  2 20:08:09.082: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar  2 20:08:09.082: INFO: validating pod update-demo-nautilus-s7g8w
Mar  2 20:08:09.145: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 20:08:09.145: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 20:08:09.145: INFO: update-demo-nautilus-s7g8w is verified up and running
STEP: using delete to clean up resources
Mar  2 20:08:09.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 delete --grace-period=0 --force -f -'
Mar  2 20:08:09.244: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 20:08:09.244: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 20:08:09.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get rc,svc -l name=update-demo --no-headers'
Mar  2 20:08:09.334: INFO: stderr: "No resources found in kubectl-7262 namespace.\n"
Mar  2 20:08:09.334: INFO: stdout: ""
Mar  2 20:08:09.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7262 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 20:08:09.425: INFO: stderr: ""
Mar  2 20:08:09.425: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:08:09.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7262" for this suite.

• [SLOW TEST:26.654 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":135,"skipped":2450,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:08:09.472: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-43762385-86f5-4ace-b0f6-f723f56c930a
STEP: Creating a pod to test consume configMaps
Mar  2 20:08:09.791: INFO: Waiting up to 5m0s for pod "pod-configmaps-136be591-5dd2-4b9f-960f-ab272e812e66" in namespace "configmap-6576" to be "Succeeded or Failed"
Mar  2 20:08:09.804: INFO: Pod "pod-configmaps-136be591-5dd2-4b9f-960f-ab272e812e66": Phase="Pending", Reason="", readiness=false. Elapsed: 13.17211ms
Mar  2 20:08:11.836: INFO: Pod "pod-configmaps-136be591-5dd2-4b9f-960f-ab272e812e66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045355545s
STEP: Saw pod success
Mar  2 20:08:11.836: INFO: Pod "pod-configmaps-136be591-5dd2-4b9f-960f-ab272e812e66" satisfied condition "Succeeded or Failed"
Mar  2 20:08:11.887: INFO: Trying to get logs from node 10.245.0.4 pod pod-configmaps-136be591-5dd2-4b9f-960f-ab272e812e66 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 20:08:12.057: INFO: Waiting for pod pod-configmaps-136be591-5dd2-4b9f-960f-ab272e812e66 to disappear
Mar  2 20:08:12.086: INFO: Pod pod-configmaps-136be591-5dd2-4b9f-960f-ab272e812e66 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:08:12.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6576" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":136,"skipped":2456,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:08:12.134: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2330
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-8291
STEP: Creating secret with name secret-test-bf5dbb81-e8f5-4c15-9a88-d6327f63815f
STEP: Creating a pod to test consume secrets
Mar  2 20:08:12.716: INFO: Waiting up to 5m0s for pod "pod-secrets-490b0318-5936-48d8-b243-4cb775b7df37" in namespace "secrets-2330" to be "Succeeded or Failed"
Mar  2 20:08:12.742: INFO: Pod "pod-secrets-490b0318-5936-48d8-b243-4cb775b7df37": Phase="Pending", Reason="", readiness=false. Elapsed: 25.310039ms
Mar  2 20:08:14.778: INFO: Pod "pod-secrets-490b0318-5936-48d8-b243-4cb775b7df37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.061096328s
STEP: Saw pod success
Mar  2 20:08:14.778: INFO: Pod "pod-secrets-490b0318-5936-48d8-b243-4cb775b7df37" satisfied condition "Succeeded or Failed"
Mar  2 20:08:14.788: INFO: Trying to get logs from node 10.245.0.4 pod pod-secrets-490b0318-5936-48d8-b243-4cb775b7df37 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 20:08:14.909: INFO: Waiting for pod pod-secrets-490b0318-5936-48d8-b243-4cb775b7df37 to disappear
Mar  2 20:08:14.946: INFO: Pod pod-secrets-490b0318-5936-48d8-b243-4cb775b7df37 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:08:14.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2330" for this suite.
STEP: Destroying namespace "secret-namespace-8291" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2495,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:08:15.093: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 20:08:15.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e181331f-3b25-40e7-9c9c-ab7fc4067730" in namespace "downward-api-3628" to be "Succeeded or Failed"
Mar  2 20:08:15.485: INFO: Pod "downwardapi-volume-e181331f-3b25-40e7-9c9c-ab7fc4067730": Phase="Pending", Reason="", readiness=false. Elapsed: 13.102321ms
Mar  2 20:08:17.503: INFO: Pod "downwardapi-volume-e181331f-3b25-40e7-9c9c-ab7fc4067730": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031748387s
STEP: Saw pod success
Mar  2 20:08:17.503: INFO: Pod "downwardapi-volume-e181331f-3b25-40e7-9c9c-ab7fc4067730" satisfied condition "Succeeded or Failed"
Mar  2 20:08:17.514: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-e181331f-3b25-40e7-9c9c-ab7fc4067730 container client-container: <nil>
STEP: delete the pod
Mar  2 20:08:17.574: INFO: Waiting for pod downwardapi-volume-e181331f-3b25-40e7-9c9c-ab7fc4067730 to disappear
Mar  2 20:08:17.585: INFO: Pod downwardapi-volume-e181331f-3b25-40e7-9c9c-ab7fc4067730 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:08:17.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3628" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":138,"skipped":2504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:08:17.636: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Mar  2 20:08:17.927: INFO: Waiting up to 5m0s for pod "var-expansion-2ce2a90f-16b3-40df-aae4-1a9529b13317" in namespace "var-expansion-3492" to be "Succeeded or Failed"
Mar  2 20:08:17.941: INFO: Pod "var-expansion-2ce2a90f-16b3-40df-aae4-1a9529b13317": Phase="Pending", Reason="", readiness=false. Elapsed: 13.775053ms
Mar  2 20:08:19.971: INFO: Pod "var-expansion-2ce2a90f-16b3-40df-aae4-1a9529b13317": Phase="Running", Reason="", readiness=true. Elapsed: 2.043556034s
Mar  2 20:08:21.992: INFO: Pod "var-expansion-2ce2a90f-16b3-40df-aae4-1a9529b13317": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064374335s
STEP: Saw pod success
Mar  2 20:08:21.992: INFO: Pod "var-expansion-2ce2a90f-16b3-40df-aae4-1a9529b13317" satisfied condition "Succeeded or Failed"
Mar  2 20:08:22.006: INFO: Trying to get logs from node 10.245.0.4 pod var-expansion-2ce2a90f-16b3-40df-aae4-1a9529b13317 container dapi-container: <nil>
STEP: delete the pod
Mar  2 20:08:22.070: INFO: Waiting for pod var-expansion-2ce2a90f-16b3-40df-aae4-1a9529b13317 to disappear
Mar  2 20:08:22.083: INFO: Pod var-expansion-2ce2a90f-16b3-40df-aae4-1a9529b13317 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:08:22.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3492" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":139,"skipped":2542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:08:22.138: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5911
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5911
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-5911
I0302 20:08:22.500535      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5911, replica count: 2
I0302 20:08:25.552415      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 20:08:25.552: INFO: Creating new exec pod
Mar  2 20:08:28.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5911 exec execpodjw6mq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 20:08:28.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 20:08:28.890: INFO: stdout: ""
Mar  2 20:08:29.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5911 exec execpodjw6mq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 20:08:30.183: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 20:08:30.183: INFO: stdout: ""
Mar  2 20:08:30.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5911 exec execpodjw6mq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 20:08:31.119: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 20:08:31.119: INFO: stdout: "externalname-service-gqbgh"
Mar  2 20:08:31.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5911 exec execpodjw6mq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.110.134 80'
Mar  2 20:08:31.324: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.110.134 80\nConnection to 172.21.110.134 80 port [tcp/http] succeeded!\n"
Mar  2 20:08:31.324: INFO: stdout: ""
Mar  2 20:08:32.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5911 exec execpodjw6mq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.110.134 80'
Mar  2 20:08:32.575: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.110.134 80\nConnection to 172.21.110.134 80 port [tcp/http] succeeded!\n"
Mar  2 20:08:32.575: INFO: stdout: "externalname-service-gqbgh"
Mar  2 20:08:32.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5911 exec execpodjw6mq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.245.0.5 30450'
Mar  2 20:08:32.806: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.245.0.5 30450\nConnection to 10.245.0.5 30450 port [tcp/*] succeeded!\n"
Mar  2 20:08:32.806: INFO: stdout: "externalname-service-6nc8j"
Mar  2 20:08:32.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5911 exec execpodjw6mq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.245.0.4 30450'
Mar  2 20:08:33.083: INFO: stderr: "+ + nc -v -t -w 2 10.245.0.4 30450\necho hostName\nConnection to 10.245.0.4 30450 port [tcp/*] succeeded!\n"
Mar  2 20:08:33.083: INFO: stdout: "externalname-service-gqbgh"
Mar  2 20:08:33.083: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:08:33.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5911" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.069 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":140,"skipped":2568,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:08:33.206: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-524
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Mar  2 20:08:33.454: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:08:58.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-524" for this suite.

• [SLOW TEST:25.050 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":141,"skipped":2575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:08:58.257: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 20:09:00.587: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:09:00.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-534" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":142,"skipped":2610,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:09:00.700: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-281
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 20:09:01.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d5dd5b8-8a97-4db5-979c-4b6fa91deac9" in namespace "downward-api-281" to be "Succeeded or Failed"
Mar  2 20:09:01.047: INFO: Pod "downwardapi-volume-2d5dd5b8-8a97-4db5-979c-4b6fa91deac9": Phase="Pending", Reason="", readiness=false. Elapsed: 43.910436ms
Mar  2 20:09:03.066: INFO: Pod "downwardapi-volume-2d5dd5b8-8a97-4db5-979c-4b6fa91deac9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062621018s
Mar  2 20:09:05.084: INFO: Pod "downwardapi-volume-2d5dd5b8-8a97-4db5-979c-4b6fa91deac9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.080084357s
STEP: Saw pod success
Mar  2 20:09:05.084: INFO: Pod "downwardapi-volume-2d5dd5b8-8a97-4db5-979c-4b6fa91deac9" satisfied condition "Succeeded or Failed"
Mar  2 20:09:05.096: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-2d5dd5b8-8a97-4db5-979c-4b6fa91deac9 container client-container: <nil>
STEP: delete the pod
Mar  2 20:09:05.211: INFO: Waiting for pod downwardapi-volume-2d5dd5b8-8a97-4db5-979c-4b6fa91deac9 to disappear
Mar  2 20:09:05.222: INFO: Pod downwardapi-volume-2d5dd5b8-8a97-4db5-979c-4b6fa91deac9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:09:05.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-281" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":143,"skipped":2631,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:09:05.258: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5546
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Mar  2 20:09:05.557: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:09:07.575: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  2 20:09:08.652: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:09:09.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5546" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":144,"skipped":2737,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:09:09.750: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-2767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  2 20:09:10.064: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 20:10:10.157: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:10:10.175: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-9151
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:10:10.441: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Mar  2 20:10:10.455: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:10:10.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9151" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:10:10.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2767" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:61.036 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":145,"skipped":2761,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:10:10.788: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9548
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:10:11.020: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:10:11.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9548" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":146,"skipped":2765,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:10:11.716: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-471
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 20:10:12.715: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  2 20:10:14.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781848612, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781848612, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781848612, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781848612, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 20:10:17.812: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:10:18.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-471" for this suite.
STEP: Destroying namespace "webhook-471-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.936 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":147,"skipped":2767,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:10:18.654: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4959
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-7a260477-b93a-45c7-a786-7a3b4ce52372
STEP: Creating configMap with name cm-test-opt-upd-1d2417a4-be07-4209-be51-b3c80c1df4b1
STEP: Creating the pod
Mar  2 20:10:19.019: INFO: The status of Pod pod-projected-configmaps-b6ec22a1-1e29-4ba9-98df-08e0ad177ab4 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:10:21.033: INFO: The status of Pod pod-projected-configmaps-b6ec22a1-1e29-4ba9-98df-08e0ad177ab4 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:10:23.040: INFO: The status of Pod pod-projected-configmaps-b6ec22a1-1e29-4ba9-98df-08e0ad177ab4 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-7a260477-b93a-45c7-a786-7a3b4ce52372
STEP: Updating configmap cm-test-opt-upd-1d2417a4-be07-4209-be51-b3c80c1df4b1
STEP: Creating configMap with name cm-test-opt-create-be9c4045-8030-4d61-95e7-2df847efcef9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:11:36.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4959" for this suite.

• [SLOW TEST:77.794 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":148,"skipped":2863,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:11:36.450: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3337
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:11:36.712: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  2 20:11:41.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-3337 --namespace=crd-publish-openapi-3337 create -f -'
Mar  2 20:11:42.853: INFO: stderr: ""
Mar  2 20:11:42.853: INFO: stdout: "e2e-test-crd-publish-openapi-2494-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 20:11:42.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-3337 --namespace=crd-publish-openapi-3337 delete e2e-test-crd-publish-openapi-2494-crds test-cr'
Mar  2 20:11:43.049: INFO: stderr: ""
Mar  2 20:11:43.049: INFO: stdout: "e2e-test-crd-publish-openapi-2494-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  2 20:11:43.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-3337 --namespace=crd-publish-openapi-3337 apply -f -'
Mar  2 20:11:43.923: INFO: stderr: ""
Mar  2 20:11:43.923: INFO: stdout: "e2e-test-crd-publish-openapi-2494-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 20:11:43.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-3337 --namespace=crd-publish-openapi-3337 delete e2e-test-crd-publish-openapi-2494-crds test-cr'
Mar  2 20:11:44.004: INFO: stderr: ""
Mar  2 20:11:44.004: INFO: stdout: "e2e-test-crd-publish-openapi-2494-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  2 20:11:44.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=crd-publish-openapi-3337 explain e2e-test-crd-publish-openapi-2494-crds'
Mar  2 20:11:44.201: INFO: stderr: ""
Mar  2 20:11:44.201: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2494-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:11:48.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3337" for this suite.

• [SLOW TEST:12.259 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":149,"skipped":2904,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:11:48.709: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename hostport
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-3793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Mar  2 20:11:49.026: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:11:51.043: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:11:53.044: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.245.0.4 on the node which pod1 resides and expect scheduled
Mar  2 20:11:53.073: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:11:55.089: INFO: The status of Pod pod2 is Running (Ready = false)
Mar  2 20:11:57.085: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.245.0.4 but use UDP protocol on the node which pod2 resides
Mar  2 20:11:57.127: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:11:59.148: INFO: The status of Pod pod3 is Running (Ready = true)
Mar  2 20:11:59.177: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:12:01.211: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Mar  2 20:12:01.224: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.245.0.4 http://127.0.0.1:54323/hostname] Namespace:hostport-3793 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 20:12:01.224: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.245.0.4, port: 54323
Mar  2 20:12:01.441: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.245.0.4:54323/hostname] Namespace:hostport-3793 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 20:12:01.441: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.245.0.4, port: 54323 UDP
Mar  2 20:12:01.629: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.245.0.4 54323] Namespace:hostport-3793 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 20:12:01.629: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:12:06.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-3793" for this suite.

• [SLOW TEST:18.140 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":150,"skipped":2918,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:12:06.852: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-7630
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:12:07.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7630" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":151,"skipped":2928,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:12:07.167: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-192
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar  2 20:12:07.465: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 20:12:10.775: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:12:27.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-192" for this suite.

• [SLOW TEST:20.379 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":152,"skipped":2929,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:12:27.547: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-7727
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Mar  2 20:12:29.925: INFO: running pods: 0 < 3
Mar  2 20:12:31.961: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:12:33.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7727" for this suite.

• [SLOW TEST:6.467 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":153,"skipped":2943,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:12:34.015: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-856
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:12:34.253: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:12:41.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-856" for this suite.

• [SLOW TEST:7.249 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":154,"skipped":2943,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:12:41.280: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9394
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:12:58.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9394" for this suite.

• [SLOW TEST:17.455 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":155,"skipped":2957,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:12:58.737: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9298
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-3b22bbf8-c186-49d2-8779-fbd0fc44c8a7 in namespace container-probe-9298
Mar  2 20:13:03.064: INFO: Started pod liveness-3b22bbf8-c186-49d2-8779-fbd0fc44c8a7 in namespace container-probe-9298
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 20:13:03.101: INFO: Initial restart count of pod liveness-3b22bbf8-c186-49d2-8779-fbd0fc44c8a7 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:17:03.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9298" for this suite.

• [SLOW TEST:245.039 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":156,"skipped":2963,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:17:03.777: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 20:17:04.609: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 20:17:06.647: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781849024, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781849024, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781849024, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781849024, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 20:17:09.698: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:17:09.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4395" for this suite.
STEP: Destroying namespace "webhook-4395-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.162 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":157,"skipped":2977,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:17:09.940: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar  2 20:17:10.199: INFO: Waiting up to 5m0s for pod "downward-api-fd09a6a7-56b4-4ff9-b18b-f8b299ef9ecb" in namespace "downward-api-7214" to be "Succeeded or Failed"
Mar  2 20:17:10.216: INFO: Pod "downward-api-fd09a6a7-56b4-4ff9-b18b-f8b299ef9ecb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.797134ms
Mar  2 20:17:12.238: INFO: Pod "downward-api-fd09a6a7-56b4-4ff9-b18b-f8b299ef9ecb": Phase="Running", Reason="", readiness=true. Elapsed: 2.038679163s
Mar  2 20:17:14.271: INFO: Pod "downward-api-fd09a6a7-56b4-4ff9-b18b-f8b299ef9ecb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071932658s
STEP: Saw pod success
Mar  2 20:17:14.271: INFO: Pod "downward-api-fd09a6a7-56b4-4ff9-b18b-f8b299ef9ecb" satisfied condition "Succeeded or Failed"
Mar  2 20:17:14.283: INFO: Trying to get logs from node 10.245.0.4 pod downward-api-fd09a6a7-56b4-4ff9-b18b-f8b299ef9ecb container dapi-container: <nil>
STEP: delete the pod
Mar  2 20:17:14.414: INFO: Waiting for pod downward-api-fd09a6a7-56b4-4ff9-b18b-f8b299ef9ecb to disappear
Mar  2 20:17:14.431: INFO: Pod downward-api-fd09a6a7-56b4-4ff9-b18b-f8b299ef9ecb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:17:14.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7214" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":158,"skipped":2994,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:17:14.469: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  2 20:17:14.761: INFO: Waiting up to 5m0s for pod "pod-c04fa30f-025a-47a0-80a1-2112026d53e8" in namespace "emptydir-7345" to be "Succeeded or Failed"
Mar  2 20:17:14.775: INFO: Pod "pod-c04fa30f-025a-47a0-80a1-2112026d53e8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.127562ms
Mar  2 20:17:16.789: INFO: Pod "pod-c04fa30f-025a-47a0-80a1-2112026d53e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028458157s
STEP: Saw pod success
Mar  2 20:17:16.789: INFO: Pod "pod-c04fa30f-025a-47a0-80a1-2112026d53e8" satisfied condition "Succeeded or Failed"
Mar  2 20:17:16.801: INFO: Trying to get logs from node 10.245.0.4 pod pod-c04fa30f-025a-47a0-80a1-2112026d53e8 container test-container: <nil>
STEP: delete the pod
Mar  2 20:17:16.862: INFO: Waiting for pod pod-c04fa30f-025a-47a0-80a1-2112026d53e8 to disappear
Mar  2 20:17:16.874: INFO: Pod pod-c04fa30f-025a-47a0-80a1-2112026d53e8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:17:16.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7345" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":159,"skipped":3010,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:17:16.909: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6492
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Mar  2 20:17:17.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-6492 create -f -'
Mar  2 20:17:17.427: INFO: stderr: ""
Mar  2 20:17:17.427: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar  2 20:17:18.442: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 20:17:18.442: INFO: Found 0 / 1
Mar  2 20:17:19.469: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 20:17:19.469: INFO: Found 0 / 1
Mar  2 20:17:20.443: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 20:17:20.443: INFO: Found 1 / 1
Mar  2 20:17:20.443: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  2 20:17:20.456: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 20:17:20.456: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 20:17:20.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-6492 patch pod agnhost-primary-rkg98 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  2 20:17:20.561: INFO: stderr: ""
Mar  2 20:17:20.561: INFO: stdout: "pod/agnhost-primary-rkg98 patched\n"
STEP: checking annotations
Mar  2 20:17:20.573: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 20:17:20.573: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:17:20.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6492" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":160,"skipped":3019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:17:20.613: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-2217
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2217
STEP: Waiting until pod test-pod will start running in namespace statefulset-2217
STEP: Creating statefulset with conflicting port in namespace statefulset-2217
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2217
Mar  2 20:17:25.069: INFO: Observed stateful pod in namespace: statefulset-2217, name: ss-0, uid: 9e70c506-ce84-47dd-9430-3656514fe1ad, status phase: Pending. Waiting for statefulset controller to delete.
Mar  2 20:17:25.100: INFO: Observed stateful pod in namespace: statefulset-2217, name: ss-0, uid: 9e70c506-ce84-47dd-9430-3656514fe1ad, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 20:17:25.145: INFO: Observed stateful pod in namespace: statefulset-2217, name: ss-0, uid: 9e70c506-ce84-47dd-9430-3656514fe1ad, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 20:17:25.160: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2217
STEP: Removing pod with conflicting port in namespace statefulset-2217
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2217 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Mar  2 20:17:27.249: INFO: Deleting all statefulset in ns statefulset-2217
Mar  2 20:17:27.261: INFO: Scaling statefulset ss to 0
Mar  2 20:17:37.365: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 20:17:37.379: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:17:37.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2217" for this suite.

• [SLOW TEST:16.883 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":161,"skipped":3043,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:17:37.497: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-f0af76e4-3bbf-44d8-9ebc-beae5164e56b
STEP: Creating a pod to test consume configMaps
Mar  2 20:17:37.779: INFO: Waiting up to 5m0s for pod "pod-configmaps-e230398a-ef3a-4139-8bbc-df5f9ae82628" in namespace "configmap-7317" to be "Succeeded or Failed"
Mar  2 20:17:37.791: INFO: Pod "pod-configmaps-e230398a-ef3a-4139-8bbc-df5f9ae82628": Phase="Pending", Reason="", readiness=false. Elapsed: 12.154498ms
Mar  2 20:17:39.814: INFO: Pod "pod-configmaps-e230398a-ef3a-4139-8bbc-df5f9ae82628": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034410763s
Mar  2 20:17:41.830: INFO: Pod "pod-configmaps-e230398a-ef3a-4139-8bbc-df5f9ae82628": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05085038s
STEP: Saw pod success
Mar  2 20:17:41.830: INFO: Pod "pod-configmaps-e230398a-ef3a-4139-8bbc-df5f9ae82628" satisfied condition "Succeeded or Failed"
Mar  2 20:17:41.841: INFO: Trying to get logs from node 10.245.0.4 pod pod-configmaps-e230398a-ef3a-4139-8bbc-df5f9ae82628 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 20:17:41.966: INFO: Waiting for pod pod-configmaps-e230398a-ef3a-4139-8bbc-df5f9ae82628 to disappear
Mar  2 20:17:41.977: INFO: Pod pod-configmaps-e230398a-ef3a-4139-8bbc-df5f9ae82628 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:17:41.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7317" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":162,"skipped":3044,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:17:42.027: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4448
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar  2 20:17:44.338: INFO: &Pod{ObjectMeta:{send-events-c1349399-a8b8-42c5-9260-6c32a93a78e7  events-4448  63281028-d817-4c39-8830-5349334bbb5d 32438 0 2022-03-02 20:17:42 +0000 UTC <nil> <nil> map[name:foo time:249975493] map[cni.projectcalico.org/containerID:b5956d61b94fcb83a37a125fb6037e8a6a39f6d8c17fa3b009e425ab27b6c98f cni.projectcalico.org/podIP:172.17.100.190/32 cni.projectcalico.org/podIPs:172.17.100.190/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2022-03-02 20:17:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 20:17:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 20:17:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.100.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-znnfk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-znnfk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:17:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:17:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:17:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:17:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.4,PodIP:172.17.100.190,StartTime:2022-03-02 20:17:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 20:17:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://c71cffc049132c900bda54a679a6e82244c8f6aa75baa33504e99687611faafe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.100.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar  2 20:17:46.349: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar  2 20:17:48.370: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:17:48.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4448" for this suite.

• [SLOW TEST:6.409 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":163,"skipped":3073,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:17:48.436: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9249
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  2 20:17:48.722: INFO: Waiting up to 5m0s for pod "pod-1c608f77-8952-4698-9c63-46eaa75f62ae" in namespace "emptydir-9249" to be "Succeeded or Failed"
Mar  2 20:17:48.742: INFO: Pod "pod-1c608f77-8952-4698-9c63-46eaa75f62ae": Phase="Pending", Reason="", readiness=false. Elapsed: 20.258909ms
Mar  2 20:17:50.769: INFO: Pod "pod-1c608f77-8952-4698-9c63-46eaa75f62ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046864768s
Mar  2 20:17:52.791: INFO: Pod "pod-1c608f77-8952-4698-9c63-46eaa75f62ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068843858s
STEP: Saw pod success
Mar  2 20:17:52.791: INFO: Pod "pod-1c608f77-8952-4698-9c63-46eaa75f62ae" satisfied condition "Succeeded or Failed"
Mar  2 20:17:52.803: INFO: Trying to get logs from node 10.245.0.4 pod pod-1c608f77-8952-4698-9c63-46eaa75f62ae container test-container: <nil>
STEP: delete the pod
Mar  2 20:17:52.893: INFO: Waiting for pod pod-1c608f77-8952-4698-9c63-46eaa75f62ae to disappear
Mar  2 20:17:52.921: INFO: Pod pod-1c608f77-8952-4698-9c63-46eaa75f62ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:17:52.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9249" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":164,"skipped":3082,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:17:52.996: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-1445
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Mar  2 20:17:53.260: INFO: Waiting up to 5m0s for pod "security-context-f5ab429e-2081-43d7-9f90-259d155b5c3c" in namespace "security-context-1445" to be "Succeeded or Failed"
Mar  2 20:17:53.331: INFO: Pod "security-context-f5ab429e-2081-43d7-9f90-259d155b5c3c": Phase="Pending", Reason="", readiness=false. Elapsed: 71.302371ms
Mar  2 20:17:55.354: INFO: Pod "security-context-f5ab429e-2081-43d7-9f90-259d155b5c3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.093607356s
STEP: Saw pod success
Mar  2 20:17:55.354: INFO: Pod "security-context-f5ab429e-2081-43d7-9f90-259d155b5c3c" satisfied condition "Succeeded or Failed"
Mar  2 20:17:55.366: INFO: Trying to get logs from node 10.245.0.4 pod security-context-f5ab429e-2081-43d7-9f90-259d155b5c3c container test-container: <nil>
STEP: delete the pod
Mar  2 20:17:55.427: INFO: Waiting for pod security-context-f5ab429e-2081-43d7-9f90-259d155b5c3c to disappear
Mar  2 20:17:55.437: INFO: Pod security-context-f5ab429e-2081-43d7-9f90-259d155b5c3c no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:17:55.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1445" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":165,"skipped":3089,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:17:55.497: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 20:17:55.947: INFO: Number of nodes with available pods: 0
Mar  2 20:17:55.948: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:17:56.981: INFO: Number of nodes with available pods: 0
Mar  2 20:17:56.981: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:17:57.990: INFO: Number of nodes with available pods: 1
Mar  2 20:17:57.990: INFO: Node 10.245.0.5 is running more than one daemon pod
Mar  2 20:17:59.011: INFO: Number of nodes with available pods: 3
Mar  2 20:17:59.011: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Getting /status
Mar  2 20:17:59.040: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Mar  2 20:17:59.095: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Mar  2 20:17:59.101: INFO: Observed &DaemonSet event: ADDED
Mar  2 20:17:59.101: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 20:17:59.102: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 20:17:59.102: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 20:17:59.102: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 20:17:59.102: INFO: Found daemon set daemon-set in namespace daemonsets-1850 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 20:17:59.102: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Mar  2 20:17:59.121: INFO: Observed &DaemonSet event: ADDED
Mar  2 20:17:59.121: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 20:17:59.121: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 20:17:59.122: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 20:17:59.122: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 20:17:59.122: INFO: Observed daemon set daemon-set in namespace daemonsets-1850 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 20:17:59.122: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 20:17:59.122: INFO: Found daemon set daemon-set in namespace daemonsets-1850 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar  2 20:17:59.122: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1850, will wait for the garbage collector to delete the pods
Mar  2 20:17:59.217: INFO: Deleting DaemonSet.extensions daemon-set took: 19.933411ms
Mar  2 20:17:59.317: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.410136ms
Mar  2 20:18:02.347: INFO: Number of nodes with available pods: 0
Mar  2 20:18:02.347: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 20:18:02.358: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32655"},"items":null}

Mar  2 20:18:02.369: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32655"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:18:02.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1850" for this suite.

• [SLOW TEST:6.998 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":166,"skipped":3121,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:18:02.496: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8881
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  2 20:18:02.769: INFO: Waiting up to 5m0s for pod "pod-8ef1b0c3-4b1d-478f-80a8-ae6100876042" in namespace "emptydir-8881" to be "Succeeded or Failed"
Mar  2 20:18:02.781: INFO: Pod "pod-8ef1b0c3-4b1d-478f-80a8-ae6100876042": Phase="Pending", Reason="", readiness=false. Elapsed: 11.921074ms
Mar  2 20:18:04.801: INFO: Pod "pod-8ef1b0c3-4b1d-478f-80a8-ae6100876042": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03240948s
Mar  2 20:18:06.815: INFO: Pod "pod-8ef1b0c3-4b1d-478f-80a8-ae6100876042": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045905848s
STEP: Saw pod success
Mar  2 20:18:06.815: INFO: Pod "pod-8ef1b0c3-4b1d-478f-80a8-ae6100876042" satisfied condition "Succeeded or Failed"
Mar  2 20:18:06.826: INFO: Trying to get logs from node 10.245.0.4 pod pod-8ef1b0c3-4b1d-478f-80a8-ae6100876042 container test-container: <nil>
STEP: delete the pod
Mar  2 20:18:06.881: INFO: Waiting for pod pod-8ef1b0c3-4b1d-478f-80a8-ae6100876042 to disappear
Mar  2 20:18:06.892: INFO: Pod pod-8ef1b0c3-4b1d-478f-80a8-ae6100876042 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:18:06.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8881" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":167,"skipped":3127,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:18:06.974: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5097
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5097
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5097
STEP: creating replication controller externalsvc in namespace services-5097
I0302 20:18:07.333173      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-5097, replica count: 2
I0302 20:18:10.384725      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar  2 20:18:10.480: INFO: Creating new exec pod
Mar  2 20:18:12.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-5097 exec execpodsdfzc -- /bin/sh -x -c nslookup nodeport-service.services-5097.svc.cluster.local'
Mar  2 20:18:12.836: INFO: stderr: "+ nslookup nodeport-service.services-5097.svc.cluster.local\n"
Mar  2 20:18:12.837: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-5097.svc.cluster.local\tcanonical name = externalsvc.services-5097.svc.cluster.local.\nName:\texternalsvc.services-5097.svc.cluster.local\nAddress: 172.21.134.0\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5097, will wait for the garbage collector to delete the pods
Mar  2 20:18:12.931: INFO: Deleting ReplicationController externalsvc took: 28.800098ms
Mar  2 20:18:13.032: INFO: Terminating ReplicationController externalsvc pods took: 101.14104ms
Mar  2 20:18:15.301: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:18:15.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5097" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.406 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":168,"skipped":3142,"failed":0}
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:18:15.381: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3916
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:18:15.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3916" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":169,"skipped":3142,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:18:15.783: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 20:18:16.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34d91fcf-90c2-4743-8ce0-6b3803db6bca" in namespace "projected-2037" to be "Succeeded or Failed"
Mar  2 20:18:16.084: INFO: Pod "downwardapi-volume-34d91fcf-90c2-4743-8ce0-6b3803db6bca": Phase="Pending", Reason="", readiness=false. Elapsed: 14.703966ms
Mar  2 20:18:18.104: INFO: Pod "downwardapi-volume-34d91fcf-90c2-4743-8ce0-6b3803db6bca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034667294s
STEP: Saw pod success
Mar  2 20:18:18.104: INFO: Pod "downwardapi-volume-34d91fcf-90c2-4743-8ce0-6b3803db6bca" satisfied condition "Succeeded or Failed"
Mar  2 20:18:18.115: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-34d91fcf-90c2-4743-8ce0-6b3803db6bca container client-container: <nil>
STEP: delete the pod
Mar  2 20:18:18.169: INFO: Waiting for pod downwardapi-volume-34d91fcf-90c2-4743-8ce0-6b3803db6bca to disappear
Mar  2 20:18:18.185: INFO: Pod downwardapi-volume-34d91fcf-90c2-4743-8ce0-6b3803db6bca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:18:18.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2037" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":170,"skipped":3153,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:18:18.230: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4760
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Mar  2 20:18:18.503: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:18:20.524: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Mar  2 20:18:20.568: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:18:22.590: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:18:24.599: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  2 20:18:24.689: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 20:18:24.720: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 20:18:26.720: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 20:18:26.735: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 20:18:28.721: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 20:18:28.747: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:18:28.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4760" for this suite.

• [SLOW TEST:10.569 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":171,"skipped":3164,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:18:28.799: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8001
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-8001
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 20:18:29.047: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 20:18:29.162: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:18:31.191: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:18:33.185: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:18:35.209: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:18:37.173: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:18:39.195: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:18:41.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:18:43.183: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:18:45.188: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:18:47.177: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 20:18:49.191: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 20:18:49.215: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 20:18:49.237: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  2 20:18:51.358: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 20:18:51.358: INFO: Going to poll 172.17.100.129 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar  2 20:18:51.378: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.17.100.129:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8001 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 20:18:51.378: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 20:18:51.544: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  2 20:18:51.544: INFO: Going to poll 172.17.125.233 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar  2 20:18:51.565: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.17.125.233:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8001 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 20:18:51.565: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 20:18:51.768: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  2 20:18:51.768: INFO: Going to poll 172.17.74.47 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar  2 20:18:51.829: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.17.74.47:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8001 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 20:18:51.829: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 20:18:52.073: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:18:52.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8001" for this suite.

• [SLOW TEST:23.332 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":172,"skipped":3165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:18:52.132: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-4498
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-4498
Mar  2 20:18:52.410: INFO: Found 0 stateful pods, waiting for 1
Mar  2 20:19:02.433: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Mar  2 20:19:02.594: INFO: Deleting all statefulset in ns statefulset-4498
Mar  2 20:19:02.610: INFO: Scaling statefulset ss to 0
Mar  2 20:19:12.728: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 20:19:12.743: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:19:12.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4498" for this suite.

• [SLOW TEST:20.717 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":173,"skipped":3187,"failed":0}
SSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:19:12.851: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-761
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:19:15.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-761" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":174,"skipped":3191,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:19:15.376: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar  2 20:19:15.656: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:19:21.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6885" for this suite.

• [SLOW TEST:5.847 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3195,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:19:21.223: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 20:19:21.545: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00e68e1a-cb76-4e47-bfaf-86fe3dd85890" in namespace "projected-7108" to be "Succeeded or Failed"
Mar  2 20:19:21.557: INFO: Pod "downwardapi-volume-00e68e1a-cb76-4e47-bfaf-86fe3dd85890": Phase="Pending", Reason="", readiness=false. Elapsed: 11.998349ms
Mar  2 20:19:23.585: INFO: Pod "downwardapi-volume-00e68e1a-cb76-4e47-bfaf-86fe3dd85890": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040551188s
Mar  2 20:19:25.606: INFO: Pod "downwardapi-volume-00e68e1a-cb76-4e47-bfaf-86fe3dd85890": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060964557s
STEP: Saw pod success
Mar  2 20:19:25.606: INFO: Pod "downwardapi-volume-00e68e1a-cb76-4e47-bfaf-86fe3dd85890" satisfied condition "Succeeded or Failed"
Mar  2 20:19:25.620: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-00e68e1a-cb76-4e47-bfaf-86fe3dd85890 container client-container: <nil>
STEP: delete the pod
Mar  2 20:19:25.707: INFO: Waiting for pod downwardapi-volume-00e68e1a-cb76-4e47-bfaf-86fe3dd85890 to disappear
Mar  2 20:19:25.719: INFO: Pod downwardapi-volume-00e68e1a-cb76-4e47-bfaf-86fe3dd85890 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:19:25.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7108" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":176,"skipped":3202,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:19:25.805: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7583
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  2 20:19:26.160: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7583  66fb1776-de7d-4694-b0c6-17623ed13e23 33442 0 2022-03-02 20:19:26 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-02 20:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 20:19:26.160: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7583  66fb1776-de7d-4694-b0c6-17623ed13e23 33443 0 2022-03-02 20:19:26 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-02 20:19:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 20:19:26.161: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7583  66fb1776-de7d-4694-b0c6-17623ed13e23 33444 0 2022-03-02 20:19:26 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-02 20:19:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  2 20:19:36.284: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7583  66fb1776-de7d-4694-b0c6-17623ed13e23 33486 0 2022-03-02 20:19:26 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-02 20:19:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 20:19:36.284: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7583  66fb1776-de7d-4694-b0c6-17623ed13e23 33487 0 2022-03-02 20:19:26 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-02 20:19:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 20:19:36.284: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7583  66fb1776-de7d-4694-b0c6-17623ed13e23 33488 0 2022-03-02 20:19:26 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-02 20:19:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:19:36.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7583" for this suite.

• [SLOW TEST:10.522 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":177,"skipped":3210,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:19:36.328: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2404
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2404
Mar  2 20:19:36.598: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:19:38.617: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar  2 20:19:38.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2404 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  2 20:19:38.923: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  2 20:19:38.923: INFO: stdout: "iptables"
Mar  2 20:19:38.923: INFO: proxyMode: iptables
Mar  2 20:19:38.960: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  2 20:19:38.972: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-2404
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2404
I0302 20:19:39.041684      21 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2404, replica count: 3
I0302 20:19:42.092513      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 20:19:42.149: INFO: Creating new exec pod
Mar  2 20:19:45.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2404 exec execpod-affinitynhp6p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Mar  2 20:19:45.454: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar  2 20:19:45.454: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 20:19:45.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2404 exec execpod-affinitynhp6p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.245.55 80'
Mar  2 20:19:45.690: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.245.55 80\nConnection to 172.21.245.55 80 port [tcp/http] succeeded!\n"
Mar  2 20:19:45.690: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 20:19:45.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2404 exec execpod-affinitynhp6p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.245.0.5 32218'
Mar  2 20:19:45.932: INFO: stderr: "+ nc -v -t -w 2 10.245.0.5 32218\n+ echo hostName\nConnection to 10.245.0.5 32218 port [tcp/*] succeeded!\n"
Mar  2 20:19:45.932: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 20:19:45.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2404 exec execpod-affinitynhp6p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.245.0.4 32218'
Mar  2 20:19:46.194: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.245.0.4 32218\nConnection to 10.245.0.4 32218 port [tcp/*] succeeded!\n"
Mar  2 20:19:46.194: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 20:19:46.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2404 exec execpod-affinitynhp6p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.245.0.4:32218/ ; done'
Mar  2 20:19:46.468: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n"
Mar  2 20:19:46.468: INFO: stdout: "\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624\naffinity-nodeport-timeout-vx624"
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Received response from host: affinity-nodeport-timeout-vx624
Mar  2 20:19:46.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2404 exec execpod-affinitynhp6p -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.245.0.4:32218/'
Mar  2 20:19:46.688: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n"
Mar  2 20:19:46.688: INFO: stdout: "affinity-nodeport-timeout-vx624"
Mar  2 20:20:06.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2404 exec execpod-affinitynhp6p -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.245.0.4:32218/'
Mar  2 20:20:06.959: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n"
Mar  2 20:20:06.959: INFO: stdout: "affinity-nodeport-timeout-vx624"
Mar  2 20:20:26.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2404 exec execpod-affinitynhp6p -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.245.0.4:32218/'
Mar  2 20:20:27.192: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.245.0.4:32218/\n"
Mar  2 20:20:27.192: INFO: stdout: "affinity-nodeport-timeout-cqn4p"
Mar  2 20:20:27.192: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2404, will wait for the garbage collector to delete the pods
Mar  2 20:20:27.351: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 21.505448ms
Mar  2 20:20:27.552: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.904356ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:20:29.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2404" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:53.551 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":178,"skipped":3223,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:20:29.880: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7074
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Mar  2 20:20:30.152: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7074 proxy --unix-socket=/tmp/kubectl-proxy-unix763836609/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:20:30.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7074" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":179,"skipped":3224,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:20:30.224: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1036
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1036
Mar  2 20:20:30.498: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:20:32.549: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar  2 20:20:32.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-1036 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  2 20:20:32.804: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  2 20:20:32.804: INFO: stdout: "iptables"
Mar  2 20:20:32.804: INFO: proxyMode: iptables
Mar  2 20:20:32.847: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  2 20:20:32.859: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1036
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1036
I0302 20:20:32.911730      21 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1036, replica count: 3
I0302 20:20:35.973623      21 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 20:20:36.030: INFO: Creating new exec pod
Mar  2 20:20:41.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-1036 exec execpod-affinityg6445 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Mar  2 20:20:41.395: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout 80\n+ echo hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar  2 20:20:41.395: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 20:20:41.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-1036 exec execpod-affinityg6445 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.157.148 80'
Mar  2 20:20:41.629: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.157.148 80\nConnection to 172.21.157.148 80 port [tcp/http] succeeded!\n"
Mar  2 20:20:41.629: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 20:20:41.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-1036 exec execpod-affinityg6445 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.157.148:80/ ; done'
Mar  2 20:20:41.959: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n"
Mar  2 20:20:41.959: INFO: stdout: "\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d\naffinity-clusterip-timeout-cpl8d"
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Received response from host: affinity-clusterip-timeout-cpl8d
Mar  2 20:20:41.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-1036 exec execpod-affinityg6445 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.157.148:80/'
Mar  2 20:20:42.246: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n"
Mar  2 20:20:42.246: INFO: stdout: "affinity-clusterip-timeout-cpl8d"
Mar  2 20:21:02.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-1036 exec execpod-affinityg6445 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.157.148:80/'
Mar  2 20:21:02.495: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.157.148:80/\n"
Mar  2 20:21:02.495: INFO: stdout: "affinity-clusterip-timeout-xhgl4"
Mar  2 20:21:02.495: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1036, will wait for the garbage collector to delete the pods
Mar  2 20:21:02.630: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 18.734433ms
Mar  2 20:21:02.830: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 200.312078ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:21:05.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1036" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:34.903 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":180,"skipped":3242,"failed":0}
SSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:21:05.128: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-9376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:23:01.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9376" for this suite.

• [SLOW TEST:116.447 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":181,"skipped":3248,"failed":0}
S
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:23:01.575: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9436
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Mar  2 20:23:01.911: INFO: Waiting up to 5m0s for pod "client-containers-da1ce472-a340-4b37-91ca-a069fa83262b" in namespace "containers-9436" to be "Succeeded or Failed"
Mar  2 20:23:01.959: INFO: Pod "client-containers-da1ce472-a340-4b37-91ca-a069fa83262b": Phase="Pending", Reason="", readiness=false. Elapsed: 47.793556ms
Mar  2 20:23:03.974: INFO: Pod "client-containers-da1ce472-a340-4b37-91ca-a069fa83262b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.063246677s
STEP: Saw pod success
Mar  2 20:23:03.974: INFO: Pod "client-containers-da1ce472-a340-4b37-91ca-a069fa83262b" satisfied condition "Succeeded or Failed"
Mar  2 20:23:03.986: INFO: Trying to get logs from node 10.245.0.4 pod client-containers-da1ce472-a340-4b37-91ca-a069fa83262b container agnhost-container: <nil>
STEP: delete the pod
Mar  2 20:23:04.106: INFO: Waiting for pod client-containers-da1ce472-a340-4b37-91ca-a069fa83262b to disappear
Mar  2 20:23:04.117: INFO: Pod client-containers-da1ce472-a340-4b37-91ca-a069fa83262b no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:23:04.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9436" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:23:04.150: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Mar  2 20:23:04.377: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:23:08.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4003" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":183,"skipped":3274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:23:08.743: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5453
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:23:11.072: INFO: Deleting pod "var-expansion-3cc0412d-8eb4-41f3-8854-ce9414431f33" in namespace "var-expansion-5453"
Mar  2 20:23:11.094: INFO: Wait up to 5m0s for pod "var-expansion-3cc0412d-8eb4-41f3-8854-ce9414431f33" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:23:15.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5453" for this suite.

• [SLOW TEST:6.432 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":184,"skipped":3310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:23:15.178: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5292
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  2 20:23:15.505: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 20:24:15.630: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:24:15.643: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-1668
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Mar  2 20:24:18.047: INFO: found a healthy node: 10.245.0.4
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:24:32.287: INFO: pods created so far: [1 1 1]
Mar  2 20:24:32.287: INFO: length of pods created so far: 3
Mar  2 20:24:36.369: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:24:43.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1668" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:24:43.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5292" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:88.472 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":185,"skipped":3403,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:24:43.651: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8105
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:24:43.973: INFO: The status of Pod pod-secrets-430a887a-4aeb-468f-93ca-41a269b07cf1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:24:45.993: INFO: The status of Pod pod-secrets-430a887a-4aeb-468f-93ca-41a269b07cf1 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:24:46.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8105" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":186,"skipped":3407,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:24:46.166: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 20:24:46.882: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 20:24:48.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781849486, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781849486, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781849486, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781849486, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 20:24:52.048: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:24:52.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-972" for this suite.
STEP: Destroying namespace "webhook-972-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.312 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":187,"skipped":3417,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:24:52.478: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6739
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:25:13.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6739" for this suite.

• [SLOW TEST:21.160 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":188,"skipped":3436,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:25:13.639: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5572
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5572.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5572.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 20:25:18.095: INFO: DNS probes using dns-test-589fb6c1-3096-43f5-bbfc-f548d3cfccf3 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5572.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5572.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 20:25:20.277: INFO: File wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local from pod  dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 20:25:20.314: INFO: File jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local from pod  dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 20:25:20.314: INFO: Lookups using dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 failed for: [wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local]

Mar  2 20:25:25.332: INFO: File wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local from pod  dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 20:25:25.350: INFO: File jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local from pod  dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 20:25:25.350: INFO: Lookups using dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 failed for: [wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local]

Mar  2 20:25:30.332: INFO: File wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local from pod  dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 20:25:30.351: INFO: File jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local from pod  dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 20:25:30.351: INFO: Lookups using dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 failed for: [wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local]

Mar  2 20:25:35.348: INFO: File wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local from pod  dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 20:25:35.365: INFO: File jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local from pod  dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 20:25:35.365: INFO: Lookups using dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 failed for: [wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local]

Mar  2 20:25:40.335: INFO: File wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local from pod  dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 20:25:40.377: INFO: File jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local from pod  dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 20:25:40.377: INFO: Lookups using dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 failed for: [wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local]

Mar  2 20:25:45.332: INFO: File wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local from pod  dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 20:25:45.371: INFO: File jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local from pod  dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 20:25:45.371: INFO: Lookups using dns-5572/dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 failed for: [wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local]

Mar  2 20:25:50.376: INFO: DNS probes using dns-test-85ca8d62-1711-46bb-8ac6-1a32c925c2c9 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5572.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5572.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5572.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5572.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 20:25:54.726: INFO: DNS probes using dns-test-f8c3e602-7d45-42da-beb6-d3ad8ae5bb2f succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:25:54.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5572" for this suite.

• [SLOW TEST:41.255 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":189,"skipped":3472,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:25:54.895: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5130
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 20:25:55.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a9162a0f-ca19-40e2-a700-b00eed53548a" in namespace "downward-api-5130" to be "Succeeded or Failed"
Mar  2 20:25:55.165: INFO: Pod "downwardapi-volume-a9162a0f-ca19-40e2-a700-b00eed53548a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.0921ms
Mar  2 20:25:57.179: INFO: Pod "downwardapi-volume-a9162a0f-ca19-40e2-a700-b00eed53548a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027346357s
STEP: Saw pod success
Mar  2 20:25:57.179: INFO: Pod "downwardapi-volume-a9162a0f-ca19-40e2-a700-b00eed53548a" satisfied condition "Succeeded or Failed"
Mar  2 20:25:57.192: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-a9162a0f-ca19-40e2-a700-b00eed53548a container client-container: <nil>
STEP: delete the pod
Mar  2 20:25:57.317: INFO: Waiting for pod downwardapi-volume-a9162a0f-ca19-40e2-a700-b00eed53548a to disappear
Mar  2 20:25:57.337: INFO: Pod downwardapi-volume-a9162a0f-ca19-40e2-a700-b00eed53548a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:25:57.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5130" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":190,"skipped":3486,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:25:57.374: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar  2 20:25:57.675: INFO: Waiting up to 5m0s for pod "downward-api-0f87b76c-f68a-46af-9dd4-b755a9783fdd" in namespace "downward-api-4836" to be "Succeeded or Failed"
Mar  2 20:25:57.687: INFO: Pod "downward-api-0f87b76c-f68a-46af-9dd4-b755a9783fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.394607ms
Mar  2 20:25:59.731: INFO: Pod "downward-api-0f87b76c-f68a-46af-9dd4-b755a9783fdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.056881369s
STEP: Saw pod success
Mar  2 20:25:59.731: INFO: Pod "downward-api-0f87b76c-f68a-46af-9dd4-b755a9783fdd" satisfied condition "Succeeded or Failed"
Mar  2 20:25:59.744: INFO: Trying to get logs from node 10.245.0.4 pod downward-api-0f87b76c-f68a-46af-9dd4-b755a9783fdd container dapi-container: <nil>
STEP: delete the pod
Mar  2 20:25:59.801: INFO: Waiting for pod downward-api-0f87b76c-f68a-46af-9dd4-b755a9783fdd to disappear
Mar  2 20:25:59.813: INFO: Pod downward-api-0f87b76c-f68a-46af-9dd4-b755a9783fdd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:25:59.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4836" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3499,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:25:59.849: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar  2 20:26:00.148: INFO: The status of Pod pod-update-activedeadlineseconds-be66c3f9-ef9f-4803-a254-59ac5b0ac3d7 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:26:02.166: INFO: The status of Pod pod-update-activedeadlineseconds-be66c3f9-ef9f-4803-a254-59ac5b0ac3d7 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:26:04.176: INFO: The status of Pod pod-update-activedeadlineseconds-be66c3f9-ef9f-4803-a254-59ac5b0ac3d7 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  2 20:26:04.733: INFO: Successfully updated pod "pod-update-activedeadlineseconds-be66c3f9-ef9f-4803-a254-59ac5b0ac3d7"
Mar  2 20:26:04.733: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-be66c3f9-ef9f-4803-a254-59ac5b0ac3d7" in namespace "pods-2344" to be "terminated due to deadline exceeded"
Mar  2 20:26:04.747: INFO: Pod "pod-update-activedeadlineseconds-be66c3f9-ef9f-4803-a254-59ac5b0ac3d7": Phase="Running", Reason="", readiness=true. Elapsed: 14.313236ms
Mar  2 20:26:06.764: INFO: Pod "pod-update-activedeadlineseconds-be66c3f9-ef9f-4803-a254-59ac5b0ac3d7": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 2.030753528s
Mar  2 20:26:06.764: INFO: Pod "pod-update-activedeadlineseconds-be66c3f9-ef9f-4803-a254-59ac5b0ac3d7" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:26:06.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2344" for this suite.

• [SLOW TEST:6.954 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":192,"skipped":3505,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:26:06.806: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2812
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-cf6666f3-d483-4757-9aeb-c5112abb19d4
STEP: Creating a pod to test consume configMaps
Mar  2 20:26:07.134: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8f7d4ad-a254-43cd-a8b0-e88169ab97fd" in namespace "projected-2812" to be "Succeeded or Failed"
Mar  2 20:26:07.178: INFO: Pod "pod-projected-configmaps-c8f7d4ad-a254-43cd-a8b0-e88169ab97fd": Phase="Pending", Reason="", readiness=false. Elapsed: 43.52099ms
Mar  2 20:26:09.214: INFO: Pod "pod-projected-configmaps-c8f7d4ad-a254-43cd-a8b0-e88169ab97fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080192405s
Mar  2 20:26:11.232: INFO: Pod "pod-projected-configmaps-c8f7d4ad-a254-43cd-a8b0-e88169ab97fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.097417903s
STEP: Saw pod success
Mar  2 20:26:11.232: INFO: Pod "pod-projected-configmaps-c8f7d4ad-a254-43cd-a8b0-e88169ab97fd" satisfied condition "Succeeded or Failed"
Mar  2 20:26:11.243: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-configmaps-c8f7d4ad-a254-43cd-a8b0-e88169ab97fd container agnhost-container: <nil>
STEP: delete the pod
Mar  2 20:26:11.308: INFO: Waiting for pod pod-projected-configmaps-c8f7d4ad-a254-43cd-a8b0-e88169ab97fd to disappear
Mar  2 20:26:11.320: INFO: Pod pod-projected-configmaps-c8f7d4ad-a254-43cd-a8b0-e88169ab97fd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:26:11.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2812" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":193,"skipped":3524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:26:11.361: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6200
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-3a7ab78f-b4f1-4d0c-bce0-e253ed744115
STEP: Creating a pod to test consume secrets
Mar  2 20:26:11.650: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-066a8667-b7f6-45df-adfd-3217e61c7f37" in namespace "projected-6200" to be "Succeeded or Failed"
Mar  2 20:26:11.661: INFO: Pod "pod-projected-secrets-066a8667-b7f6-45df-adfd-3217e61c7f37": Phase="Pending", Reason="", readiness=false. Elapsed: 11.691877ms
Mar  2 20:26:13.682: INFO: Pod "pod-projected-secrets-066a8667-b7f6-45df-adfd-3217e61c7f37": Phase="Running", Reason="", readiness=true. Elapsed: 2.032450799s
Mar  2 20:26:15.703: INFO: Pod "pod-projected-secrets-066a8667-b7f6-45df-adfd-3217e61c7f37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053796684s
STEP: Saw pod success
Mar  2 20:26:15.703: INFO: Pod "pod-projected-secrets-066a8667-b7f6-45df-adfd-3217e61c7f37" satisfied condition "Succeeded or Failed"
Mar  2 20:26:15.716: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-secrets-066a8667-b7f6-45df-adfd-3217e61c7f37 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 20:26:15.798: INFO: Waiting for pod pod-projected-secrets-066a8667-b7f6-45df-adfd-3217e61c7f37 to disappear
Mar  2 20:26:15.811: INFO: Pod pod-projected-secrets-066a8667-b7f6-45df-adfd-3217e61c7f37 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:26:15.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6200" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":194,"skipped":3564,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:26:15.857: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5247
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar  2 20:26:16.178: INFO: Waiting up to 5m0s for pod "pod-5a3e2858-1b7c-4b62-bd4e-ff3a26d6bfb1" in namespace "emptydir-5247" to be "Succeeded or Failed"
Mar  2 20:26:16.192: INFO: Pod "pod-5a3e2858-1b7c-4b62-bd4e-ff3a26d6bfb1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.18019ms
Mar  2 20:26:18.209: INFO: Pod "pod-5a3e2858-1b7c-4b62-bd4e-ff3a26d6bfb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030296914s
Mar  2 20:26:20.233: INFO: Pod "pod-5a3e2858-1b7c-4b62-bd4e-ff3a26d6bfb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055128692s
STEP: Saw pod success
Mar  2 20:26:20.233: INFO: Pod "pod-5a3e2858-1b7c-4b62-bd4e-ff3a26d6bfb1" satisfied condition "Succeeded or Failed"
Mar  2 20:26:20.246: INFO: Trying to get logs from node 10.245.0.4 pod pod-5a3e2858-1b7c-4b62-bd4e-ff3a26d6bfb1 container test-container: <nil>
STEP: delete the pod
Mar  2 20:26:20.304: INFO: Waiting for pod pod-5a3e2858-1b7c-4b62-bd4e-ff3a26d6bfb1 to disappear
Mar  2 20:26:20.317: INFO: Pod pod-5a3e2858-1b7c-4b62-bd4e-ff3a26d6bfb1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:26:20.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5247" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":195,"skipped":3574,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:26:20.353: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1612
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Mar  2 20:26:20.629: INFO: Waiting up to 5m0s for pod "var-expansion-3844c1f4-33c6-462a-89dd-102f01bc65f9" in namespace "var-expansion-1612" to be "Succeeded or Failed"
Mar  2 20:26:20.644: INFO: Pod "var-expansion-3844c1f4-33c6-462a-89dd-102f01bc65f9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.108024ms
Mar  2 20:26:22.660: INFO: Pod "var-expansion-3844c1f4-33c6-462a-89dd-102f01bc65f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031041637s
STEP: Saw pod success
Mar  2 20:26:22.660: INFO: Pod "var-expansion-3844c1f4-33c6-462a-89dd-102f01bc65f9" satisfied condition "Succeeded or Failed"
Mar  2 20:26:22.674: INFO: Trying to get logs from node 10.245.0.4 pod var-expansion-3844c1f4-33c6-462a-89dd-102f01bc65f9 container dapi-container: <nil>
STEP: delete the pod
Mar  2 20:26:22.732: INFO: Waiting for pod var-expansion-3844c1f4-33c6-462a-89dd-102f01bc65f9 to disappear
Mar  2 20:26:22.744: INFO: Pod var-expansion-3844c1f4-33c6-462a-89dd-102f01bc65f9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:26:22.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1612" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":196,"skipped":3621,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:26:22.784: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6065
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-efbeb4ff-01a2-4292-9c6a-692fdd8bd79c in namespace container-probe-6065
Mar  2 20:26:27.103: INFO: Started pod busybox-efbeb4ff-01a2-4292-9c6a-692fdd8bd79c in namespace container-probe-6065
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 20:26:27.116: INFO: Initial restart count of pod busybox-efbeb4ff-01a2-4292-9c6a-692fdd8bd79c is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:30:27.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6065" for this suite.

• [SLOW TEST:245.017 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":197,"skipped":3633,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:30:27.801: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Mar  2 20:30:28.128: INFO: The status of Pod pod-hostip-7b967faf-9d58-4b8b-b35d-f97cba9f3e23 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:30:30.142: INFO: The status of Pod pod-hostip-7b967faf-9d58-4b8b-b35d-f97cba9f3e23 is Running (Ready = true)
Mar  2 20:30:30.171: INFO: Pod pod-hostip-7b967faf-9d58-4b8b-b35d-f97cba9f3e23 has hostIP: 10.245.0.4
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:30:30.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-406" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":198,"skipped":3660,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:30:30.215: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1291
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-4m4s
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 20:30:30.507: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4m4s" in namespace "subpath-1291" to be "Succeeded or Failed"
Mar  2 20:30:30.521: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Pending", Reason="", readiness=false. Elapsed: 13.895767ms
Mar  2 20:30:32.539: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Running", Reason="", readiness=true. Elapsed: 2.031871719s
Mar  2 20:30:34.559: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Running", Reason="", readiness=true. Elapsed: 4.05180868s
Mar  2 20:30:36.576: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Running", Reason="", readiness=true. Elapsed: 6.069554754s
Mar  2 20:30:38.596: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Running", Reason="", readiness=true. Elapsed: 8.08904262s
Mar  2 20:30:40.666: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Running", Reason="", readiness=true. Elapsed: 10.158818795s
Mar  2 20:30:42.691: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Running", Reason="", readiness=true. Elapsed: 12.184287045s
Mar  2 20:30:44.714: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Running", Reason="", readiness=true. Elapsed: 14.206857209s
Mar  2 20:30:46.732: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Running", Reason="", readiness=true. Elapsed: 16.225102061s
Mar  2 20:30:48.752: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Running", Reason="", readiness=true. Elapsed: 18.245472188s
Mar  2 20:30:50.771: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Running", Reason="", readiness=true. Elapsed: 20.26403918s
Mar  2 20:30:52.790: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Running", Reason="", readiness=true. Elapsed: 22.283073757s
Mar  2 20:30:54.836: INFO: Pod "pod-subpath-test-configmap-4m4s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.329620212s
STEP: Saw pod success
Mar  2 20:30:54.837: INFO: Pod "pod-subpath-test-configmap-4m4s" satisfied condition "Succeeded or Failed"
Mar  2 20:30:54.851: INFO: Trying to get logs from node 10.245.0.4 pod pod-subpath-test-configmap-4m4s container test-container-subpath-configmap-4m4s: <nil>
STEP: delete the pod
Mar  2 20:30:55.023: INFO: Waiting for pod pod-subpath-test-configmap-4m4s to disappear
Mar  2 20:30:55.036: INFO: Pod pod-subpath-test-configmap-4m4s no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4m4s
Mar  2 20:30:55.036: INFO: Deleting pod "pod-subpath-test-configmap-4m4s" in namespace "subpath-1291"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:30:55.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1291" for this suite.

• [SLOW TEST:24.874 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":199,"skipped":3691,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:30:55.092: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 20:30:55.357: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7fedae13-b35c-4eb4-b4b4-589c6e022bd5" in namespace "downward-api-4141" to be "Succeeded or Failed"
Mar  2 20:30:55.373: INFO: Pod "downwardapi-volume-7fedae13-b35c-4eb4-b4b4-589c6e022bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 15.447852ms
Mar  2 20:30:57.391: INFO: Pod "downwardapi-volume-7fedae13-b35c-4eb4-b4b4-589c6e022bd5": Phase="Running", Reason="", readiness=true. Elapsed: 2.033411784s
Mar  2 20:30:59.410: INFO: Pod "downwardapi-volume-7fedae13-b35c-4eb4-b4b4-589c6e022bd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052614024s
STEP: Saw pod success
Mar  2 20:30:59.410: INFO: Pod "downwardapi-volume-7fedae13-b35c-4eb4-b4b4-589c6e022bd5" satisfied condition "Succeeded or Failed"
Mar  2 20:30:59.421: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-7fedae13-b35c-4eb4-b4b4-589c6e022bd5 container client-container: <nil>
STEP: delete the pod
Mar  2 20:30:59.484: INFO: Waiting for pod downwardapi-volume-7fedae13-b35c-4eb4-b4b4-589c6e022bd5 to disappear
Mar  2 20:30:59.496: INFO: Pod downwardapi-volume-7fedae13-b35c-4eb4-b4b4-589c6e022bd5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:30:59.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4141" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3705,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:30:59.532: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:30:59.802: INFO: Got root ca configmap in namespace "svcaccounts-6810"
Mar  2 20:30:59.821: INFO: Deleted root ca configmap in namespace "svcaccounts-6810"
STEP: waiting for a new root ca configmap created
Mar  2 20:31:00.333: INFO: Recreated root ca configmap in namespace "svcaccounts-6810"
Mar  2 20:31:00.346: INFO: Updated root ca configmap in namespace "svcaccounts-6810"
STEP: waiting for the root ca configmap reconciled
Mar  2 20:31:00.860: INFO: Reconciled root ca configmap in namespace "svcaccounts-6810"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:31:00.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6810" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":201,"skipped":3727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:31:00.896: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1963
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-bb624acd-fcb9-4fec-bd74-606ef8a4340d
STEP: Creating a pod to test consume configMaps
Mar  2 20:31:01.168: INFO: Waiting up to 5m0s for pod "pod-configmaps-f0a27d37-49e4-46cd-90ea-a413c325c6f3" in namespace "configmap-1963" to be "Succeeded or Failed"
Mar  2 20:31:01.180: INFO: Pod "pod-configmaps-f0a27d37-49e4-46cd-90ea-a413c325c6f3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.722353ms
Mar  2 20:31:03.218: INFO: Pod "pod-configmaps-f0a27d37-49e4-46cd-90ea-a413c325c6f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.049980284s
STEP: Saw pod success
Mar  2 20:31:03.218: INFO: Pod "pod-configmaps-f0a27d37-49e4-46cd-90ea-a413c325c6f3" satisfied condition "Succeeded or Failed"
Mar  2 20:31:03.249: INFO: Trying to get logs from node 10.245.0.4 pod pod-configmaps-f0a27d37-49e4-46cd-90ea-a413c325c6f3 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 20:31:03.386: INFO: Waiting for pod pod-configmaps-f0a27d37-49e4-46cd-90ea-a413c325c6f3 to disappear
Mar  2 20:31:03.422: INFO: Pod pod-configmaps-f0a27d37-49e4-46cd-90ea-a413c325c6f3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:31:03.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1963" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":202,"skipped":3764,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:31:03.475: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-1332
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Mar  2 20:31:03.719: INFO: Major version: 1
STEP: Confirm minor version
Mar  2 20:31:03.719: INFO: cleanMinorVersion: 22
Mar  2 20:31:03.719: INFO: Minor version: 22
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:31:03.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1332" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":203,"skipped":3780,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:31:03.757: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7429
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar  2 20:31:03.991: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar  2 20:31:18.994: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 20:31:22.452: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:31:39.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7429" for this suite.

• [SLOW TEST:35.919 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":204,"skipped":3786,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:31:39.676: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 20:31:39.990: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c989c28e-bcd8-4760-832e-88edf6ef087f" in namespace "projected-6432" to be "Succeeded or Failed"
Mar  2 20:31:40.002: INFO: Pod "downwardapi-volume-c989c28e-bcd8-4760-832e-88edf6ef087f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.284288ms
Mar  2 20:31:42.021: INFO: Pod "downwardapi-volume-c989c28e-bcd8-4760-832e-88edf6ef087f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031458328s
STEP: Saw pod success
Mar  2 20:31:42.021: INFO: Pod "downwardapi-volume-c989c28e-bcd8-4760-832e-88edf6ef087f" satisfied condition "Succeeded or Failed"
Mar  2 20:31:42.032: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-c989c28e-bcd8-4760-832e-88edf6ef087f container client-container: <nil>
STEP: delete the pod
Mar  2 20:31:42.098: INFO: Waiting for pod downwardapi-volume-c989c28e-bcd8-4760-832e-88edf6ef087f to disappear
Mar  2 20:31:42.109: INFO: Pod downwardapi-volume-c989c28e-bcd8-4760-832e-88edf6ef087f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:31:42.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6432" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":205,"skipped":3796,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:31:42.191: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-870
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Mar  2 20:31:42.461: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:32:04.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-870" for this suite.

• [SLOW TEST:21.953 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":206,"skipped":3797,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:32:04.144: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5874
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:32:09.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5874" for this suite.

• [SLOW TEST:5.525 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":207,"skipped":3798,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:32:09.670: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5066
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-ls8v
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 20:32:09.985: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ls8v" in namespace "subpath-5066" to be "Succeeded or Failed"
Mar  2 20:32:09.996: INFO: Pod "pod-subpath-test-secret-ls8v": Phase="Pending", Reason="", readiness=false. Elapsed: 11.34288ms
Mar  2 20:32:12.008: INFO: Pod "pod-subpath-test-secret-ls8v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022950257s
Mar  2 20:32:14.027: INFO: Pod "pod-subpath-test-secret-ls8v": Phase="Running", Reason="", readiness=true. Elapsed: 4.041960717s
Mar  2 20:32:16.043: INFO: Pod "pod-subpath-test-secret-ls8v": Phase="Running", Reason="", readiness=true. Elapsed: 6.058496585s
Mar  2 20:32:18.065: INFO: Pod "pod-subpath-test-secret-ls8v": Phase="Running", Reason="", readiness=true. Elapsed: 8.079773962s
Mar  2 20:32:20.089: INFO: Pod "pod-subpath-test-secret-ls8v": Phase="Running", Reason="", readiness=true. Elapsed: 10.103863167s
Mar  2 20:32:22.103: INFO: Pod "pod-subpath-test-secret-ls8v": Phase="Running", Reason="", readiness=true. Elapsed: 12.118457894s
Mar  2 20:32:24.123: INFO: Pod "pod-subpath-test-secret-ls8v": Phase="Running", Reason="", readiness=true. Elapsed: 14.13802482s
Mar  2 20:32:26.146: INFO: Pod "pod-subpath-test-secret-ls8v": Phase="Running", Reason="", readiness=true. Elapsed: 16.160936198s
Mar  2 20:32:28.172: INFO: Pod "pod-subpath-test-secret-ls8v": Phase="Running", Reason="", readiness=true. Elapsed: 18.186969683s
Mar  2 20:32:30.193: INFO: Pod "pod-subpath-test-secret-ls8v": Phase="Running", Reason="", readiness=true. Elapsed: 20.208549216s
Mar  2 20:32:32.221: INFO: Pod "pod-subpath-test-secret-ls8v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.236196177s
STEP: Saw pod success
Mar  2 20:32:32.221: INFO: Pod "pod-subpath-test-secret-ls8v" satisfied condition "Succeeded or Failed"
Mar  2 20:32:32.232: INFO: Trying to get logs from node 10.245.0.4 pod pod-subpath-test-secret-ls8v container test-container-subpath-secret-ls8v: <nil>
STEP: delete the pod
Mar  2 20:32:32.376: INFO: Waiting for pod pod-subpath-test-secret-ls8v to disappear
Mar  2 20:32:32.385: INFO: Pod pod-subpath-test-secret-ls8v no longer exists
STEP: Deleting pod pod-subpath-test-secret-ls8v
Mar  2 20:32:32.386: INFO: Deleting pod "pod-subpath-test-secret-ls8v" in namespace "subpath-5066"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:32:32.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5066" for this suite.

• [SLOW TEST:22.764 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":208,"skipped":3802,"failed":0}
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:32:32.435: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6099
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 20:32:35.803: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:32:35.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6099" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":209,"skipped":3803,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:32:35.896: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:32:36.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-1929 create -f -'
Mar  2 20:32:37.044: INFO: stderr: ""
Mar  2 20:32:37.044: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar  2 20:32:37.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-1929 create -f -'
Mar  2 20:32:37.269: INFO: stderr: ""
Mar  2 20:32:37.269: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar  2 20:32:38.288: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 20:32:38.288: INFO: Found 0 / 1
Mar  2 20:32:39.288: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 20:32:39.288: INFO: Found 1 / 1
Mar  2 20:32:39.288: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 20:32:39.322: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 20:32:39.322: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 20:32:39.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-1929 describe pod agnhost-primary-rtl49'
Mar  2 20:32:39.468: INFO: stderr: ""
Mar  2 20:32:39.468: INFO: stdout: "Name:         agnhost-primary-rtl49\nNamespace:    kubectl-1929\nPriority:     0\nNode:         10.245.0.4/10.245.0.4\nStart Time:   Wed, 02 Mar 2022 20:32:37 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: cb6e286bd6dc5e875f7416ba0d5440ecdff012674d73724d5f999a790a283f7c\n              cni.projectcalico.org/podIP: 172.17.100.181/32\n              cni.projectcalico.org/podIPs: 172.17.100.181/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.17.100.181\nIPs:\n  IP:           172.17.100.181\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6db8c3d432383a14d15bb27c69996e437b6c621656e601f2b6b5dbeffa4fb4cc\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 02 Mar 2022 20:32:38 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cnj4v (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-cnj4v:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1929/agnhost-primary-rtl49 to 10.245.0.4\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Mar  2 20:32:39.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-1929 describe rc agnhost-primary'
Mar  2 20:32:39.588: INFO: stderr: ""
Mar  2 20:32:39.588: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1929\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-rtl49\n"
Mar  2 20:32:39.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-1929 describe service agnhost-primary'
Mar  2 20:32:39.695: INFO: stderr: ""
Mar  2 20:32:39.695: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1929\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                172.21.198.143\nIPs:               172.21.198.143\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.17.100.181:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  2 20:32:39.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-1929 describe node 10.245.0.4'
Mar  2 20:32:39.909: INFO: stderr: ""
Mar  2 20:32:39.909: INFO: stdout: "Name:               10.245.0.4\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=cx2.2x4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=au-syd\n                    failure-domain.beta.kubernetes.io/zone=au-syd-1\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=g2\n                    ibm-cloud.kubernetes.io/instance-id=02h7_b0eb8b1c-dbe0-486d-a572-b806cd8c5a94\n                    ibm-cloud.kubernetes.io/internal-ip=10.245.0.4\n                    ibm-cloud.kubernetes.io/machine-type=cx2.2x4\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=au-syd\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/subnet-id=02h7-85dea4ef-860b-45cf-aedc-bf6160bac07a\n                    ibm-cloud.kubernetes.io/worker-id=kube-c8fqf88s0ubnto7pbi70-kubee2epvgf-default-0000023f\n                    ibm-cloud.kubernetes.io/worker-pool-id=c8fqf88s0ubnto7pbi70-57b6434\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.22.7_1542\n                    ibm-cloud.kubernetes.io/zone=au-syd-1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.245.0.4\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=cx2.2x4\n                    topology.kubernetes.io/region=au-syd\n                    topology.kubernetes.io/zone=au-syd-1\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"vpc.block.csi.ibm.io\":\"kube-c8fqf88s0ubnto7pbi70-kubee2epvgf-default-0000023f\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.245.0.4/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.17.100.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 02 Mar 2022 17:40:06 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.245.0.4\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 02 Mar 2022 20:32:37 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 02 Mar 2022 17:40:39 +0000   Wed, 02 Mar 2022 17:40:39 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 02 Mar 2022 20:29:25 +0000   Wed, 02 Mar 2022 17:40:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 02 Mar 2022 20:29:25 +0000   Wed, 02 Mar 2022 17:40:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 02 Mar 2022 20:29:25 +0000   Wed, 02 Mar 2022 17:40:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 02 Mar 2022 20:29:25 +0000   Wed, 02 Mar 2022 17:40:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.245.0.4\n  ExternalIP:  10.245.0.4\n  Hostname:    10.245.0.4\nCapacity:\n  cpu:                  2\n  ephemeral-storage:    102821812Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               3842424Ki\n  pods:                 110\nAllocatable:\n  cpu:                  1920m\n  ephemeral-storage:    94234134186\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               2779512Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 b0eb8b1cdbe0486da572b806cd8c5a94\n  System UUID:                B0EB8B1C-DBE0-486D-A572-B806CD8C5A94\n  Boot ID:                    40eba8a7-d171-4bc2-9edb-5541de980cb0\n  Kernel Version:             4.15.0-169-generic\n  OS Image:                   Ubuntu 18.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.9\n  Kubelet Version:            v1.22.7+IKS\n  Kube-Proxy Version:         v1.22.7+IKS\nProviderID:                   ibm://68010fd8df4f467681ddec1e065d7a48///c8fqf88s0ubnto7pbi70/kube-c8fqf88s0ubnto7pbi70-kubee2epvgf-default-0000023f\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-6bmpc                                          250m (13%)    0 (0%)      80Mi (2%)        0 (0%)         172m\n  kube-system                 calico-typha-7cdb864b94-kg6vk                              250m (13%)    0 (0%)      80Mi (2%)        0 (0%)         32m\n  kube-system                 ibm-master-proxy-static-10.245.0.4                         25m (1%)      300m (15%)  32M (1%)         512M (17%)     171m\n  kube-system                 ibm-vpc-block-csi-node-m9dxv                               48m (2%)      192m (10%)  110Mi (4%)       440Mi (16%)    172m\n  kube-system                 konnectivity-agent-cnbg9                                   10m (0%)      0 (0%)      10Mi (0%)        50Mi (1%)      164m\n  kubectl-1929                agnhost-primary-rtl49                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         71m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-867x2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         71m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests        Limits\n  --------             --------        ------\n  cpu                  583m (30%)      492m (25%)\n  memory               317970Ki (11%)  1025802240 (36%)\n  ephemeral-storage    0 (0%)          0 (0%)\n  hugepages-1Gi        0 (0%)          0 (0%)\n  hugepages-2Mi        0 (0%)          0 (0%)\n  example.com/fakecpu  0               0\nEvents:                <none>\n"
Mar  2 20:32:39.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-1929 describe namespace kubectl-1929'
Mar  2 20:32:40.021: INFO: stderr: ""
Mar  2 20:32:40.021: INFO: stdout: "Name:         kubectl-1929\nLabels:       e2e-framework=kubectl\n              e2e-run=2ade9e29-0f64-4727-8ad9-030358750b63\n              kubernetes.io/metadata.name=kubectl-1929\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:32:40.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1929" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":210,"skipped":3816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:32:40.106: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:32:40.563: INFO: Create a RollingUpdate DaemonSet
Mar  2 20:32:40.597: INFO: Check that daemon pods launch on every node of the cluster
Mar  2 20:32:40.653: INFO: Number of nodes with available pods: 0
Mar  2 20:32:40.653: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:32:41.686: INFO: Number of nodes with available pods: 0
Mar  2 20:32:41.686: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:32:42.689: INFO: Number of nodes with available pods: 2
Mar  2 20:32:42.689: INFO: Node 10.245.0.5 is running more than one daemon pod
Mar  2 20:32:43.686: INFO: Number of nodes with available pods: 3
Mar  2 20:32:43.686: INFO: Number of running nodes: 3, number of available pods: 3
Mar  2 20:32:43.686: INFO: Update the DaemonSet to trigger a rollout
Mar  2 20:32:43.716: INFO: Updating DaemonSet daemon-set
Mar  2 20:32:45.794: INFO: Roll back the DaemonSet before rollout is complete
Mar  2 20:32:45.823: INFO: Updating DaemonSet daemon-set
Mar  2 20:32:45.823: INFO: Make sure DaemonSet rollback is complete
Mar  2 20:32:45.839: INFO: Wrong image for pod: daemon-set-qvmtz. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Mar  2 20:32:45.840: INFO: Pod daemon-set-qvmtz is not available
Mar  2 20:32:52.875: INFO: Pod daemon-set-5zqcw is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7767, will wait for the garbage collector to delete the pods
Mar  2 20:32:53.058: INFO: Deleting DaemonSet.extensions daemon-set took: 22.752629ms
Mar  2 20:32:53.258: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.328236ms
Mar  2 20:32:55.280: INFO: Number of nodes with available pods: 0
Mar  2 20:32:55.280: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 20:32:55.291: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36759"},"items":null}

Mar  2 20:32:55.302: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36759"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:32:55.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7767" for this suite.

• [SLOW TEST:15.330 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":211,"skipped":3843,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:32:55.436: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-9345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:32:59.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9345" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":212,"skipped":3862,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:32:59.849: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7804
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:33:00.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7804" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":213,"skipped":3871,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:33:00.281: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7734
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-537e7a36-0bb3-4b19-aa0c-a3afd5496fe4
STEP: Creating a pod to test consume secrets
Mar  2 20:33:00.723: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-55438174-fa7b-4a57-9d2e-f27ce3c11c15" in namespace "projected-7734" to be "Succeeded or Failed"
Mar  2 20:33:00.762: INFO: Pod "pod-projected-secrets-55438174-fa7b-4a57-9d2e-f27ce3c11c15": Phase="Pending", Reason="", readiness=false. Elapsed: 39.24229ms
Mar  2 20:33:02.789: INFO: Pod "pod-projected-secrets-55438174-fa7b-4a57-9d2e-f27ce3c11c15": Phase="Running", Reason="", readiness=true. Elapsed: 2.065802369s
Mar  2 20:33:04.820: INFO: Pod "pod-projected-secrets-55438174-fa7b-4a57-9d2e-f27ce3c11c15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.096962618s
STEP: Saw pod success
Mar  2 20:33:04.820: INFO: Pod "pod-projected-secrets-55438174-fa7b-4a57-9d2e-f27ce3c11c15" satisfied condition "Succeeded or Failed"
Mar  2 20:33:04.842: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-secrets-55438174-fa7b-4a57-9d2e-f27ce3c11c15 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 20:33:04.980: INFO: Waiting for pod pod-projected-secrets-55438174-fa7b-4a57-9d2e-f27ce3c11c15 to disappear
Mar  2 20:33:04.992: INFO: Pod pod-projected-secrets-55438174-fa7b-4a57-9d2e-f27ce3c11c15 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:33:04.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7734" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":3937,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:33:05.065: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 20:33:05.897: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 20:33:07.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781849985, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781849985, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781849985, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781849985, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 20:33:10.997: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:33:11.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5667" for this suite.
STEP: Destroying namespace "webhook-5667-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.380 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":215,"skipped":3937,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:33:11.446: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4453
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar  2 20:33:11.761: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 20:33:16.638: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:33:34.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4453" for this suite.

• [SLOW TEST:22.908 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":216,"skipped":3972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:33:34.354: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-9127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Mar  2 20:33:34.736: INFO: created test-podtemplate-1
Mar  2 20:33:34.749: INFO: created test-podtemplate-2
Mar  2 20:33:34.763: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Mar  2 20:33:34.775: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Mar  2 20:33:34.853: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:33:34.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9127" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":217,"skipped":3995,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:33:35.001: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-967
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:33:39.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-967" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":218,"skipped":4005,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:33:39.596: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-786
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-kjnpm in namespace proxy-786
I0302 20:33:39.922395      21 runners.go:190] Created replication controller with name: proxy-service-kjnpm, namespace: proxy-786, replica count: 1
I0302 20:33:40.974052      21 runners.go:190] proxy-service-kjnpm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 20:33:41.975044      21 runners.go:190] proxy-service-kjnpm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 20:33:42.977789      21 runners.go:190] proxy-service-kjnpm Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 20:33:42.996: INFO: setup took 3.149226826s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  2 20:33:43.049: INFO: (0) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 52.957716ms)
Mar  2 20:33:43.058: INFO: (0) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 60.468083ms)
Mar  2 20:33:43.058: INFO: (0) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 60.599363ms)
Mar  2 20:33:43.066: INFO: (0) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 69.692892ms)
Mar  2 20:33:43.066: INFO: (0) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 68.850648ms)
Mar  2 20:33:43.067: INFO: (0) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 70.328273ms)
Mar  2 20:33:43.073: INFO: (0) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 76.235984ms)
Mar  2 20:33:43.073: INFO: (0) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 76.389704ms)
Mar  2 20:33:43.074: INFO: (0) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 77.718411ms)
Mar  2 20:33:43.074: INFO: (0) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 77.967002ms)
Mar  2 20:33:43.074: INFO: (0) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 77.39342ms)
Mar  2 20:33:43.076: INFO: (0) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 79.432594ms)
Mar  2 20:33:43.079: INFO: (0) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 82.409528ms)
Mar  2 20:33:43.084: INFO: (0) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 87.549829ms)
Mar  2 20:33:43.084: INFO: (0) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 86.572201ms)
Mar  2 20:33:43.121: INFO: (0) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 124.272401ms)
Mar  2 20:33:43.139: INFO: (1) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 17.935649ms)
Mar  2 20:33:43.144: INFO: (1) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 23.012571ms)
Mar  2 20:33:43.144: INFO: (1) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 23.535938ms)
Mar  2 20:33:43.145: INFO: (1) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 23.521199ms)
Mar  2 20:33:43.145: INFO: (1) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 23.951709ms)
Mar  2 20:33:43.145: INFO: (1) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 23.745647ms)
Mar  2 20:33:43.145: INFO: (1) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 24.34922ms)
Mar  2 20:33:43.145: INFO: (1) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 24.577145ms)
Mar  2 20:33:43.145: INFO: (1) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 24.83316ms)
Mar  2 20:33:43.147: INFO: (1) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 25.912578ms)
Mar  2 20:33:43.148: INFO: (1) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 26.526915ms)
Mar  2 20:33:43.148: INFO: (1) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 27.059601ms)
Mar  2 20:33:43.148: INFO: (1) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 27.013551ms)
Mar  2 20:33:43.150: INFO: (1) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 29.18924ms)
Mar  2 20:33:43.152: INFO: (1) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 30.870041ms)
Mar  2 20:33:43.152: INFO: (1) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 30.894017ms)
Mar  2 20:33:43.219: INFO: (2) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 67.106148ms)
Mar  2 20:33:43.219: INFO: (2) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 67.078594ms)
Mar  2 20:33:43.226: INFO: (2) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 74.265382ms)
Mar  2 20:33:43.226: INFO: (2) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 74.220379ms)
Mar  2 20:33:43.227: INFO: (2) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 74.71456ms)
Mar  2 20:33:43.227: INFO: (2) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 74.443642ms)
Mar  2 20:33:43.227: INFO: (2) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 74.752315ms)
Mar  2 20:33:43.227: INFO: (2) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 74.379267ms)
Mar  2 20:33:43.227: INFO: (2) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 74.603406ms)
Mar  2 20:33:43.227: INFO: (2) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 74.587735ms)
Mar  2 20:33:43.229: INFO: (2) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 76.420557ms)
Mar  2 20:33:43.265: INFO: (2) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 113.333924ms)
Mar  2 20:33:43.266: INFO: (2) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 113.866705ms)
Mar  2 20:33:43.266: INFO: (2) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 114.242081ms)
Mar  2 20:33:43.266: INFO: (2) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 114.347018ms)
Mar  2 20:33:43.267: INFO: (2) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 114.511288ms)
Mar  2 20:33:43.309: INFO: (3) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 42.17854ms)
Mar  2 20:33:43.310: INFO: (3) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 43.020724ms)
Mar  2 20:33:43.310: INFO: (3) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 43.644699ms)
Mar  2 20:33:43.310: INFO: (3) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 42.847886ms)
Mar  2 20:33:43.310: INFO: (3) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 43.104646ms)
Mar  2 20:33:43.311: INFO: (3) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 43.606991ms)
Mar  2 20:33:43.311: INFO: (3) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 43.616433ms)
Mar  2 20:33:43.311: INFO: (3) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 43.491227ms)
Mar  2 20:33:43.311: INFO: (3) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 44.561377ms)
Mar  2 20:33:43.311: INFO: (3) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 43.834516ms)
Mar  2 20:33:43.311: INFO: (3) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 44.447637ms)
Mar  2 20:33:43.315: INFO: (3) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 48.097759ms)
Mar  2 20:33:43.322: INFO: (3) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 55.076912ms)
Mar  2 20:33:43.322: INFO: (3) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 55.08219ms)
Mar  2 20:33:43.323: INFO: (3) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 55.055775ms)
Mar  2 20:33:43.323: INFO: (3) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 55.160349ms)
Mar  2 20:33:43.339: INFO: (4) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 16.086148ms)
Mar  2 20:33:43.346: INFO: (4) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 22.473811ms)
Mar  2 20:33:43.346: INFO: (4) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 22.845106ms)
Mar  2 20:33:43.347: INFO: (4) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 23.688611ms)
Mar  2 20:33:43.348: INFO: (4) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 24.456157ms)
Mar  2 20:33:43.348: INFO: (4) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 24.764405ms)
Mar  2 20:33:43.348: INFO: (4) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 25.029956ms)
Mar  2 20:33:43.351: INFO: (4) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 28.436291ms)
Mar  2 20:33:43.351: INFO: (4) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 28.287454ms)
Mar  2 20:33:43.351: INFO: (4) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 28.160519ms)
Mar  2 20:33:43.358: INFO: (4) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 34.80629ms)
Mar  2 20:33:43.358: INFO: (4) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 35.127451ms)
Mar  2 20:33:43.359: INFO: (4) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 35.430516ms)
Mar  2 20:33:43.381: INFO: (4) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 58.475893ms)
Mar  2 20:33:43.381: INFO: (4) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 58.194712ms)
Mar  2 20:33:43.383: INFO: (4) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 60.118929ms)
Mar  2 20:33:43.426: INFO: (5) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 42.357903ms)
Mar  2 20:33:43.426: INFO: (5) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 42.589329ms)
Mar  2 20:33:43.427: INFO: (5) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 43.093255ms)
Mar  2 20:33:43.427: INFO: (5) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 43.215777ms)
Mar  2 20:33:43.427: INFO: (5) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 43.371826ms)
Mar  2 20:33:43.427: INFO: (5) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 43.377754ms)
Mar  2 20:33:43.427: INFO: (5) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 43.854751ms)
Mar  2 20:33:43.427: INFO: (5) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 43.970375ms)
Mar  2 20:33:43.427: INFO: (5) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 43.648444ms)
Mar  2 20:33:43.427: INFO: (5) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 43.529267ms)
Mar  2 20:33:43.427: INFO: (5) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 43.64158ms)
Mar  2 20:33:43.427: INFO: (5) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 43.677391ms)
Mar  2 20:33:43.431: INFO: (5) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 47.545416ms)
Mar  2 20:33:43.439: INFO: (5) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 55.246012ms)
Mar  2 20:33:43.466: INFO: (5) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 82.667653ms)
Mar  2 20:33:43.466: INFO: (5) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 82.659398ms)
Mar  2 20:33:43.492: INFO: (6) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 24.640262ms)
Mar  2 20:33:43.492: INFO: (6) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 24.96655ms)
Mar  2 20:33:43.493: INFO: (6) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 25.830035ms)
Mar  2 20:33:43.493: INFO: (6) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 25.762085ms)
Mar  2 20:33:43.495: INFO: (6) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 28.695258ms)
Mar  2 20:33:43.496: INFO: (6) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 29.102051ms)
Mar  2 20:33:43.496: INFO: (6) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 29.879015ms)
Mar  2 20:33:43.512: INFO: (6) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 45.934539ms)
Mar  2 20:33:43.512: INFO: (6) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 45.575717ms)
Mar  2 20:33:43.513: INFO: (6) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 45.968244ms)
Mar  2 20:33:43.513: INFO: (6) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 45.654651ms)
Mar  2 20:33:43.513: INFO: (6) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 46.343854ms)
Mar  2 20:33:43.513: INFO: (6) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 46.208133ms)
Mar  2 20:33:43.513: INFO: (6) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 45.760013ms)
Mar  2 20:33:43.513: INFO: (6) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 45.710025ms)
Mar  2 20:33:43.513: INFO: (6) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 45.991173ms)
Mar  2 20:33:43.536: INFO: (7) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 22.462129ms)
Mar  2 20:33:43.537: INFO: (7) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 22.627335ms)
Mar  2 20:33:43.537: INFO: (7) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 22.522125ms)
Mar  2 20:33:43.538: INFO: (7) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 23.741088ms)
Mar  2 20:33:43.547: INFO: (7) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 33.905394ms)
Mar  2 20:33:43.547: INFO: (7) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 33.38378ms)
Mar  2 20:33:43.547: INFO: (7) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 33.126643ms)
Mar  2 20:33:43.549: INFO: (7) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 35.360115ms)
Mar  2 20:33:43.549: INFO: (7) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 36.450289ms)
Mar  2 20:33:43.550: INFO: (7) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 35.68617ms)
Mar  2 20:33:43.550: INFO: (7) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 35.981082ms)
Mar  2 20:33:43.550: INFO: (7) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 36.337321ms)
Mar  2 20:33:43.550: INFO: (7) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 36.141154ms)
Mar  2 20:33:43.550: INFO: (7) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 35.469337ms)
Mar  2 20:33:43.550: INFO: (7) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 37.012528ms)
Mar  2 20:33:43.550: INFO: (7) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 37.076919ms)
Mar  2 20:33:43.568: INFO: (8) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 17.350813ms)
Mar  2 20:33:43.577: INFO: (8) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 25.834887ms)
Mar  2 20:33:43.577: INFO: (8) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 25.667718ms)
Mar  2 20:33:43.577: INFO: (8) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 25.964088ms)
Mar  2 20:33:43.577: INFO: (8) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 25.98752ms)
Mar  2 20:33:43.577: INFO: (8) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 26.408624ms)
Mar  2 20:33:43.577: INFO: (8) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 26.351666ms)
Mar  2 20:33:43.578: INFO: (8) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 27.072164ms)
Mar  2 20:33:43.578: INFO: (8) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 27.84321ms)
Mar  2 20:33:43.579: INFO: (8) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 28.123932ms)
Mar  2 20:33:43.584: INFO: (8) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 33.136353ms)
Mar  2 20:33:43.585: INFO: (8) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 34.091537ms)
Mar  2 20:33:43.585: INFO: (8) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 34.077686ms)
Mar  2 20:33:43.586: INFO: (8) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 35.090202ms)
Mar  2 20:33:43.586: INFO: (8) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 34.859856ms)
Mar  2 20:33:43.586: INFO: (8) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 35.474958ms)
Mar  2 20:33:43.610: INFO: (9) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 22.807301ms)
Mar  2 20:33:43.620: INFO: (9) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 32.887705ms)
Mar  2 20:33:43.620: INFO: (9) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 32.937802ms)
Mar  2 20:33:43.620: INFO: (9) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 33.256576ms)
Mar  2 20:33:43.620: INFO: (9) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 33.343885ms)
Mar  2 20:33:43.620: INFO: (9) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 33.44891ms)
Mar  2 20:33:43.620: INFO: (9) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 33.233195ms)
Mar  2 20:33:43.620: INFO: (9) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 33.143103ms)
Mar  2 20:33:43.620: INFO: (9) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 33.458477ms)
Mar  2 20:33:43.620: INFO: (9) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 33.452213ms)
Mar  2 20:33:43.620: INFO: (9) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 33.637926ms)
Mar  2 20:33:43.620: INFO: (9) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 33.422115ms)
Mar  2 20:33:43.620: INFO: (9) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 33.542918ms)
Mar  2 20:33:43.633: INFO: (9) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 46.55169ms)
Mar  2 20:33:43.633: INFO: (9) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 46.381444ms)
Mar  2 20:33:43.633: INFO: (9) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 46.504246ms)
Mar  2 20:33:43.649: INFO: (10) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 15.65169ms)
Mar  2 20:33:43.655: INFO: (10) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 20.654552ms)
Mar  2 20:33:43.655: INFO: (10) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 21.096348ms)
Mar  2 20:33:43.657: INFO: (10) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 22.733453ms)
Mar  2 20:33:43.657: INFO: (10) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 22.731796ms)
Mar  2 20:33:43.657: INFO: (10) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 23.068758ms)
Mar  2 20:33:43.657: INFO: (10) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 23.347163ms)
Mar  2 20:33:43.657: INFO: (10) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 23.593152ms)
Mar  2 20:33:43.658: INFO: (10) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 23.937038ms)
Mar  2 20:33:43.658: INFO: (10) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 24.106148ms)
Mar  2 20:33:43.659: INFO: (10) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 25.159789ms)
Mar  2 20:33:43.665: INFO: (10) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 31.356042ms)
Mar  2 20:33:43.666: INFO: (10) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 31.578859ms)
Mar  2 20:33:43.666: INFO: (10) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 31.625004ms)
Mar  2 20:33:43.666: INFO: (10) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 31.866541ms)
Mar  2 20:33:43.666: INFO: (10) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 32.084888ms)
Mar  2 20:33:43.681: INFO: (11) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 15.247523ms)
Mar  2 20:33:43.688: INFO: (11) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 21.399465ms)
Mar  2 20:33:43.688: INFO: (11) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 20.981036ms)
Mar  2 20:33:43.689: INFO: (11) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 21.348527ms)
Mar  2 20:33:43.689: INFO: (11) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 21.151017ms)
Mar  2 20:33:43.689: INFO: (11) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 21.319236ms)
Mar  2 20:33:43.689: INFO: (11) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 21.854224ms)
Mar  2 20:33:43.689: INFO: (11) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 22.328089ms)
Mar  2 20:33:43.690: INFO: (11) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 23.268059ms)
Mar  2 20:33:43.690: INFO: (11) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 23.159321ms)
Mar  2 20:33:43.693: INFO: (11) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 26.054048ms)
Mar  2 20:33:43.697: INFO: (11) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 29.242795ms)
Mar  2 20:33:43.697: INFO: (11) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 30.208207ms)
Mar  2 20:33:43.697: INFO: (11) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 30.310879ms)
Mar  2 20:33:43.700: INFO: (11) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 33.08328ms)
Mar  2 20:33:43.700: INFO: (11) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 32.988535ms)
Mar  2 20:33:43.721: INFO: (12) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 21.000575ms)
Mar  2 20:33:43.723: INFO: (12) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 22.009456ms)
Mar  2 20:33:43.723: INFO: (12) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 22.841003ms)
Mar  2 20:33:43.724: INFO: (12) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 22.496658ms)
Mar  2 20:33:43.724: INFO: (12) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 22.639348ms)
Mar  2 20:33:43.724: INFO: (12) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 23.63399ms)
Mar  2 20:33:43.725: INFO: (12) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 24.83081ms)
Mar  2 20:33:43.726: INFO: (12) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 25.016542ms)
Mar  2 20:33:43.726: INFO: (12) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 25.303457ms)
Mar  2 20:33:43.726: INFO: (12) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 25.369918ms)
Mar  2 20:33:43.727: INFO: (12) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 25.95237ms)
Mar  2 20:33:43.731: INFO: (12) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 30.681366ms)
Mar  2 20:33:43.732: INFO: (12) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 31.128334ms)
Mar  2 20:33:43.733: INFO: (12) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 32.787501ms)
Mar  2 20:33:43.739: INFO: (12) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 38.54381ms)
Mar  2 20:33:43.739: INFO: (12) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 39.032906ms)
Mar  2 20:33:43.758: INFO: (13) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 18.544777ms)
Mar  2 20:33:43.764: INFO: (13) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 24.289947ms)
Mar  2 20:33:43.765: INFO: (13) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 25.371438ms)
Mar  2 20:33:43.765: INFO: (13) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 24.603824ms)
Mar  2 20:33:43.765: INFO: (13) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 25.129922ms)
Mar  2 20:33:43.766: INFO: (13) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 25.689593ms)
Mar  2 20:33:43.766: INFO: (13) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 25.338982ms)
Mar  2 20:33:43.766: INFO: (13) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 25.83111ms)
Mar  2 20:33:43.766: INFO: (13) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 25.890294ms)
Mar  2 20:33:43.767: INFO: (13) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 27.618446ms)
Mar  2 20:33:43.769: INFO: (13) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 29.936094ms)
Mar  2 20:33:43.773: INFO: (13) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 33.810664ms)
Mar  2 20:33:43.780: INFO: (13) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 40.207578ms)
Mar  2 20:33:43.781: INFO: (13) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 40.649199ms)
Mar  2 20:33:43.781: INFO: (13) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 41.032704ms)
Mar  2 20:33:43.781: INFO: (13) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 41.115806ms)
Mar  2 20:33:43.797: INFO: (14) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 15.729439ms)
Mar  2 20:33:43.804: INFO: (14) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 21.775334ms)
Mar  2 20:33:43.804: INFO: (14) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 23.016433ms)
Mar  2 20:33:43.804: INFO: (14) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 22.031906ms)
Mar  2 20:33:43.804: INFO: (14) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 22.910183ms)
Mar  2 20:33:43.805: INFO: (14) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 23.697806ms)
Mar  2 20:33:43.806: INFO: (14) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 24.462989ms)
Mar  2 20:33:43.807: INFO: (14) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 25.54421ms)
Mar  2 20:33:43.807: INFO: (14) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 25.924484ms)
Mar  2 20:33:43.807: INFO: (14) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 26.316031ms)
Mar  2 20:33:43.809: INFO: (14) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 27.098855ms)
Mar  2 20:33:43.813: INFO: (14) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 31.707494ms)
Mar  2 20:33:43.817: INFO: (14) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 34.884203ms)
Mar  2 20:33:43.817: INFO: (14) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 35.81375ms)
Mar  2 20:33:43.817: INFO: (14) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 35.247108ms)
Mar  2 20:33:43.817: INFO: (14) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 35.026917ms)
Mar  2 20:33:43.841: INFO: (15) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 24.567735ms)
Mar  2 20:33:43.885: INFO: (15) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 67.316593ms)
Mar  2 20:33:43.885: INFO: (15) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 67.794936ms)
Mar  2 20:33:43.885: INFO: (15) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 67.487591ms)
Mar  2 20:33:43.886: INFO: (15) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 67.719105ms)
Mar  2 20:33:43.886: INFO: (15) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 68.37938ms)
Mar  2 20:33:43.886: INFO: (15) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 68.302553ms)
Mar  2 20:33:43.886: INFO: (15) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 68.957088ms)
Mar  2 20:33:43.886: INFO: (15) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 69.204416ms)
Mar  2 20:33:43.887: INFO: (15) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 69.109641ms)
Mar  2 20:33:43.887: INFO: (15) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 69.633832ms)
Mar  2 20:33:43.886: INFO: (15) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 68.926623ms)
Mar  2 20:33:43.891: INFO: (15) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 73.135429ms)
Mar  2 20:33:43.908: INFO: (15) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 91.734494ms)
Mar  2 20:33:43.908: INFO: (15) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 90.953198ms)
Mar  2 20:33:43.909: INFO: (15) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 91.532292ms)
Mar  2 20:33:43.941: INFO: (16) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 31.265184ms)
Mar  2 20:33:43.941: INFO: (16) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 31.302624ms)
Mar  2 20:33:43.941: INFO: (16) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 31.421834ms)
Mar  2 20:33:43.941: INFO: (16) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 32.075445ms)
Mar  2 20:33:43.941: INFO: (16) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 31.516437ms)
Mar  2 20:33:43.941: INFO: (16) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 31.459371ms)
Mar  2 20:33:43.942: INFO: (16) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 32.841788ms)
Mar  2 20:33:43.942: INFO: (16) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 33.336439ms)
Mar  2 20:33:43.942: INFO: (16) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 33.012455ms)
Mar  2 20:33:43.942: INFO: (16) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 32.850023ms)
Mar  2 20:33:43.947: INFO: (16) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 37.988633ms)
Mar  2 20:33:43.951: INFO: (16) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 41.52914ms)
Mar  2 20:33:43.951: INFO: (16) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 41.456074ms)
Mar  2 20:33:43.951: INFO: (16) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 42.554458ms)
Mar  2 20:33:43.952: INFO: (16) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 42.384969ms)
Mar  2 20:33:43.952: INFO: (16) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 43.571884ms)
Mar  2 20:33:43.972: INFO: (17) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 20.176529ms)
Mar  2 20:33:43.978: INFO: (17) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 25.737336ms)
Mar  2 20:33:43.979: INFO: (17) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 26.234554ms)
Mar  2 20:33:43.979: INFO: (17) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 26.070551ms)
Mar  2 20:33:43.980: INFO: (17) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 27.301379ms)
Mar  2 20:33:43.980: INFO: (17) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 27.431699ms)
Mar  2 20:33:43.980: INFO: (17) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 27.202041ms)
Mar  2 20:33:43.981: INFO: (17) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 28.260199ms)
Mar  2 20:33:43.981: INFO: (17) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 28.863813ms)
Mar  2 20:33:43.982: INFO: (17) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 28.853497ms)
Mar  2 20:33:43.982: INFO: (17) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 29.217144ms)
Mar  2 20:33:43.989: INFO: (17) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 36.405183ms)
Mar  2 20:33:43.989: INFO: (17) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 36.406421ms)
Mar  2 20:33:43.989: INFO: (17) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 36.247377ms)
Mar  2 20:33:43.989: INFO: (17) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 36.524154ms)
Mar  2 20:33:43.989: INFO: (17) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 36.625444ms)
Mar  2 20:33:44.066: INFO: (18) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 76.907768ms)
Mar  2 20:33:44.066: INFO: (18) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 76.203563ms)
Mar  2 20:33:44.066: INFO: (18) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 76.731222ms)
Mar  2 20:33:44.066: INFO: (18) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 76.467282ms)
Mar  2 20:33:44.066: INFO: (18) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 76.29994ms)
Mar  2 20:33:44.066: INFO: (18) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 76.38922ms)
Mar  2 20:33:44.066: INFO: (18) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 76.946658ms)
Mar  2 20:33:44.066: INFO: (18) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 76.040256ms)
Mar  2 20:33:44.066: INFO: (18) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 76.624497ms)
Mar  2 20:33:44.066: INFO: (18) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 76.76925ms)
Mar  2 20:33:44.071: INFO: (18) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 80.617754ms)
Mar  2 20:33:44.078: INFO: (18) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 88.257428ms)
Mar  2 20:33:44.078: INFO: (18) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 88.077152ms)
Mar  2 20:33:44.078: INFO: (18) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 87.883797ms)
Mar  2 20:33:44.078: INFO: (18) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 87.674218ms)
Mar  2 20:33:44.078: INFO: (18) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 87.794518ms)
Mar  2 20:33:44.122: INFO: (19) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:443/proxy/tlsrewriteme... (200; 44.420991ms)
Mar  2 20:33:44.126: INFO: (19) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz/proxy/rewriteme">test</a> (200; 47.991724ms)
Mar  2 20:33:44.127: INFO: (19) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">test</... (200; 48.701748ms)
Mar  2 20:33:44.128: INFO: (19) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 49.597105ms)
Mar  2 20:33:44.128: INFO: (19) /api/v1/namespaces/proxy-786/pods/proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 50.102725ms)
Mar  2 20:33:44.128: INFO: (19) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:460/proxy/: tls baz (200; 49.342738ms)
Mar  2 20:33:44.133: INFO: (19) /api/v1/namespaces/proxy-786/pods/https:proxy-service-kjnpm-qd8cz:462/proxy/: tls qux (200; 53.674388ms)
Mar  2 20:33:44.133: INFO: (19) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:162/proxy/: bar (200; 53.971901ms)
Mar  2 20:33:44.133: INFO: (19) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:160/proxy/: foo (200; 54.178075ms)
Mar  2 20:33:44.133: INFO: (19) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname2/proxy/: tls qux (200; 53.792137ms)
Mar  2 20:33:44.133: INFO: (19) /api/v1/namespaces/proxy-786/services/https:proxy-service-kjnpm:tlsportname1/proxy/: tls baz (200; 54.267211ms)
Mar  2 20:33:44.133: INFO: (19) /api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/: <a href="/api/v1/namespaces/proxy-786/pods/http:proxy-service-kjnpm-qd8cz:1080/proxy/rewriteme">t... (200; 53.953377ms)
Mar  2 20:33:44.144: INFO: (19) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname2/proxy/: bar (200; 65.640519ms)
Mar  2 20:33:44.144: INFO: (19) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname2/proxy/: bar (200; 65.870093ms)
Mar  2 20:33:44.144: INFO: (19) /api/v1/namespaces/proxy-786/services/proxy-service-kjnpm:portname1/proxy/: foo (200; 65.367967ms)
Mar  2 20:33:44.145: INFO: (19) /api/v1/namespaces/proxy-786/services/http:proxy-service-kjnpm:portname1/proxy/: foo (200; 66.133687ms)
STEP: deleting ReplicationController proxy-service-kjnpm in namespace proxy-786, will wait for the garbage collector to delete the pods
Mar  2 20:33:44.259: INFO: Deleting ReplicationController proxy-service-kjnpm took: 24.726249ms
Mar  2 20:33:44.359: INFO: Terminating ReplicationController proxy-service-kjnpm pods took: 100.682545ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:33:46.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-786" for this suite.

• [SLOW TEST:6.999 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":219,"skipped":4013,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:33:46.596: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-6498
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-6498
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6498
Mar  2 20:33:46.899: INFO: Found 0 stateful pods, waiting for 1
Mar  2 20:33:56.925: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  2 20:33:56.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-6498 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 20:33:57.196: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 20:33:57.196: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 20:33:57.196: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 20:33:57.209: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 20:34:07.232: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 20:34:07.232: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 20:34:07.296: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Mar  2 20:34:07.296: INFO: ss-0  10.245.0.4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:33:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:33:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:33:46 +0000 UTC  }]
Mar  2 20:34:07.296: INFO: 
Mar  2 20:34:07.296: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  2 20:34:08.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.973123735s
Mar  2 20:34:09.334: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.951904272s
Mar  2 20:34:10.373: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.935472457s
Mar  2 20:34:11.417: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.895779475s
Mar  2 20:34:12.431: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.852579675s
Mar  2 20:34:13.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.838200668s
Mar  2 20:34:14.479: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.821704947s
Mar  2 20:34:15.497: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.790328902s
Mar  2 20:34:16.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 771.485771ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6498
Mar  2 20:34:17.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-6498 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 20:34:17.764: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 20:34:17.764: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 20:34:17.764: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 20:34:17.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-6498 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 20:34:18.010: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 20:34:18.010: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 20:34:18.010: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 20:34:18.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-6498 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 20:34:18.255: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 20:34:18.255: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 20:34:18.255: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 20:34:18.269: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 20:34:18.269: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 20:34:18.269: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  2 20:34:18.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-6498 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 20:34:18.525: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 20:34:18.525: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 20:34:18.525: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 20:34:18.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-6498 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 20:34:18.762: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 20:34:18.762: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 20:34:18.762: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 20:34:18.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-6498 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 20:34:18.984: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 20:34:18.984: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 20:34:18.984: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 20:34:18.984: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 20:34:18.997: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  2 20:34:29.024: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 20:34:29.024: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 20:34:29.024: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 20:34:29.061: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Mar  2 20:34:29.061: INFO: ss-0  10.245.0.4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:33:46 +0000 UTC  }]
Mar  2 20:34:29.061: INFO: ss-1  10.245.0.5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:07 +0000 UTC  }]
Mar  2 20:34:29.061: INFO: ss-2  10.245.0.6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:07 +0000 UTC  }]
Mar  2 20:34:29.061: INFO: 
Mar  2 20:34:29.061: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 20:34:30.078: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Mar  2 20:34:30.078: INFO: ss-0  10.245.0.4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:33:46 +0000 UTC  }]
Mar  2 20:34:30.078: INFO: ss-1  10.245.0.5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:07 +0000 UTC  }]
Mar  2 20:34:30.078: INFO: ss-2  10.245.0.6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:34:07 +0000 UTC  }]
Mar  2 20:34:30.078: INFO: 
Mar  2 20:34:30.078: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 20:34:31.092: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.970854264s
Mar  2 20:34:32.106: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.957203641s
Mar  2 20:34:33.123: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.942406037s
Mar  2 20:34:34.138: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.926060692s
Mar  2 20:34:35.155: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.910571781s
Mar  2 20:34:36.196: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.893644095s
Mar  2 20:34:37.211: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.852589946s
Mar  2 20:34:38.225: INFO: Verifying statefulset ss doesn't scale past 0 for another 838.025618ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6498
Mar  2 20:34:39.241: INFO: Scaling statefulset ss to 0
Mar  2 20:34:39.310: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Mar  2 20:34:39.323: INFO: Deleting all statefulset in ns statefulset-6498
Mar  2 20:34:39.363: INFO: Scaling statefulset ss to 0
Mar  2 20:34:39.416: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 20:34:39.426: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:34:39.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6498" for this suite.

• [SLOW TEST:52.965 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":220,"skipped":4082,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:34:39.561: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 20:34:39.977: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 20:34:40.011: INFO: waiting for watch events with expected annotations
Mar  2 20:34:40.012: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:34:40.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-849" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":221,"skipped":4103,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:34:40.146: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3157
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:34:40.382: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:34:41.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3157" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":222,"skipped":4108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:34:41.509: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-738
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 20:34:41.806: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5871e9a-fb62-4674-8309-c6a644a0b86d" in namespace "projected-738" to be "Succeeded or Failed"
Mar  2 20:34:41.825: INFO: Pod "downwardapi-volume-f5871e9a-fb62-4674-8309-c6a644a0b86d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.081242ms
Mar  2 20:34:43.852: INFO: Pod "downwardapi-volume-f5871e9a-fb62-4674-8309-c6a644a0b86d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045471754s
STEP: Saw pod success
Mar  2 20:34:43.852: INFO: Pod "downwardapi-volume-f5871e9a-fb62-4674-8309-c6a644a0b86d" satisfied condition "Succeeded or Failed"
Mar  2 20:34:43.870: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-f5871e9a-fb62-4674-8309-c6a644a0b86d container client-container: <nil>
STEP: delete the pod
Mar  2 20:34:43.984: INFO: Waiting for pod downwardapi-volume-f5871e9a-fb62-4674-8309-c6a644a0b86d to disappear
Mar  2 20:34:44.011: INFO: Pod downwardapi-volume-f5871e9a-fb62-4674-8309-c6a644a0b86d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:34:44.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-738" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":223,"skipped":4135,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:34:44.096: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1278
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 20:34:44.661: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 20:34:46.700: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850084, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850084, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850084, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850084, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 20:34:49.761: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:34:49.783: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4492-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:34:53.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1278" for this suite.
STEP: Destroying namespace "webhook-1278-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.205 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":224,"skipped":4139,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:34:53.302: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-a24e877b-33d6-4fc8-ae08-1898fb6797fc
STEP: Creating a pod to test consume configMaps
Mar  2 20:34:53.771: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6343c2e0-909d-49c0-a17a-6ffa77e3bf3a" in namespace "projected-1026" to be "Succeeded or Failed"
Mar  2 20:34:53.785: INFO: Pod "pod-projected-configmaps-6343c2e0-909d-49c0-a17a-6ffa77e3bf3a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.135183ms
Mar  2 20:34:55.804: INFO: Pod "pod-projected-configmaps-6343c2e0-909d-49c0-a17a-6ffa77e3bf3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033233924s
STEP: Saw pod success
Mar  2 20:34:55.804: INFO: Pod "pod-projected-configmaps-6343c2e0-909d-49c0-a17a-6ffa77e3bf3a" satisfied condition "Succeeded or Failed"
Mar  2 20:34:55.815: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-configmaps-6343c2e0-909d-49c0-a17a-6ffa77e3bf3a container agnhost-container: <nil>
STEP: delete the pod
Mar  2 20:34:55.886: INFO: Waiting for pod pod-projected-configmaps-6343c2e0-909d-49c0-a17a-6ffa77e3bf3a to disappear
Mar  2 20:34:55.925: INFO: Pod pod-projected-configmaps-6343c2e0-909d-49c0-a17a-6ffa77e3bf3a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:34:55.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1026" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":4172,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:34:55.966: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1641
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Mar  2 20:34:56.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-1641 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Mar  2 20:34:56.313: INFO: stderr: ""
Mar  2 20:34:56.313: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Mar  2 20:34:56.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-1641 delete pods e2e-test-httpd-pod'
Mar  2 20:34:59.658: INFO: stderr: ""
Mar  2 20:34:59.658: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:34:59.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1641" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":226,"skipped":4186,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:34:59.721: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-2729
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:35:00.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2729" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":227,"skipped":4205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:35:00.353: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5500
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Mar  2 20:35:01.286: INFO: created pod pod-service-account-defaultsa
Mar  2 20:35:01.286: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  2 20:35:01.306: INFO: created pod pod-service-account-mountsa
Mar  2 20:35:01.306: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  2 20:35:01.356: INFO: created pod pod-service-account-nomountsa
Mar  2 20:35:01.356: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  2 20:35:01.381: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  2 20:35:01.381: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  2 20:35:01.405: INFO: created pod pod-service-account-mountsa-mountspec
Mar  2 20:35:01.405: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  2 20:35:01.429: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  2 20:35:01.429: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  2 20:35:01.454: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  2 20:35:01.454: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  2 20:35:01.474: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  2 20:35:01.474: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  2 20:35:01.502: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  2 20:35:01.502: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:35:01.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5500" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":228,"skipped":4252,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:35:01.545: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8965
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Mar  2 20:35:01.861: INFO: observed Pod pod-test in namespace pods-8965 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar  2 20:35:01.888: INFO: observed Pod pod-test in namespace pods-8965 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:01 +0000 UTC  }]
Mar  2 20:35:01.942: INFO: observed Pod pod-test in namespace pods-8965 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:01 +0000 UTC  }]
Mar  2 20:35:03.292: INFO: observed Pod pod-test in namespace pods-8965 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:01 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:01 +0000 UTC  }]
Mar  2 20:35:04.613: INFO: Found Pod pod-test in namespace pods-8965 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-02 20:35:01 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Mar  2 20:35:04.646: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Mar  2 20:35:04.773: INFO: observed event type ADDED
Mar  2 20:35:04.774: INFO: observed event type MODIFIED
Mar  2 20:35:04.774: INFO: observed event type MODIFIED
Mar  2 20:35:04.774: INFO: observed event type MODIFIED
Mar  2 20:35:04.774: INFO: observed event type MODIFIED
Mar  2 20:35:04.775: INFO: observed event type MODIFIED
Mar  2 20:35:04.775: INFO: observed event type MODIFIED
Mar  2 20:35:04.775: INFO: observed event type MODIFIED
Mar  2 20:35:06.655: INFO: observed event type MODIFIED
Mar  2 20:35:06.905: INFO: observed event type MODIFIED
Mar  2 20:35:07.636: INFO: observed event type MODIFIED
Mar  2 20:35:07.662: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:35:07.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8965" for this suite.

• [SLOW TEST:6.167 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":229,"skipped":4262,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:35:07.712: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1022
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ed0f7c52-7620-4dd4-9980-e4d576ce52ab
STEP: Creating the pod
Mar  2 20:35:08.106: INFO: The status of Pod pod-projected-configmaps-0a3520a6-767f-49eb-b5df-da7b8f357b04 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:35:10.124: INFO: The status of Pod pod-projected-configmaps-0a3520a6-767f-49eb-b5df-da7b8f357b04 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:35:12.131: INFO: The status of Pod pod-projected-configmaps-0a3520a6-767f-49eb-b5df-da7b8f357b04 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-ed0f7c52-7620-4dd4-9980-e4d576ce52ab
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:36:35.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1022" for this suite.

• [SLOW TEST:88.113 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":230,"skipped":4264,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:36:35.825: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-234
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar  2 20:36:36.123: INFO: Waiting up to 5m0s for pod "downward-api-20dcc5f8-e291-4d93-a4ee-37143b499cf7" in namespace "downward-api-234" to be "Succeeded or Failed"
Mar  2 20:36:36.137: INFO: Pod "downward-api-20dcc5f8-e291-4d93-a4ee-37143b499cf7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.863734ms
Mar  2 20:36:38.170: INFO: Pod "downward-api-20dcc5f8-e291-4d93-a4ee-37143b499cf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047545967s
STEP: Saw pod success
Mar  2 20:36:38.170: INFO: Pod "downward-api-20dcc5f8-e291-4d93-a4ee-37143b499cf7" satisfied condition "Succeeded or Failed"
Mar  2 20:36:38.223: INFO: Trying to get logs from node 10.245.0.4 pod downward-api-20dcc5f8-e291-4d93-a4ee-37143b499cf7 container dapi-container: <nil>
STEP: delete the pod
Mar  2 20:36:38.301: INFO: Waiting for pod downward-api-20dcc5f8-e291-4d93-a4ee-37143b499cf7 to disappear
Mar  2 20:36:38.313: INFO: Pod downward-api-20dcc5f8-e291-4d93-a4ee-37143b499cf7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:36:38.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-234" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":231,"skipped":4271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:36:38.374: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9586
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-9586
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Mar  2 20:36:38.678: INFO: Found 0 stateful pods, waiting for 3
Mar  2 20:36:48.695: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 20:36:48.695: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 20:36:48.695: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 20:36:48.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-9586 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 20:36:49.001: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 20:36:49.001: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 20:36:49.001: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Mar  2 20:36:59.180: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  2 20:37:09.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-9586 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 20:37:09.483: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 20:37:09.483: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 20:37:09.483: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 20:37:19.592: INFO: Waiting for StatefulSet statefulset-9586/ss2 to complete update
Mar  2 20:37:19.592: INFO: Waiting for Pod statefulset-9586/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Mar  2 20:37:19.592: INFO: Waiting for Pod statefulset-9586/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Mar  2 20:37:29.625: INFO: Waiting for StatefulSet statefulset-9586/ss2 to complete update
Mar  2 20:37:29.625: INFO: Waiting for Pod statefulset-9586/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Mar  2 20:37:39.641: INFO: Waiting for StatefulSet statefulset-9586/ss2 to complete update
STEP: Rolling back to a previous revision
Mar  2 20:37:49.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-9586 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 20:37:49.890: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 20:37:49.890: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 20:37:49.890: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 20:38:00.093: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  2 20:38:10.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-9586 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 20:38:10.445: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 20:38:10.445: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 20:38:10.445: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 20:38:20.556: INFO: Waiting for StatefulSet statefulset-9586/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Mar  2 20:38:30.620: INFO: Deleting all statefulset in ns statefulset-9586
Mar  2 20:38:30.634: INFO: Scaling statefulset ss2 to 0
Mar  2 20:38:40.713: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 20:38:40.728: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:38:40.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9586" for this suite.

• [SLOW TEST:122.492 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":232,"skipped":4297,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:38:40.866: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Mar  2 20:38:41.237: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:38:41.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3960" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":233,"skipped":4299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:38:41.431: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8498.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8498.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 20:38:46.070: INFO: DNS probes using dns-8498/dns-test-782c7ddd-d25d-4bd6-ab8d-b0e94996a2f5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:38:46.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8498" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":234,"skipped":4349,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:38:46.157: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-731
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-7df068ac-7107-4969-8725-0c3ee1247859
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:38:46.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-731" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":235,"skipped":4361,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:38:46.471: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-976
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 20:38:47.287: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 20:38:50.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:38:50.411: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8320-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:38:53.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-976" for this suite.
STEP: Destroying namespace "webhook-976-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.265 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":236,"skipped":4372,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:38:53.736: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-7391
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 20:38:54.032: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar  2 20:38:54.052: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 20:38:54.136: INFO: waiting for watch events with expected annotations
Mar  2 20:38:54.136: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:38:54.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7391" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":237,"skipped":4382,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:38:54.549: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-9966
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:38:54.811: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Creating first CR 
Mar  2 20:38:57.500: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-02T20:38:57Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-02T20:38:57Z]] name:name1 resourceVersion:39238 uid:9d4be163-aad5-48f3-acde-cee35e28c1aa] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar  2 20:39:07.526: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-02T20:39:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-02T20:39:07Z]] name:name2 resourceVersion:39282 uid:99fa84b4-1d4c-496b-a3e1-9e2dc906f68a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar  2 20:39:17.556: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-02T20:38:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-02T20:39:17Z]] name:name1 resourceVersion:39296 uid:9d4be163-aad5-48f3-acde-cee35e28c1aa] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar  2 20:39:27.593: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-02T20:39:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-02T20:39:27Z]] name:name2 resourceVersion:39310 uid:99fa84b4-1d4c-496b-a3e1-9e2dc906f68a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar  2 20:39:37.625: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-02T20:38:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-02T20:39:17Z]] name:name1 resourceVersion:39325 uid:9d4be163-aad5-48f3-acde-cee35e28c1aa] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar  2 20:39:47.660: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-02T20:39:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-02T20:39:27Z]] name:name2 resourceVersion:39339 uid:99fa84b4-1d4c-496b-a3e1-9e2dc906f68a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:39:58.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9966" for this suite.

• [SLOW TEST:63.710 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":238,"skipped":4395,"failed":0}
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:39:58.260: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-b19887b2-da65-4a5b-9a4c-7cddf3f64413 in namespace container-probe-4140
Mar  2 20:40:00.576: INFO: Started pod busybox-b19887b2-da65-4a5b-9a4c-7cddf3f64413 in namespace container-probe-4140
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 20:40:00.586: INFO: Initial restart count of pod busybox-b19887b2-da65-4a5b-9a4c-7cddf3f64413 is 0
Mar  2 20:40:51.195: INFO: Restart count of pod container-probe-4140/busybox-b19887b2-da65-4a5b-9a4c-7cddf3f64413 is now 1 (50.609048028s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:40:51.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4140" for this suite.

• [SLOW TEST:53.014 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":239,"skipped":4395,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:40:51.275: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Mar  2 20:42:52.158: INFO: Successfully updated pod "var-expansion-fd579261-b728-403f-8d38-99d7098503ec"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Mar  2 20:42:54.195: INFO: Deleting pod "var-expansion-fd579261-b728-403f-8d38-99d7098503ec" in namespace "var-expansion-3498"
Mar  2 20:42:54.250: INFO: Wait up to 5m0s for pod "var-expansion-fd579261-b728-403f-8d38-99d7098503ec" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:43:26.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3498" for this suite.

• [SLOW TEST:155.062 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":240,"skipped":4405,"failed":0}
SS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:43:26.338: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:43:26.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9428" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":241,"skipped":4407,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:43:26.802: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8212
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Mar  2 20:43:27.041: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:43:29.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8212" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":242,"skipped":4462,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:43:29.977: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 20:43:30.678: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 20:43:32.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850610, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850610, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850610, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850610, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 20:43:35.813: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:43:36.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7743" for this suite.
STEP: Destroying namespace "webhook-7743-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.586 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":243,"skipped":4469,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:43:36.563: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7941
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  2 20:43:36.873: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7941  f163e598-8ba2-4d47-a100-16885f23ca60 39908 0 2022-03-02 20:43:36 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-02 20:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 20:43:36.873: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7941  f163e598-8ba2-4d47-a100-16885f23ca60 39909 0 2022-03-02 20:43:36 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-02 20:43:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  2 20:43:36.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7941  f163e598-8ba2-4d47-a100-16885f23ca60 39910 0 2022-03-02 20:43:36 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-02 20:43:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 20:43:36.917: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7941  f163e598-8ba2-4d47-a100-16885f23ca60 39911 0 2022-03-02 20:43:36 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-02 20:43:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:43:36.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7941" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":244,"skipped":4475,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:43:36.957: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 20:43:37.817: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 20:43:39.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850617, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850617, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850617, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850617, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 20:43:42.918: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:43:42.936: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Mar  2 20:43:48.628: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:43:49.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2218" for this suite.
STEP: Destroying namespace "webhook-2218-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.077 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":245,"skipped":4478,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:43:50.035: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-dd9ff24d-4675-4d58-8cf2-1b0a35449c04
STEP: Creating a pod to test consume secrets
Mar  2 20:43:50.369: INFO: Waiting up to 5m0s for pod "pod-secrets-a8f289a3-c0af-4564-a7e4-3b06e50462e7" in namespace "secrets-1816" to be "Succeeded or Failed"
Mar  2 20:43:50.386: INFO: Pod "pod-secrets-a8f289a3-c0af-4564-a7e4-3b06e50462e7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.387162ms
Mar  2 20:43:52.405: INFO: Pod "pod-secrets-a8f289a3-c0af-4564-a7e4-3b06e50462e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036289989s
STEP: Saw pod success
Mar  2 20:43:52.405: INFO: Pod "pod-secrets-a8f289a3-c0af-4564-a7e4-3b06e50462e7" satisfied condition "Succeeded or Failed"
Mar  2 20:43:52.421: INFO: Trying to get logs from node 10.245.0.4 pod pod-secrets-a8f289a3-c0af-4564-a7e4-3b06e50462e7 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 20:43:52.553: INFO: Waiting for pod pod-secrets-a8f289a3-c0af-4564-a7e4-3b06e50462e7 to disappear
Mar  2 20:43:52.565: INFO: Pod pod-secrets-a8f289a3-c0af-4564-a7e4-3b06e50462e7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:43:52.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1816" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":246,"skipped":4482,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:43:52.607: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8552
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 20:43:53.645: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 20:43:55.708: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850633, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850633, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850633, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850633, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 20:43:58.753: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
Mar  2 20:44:03.877: INFO: Waiting for webhook configuration to be ready...
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:44:04.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8552" for this suite.
STEP: Destroying namespace "webhook-8552-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.859 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":247,"skipped":4483,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:44:04.467: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9265
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-bb61118d-dde4-45bf-a0a9-1b6f54096434
Mar  2 20:44:04.744: INFO: Pod name my-hostname-basic-bb61118d-dde4-45bf-a0a9-1b6f54096434: Found 0 pods out of 1
Mar  2 20:44:09.772: INFO: Pod name my-hostname-basic-bb61118d-dde4-45bf-a0a9-1b6f54096434: Found 1 pods out of 1
Mar  2 20:44:09.772: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-bb61118d-dde4-45bf-a0a9-1b6f54096434" are running
Mar  2 20:44:09.787: INFO: Pod "my-hostname-basic-bb61118d-dde4-45bf-a0a9-1b6f54096434-4b8wd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 20:44:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 20:44:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 20:44:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-02 20:44:04 +0000 UTC Reason: Message:}])
Mar  2 20:44:09.788: INFO: Trying to dial the pod
Mar  2 20:44:14.903: INFO: Controller my-hostname-basic-bb61118d-dde4-45bf-a0a9-1b6f54096434: Got expected result from replica 1 [my-hostname-basic-bb61118d-dde4-45bf-a0a9-1b6f54096434-4b8wd]: "my-hostname-basic-bb61118d-dde4-45bf-a0a9-1b6f54096434-4b8wd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:44:14.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9265" for this suite.

• [SLOW TEST:10.535 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":248,"skipped":4501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:44:15.003: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5777
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:44:15.301: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8771dcb0-2a52-4515-b9d4-9210327d20f8" in namespace "security-context-test-5777" to be "Succeeded or Failed"
Mar  2 20:44:15.314: INFO: Pod "alpine-nnp-false-8771dcb0-2a52-4515-b9d4-9210327d20f8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.974278ms
Mar  2 20:44:17.326: INFO: Pod "alpine-nnp-false-8771dcb0-2a52-4515-b9d4-9210327d20f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02563872s
Mar  2 20:44:19.344: INFO: Pod "alpine-nnp-false-8771dcb0-2a52-4515-b9d4-9210327d20f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04310563s
Mar  2 20:44:21.368: INFO: Pod "alpine-nnp-false-8771dcb0-2a52-4515-b9d4-9210327d20f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067657127s
Mar  2 20:44:21.368: INFO: Pod "alpine-nnp-false-8771dcb0-2a52-4515-b9d4-9210327d20f8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:44:21.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5777" for this suite.

• [SLOW TEST:6.436 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":249,"skipped":4539,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:44:21.441: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-7e33cfcc-f138-456f-bcaf-32344152a7eb
STEP: Creating a pod to test consume secrets
Mar  2 20:44:21.769: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2e4ec82f-48fb-4e68-97a4-d72e99814096" in namespace "projected-9813" to be "Succeeded or Failed"
Mar  2 20:44:21.780: INFO: Pod "pod-projected-secrets-2e4ec82f-48fb-4e68-97a4-d72e99814096": Phase="Pending", Reason="", readiness=false. Elapsed: 11.30071ms
Mar  2 20:44:23.812: INFO: Pod "pod-projected-secrets-2e4ec82f-48fb-4e68-97a4-d72e99814096": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042794713s
Mar  2 20:44:25.885: INFO: Pod "pod-projected-secrets-2e4ec82f-48fb-4e68-97a4-d72e99814096": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.115894803s
STEP: Saw pod success
Mar  2 20:44:25.885: INFO: Pod "pod-projected-secrets-2e4ec82f-48fb-4e68-97a4-d72e99814096" satisfied condition "Succeeded or Failed"
Mar  2 20:44:25.969: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-secrets-2e4ec82f-48fb-4e68-97a4-d72e99814096 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 20:44:26.111: INFO: Waiting for pod pod-projected-secrets-2e4ec82f-48fb-4e68-97a4-d72e99814096 to disappear
Mar  2 20:44:26.146: INFO: Pod pod-projected-secrets-2e4ec82f-48fb-4e68-97a4-d72e99814096 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:44:26.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9813" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":250,"skipped":4587,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:44:26.240: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:44:26.590: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar  2 20:44:27.805: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:44:27.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9217" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":251,"skipped":4605,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:44:27.887: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7749
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Mar  2 20:44:30.229: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7749 PodName:pod-sharedvolume-c68457d4-71f0-43e6-8261-90db3c9b26c7 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 20:44:30.229: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
Mar  2 20:44:30.391: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:44:30.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7749" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":252,"skipped":4613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:44:30.496: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5542
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar  2 20:44:30.833: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 20:44:30.973: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 20:44:31.008: INFO: 
Logging pods the apiserver thinks is on node 10.245.0.4 before test
Mar  2 20:44:31.061: INFO: pod-sharedvolume-c68457d4-71f0-43e6-8261-90db3c9b26c7 from emptydir-7749 started at 2022-03-02 20:44:28 +0000 UTC (2 container statuses recorded)
Mar  2 20:44:31.062: INFO: 	Container busybox-main-container ready: true, restart count 0
Mar  2 20:44:31.062: INFO: 	Container busybox-sub-container ready: false, restart count 0
Mar  2 20:44:31.062: INFO: calico-node-6bmpc from kube-system started at 2022-03-02 17:40:12 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.062: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 20:44:31.062: INFO: calico-typha-7cdb864b94-kg6vk from kube-system started at 2022-03-02 20:01:16 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.062: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 20:44:31.062: INFO: ibm-master-proxy-static-10.245.0.4 from kube-system started at 2022-03-02 17:39:53 +0000 UTC (2 container statuses recorded)
Mar  2 20:44:31.062: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 20:44:31.062: INFO: 	Container pause ready: true, restart count 0
Mar  2 20:44:31.062: INFO: ibm-vpc-block-csi-node-m9dxv from kube-system started at 2022-03-02 17:40:12 +0000 UTC (4 container statuses recorded)
Mar  2 20:44:31.062: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Mar  2 20:44:31.062: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Mar  2 20:44:31.062: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 20:44:31.062: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 20:44:31.062: INFO: konnectivity-agent-cnbg9 from kube-system started at 2022-03-02 17:48:31 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.062: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  2 20:44:31.062: INFO: condition-test-dss5r from replication-controller-9217 started at 2022-03-02 20:44:26 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.062: INFO: 	Container httpd ready: true, restart count 0
Mar  2 20:44:31.062: INFO: sonobuoy from sonobuoy started at 2022-03-02 19:20:48 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.062: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 20:44:31.062: INFO: sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-867x2 from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 20:44:31.062: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 20:44:31.062: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 20:44:31.062: INFO: 
Logging pods the apiserver thinks is on node 10.245.0.5 before test
Mar  2 20:44:31.121: INFO: catalog-operator-6c4b4d7c9-gnzj6 from ibm-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 20:44:31.121: INFO: olm-operator-785cdc5884-8vxvx from ibm-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 20:44:31.121: INFO: calico-kube-controllers-dcd8d986c-g4lwp from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 20:44:31.121: INFO: calico-node-7n2gc from kube-system started at 2022-03-02 17:38:33 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 20:44:31.121: INFO: calico-typha-7cdb864b94-t4bmt from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 20:44:31.121: INFO: coredns-autoscaler-689fb74d49-hqj9c from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container autoscaler ready: true, restart count 0
Mar  2 20:44:31.121: INFO: coredns-b58d5f584-m7b7m from kube-system started at 2022-03-02 17:49:06 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container coredns ready: true, restart count 0
Mar  2 20:44:31.121: INFO: coredns-b58d5f584-s8nv6 from kube-system started at 2022-03-02 20:00:39 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container coredns ready: true, restart count 0
Mar  2 20:44:31.121: INFO: dashboard-metrics-scraper-6747f89c97-4dq9w from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar  2 20:44:31.121: INFO: ibm-master-proxy-static-10.245.0.5 from kube-system started at 2022-03-02 17:38:18 +0000 UTC (2 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 20:44:31.121: INFO: 	Container pause ready: true, restart count 0
Mar  2 20:44:31.121: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2022-03-02 17:38:51 +0000 UTC (6 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container csi-attacher ready: true, restart count 0
Mar  2 20:44:31.121: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar  2 20:44:31.121: INFO: 	Container csi-resizer ready: true, restart count 0
Mar  2 20:44:31.121: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Mar  2 20:44:31.121: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 20:44:31.121: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 20:44:31.121: INFO: ibm-vpc-block-csi-node-n5lr5 from kube-system started at 2022-03-02 17:38:33 +0000 UTC (4 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Mar  2 20:44:31.121: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Mar  2 20:44:31.121: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 20:44:31.121: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 20:44:31.121: INFO: konnectivity-agent-gkp9m from kube-system started at 2022-03-02 17:48:34 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  2 20:44:31.121: INFO: kubernetes-dashboard-54c47dd995-vkxsd from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  2 20:44:31.121: INFO: public-crc8fqf88s0ubnto7pbi70-alb1-6559fd4c4b-kzxmb from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  2 20:44:31.121: INFO: condition-test-6w8p5 from replication-controller-9217 started at 2022-03-02 20:44:26 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container httpd ready: true, restart count 0
Mar  2 20:44:31.121: INFO: sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-4g544 from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 20:44:31.121: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 20:44:31.121: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 20:44:31.121: INFO: 
Logging pods the apiserver thinks is on node 10.245.0.6 before test
Mar  2 20:44:31.170: INFO: calico-node-tl6xm from kube-system started at 2022-03-02 17:38:42 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.170: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 20:44:31.170: INFO: calico-typha-7cdb864b94-znnrz from kube-system started at 2022-03-02 17:39:02 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.170: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 20:44:31.170: INFO: coredns-b58d5f584-sdfgx from kube-system started at 2022-03-02 17:49:06 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.170: INFO: 	Container coredns ready: true, restart count 0
Mar  2 20:44:31.170: INFO: ibm-master-proxy-static-10.245.0.6 from kube-system started at 2022-03-02 17:38:23 +0000 UTC (2 container statuses recorded)
Mar  2 20:44:31.170: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 20:44:31.170: INFO: 	Container pause ready: true, restart count 0
Mar  2 20:44:31.170: INFO: ibm-vpc-block-csi-node-nkxms from kube-system started at 2022-03-02 17:38:42 +0000 UTC (4 container statuses recorded)
Mar  2 20:44:31.170: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Mar  2 20:44:31.170: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Mar  2 20:44:31.170: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 20:44:31.170: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 20:44:31.170: INFO: konnectivity-agent-bnglr from kube-system started at 2022-03-02 17:48:28 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.170: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  2 20:44:31.170: INFO: metrics-server-7b97867cc5-fpb5z from kube-system started at 2022-03-02 18:23:06 +0000 UTC (3 container statuses recorded)
Mar  2 20:44:31.170: INFO: 	Container config-watcher ready: true, restart count 0
Mar  2 20:44:31.170: INFO: 	Container metrics-server ready: true, restart count 0
Mar  2 20:44:31.170: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar  2 20:44:31.170: INFO: public-crc8fqf88s0ubnto7pbi70-alb1-6559fd4c4b-lwmtq from kube-system started at 2022-03-02 17:39:02 +0000 UTC (1 container statuses recorded)
Mar  2 20:44:31.170: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  2 20:44:31.170: INFO: sonobuoy-e2e-job-6fc4581698214e95 from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 20:44:31.170: INFO: 	Container e2e ready: true, restart count 0
Mar  2 20:44:31.170: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 20:44:31.170: INFO: sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-f24tq from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 20:44:31.170: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 20:44:31.170: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16d8ab2c75ad6c94], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:44:32.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5542" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":253,"skipped":4656,"failed":0}
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:44:32.310: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-6988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Mar  2 20:44:32.572: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Mar  2 20:44:33.133: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar  2 20:44:35.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:44:37.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:44:39.367: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:44:41.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:44:43.368: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:44:45.369: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:44:47.366: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:44:49.366: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:44:51.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:44:53.365: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:44:55.372: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:44:57.367: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:44:59.370: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:45:01.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781850673, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:45:03.573: INFO: Waited 188.717795ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Mar  2 20:45:03.885: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:45:04.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6988" for this suite.

• [SLOW TEST:32.234 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":254,"skipped":4657,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:45:04.545: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:45:04.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9188" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":255,"skipped":4678,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:45:04.826: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3788
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:45:05.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3788" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":256,"skipped":4699,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:45:05.182: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-8359
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:45:05.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8359" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":257,"skipped":4731,"failed":0}

------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:45:05.741: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2099
STEP: creating service affinity-nodeport in namespace services-2099
STEP: creating replication controller affinity-nodeport in namespace services-2099
I0302 20:45:06.107636      21 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2099, replica count: 3
I0302 20:45:09.158637      21 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 20:45:09.215: INFO: Creating new exec pod
Mar  2 20:45:14.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2099 exec execpod-affinitybtqct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Mar  2 20:45:14.798: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar  2 20:45:14.798: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 20:45:14.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2099 exec execpod-affinitybtqct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.127.228 80'
Mar  2 20:45:15.049: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.127.228 80\nConnection to 172.21.127.228 80 port [tcp/http] succeeded!\n"
Mar  2 20:45:15.049: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 20:45:15.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2099 exec execpod-affinitybtqct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.245.0.6 31484'
Mar  2 20:45:15.290: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.245.0.6 31484\nConnection to 10.245.0.6 31484 port [tcp/*] succeeded!\n"
Mar  2 20:45:15.290: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 20:45:15.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2099 exec execpod-affinitybtqct -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.245.0.5 31484'
Mar  2 20:45:15.523: INFO: stderr: "+ nc -v -t -w 2 10.245.0.5 31484\n+ echo hostName\nConnection to 10.245.0.5 31484 port [tcp/*] succeeded!\n"
Mar  2 20:45:15.523: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 20:45:15.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-2099 exec execpod-affinitybtqct -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.245.0.4:31484/ ; done'
Mar  2 20:45:15.782: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.245.0.4:31484/\n"
Mar  2 20:45:15.782: INFO: stdout: "\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx\naffinity-nodeport-86rwx"
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.782: INFO: Received response from host: affinity-nodeport-86rwx
Mar  2 20:45:15.783: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2099, will wait for the garbage collector to delete the pods
Mar  2 20:45:15.907: INFO: Deleting ReplicationController affinity-nodeport took: 20.418338ms
Mar  2 20:45:16.008: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.128832ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:45:18.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2099" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.812 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":258,"skipped":4731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:45:18.555: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-5638
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Mar  2 20:45:18.866: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:45:21.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5638" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":259,"skipped":4758,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:45:21.085: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2156
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-c29ce0f6-1f91-403b-90a9-ffc877e9b54a in namespace container-probe-2156
Mar  2 20:45:23.418: INFO: Started pod liveness-c29ce0f6-1f91-403b-90a9-ffc877e9b54a in namespace container-probe-2156
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 20:45:23.429: INFO: Initial restart count of pod liveness-c29ce0f6-1f91-403b-90a9-ffc877e9b54a is 0
Mar  2 20:45:43.681: INFO: Restart count of pod container-probe-2156/liveness-c29ce0f6-1f91-403b-90a9-ffc877e9b54a is now 1 (20.252259389s elapsed)
Mar  2 20:46:03.891: INFO: Restart count of pod container-probe-2156/liveness-c29ce0f6-1f91-403b-90a9-ffc877e9b54a is now 2 (40.46278175s elapsed)
Mar  2 20:46:24.160: INFO: Restart count of pod container-probe-2156/liveness-c29ce0f6-1f91-403b-90a9-ffc877e9b54a is now 3 (1m0.731922869s elapsed)
Mar  2 20:46:44.398: INFO: Restart count of pod container-probe-2156/liveness-c29ce0f6-1f91-403b-90a9-ffc877e9b54a is now 4 (1m20.969566601s elapsed)
Mar  2 20:47:47.120: INFO: Restart count of pod container-probe-2156/liveness-c29ce0f6-1f91-403b-90a9-ffc877e9b54a is now 5 (2m23.691442622s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:47:47.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2156" for this suite.

• [SLOW TEST:146.146 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":260,"skipped":4761,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:47:47.233: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9996
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 20:47:47.510: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4983781b-0bf2-4f2f-9456-4ab5ed88e19b" in namespace "projected-9996" to be "Succeeded or Failed"
Mar  2 20:47:47.523: INFO: Pod "downwardapi-volume-4983781b-0bf2-4f2f-9456-4ab5ed88e19b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.595268ms
Mar  2 20:47:49.545: INFO: Pod "downwardapi-volume-4983781b-0bf2-4f2f-9456-4ab5ed88e19b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035280381s
Mar  2 20:47:51.569: INFO: Pod "downwardapi-volume-4983781b-0bf2-4f2f-9456-4ab5ed88e19b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058569473s
STEP: Saw pod success
Mar  2 20:47:51.569: INFO: Pod "downwardapi-volume-4983781b-0bf2-4f2f-9456-4ab5ed88e19b" satisfied condition "Succeeded or Failed"
Mar  2 20:47:51.581: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-4983781b-0bf2-4f2f-9456-4ab5ed88e19b container client-container: <nil>
STEP: delete the pod
Mar  2 20:47:51.706: INFO: Waiting for pod downwardapi-volume-4983781b-0bf2-4f2f-9456-4ab5ed88e19b to disappear
Mar  2 20:47:51.718: INFO: Pod downwardapi-volume-4983781b-0bf2-4f2f-9456-4ab5ed88e19b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:47:51.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9996" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":261,"skipped":4800,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:47:51.764: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8085
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Mar  2 20:47:52.079: INFO: Found Service test-service-jkmvm in namespace services-8085 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar  2 20:47:52.079: INFO: Service test-service-jkmvm created
STEP: Getting /status
Mar  2 20:47:52.097: INFO: Service test-service-jkmvm has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Mar  2 20:47:52.145: INFO: observed Service test-service-jkmvm in namespace services-8085 with annotations: map[] & LoadBalancer: {[]}
Mar  2 20:47:52.145: INFO: Found Service test-service-jkmvm in namespace services-8085 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar  2 20:47:52.145: INFO: Service test-service-jkmvm has service status patched
STEP: updating the ServiceStatus
Mar  2 20:47:52.176: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Mar  2 20:47:52.181: INFO: Observed Service test-service-jkmvm in namespace services-8085 with annotations: map[] & Conditions: {[]}
Mar  2 20:47:52.181: INFO: Observed event: &Service{ObjectMeta:{test-service-jkmvm  services-8085  4f3c93f0-4299-45bc-84e9-d978a66f13f4 41238 0 2022-03-02 20:47:52 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-03-02 20:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-03-02 20:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.181.83,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:nil,ClusterIPs:[172.21.181.83],IPFamilies:[],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar  2 20:47:52.182: INFO: Found Service test-service-jkmvm in namespace services-8085 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 20:47:52.182: INFO: Service test-service-jkmvm has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Mar  2 20:47:52.213: INFO: observed Service test-service-jkmvm in namespace services-8085 with labels: map[test-service-static:true]
Mar  2 20:47:52.213: INFO: observed Service test-service-jkmvm in namespace services-8085 with labels: map[test-service-static:true]
Mar  2 20:47:52.213: INFO: observed Service test-service-jkmvm in namespace services-8085 with labels: map[test-service-static:true]
Mar  2 20:47:52.213: INFO: Found Service test-service-jkmvm in namespace services-8085 with labels: map[test-service:patched test-service-static:true]
Mar  2 20:47:52.213: INFO: Service test-service-jkmvm patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Mar  2 20:47:52.297: INFO: Observed event: ADDED
Mar  2 20:47:52.297: INFO: Observed event: MODIFIED
Mar  2 20:47:52.298: INFO: Observed event: MODIFIED
Mar  2 20:47:52.298: INFO: Observed event: MODIFIED
Mar  2 20:47:52.298: INFO: Found Service test-service-jkmvm in namespace services-8085 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar  2 20:47:52.298: INFO: Service test-service-jkmvm deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:47:52.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8085" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":262,"skipped":4810,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:47:52.346: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-6010
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:47:52.588: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename disruption-2
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-7582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-6010
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:47:59.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-7582" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:47:59.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6010" for this suite.

• [SLOW TEST:6.860 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":263,"skipped":4837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:47:59.207: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-3847aca3-caeb-4b72-8ea4-8170156824c4
STEP: Creating a pod to test consume configMaps
Mar  2 20:47:59.506: INFO: Waiting up to 5m0s for pod "pod-configmaps-ded78cab-d8b7-4034-9c40-0510f8c05a93" in namespace "configmap-1922" to be "Succeeded or Failed"
Mar  2 20:47:59.518: INFO: Pod "pod-configmaps-ded78cab-d8b7-4034-9c40-0510f8c05a93": Phase="Pending", Reason="", readiness=false. Elapsed: 11.289557ms
Mar  2 20:48:01.569: INFO: Pod "pod-configmaps-ded78cab-d8b7-4034-9c40-0510f8c05a93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0622897s
Mar  2 20:48:03.606: INFO: Pod "pod-configmaps-ded78cab-d8b7-4034-9c40-0510f8c05a93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.099743053s
STEP: Saw pod success
Mar  2 20:48:03.606: INFO: Pod "pod-configmaps-ded78cab-d8b7-4034-9c40-0510f8c05a93" satisfied condition "Succeeded or Failed"
Mar  2 20:48:03.617: INFO: Trying to get logs from node 10.245.0.4 pod pod-configmaps-ded78cab-d8b7-4034-9c40-0510f8c05a93 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 20:48:03.685: INFO: Waiting for pod pod-configmaps-ded78cab-d8b7-4034-9c40-0510f8c05a93 to disappear
Mar  2 20:48:03.697: INFO: Pod pod-configmaps-ded78cab-d8b7-4034-9c40-0510f8c05a93 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:48:03.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1922" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":4872,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:48:03.742: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Mar  2 20:48:04.014: INFO: The status of Pod annotationupdated69f99d8-0b48-4b4c-9f64-2d9815ca36e9 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:48:06.041: INFO: The status of Pod annotationupdated69f99d8-0b48-4b4c-9f64-2d9815ca36e9 is Running (Ready = true)
Mar  2 20:48:06.618: INFO: Successfully updated pod "annotationupdated69f99d8-0b48-4b4c-9f64-2d9815ca36e9"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:48:08.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1927" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":265,"skipped":4926,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:48:08.717: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2793
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-420c673a-f4b6-4677-b2e5-423308a38c6e
STEP: Creating the pod
Mar  2 20:48:09.069: INFO: The status of Pod pod-configmaps-f77f356b-e1b4-4fad-a2e4-65eabc279a41 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:48:11.107: INFO: The status of Pod pod-configmaps-f77f356b-e1b4-4fad-a2e4-65eabc279a41 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-420c673a-f4b6-4677-b2e5-423308a38c6e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:48:13.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2793" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":266,"skipped":4927,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:48:13.314: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6520
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 20:48:13.614: INFO: Waiting up to 5m0s for pod "downwardapi-volume-017643df-8894-411c-b80d-9b0d63009556" in namespace "downward-api-6520" to be "Succeeded or Failed"
Mar  2 20:48:13.629: INFO: Pod "downwardapi-volume-017643df-8894-411c-b80d-9b0d63009556": Phase="Pending", Reason="", readiness=false. Elapsed: 15.166578ms
Mar  2 20:48:15.653: INFO: Pod "downwardapi-volume-017643df-8894-411c-b80d-9b0d63009556": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039530805s
Mar  2 20:48:17.680: INFO: Pod "downwardapi-volume-017643df-8894-411c-b80d-9b0d63009556": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066667724s
STEP: Saw pod success
Mar  2 20:48:17.680: INFO: Pod "downwardapi-volume-017643df-8894-411c-b80d-9b0d63009556" satisfied condition "Succeeded or Failed"
Mar  2 20:48:17.714: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-017643df-8894-411c-b80d-9b0d63009556 container client-container: <nil>
STEP: delete the pod
Mar  2 20:48:17.789: INFO: Waiting for pod downwardapi-volume-017643df-8894-411c-b80d-9b0d63009556 to disappear
Mar  2 20:48:17.800: INFO: Pod downwardapi-volume-017643df-8894-411c-b80d-9b0d63009556 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:48:17.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6520" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":267,"skipped":4931,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:48:17.888: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:48:26.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5872" for this suite.

• [SLOW TEST:8.340 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":268,"skipped":4949,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:48:26.229: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Mar  2 20:48:26.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-7767 cluster-info'
Mar  2 20:48:26.624: INFO: stderr: ""
Mar  2 20:48:26.624: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:48:26.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7767" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":269,"skipped":4950,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:48:26.709: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8554
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  2 20:48:27.005: INFO: Waiting up to 5m0s for pod "pod-fba4e391-5cef-4bed-b53b-26967ef0533d" in namespace "emptydir-8554" to be "Succeeded or Failed"
Mar  2 20:48:27.016: INFO: Pod "pod-fba4e391-5cef-4bed-b53b-26967ef0533d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.242367ms
Mar  2 20:48:29.051: INFO: Pod "pod-fba4e391-5cef-4bed-b53b-26967ef0533d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046255749s
STEP: Saw pod success
Mar  2 20:48:29.052: INFO: Pod "pod-fba4e391-5cef-4bed-b53b-26967ef0533d" satisfied condition "Succeeded or Failed"
Mar  2 20:48:29.065: INFO: Trying to get logs from node 10.245.0.4 pod pod-fba4e391-5cef-4bed-b53b-26967ef0533d container test-container: <nil>
STEP: delete the pod
Mar  2 20:48:29.169: INFO: Waiting for pod pod-fba4e391-5cef-4bed-b53b-26967ef0533d to disappear
Mar  2 20:48:29.183: INFO: Pod pod-fba4e391-5cef-4bed-b53b-26967ef0533d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:48:29.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8554" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":5046,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:48:29.226: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9910
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:48:29.497: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:48:32.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9910" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":271,"skipped":5047,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:48:32.958: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-7308
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7308
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7308
Mar  2 20:48:33.362: INFO: Found 0 stateful pods, waiting for 1
Mar  2 20:48:43.394: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  2 20:48:43.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-7308 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 20:48:43.636: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 20:48:43.636: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 20:48:43.636: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 20:48:43.650: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 20:48:53.682: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 20:48:53.682: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 20:48:53.742: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999688s
Mar  2 20:48:54.759: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.986965324s
Mar  2 20:48:55.777: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.969980088s
Mar  2 20:48:56.797: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.952007837s
Mar  2 20:48:57.829: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.932572797s
Mar  2 20:48:58.849: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.900705125s
Mar  2 20:48:59.871: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.880226549s
Mar  2 20:49:00.896: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.857807255s
Mar  2 20:49:01.916: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.833317385s
Mar  2 20:49:02.937: INFO: Verifying statefulset ss doesn't scale past 1 for another 813.115629ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7308
Mar  2 20:49:03.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-7308 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 20:49:04.226: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 20:49:04.226: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 20:49:04.226: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 20:49:04.241: INFO: Found 1 stateful pods, waiting for 3
Mar  2 20:49:14.267: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 20:49:14.267: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 20:49:14.267: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  2 20:49:14.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-7308 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 20:49:14.599: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 20:49:14.599: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 20:49:14.599: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 20:49:14.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-7308 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 20:49:14.860: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 20:49:14.860: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 20:49:14.860: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 20:49:14.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-7308 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 20:49:15.110: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 20:49:15.110: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 20:49:15.110: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 20:49:15.110: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 20:49:15.150: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  2 20:49:25.194: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 20:49:25.194: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 20:49:25.194: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 20:49:25.269: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999744s
Mar  2 20:49:26.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.963238865s
Mar  2 20:49:27.311: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.940995205s
Mar  2 20:49:28.326: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.922012206s
Mar  2 20:49:29.343: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.906987429s
Mar  2 20:49:30.362: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.889489908s
Mar  2 20:49:31.380: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.870474546s
Mar  2 20:49:32.419: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.852456238s
Mar  2 20:49:33.445: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.813123524s
Mar  2 20:49:34.468: INFO: Verifying statefulset ss doesn't scale past 3 for another 787.137046ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7308
Mar  2 20:49:35.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-7308 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 20:49:35.786: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 20:49:35.786: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 20:49:35.786: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 20:49:35.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-7308 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 20:49:36.031: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 20:49:36.031: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 20:49:36.031: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 20:49:36.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=statefulset-7308 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 20:49:36.344: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 20:49:36.344: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 20:49:36.345: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 20:49:36.345: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Mar  2 20:49:46.494: INFO: Deleting all statefulset in ns statefulset-7308
Mar  2 20:49:46.508: INFO: Scaling statefulset ss to 0
Mar  2 20:49:46.553: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 20:49:46.571: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:49:46.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7308" for this suite.

• [SLOW TEST:73.712 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":272,"skipped":5051,"failed":0}
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:49:46.670: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Mar  2 20:49:47.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-3819 create -f -'
Mar  2 20:49:48.120: INFO: stderr: ""
Mar  2 20:49:48.120: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Mar  2 20:49:48.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-3819 diff -f -'
Mar  2 20:49:49.360: INFO: rc: 1
Mar  2 20:49:49.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-3819 delete -f -'
Mar  2 20:49:49.442: INFO: stderr: ""
Mar  2 20:49:49.442: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:49:49.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3819" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":273,"skipped":5051,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:49:49.490: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:49:49.891: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 20:49:49.939: INFO: Number of nodes with available pods: 0
Mar  2 20:49:49.939: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:49:50.977: INFO: Number of nodes with available pods: 0
Mar  2 20:49:50.977: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:49:51.978: INFO: Number of nodes with available pods: 1
Mar  2 20:49:51.978: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:49:52.995: INFO: Number of nodes with available pods: 3
Mar  2 20:49:52.995: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  2 20:49:53.114: INFO: Wrong image for pod: daemon-set-8qjxq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:49:53.114: INFO: Wrong image for pod: daemon-set-l4lkw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:49:54.146: INFO: Wrong image for pod: daemon-set-8qjxq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:49:54.146: INFO: Wrong image for pod: daemon-set-l4lkw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:49:55.178: INFO: Wrong image for pod: daemon-set-8qjxq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:49:55.178: INFO: Wrong image for pod: daemon-set-l4lkw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:49:56.148: INFO: Wrong image for pod: daemon-set-8qjxq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:49:56.148: INFO: Pod daemon-set-c6j88 is not available
Mar  2 20:49:56.148: INFO: Wrong image for pod: daemon-set-l4lkw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:49:57.147: INFO: Wrong image for pod: daemon-set-8qjxq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:49:57.147: INFO: Pod daemon-set-c6j88 is not available
Mar  2 20:49:57.147: INFO: Wrong image for pod: daemon-set-l4lkw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:49:58.147: INFO: Wrong image for pod: daemon-set-8qjxq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:49:59.148: INFO: Wrong image for pod: daemon-set-8qjxq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:50:00.146: INFO: Pod daemon-set-8jr4f is not available
Mar  2 20:50:00.146: INFO: Wrong image for pod: daemon-set-8qjxq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar  2 20:50:03.147: INFO: Pod daemon-set-hr59m is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  2 20:50:03.192: INFO: Number of nodes with available pods: 2
Mar  2 20:50:03.192: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 20:50:04.236: INFO: Number of nodes with available pods: 2
Mar  2 20:50:04.236: INFO: Node 10.245.0.6 is running more than one daemon pod
Mar  2 20:50:05.237: INFO: Number of nodes with available pods: 3
Mar  2 20:50:05.237: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6969, will wait for the garbage collector to delete the pods
Mar  2 20:50:05.411: INFO: Deleting DaemonSet.extensions daemon-set took: 23.014109ms
Mar  2 20:50:05.511: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.229436ms
Mar  2 20:50:07.634: INFO: Number of nodes with available pods: 0
Mar  2 20:50:07.634: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 20:50:07.650: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42383"},"items":null}

Mar  2 20:50:07.666: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42383"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:50:07.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6969" for this suite.

• [SLOW TEST:18.274 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":274,"skipped":5054,"failed":0}
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:50:07.764: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9095
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:50:08.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9095" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":275,"skipped":5054,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:50:08.195: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6091
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:50:08.485: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  2 20:50:13.505: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 20:50:13.505: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  2 20:50:15.525: INFO: Creating deployment "test-rollover-deployment"
Mar  2 20:50:15.557: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  2 20:50:17.586: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  2 20:50:17.623: INFO: Ensure that both replica sets have 1 created replica
Mar  2 20:50:17.644: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  2 20:50:17.672: INFO: Updating deployment test-rollover-deployment
Mar  2 20:50:17.672: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  2 20:50:19.712: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  2 20:50:19.752: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  2 20:50:19.849: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 20:50:19.849: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851017, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:50:21.877: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 20:50:21.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851020, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:50:23.904: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 20:50:23.904: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851020, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:50:25.875: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 20:50:25.875: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851020, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:50:27.877: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 20:50:27.878: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851020, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:50:29.881: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 20:50:29.882: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851020, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851015, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 20:50:31.889: INFO: 
Mar  2 20:50:31.889: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar  2 20:50:32.006: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6091  08a7523a-327d-4116-8519-e24746de5681 42568 2 2022-03-02 20:50:15 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-03-02 20:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 20:50:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d8b6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-02 20:50:15 +0000 UTC,LastTransitionTime:2022-03-02 20:50:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2022-03-02 20:50:30 +0000 UTC,LastTransitionTime:2022-03-02 20:50:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 20:50:32.042: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-6091  6cf126e1-7942-4e86-9115-356a4abae1c8 42558 2 2022-03-02 20:50:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 08a7523a-327d-4116-8519-e24746de5681 0xc005ac87f0 0xc005ac87f1}] []  [{kube-controller-manager Update apps/v1 2022-03-02 20:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08a7523a-327d-4116-8519-e24746de5681\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 20:50:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac8888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 20:50:32.042: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  2 20:50:32.042: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6091  ef0989ca-3948-4ace-8d47-f0a27a4138a2 42567 2 2022-03-02 20:50:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 08a7523a-327d-4116-8519-e24746de5681 0xc005ac85a7 0xc005ac85a8}] []  [{e2e.test Update apps/v1 2022-03-02 20:50:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 20:50:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08a7523a-327d-4116-8519-e24746de5681\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-03-02 20:50:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005ac8668 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 20:50:32.042: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-6091  c22a2754-921e-4e59-8b6f-165b3cf38418 42522 2 2022-03-02 20:50:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 08a7523a-327d-4116-8519-e24746de5681 0xc005ac86d7 0xc005ac86d8}] []  [{kube-controller-manager Update apps/v1 2022-03-02 20:50:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08a7523a-327d-4116-8519-e24746de5681\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 20:50:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac8788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 20:50:32.090: INFO: Pod "test-rollover-deployment-98c5f4599-rsxgn" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-rsxgn test-rollover-deployment-98c5f4599- deployment-6091  363383b3-7ac2-493d-acf7-053ea67dd296 42540 0 2022-03-02 20:50:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/containerID:6aa3b6716a279cc0b40d9e8f013c9656d48ac03ae44c7f6290749d7cdfcb306a cni.projectcalico.org/podIP:172.17.100.179/32 cni.projectcalico.org/podIPs:172.17.100.179/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 6cf126e1-7942-4e86-9115-356a4abae1c8 0xc005ac8de0 0xc005ac8de1}] []  [{kube-controller-manager Update v1 2022-03-02 20:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cf126e1-7942-4e86-9115-356a4abae1c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 20:50:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 20:50:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.100.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mnsw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mnsw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:50:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:50:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:50:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:50:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.4,PodIP:172.17.100.179,StartTime:2022-03-02 20:50:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 20:50:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://ecd7917d29ecbe7ccee4959bbd4330e21725253cb886f0b427d6b3c7f8942d29,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.100.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:50:32.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6091" for this suite.

• [SLOW TEST:23.943 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":276,"skipped":5063,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:50:32.138: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Mar  2 20:50:32.453: INFO: The status of Pod labelsupdate37c930a3-5ab6-4c16-bbd6-3c4940a18bc4 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:50:34.469: INFO: The status of Pod labelsupdate37c930a3-5ab6-4c16-bbd6-3c4940a18bc4 is Running (Ready = true)
Mar  2 20:50:35.099: INFO: Successfully updated pod "labelsupdate37c930a3-5ab6-4c16-bbd6-3c4940a18bc4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:50:37.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1643" for this suite.

• [SLOW TEST:5.067 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":277,"skipped":5074,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:50:37.206: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:50:37.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4264" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":278,"skipped":5137,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:50:37.580: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3732
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:51:06.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3732" for this suite.

• [SLOW TEST:28.594 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":279,"skipped":5141,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:51:06.174: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2111
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-cb13ab52-4c8a-4c8a-91a2-8458116c5e2b
STEP: Creating a pod to test consume configMaps
Mar  2 20:51:06.615: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f908a048-3456-4b8a-9f03-9f875f5b3533" in namespace "projected-2111" to be "Succeeded or Failed"
Mar  2 20:51:06.630: INFO: Pod "pod-projected-configmaps-f908a048-3456-4b8a-9f03-9f875f5b3533": Phase="Pending", Reason="", readiness=false. Elapsed: 14.399154ms
Mar  2 20:51:08.650: INFO: Pod "pod-projected-configmaps-f908a048-3456-4b8a-9f03-9f875f5b3533": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034570751s
STEP: Saw pod success
Mar  2 20:51:08.650: INFO: Pod "pod-projected-configmaps-f908a048-3456-4b8a-9f03-9f875f5b3533" satisfied condition "Succeeded or Failed"
Mar  2 20:51:08.662: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-configmaps-f908a048-3456-4b8a-9f03-9f875f5b3533 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 20:51:08.719: INFO: Waiting for pod pod-projected-configmaps-f908a048-3456-4b8a-9f03-9f875f5b3533 to disappear
Mar  2 20:51:08.732: INFO: Pod pod-projected-configmaps-f908a048-3456-4b8a-9f03-9f875f5b3533 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:51:08.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2111" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":280,"skipped":5148,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:51:08.774: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5137
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar  2 20:51:09.054: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 20:51:09.108: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 20:51:09.119: INFO: 
Logging pods the apiserver thinks is on node 10.245.0.4 before test
Mar  2 20:51:09.142: INFO: calico-node-6bmpc from kube-system started at 2022-03-02 17:40:12 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.142: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 20:51:09.142: INFO: calico-typha-7cdb864b94-kg6vk from kube-system started at 2022-03-02 20:01:16 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.142: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 20:51:09.142: INFO: ibm-master-proxy-static-10.245.0.4 from kube-system started at 2022-03-02 17:39:53 +0000 UTC (2 container statuses recorded)
Mar  2 20:51:09.143: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 20:51:09.143: INFO: 	Container pause ready: true, restart count 0
Mar  2 20:51:09.143: INFO: ibm-vpc-block-csi-node-m9dxv from kube-system started at 2022-03-02 17:40:12 +0000 UTC (4 container statuses recorded)
Mar  2 20:51:09.143: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Mar  2 20:51:09.143: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Mar  2 20:51:09.143: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 20:51:09.143: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 20:51:09.143: INFO: konnectivity-agent-cnbg9 from kube-system started at 2022-03-02 17:48:31 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.143: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  2 20:51:09.143: INFO: sonobuoy from sonobuoy started at 2022-03-02 19:20:48 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.143: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 20:51:09.143: INFO: sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-867x2 from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 20:51:09.143: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 20:51:09.143: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 20:51:09.143: INFO: 
Logging pods the apiserver thinks is on node 10.245.0.5 before test
Mar  2 20:51:09.173: INFO: catalog-operator-6c4b4d7c9-gnzj6 from ibm-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 20:51:09.173: INFO: olm-operator-785cdc5884-8vxvx from ibm-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 20:51:09.173: INFO: calico-kube-controllers-dcd8d986c-g4lwp from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 20:51:09.173: INFO: calico-node-7n2gc from kube-system started at 2022-03-02 17:38:33 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 20:51:09.173: INFO: calico-typha-7cdb864b94-t4bmt from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 20:51:09.173: INFO: coredns-autoscaler-689fb74d49-hqj9c from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container autoscaler ready: true, restart count 0
Mar  2 20:51:09.173: INFO: coredns-b58d5f584-m7b7m from kube-system started at 2022-03-02 17:49:06 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container coredns ready: true, restart count 0
Mar  2 20:51:09.173: INFO: coredns-b58d5f584-s8nv6 from kube-system started at 2022-03-02 20:00:39 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container coredns ready: true, restart count 0
Mar  2 20:51:09.173: INFO: dashboard-metrics-scraper-6747f89c97-4dq9w from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar  2 20:51:09.173: INFO: ibm-master-proxy-static-10.245.0.5 from kube-system started at 2022-03-02 17:38:18 +0000 UTC (2 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 20:51:09.173: INFO: 	Container pause ready: true, restart count 0
Mar  2 20:51:09.173: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2022-03-02 17:38:51 +0000 UTC (6 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container csi-attacher ready: true, restart count 0
Mar  2 20:51:09.173: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar  2 20:51:09.173: INFO: 	Container csi-resizer ready: true, restart count 0
Mar  2 20:51:09.173: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Mar  2 20:51:09.173: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 20:51:09.173: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 20:51:09.173: INFO: ibm-vpc-block-csi-node-n5lr5 from kube-system started at 2022-03-02 17:38:33 +0000 UTC (4 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Mar  2 20:51:09.173: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Mar  2 20:51:09.173: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 20:51:09.173: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 20:51:09.173: INFO: konnectivity-agent-gkp9m from kube-system started at 2022-03-02 17:48:34 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  2 20:51:09.173: INFO: kubernetes-dashboard-54c47dd995-vkxsd from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar  2 20:51:09.173: INFO: public-crc8fqf88s0ubnto7pbi70-alb1-6559fd4c4b-kzxmb from kube-system started at 2022-03-02 17:38:51 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  2 20:51:09.173: INFO: sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-4g544 from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 20:51:09.173: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 20:51:09.173: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 20:51:09.173: INFO: 
Logging pods the apiserver thinks is on node 10.245.0.6 before test
Mar  2 20:51:09.195: INFO: calico-node-tl6xm from kube-system started at 2022-03-02 17:38:42 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.195: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 20:51:09.195: INFO: calico-typha-7cdb864b94-znnrz from kube-system started at 2022-03-02 17:39:02 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.195: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 20:51:09.195: INFO: coredns-b58d5f584-sdfgx from kube-system started at 2022-03-02 17:49:06 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.195: INFO: 	Container coredns ready: true, restart count 0
Mar  2 20:51:09.195: INFO: ibm-master-proxy-static-10.245.0.6 from kube-system started at 2022-03-02 17:38:23 +0000 UTC (2 container statuses recorded)
Mar  2 20:51:09.195: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 20:51:09.195: INFO: 	Container pause ready: true, restart count 0
Mar  2 20:51:09.195: INFO: ibm-vpc-block-csi-node-nkxms from kube-system started at 2022-03-02 17:38:42 +0000 UTC (4 container statuses recorded)
Mar  2 20:51:09.195: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Mar  2 20:51:09.195: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Mar  2 20:51:09.195: INFO: 	Container liveness-probe ready: true, restart count 0
Mar  2 20:51:09.195: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 20:51:09.195: INFO: konnectivity-agent-bnglr from kube-system started at 2022-03-02 17:48:28 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.195: INFO: 	Container konnectivity-agent ready: true, restart count 0
Mar  2 20:51:09.195: INFO: metrics-server-7b97867cc5-fpb5z from kube-system started at 2022-03-02 18:23:06 +0000 UTC (3 container statuses recorded)
Mar  2 20:51:09.195: INFO: 	Container config-watcher ready: true, restart count 0
Mar  2 20:51:09.195: INFO: 	Container metrics-server ready: true, restart count 0
Mar  2 20:51:09.195: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar  2 20:51:09.195: INFO: public-crc8fqf88s0ubnto7pbi70-alb1-6559fd4c4b-lwmtq from kube-system started at 2022-03-02 17:39:02 +0000 UTC (1 container statuses recorded)
Mar  2 20:51:09.195: INFO: 	Container nginx-ingress ready: true, restart count 0
Mar  2 20:51:09.195: INFO: sonobuoy-e2e-job-6fc4581698214e95 from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 20:51:09.196: INFO: 	Container e2e ready: true, restart count 0
Mar  2 20:51:09.196: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 20:51:09.196: INFO: sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-f24tq from sonobuoy started at 2022-03-02 19:20:58 +0000 UTC (2 container statuses recorded)
Mar  2 20:51:09.196: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 20:51:09.196: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node 10.245.0.4
STEP: verifying the node has the label node 10.245.0.5
STEP: verifying the node has the label node 10.245.0.6
Mar  2 20:51:09.360: INFO: Pod catalog-operator-6c4b4d7c9-gnzj6 requesting resource cpu=10m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod olm-operator-785cdc5884-8vxvx requesting resource cpu=10m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod calico-kube-controllers-dcd8d986c-g4lwp requesting resource cpu=10m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod calico-node-6bmpc requesting resource cpu=250m on Node 10.245.0.4
Mar  2 20:51:09.360: INFO: Pod calico-node-7n2gc requesting resource cpu=250m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod calico-node-tl6xm requesting resource cpu=250m on Node 10.245.0.6
Mar  2 20:51:09.360: INFO: Pod calico-typha-7cdb864b94-kg6vk requesting resource cpu=250m on Node 10.245.0.4
Mar  2 20:51:09.360: INFO: Pod calico-typha-7cdb864b94-t4bmt requesting resource cpu=250m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod calico-typha-7cdb864b94-znnrz requesting resource cpu=250m on Node 10.245.0.6
Mar  2 20:51:09.360: INFO: Pod coredns-autoscaler-689fb74d49-hqj9c requesting resource cpu=20m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod coredns-b58d5f584-m7b7m requesting resource cpu=100m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod coredns-b58d5f584-s8nv6 requesting resource cpu=100m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod coredns-b58d5f584-sdfgx requesting resource cpu=100m on Node 10.245.0.6
Mar  2 20:51:09.360: INFO: Pod dashboard-metrics-scraper-6747f89c97-4dq9w requesting resource cpu=1m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod ibm-master-proxy-static-10.245.0.4 requesting resource cpu=25m on Node 10.245.0.4
Mar  2 20:51:09.360: INFO: Pod ibm-master-proxy-static-10.245.0.5 requesting resource cpu=25m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod ibm-master-proxy-static-10.245.0.6 requesting resource cpu=25m on Node 10.245.0.6
Mar  2 20:51:09.360: INFO: Pod ibm-vpc-block-csi-controller-0 requesting resource cpu=138m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod ibm-vpc-block-csi-node-m9dxv requesting resource cpu=48m on Node 10.245.0.4
Mar  2 20:51:09.360: INFO: Pod ibm-vpc-block-csi-node-n5lr5 requesting resource cpu=48m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod ibm-vpc-block-csi-node-nkxms requesting resource cpu=48m on Node 10.245.0.6
Mar  2 20:51:09.360: INFO: Pod konnectivity-agent-bnglr requesting resource cpu=10m on Node 10.245.0.6
Mar  2 20:51:09.360: INFO: Pod konnectivity-agent-cnbg9 requesting resource cpu=10m on Node 10.245.0.4
Mar  2 20:51:09.360: INFO: Pod konnectivity-agent-gkp9m requesting resource cpu=10m on Node 10.245.0.5
Mar  2 20:51:09.360: INFO: Pod kubernetes-dashboard-54c47dd995-vkxsd requesting resource cpu=50m on Node 10.245.0.5
Mar  2 20:51:09.361: INFO: Pod metrics-server-7b97867cc5-fpb5z requesting resource cpu=126m on Node 10.245.0.6
Mar  2 20:51:09.361: INFO: Pod public-crc8fqf88s0ubnto7pbi70-alb1-6559fd4c4b-kzxmb requesting resource cpu=10m on Node 10.245.0.5
Mar  2 20:51:09.361: INFO: Pod public-crc8fqf88s0ubnto7pbi70-alb1-6559fd4c4b-lwmtq requesting resource cpu=10m on Node 10.245.0.6
Mar  2 20:51:09.361: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.245.0.4
Mar  2 20:51:09.361: INFO: Pod sonobuoy-e2e-job-6fc4581698214e95 requesting resource cpu=0m on Node 10.245.0.6
Mar  2 20:51:09.361: INFO: Pod sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-4g544 requesting resource cpu=0m on Node 10.245.0.5
Mar  2 20:51:09.361: INFO: Pod sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-867x2 requesting resource cpu=0m on Node 10.245.0.4
Mar  2 20:51:09.361: INFO: Pod sonobuoy-systemd-logs-daemon-set-ce7d98c0dd674824-f24tq requesting resource cpu=0m on Node 10.245.0.6
STEP: Starting Pods to consume most of the cluster CPU.
Mar  2 20:51:09.361: INFO: Creating a pod which consumes cpu=935m on Node 10.245.0.4
Mar  2 20:51:09.384: INFO: Creating a pod which consumes cpu=621m on Node 10.245.0.5
Mar  2 20:51:09.399: INFO: Creating a pod which consumes cpu=770m on Node 10.245.0.6
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4cd71bce-4c83-44cd-a428-2a4a9d1d58aa.16d8ab8929581fbd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5137/filler-pod-4cd71bce-4c83-44cd-a428-2a4a9d1d58aa to 10.245.0.4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4cd71bce-4c83-44cd-a428-2a4a9d1d58aa.16d8ab896cce3317], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4cd71bce-4c83-44cd-a428-2a4a9d1d58aa.16d8ab8972aca14f], Reason = [Created], Message = [Created container filler-pod-4cd71bce-4c83-44cd-a428-2a4a9d1d58aa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4cd71bce-4c83-44cd-a428-2a4a9d1d58aa.16d8ab897bcb754a], Reason = [Started], Message = [Started container filler-pod-4cd71bce-4c83-44cd-a428-2a4a9d1d58aa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-759cd2ac-4c5a-4e1e-83e9-01c45baa2d6a.16d8ab892b305e4d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5137/filler-pod-759cd2ac-4c5a-4e1e-83e9-01c45baa2d6a to 10.245.0.6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-759cd2ac-4c5a-4e1e-83e9-01c45baa2d6a.16d8ab897a8547fa], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-759cd2ac-4c5a-4e1e-83e9-01c45baa2d6a.16d8ab897ed77e72], Reason = [Created], Message = [Created container filler-pod-759cd2ac-4c5a-4e1e-83e9-01c45baa2d6a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-759cd2ac-4c5a-4e1e-83e9-01c45baa2d6a.16d8ab8986421a1e], Reason = [Started], Message = [Started container filler-pod-759cd2ac-4c5a-4e1e-83e9-01c45baa2d6a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6e16128-49a7-4377-9421-d25dde00b2dc.16d8ab892a2ee315], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5137/filler-pod-e6e16128-49a7-4377-9421-d25dde00b2dc to 10.245.0.5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6e16128-49a7-4377-9421-d25dde00b2dc.16d8ab896ae86495], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6e16128-49a7-4377-9421-d25dde00b2dc.16d8ab8970f16e71], Reason = [Created], Message = [Created container filler-pod-e6e16128-49a7-4377-9421-d25dde00b2dc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6e16128-49a7-4377-9421-d25dde00b2dc.16d8ab8978f0de32], Reason = [Started], Message = [Started container filler-pod-e6e16128-49a7-4377-9421-d25dde00b2dc]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16d8ab8a1f296489], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.245.0.4
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.245.0.5
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.245.0.6
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:51:14.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5137" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.967 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":281,"skipped":5151,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:51:14.741: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1469
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-647380a7-dbce-448f-a2fa-6ab520f4efee
STEP: Creating secret with name s-test-opt-upd-444015b0-b466-45f4-b720-97aee0a18b6a
STEP: Creating the pod
Mar  2 20:51:15.187: INFO: The status of Pod pod-secrets-aeb144d3-12a7-4454-a408-b80569cd45b4 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:51:17.206: INFO: The status of Pod pod-secrets-aeb144d3-12a7-4454-a408-b80569cd45b4 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:51:19.211: INFO: The status of Pod pod-secrets-aeb144d3-12a7-4454-a408-b80569cd45b4 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-647380a7-dbce-448f-a2fa-6ab520f4efee
STEP: Updating secret s-test-opt-upd-444015b0-b466-45f4-b720-97aee0a18b6a
STEP: Creating secret with name s-test-opt-create-343d0ff7-3225-490e-a864-a54e3f16931d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:52:22.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1469" for this suite.

• [SLOW TEST:68.107 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":282,"skipped":5161,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:52:22.849: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 20:52:23.216: INFO: Number of nodes with available pods: 0
Mar  2 20:52:23.216: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:52:24.252: INFO: Number of nodes with available pods: 0
Mar  2 20:52:24.252: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:52:25.267: INFO: Number of nodes with available pods: 1
Mar  2 20:52:25.267: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:52:26.283: INFO: Number of nodes with available pods: 3
Mar  2 20:52:26.283: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  2 20:52:26.389: INFO: Number of nodes with available pods: 2
Mar  2 20:52:26.389: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:52:27.437: INFO: Number of nodes with available pods: 2
Mar  2 20:52:27.438: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:52:28.429: INFO: Number of nodes with available pods: 2
Mar  2 20:52:28.429: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:52:29.427: INFO: Number of nodes with available pods: 2
Mar  2 20:52:29.427: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:52:30.419: INFO: Number of nodes with available pods: 2
Mar  2 20:52:30.419: INFO: Node 10.245.0.4 is running more than one daemon pod
Mar  2 20:52:31.418: INFO: Number of nodes with available pods: 3
Mar  2 20:52:31.418: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7926, will wait for the garbage collector to delete the pods
Mar  2 20:52:31.516: INFO: Deleting DaemonSet.extensions daemon-set took: 23.944404ms
Mar  2 20:52:31.717: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.344398ms
Mar  2 20:52:33.684: INFO: Number of nodes with available pods: 0
Mar  2 20:52:33.684: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 20:52:33.695: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43167"},"items":null}

Mar  2 20:52:33.709: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43167"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:52:33.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7926" for this suite.

• [SLOW TEST:10.936 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":283,"skipped":5163,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:52:33.785: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Mar  2 20:52:34.069: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 20:52:34.069: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 20:52:34.095: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 20:52:34.095: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 20:52:34.115: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 20:52:34.115: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 20:52:34.186: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 20:52:34.186: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 20:52:35.915: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  2 20:52:35.915: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  2 20:52:36.606: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Mar  2 20:52:36.633: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Mar  2 20:52:36.637: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0
Mar  2 20:52:36.637: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0
Mar  2 20:52:36.637: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0
Mar  2 20:52:36.637: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0
Mar  2 20:52:36.637: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0
Mar  2 20:52:36.637: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0
Mar  2 20:52:36.637: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0
Mar  2 20:52:36.637: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 0
Mar  2 20:52:36.638: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
Mar  2 20:52:36.638: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
Mar  2 20:52:36.638: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:36.638: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:36.638: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:36.638: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:36.718: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:36.718: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:36.757: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:36.757: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:36.801: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
Mar  2 20:52:36.801: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
Mar  2 20:52:36.815: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
Mar  2 20:52:36.815: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
Mar  2 20:52:38.691: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:38.691: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:38.751: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
STEP: listing Deployments
Mar  2 20:52:38.771: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Mar  2 20:52:38.794: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Mar  2 20:52:38.814: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 20:52:38.822: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 20:52:38.875: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 20:52:38.876: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 20:52:38.916: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 20:52:38.933: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 20:52:40.604: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 20:52:40.633: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 20:52:40.642: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 20:52:40.653: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 20:52:40.682: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 20:52:42.911: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Mar  2 20:52:43.030: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
Mar  2 20:52:43.030: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
Mar  2 20:52:43.031: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
Mar  2 20:52:43.031: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
Mar  2 20:52:43.031: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
Mar  2 20:52:43.031: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 1
Mar  2 20:52:43.031: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:43.031: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:43.031: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:43.031: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:43.031: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 2
Mar  2 20:52:43.032: INFO: observed Deployment test-deployment in namespace deployment-8417 with ReadyReplicas 3
STEP: deleting the Deployment
Mar  2 20:52:43.069: INFO: observed event type MODIFIED
Mar  2 20:52:43.069: INFO: observed event type MODIFIED
Mar  2 20:52:43.070: INFO: observed event type MODIFIED
Mar  2 20:52:43.070: INFO: observed event type MODIFIED
Mar  2 20:52:43.070: INFO: observed event type MODIFIED
Mar  2 20:52:43.070: INFO: observed event type MODIFIED
Mar  2 20:52:43.070: INFO: observed event type MODIFIED
Mar  2 20:52:43.070: INFO: observed event type MODIFIED
Mar  2 20:52:43.070: INFO: observed event type MODIFIED
Mar  2 20:52:43.070: INFO: observed event type MODIFIED
Mar  2 20:52:43.070: INFO: observed event type MODIFIED
Mar  2 20:52:43.071: INFO: observed event type MODIFIED
Mar  2 20:52:43.071: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar  2 20:52:43.085: INFO: Log out all the ReplicaSets if there is no deployment created
Mar  2 20:52:43.100: INFO: ReplicaSet "test-deployment-855f7994f9":
&ReplicaSet{ObjectMeta:{test-deployment-855f7994f9  deployment-8417  44ce3960-1795-4caf-ba05-3db4f11be5f0 43257 3 2022-03-02 20:52:34 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6c72f86f-3fb6-4dd7-875d-5f6c25969443 0xc00442cae7 0xc00442cae8}] []  [{kube-controller-manager Update apps/v1 2022-03-02 20:52:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c72f86f-3fb6-4dd7-875d-5f6c25969443\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 20:52:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 855f7994f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00442cb70 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar  2 20:52:43.112: INFO: ReplicaSet "test-deployment-d4dfddfbf":
&ReplicaSet{ObjectMeta:{test-deployment-d4dfddfbf  deployment-8417  f58d5bab-fd32-4c7d-b2d5-62d3d4ffede0 43379 2 2022-03-02 20:52:38 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6c72f86f-3fb6-4dd7-875d-5f6c25969443 0xc00442cbd7 0xc00442cbd8}] []  [{kube-controller-manager Update apps/v1 2022-03-02 20:52:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c72f86f-3fb6-4dd7-875d-5f6c25969443\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 20:52:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d4dfddfbf,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00442cc60 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Mar  2 20:52:43.124: INFO: pod: "test-deployment-d4dfddfbf-k6qfs":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-k6qfs test-deployment-d4dfddfbf- deployment-8417  3976992a-06a6-4e66-b5b7-94e888532b98 43378 0 2022-03-02 20:52:40 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:8cbd505ba7592fb975b1ccd37568dec980b62b623071bdd0b823704115210872 cni.projectcalico.org/podIP:172.17.125.252/32 cni.projectcalico.org/podIPs:172.17.125.252/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf f58d5bab-fd32-4c7d-b2d5-62d3d4ffede0 0xc00442d297 0xc00442d298}] []  [{kube-controller-manager Update v1 2022-03-02 20:52:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f58d5bab-fd32-4c7d-b2d5-62d3d4ffede0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 20:52:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 20:52:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.125.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4kr8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4kr8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:52:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:52:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:52:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:52:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.5,PodIP:172.17.125.252,StartTime:2022-03-02 20:52:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 20:52:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://067077108041b48396b4a31f5bdd193519e4642282110df78f6739c49a6fdc4b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.125.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  2 20:52:43.125: INFO: pod: "test-deployment-d4dfddfbf-rjn7s":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-rjn7s test-deployment-d4dfddfbf- deployment-8417  dcbd90f9-73ad-4aa2-ae22-db420a6b83e2 43344 0 2022-03-02 20:52:38 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:e7afa0462cc8e229009c9f0a642be722f8c17178ffc3834ffbbef4a1a21baad7 cni.projectcalico.org/podIP:172.17.100.189/32 cni.projectcalico.org/podIPs:172.17.100.189/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf f58d5bab-fd32-4c7d-b2d5-62d3d4ffede0 0xc00442d8b7 0xc00442d8b8}] []  [{kube-controller-manager Update v1 2022-03-02 20:52:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f58d5bab-fd32-4c7d-b2d5-62d3d4ffede0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 20:52:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 20:52:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.100.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5zqkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5zqkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:52:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:52:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:52:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:52:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.4,PodIP:172.17.100.189,StartTime:2022-03-02 20:52:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 20:52:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://4ebba12d5199443048c43fd4f816dd7845dc85c439c8d91d21afe4d5128670a5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.100.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:52:43.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8417" for this suite.

• [SLOW TEST:9.379 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":284,"skipped":5168,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:52:43.165: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-878
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  2 20:52:43.416: INFO: Waiting up to 5m0s for pod "pod-803d1870-0430-465d-9d27-8438465f01ac" in namespace "emptydir-878" to be "Succeeded or Failed"
Mar  2 20:52:43.429: INFO: Pod "pod-803d1870-0430-465d-9d27-8438465f01ac": Phase="Pending", Reason="", readiness=false. Elapsed: 13.119868ms
Mar  2 20:52:45.445: INFO: Pod "pod-803d1870-0430-465d-9d27-8438465f01ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028713367s
Mar  2 20:52:47.460: INFO: Pod "pod-803d1870-0430-465d-9d27-8438465f01ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043901533s
STEP: Saw pod success
Mar  2 20:52:47.460: INFO: Pod "pod-803d1870-0430-465d-9d27-8438465f01ac" satisfied condition "Succeeded or Failed"
Mar  2 20:52:47.473: INFO: Trying to get logs from node 10.245.0.4 pod pod-803d1870-0430-465d-9d27-8438465f01ac container test-container: <nil>
STEP: delete the pod
Mar  2 20:52:47.610: INFO: Waiting for pod pod-803d1870-0430-465d-9d27-8438465f01ac to disappear
Mar  2 20:52:47.623: INFO: Pod pod-803d1870-0430-465d-9d27-8438465f01ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:52:47.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-878" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":5171,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:52:47.664: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4844
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:52:48.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4844" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":286,"skipped":5177,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:52:48.096: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar  2 20:52:58.682: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0302 20:52:58.682203      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 20:52:58.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cckr" in namespace "gc-4836"
Mar  2 20:52:58.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-84v7p" in namespace "gc-4836"
Mar  2 20:52:58.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-bw2rg" in namespace "gc-4836"
Mar  2 20:52:58.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmr55" in namespace "gc-4836"
Mar  2 20:52:58.872: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhj2g" in namespace "gc-4836"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:52:58.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4836" for this suite.

• [SLOW TEST:10.884 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":287,"skipped":5186,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:52:58.980: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:53:15.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-342" for this suite.

• [SLOW TEST:16.744 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":288,"skipped":5197,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:53:15.724: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6981
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  2 20:53:16.064: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6981  2e1a510f-47f1-4ee7-bf58-523216e4b8c4 43902 0 2022-03-02 20:53:15 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-03-02 20:53:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 20:53:16.064: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6981  2e1a510f-47f1-4ee7-bf58-523216e4b8c4 43903 0 2022-03-02 20:53:15 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-03-02 20:53:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:53:16.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6981" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":289,"skipped":5215,"failed":0}
SSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:53:16.126: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6757
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:53:16.388: INFO: Creating pod...
Mar  2 20:53:16.429: INFO: Pod Quantity: 1 Status: Pending
Mar  2 20:53:17.443: INFO: Pod Quantity: 1 Status: Pending
Mar  2 20:53:18.445: INFO: Pod Quantity: 1 Status: Pending
Mar  2 20:53:19.443: INFO: Pod Status: Running
Mar  2 20:53:19.443: INFO: Creating service...
Mar  2 20:53:19.467: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/pods/agnhost/proxy/some/path/with/DELETE
Mar  2 20:53:19.512: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 20:53:19.512: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/pods/agnhost/proxy/some/path/with/GET
Mar  2 20:53:19.526: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar  2 20:53:19.526: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/pods/agnhost/proxy/some/path/with/HEAD
Mar  2 20:53:19.542: INFO: http.Client request:HEAD | StatusCode:200
Mar  2 20:53:19.542: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/pods/agnhost/proxy/some/path/with/OPTIONS
Mar  2 20:53:19.557: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 20:53:19.557: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/pods/agnhost/proxy/some/path/with/PATCH
Mar  2 20:53:19.571: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 20:53:19.571: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/pods/agnhost/proxy/some/path/with/POST
Mar  2 20:53:19.586: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 20:53:19.586: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/pods/agnhost/proxy/some/path/with/PUT
Mar  2 20:53:19.607: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  2 20:53:19.608: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/services/test-service/proxy/some/path/with/DELETE
Mar  2 20:53:19.631: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 20:53:19.631: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/services/test-service/proxy/some/path/with/GET
Mar  2 20:53:19.654: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar  2 20:53:19.654: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/services/test-service/proxy/some/path/with/HEAD
Mar  2 20:53:19.676: INFO: http.Client request:HEAD | StatusCode:200
Mar  2 20:53:19.677: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/services/test-service/proxy/some/path/with/OPTIONS
Mar  2 20:53:19.699: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 20:53:19.699: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/services/test-service/proxy/some/path/with/PATCH
Mar  2 20:53:19.721: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 20:53:19.722: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/services/test-service/proxy/some/path/with/POST
Mar  2 20:53:19.802: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 20:53:19.802: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6757/services/test-service/proxy/some/path/with/PUT
Mar  2 20:53:19.848: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:53:19.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6757" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":290,"skipped":5220,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:53:19.884: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1495
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-fdfdbf8d-4b05-4792-b1fa-41faeb92eccc
STEP: Creating secret with name s-test-opt-upd-2bd16ff9-3c9e-47ee-a113-5a87484a67e7
STEP: Creating the pod
Mar  2 20:53:20.245: INFO: The status of Pod pod-projected-secrets-3d62228b-7ccd-4624-beaa-6c96e6d4a780 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:53:22.260: INFO: The status of Pod pod-projected-secrets-3d62228b-7ccd-4624-beaa-6c96e6d4a780 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:53:24.261: INFO: The status of Pod pod-projected-secrets-3d62228b-7ccd-4624-beaa-6c96e6d4a780 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-fdfdbf8d-4b05-4792-b1fa-41faeb92eccc
STEP: Updating secret s-test-opt-upd-2bd16ff9-3c9e-47ee-a113-5a87484a67e7
STEP: Creating secret with name s-test-opt-create-ee9f14fb-cc5c-4aa7-917e-833e12a534d9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:54:31.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1495" for this suite.

• [SLOW TEST:71.737 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5228,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:54:31.622: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  2 20:54:31.891: INFO: Waiting up to 5m0s for pod "pod-e48c6013-6d20-463a-a749-885129515abd" in namespace "emptydir-670" to be "Succeeded or Failed"
Mar  2 20:54:31.903: INFO: Pod "pod-e48c6013-6d20-463a-a749-885129515abd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.323348ms
Mar  2 20:54:33.920: INFO: Pod "pod-e48c6013-6d20-463a-a749-885129515abd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029460487s
STEP: Saw pod success
Mar  2 20:54:33.920: INFO: Pod "pod-e48c6013-6d20-463a-a749-885129515abd" satisfied condition "Succeeded or Failed"
Mar  2 20:54:33.936: INFO: Trying to get logs from node 10.245.0.4 pod pod-e48c6013-6d20-463a-a749-885129515abd container test-container: <nil>
STEP: delete the pod
Mar  2 20:54:34.059: INFO: Waiting for pod pod-e48c6013-6d20-463a-a749-885129515abd to disappear
Mar  2 20:54:34.071: INFO: Pod pod-e48c6013-6d20-463a-a749-885129515abd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:54:34.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-670" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":292,"skipped":5230,"failed":0}
SSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:54:34.103: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Mar  2 20:54:34.421: INFO: Waiting up to 5m0s for pod "client-containers-947e6ff2-2b3f-458f-9ce8-44b61743e548" in namespace "containers-4433" to be "Succeeded or Failed"
Mar  2 20:54:34.446: INFO: Pod "client-containers-947e6ff2-2b3f-458f-9ce8-44b61743e548": Phase="Pending", Reason="", readiness=false. Elapsed: 24.133477ms
Mar  2 20:54:36.464: INFO: Pod "client-containers-947e6ff2-2b3f-458f-9ce8-44b61743e548": Phase="Running", Reason="", readiness=true. Elapsed: 2.042479589s
Mar  2 20:54:38.481: INFO: Pod "client-containers-947e6ff2-2b3f-458f-9ce8-44b61743e548": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059292355s
STEP: Saw pod success
Mar  2 20:54:38.481: INFO: Pod "client-containers-947e6ff2-2b3f-458f-9ce8-44b61743e548" satisfied condition "Succeeded or Failed"
Mar  2 20:54:38.492: INFO: Trying to get logs from node 10.245.0.4 pod client-containers-947e6ff2-2b3f-458f-9ce8-44b61743e548 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 20:54:38.551: INFO: Waiting for pod client-containers-947e6ff2-2b3f-458f-9ce8-44b61743e548 to disappear
Mar  2 20:54:38.562: INFO: Pod client-containers-947e6ff2-2b3f-458f-9ce8-44b61743e548 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:54:38.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4433" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":293,"skipped":5233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:54:38.634: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Mar  2 20:54:38.879: INFO: Creating simple deployment test-deployment-xn86w
Mar  2 20:54:38.952: INFO: deployment "test-deployment-xn86w" doesn't have the required revision set
STEP: Getting /status
Mar  2 20:54:41.017: INFO: Deployment test-deployment-xn86w has Conditions: [{Available True 2022-03-02 20:54:40 +0000 UTC 2022-03-02 20:54:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-03-02 20:54:40 +0000 UTC 2022-03-02 20:54:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xn86w-794dd694d8" has successfully progressed.}]
STEP: updating Deployment Status
Mar  2 20:54:41.044: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851280, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851280, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851280, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851278, loc:(*time.Location)(0xa0aaf60)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-xn86w-794dd694d8\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Mar  2 20:54:41.048: INFO: Observed &Deployment event: ADDED
Mar  2 20:54:41.048: INFO: Observed Deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-02 20:54:38 +0000 UTC 2022-03-02 20:54:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xn86w-794dd694d8"}
Mar  2 20:54:41.048: INFO: Observed &Deployment event: MODIFIED
Mar  2 20:54:41.048: INFO: Observed Deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-02 20:54:38 +0000 UTC 2022-03-02 20:54:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xn86w-794dd694d8"}
Mar  2 20:54:41.048: INFO: Observed Deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-03-02 20:54:38 +0000 UTC 2022-03-02 20:54:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 20:54:41.048: INFO: Observed &Deployment event: MODIFIED
Mar  2 20:54:41.048: INFO: Observed Deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-03-02 20:54:38 +0000 UTC 2022-03-02 20:54:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 20:54:41.048: INFO: Observed Deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-02 20:54:38 +0000 UTC 2022-03-02 20:54:38 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-xn86w-794dd694d8" is progressing.}
Mar  2 20:54:41.048: INFO: Observed &Deployment event: MODIFIED
Mar  2 20:54:41.048: INFO: Observed Deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-03-02 20:54:40 +0000 UTC 2022-03-02 20:54:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 20:54:41.049: INFO: Observed Deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-02 20:54:40 +0000 UTC 2022-03-02 20:54:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xn86w-794dd694d8" has successfully progressed.}
Mar  2 20:54:41.049: INFO: Observed &Deployment event: MODIFIED
Mar  2 20:54:41.049: INFO: Observed Deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-03-02 20:54:40 +0000 UTC 2022-03-02 20:54:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 20:54:41.049: INFO: Observed Deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-02 20:54:40 +0000 UTC 2022-03-02 20:54:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xn86w-794dd694d8" has successfully progressed.}
Mar  2 20:54:41.049: INFO: Found Deployment test-deployment-xn86w in namespace deployment-1984 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 20:54:41.049: INFO: Deployment test-deployment-xn86w has an updated status
STEP: patching the Statefulset Status
Mar  2 20:54:41.049: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  2 20:54:41.064: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Mar  2 20:54:41.067: INFO: Observed &Deployment event: ADDED
Mar  2 20:54:41.067: INFO: Observed deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-02 20:54:38 +0000 UTC 2022-03-02 20:54:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xn86w-794dd694d8"}
Mar  2 20:54:41.068: INFO: Observed &Deployment event: MODIFIED
Mar  2 20:54:41.068: INFO: Observed deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-02 20:54:38 +0000 UTC 2022-03-02 20:54:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-xn86w-794dd694d8"}
Mar  2 20:54:41.068: INFO: Observed deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-03-02 20:54:38 +0000 UTC 2022-03-02 20:54:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 20:54:41.068: INFO: Observed &Deployment event: MODIFIED
Mar  2 20:54:41.068: INFO: Observed deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-03-02 20:54:38 +0000 UTC 2022-03-02 20:54:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 20:54:41.068: INFO: Observed deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-02 20:54:38 +0000 UTC 2022-03-02 20:54:38 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-xn86w-794dd694d8" is progressing.}
Mar  2 20:54:41.068: INFO: Observed &Deployment event: MODIFIED
Mar  2 20:54:41.068: INFO: Observed deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-03-02 20:54:40 +0000 UTC 2022-03-02 20:54:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 20:54:41.068: INFO: Observed deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-02 20:54:40 +0000 UTC 2022-03-02 20:54:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xn86w-794dd694d8" has successfully progressed.}
Mar  2 20:54:41.068: INFO: Observed &Deployment event: MODIFIED
Mar  2 20:54:41.068: INFO: Observed deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-03-02 20:54:40 +0000 UTC 2022-03-02 20:54:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 20:54:41.068: INFO: Observed deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-03-02 20:54:40 +0000 UTC 2022-03-02 20:54:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-xn86w-794dd694d8" has successfully progressed.}
Mar  2 20:54:41.068: INFO: Observed deployment test-deployment-xn86w in namespace deployment-1984 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 20:54:41.069: INFO: Observed &Deployment event: MODIFIED
Mar  2 20:54:41.069: INFO: Found deployment test-deployment-xn86w in namespace deployment-1984 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar  2 20:54:41.069: INFO: Deployment test-deployment-xn86w has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar  2 20:54:41.080: INFO: Deployment "test-deployment-xn86w":
&Deployment{ObjectMeta:{test-deployment-xn86w  deployment-1984  78bb18bf-3010-406c-a668-4da08af62411 44241 1 2022-03-02 20:54:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-03-02 20:54:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-03-02 20:54:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-03-02 20:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034ccf38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-xn86w-794dd694d8",LastUpdateTime:2022-03-02 20:54:41 +0000 UTC,LastTransitionTime:2022-03-02 20:54:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 20:54:41.092: INFO: New ReplicaSet "test-deployment-xn86w-794dd694d8" of Deployment "test-deployment-xn86w":
&ReplicaSet{ObjectMeta:{test-deployment-xn86w-794dd694d8  deployment-1984  22736b6e-a4ee-4d3e-9c22-95d90b903fcd 44237 1 2022-03-02 20:54:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-xn86w 78bb18bf-3010-406c-a668-4da08af62411 0xc0034cd310 0xc0034cd311}] []  [{kube-controller-manager Update apps/v1 2022-03-02 20:54:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78bb18bf-3010-406c-a668-4da08af62411\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 20:54:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 794dd694d8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034cd3c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 20:54:41.104: INFO: Pod "test-deployment-xn86w-794dd694d8-znpzp" is available:
&Pod{ObjectMeta:{test-deployment-xn86w-794dd694d8-znpzp test-deployment-xn86w-794dd694d8- deployment-1984  a04cf6de-8dfc-429a-aa46-4357ddca94ce 44236 0 2022-03-02 20:54:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[cni.projectcalico.org/containerID:2ed4136fa157349e9a3159c44b023ab990fc779d354b25f71dc909c9e9e15adb cni.projectcalico.org/podIP:172.17.100.136/32 cni.projectcalico.org/podIPs:172.17.100.136/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-xn86w-794dd694d8 22736b6e-a4ee-4d3e-9c22-95d90b903fcd 0xc0034cd780 0xc0034cd781}] []  [{kube-controller-manager Update v1 2022-03-02 20:54:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22736b6e-a4ee-4d3e-9c22-95d90b903fcd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 20:54:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 20:54:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.100.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s92hp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s92hp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:54:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:54:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:54:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 20:54:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.4,PodIP:172.17.100.136,StartTime:2022-03-02 20:54:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 20:54:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://1f098d3cccd73c43318a482d3a7b7550ed1bb524f91183afa92ce7112ca4c18f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.100.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:54:41.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1984" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":294,"skipped":5255,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:54:41.137: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7525
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Mar  2 20:54:41.409: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:54:43.427: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Mar  2 20:54:43.492: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:54:45.509: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Mar  2 20:54:45.542: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 20:54:45.562: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 20:54:47.562: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 20:54:47.578: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 20:54:49.563: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 20:54:49.598: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:54:49.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7525" for this suite.

• [SLOW TEST:8.534 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":295,"skipped":5267,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:54:49.672: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4986
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-52c4f8c2-2853-4c02-bfa9-b980284e20ac
STEP: Creating a pod to test consume configMaps
Mar  2 20:54:49.961: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c1d1afe-35cd-49e5-92d8-f6a1f3437c3e" in namespace "projected-4986" to be "Succeeded or Failed"
Mar  2 20:54:49.975: INFO: Pod "pod-projected-configmaps-9c1d1afe-35cd-49e5-92d8-f6a1f3437c3e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.160122ms
Mar  2 20:54:51.992: INFO: Pod "pod-projected-configmaps-9c1d1afe-35cd-49e5-92d8-f6a1f3437c3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030202351s
STEP: Saw pod success
Mar  2 20:54:51.992: INFO: Pod "pod-projected-configmaps-9c1d1afe-35cd-49e5-92d8-f6a1f3437c3e" satisfied condition "Succeeded or Failed"
Mar  2 20:54:52.004: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-configmaps-9c1d1afe-35cd-49e5-92d8-f6a1f3437c3e container agnhost-container: <nil>
STEP: delete the pod
Mar  2 20:54:52.069: INFO: Waiting for pod pod-projected-configmaps-9c1d1afe-35cd-49e5-92d8-f6a1f3437c3e to disappear
Mar  2 20:54:52.094: INFO: Pod pod-projected-configmaps-9c1d1afe-35cd-49e5-92d8-f6a1f3437c3e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:54:52.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4986" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:54:52.164: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Mar  2 20:54:52.531: INFO: The status of Pod annotationupdate08f347d7-5558-4802-a6d1-58e658e6b027 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 20:54:54.573: INFO: The status of Pod annotationupdate08f347d7-5558-4802-a6d1-58e658e6b027 is Running (Ready = true)
Mar  2 20:54:55.193: INFO: Successfully updated pod "annotationupdate08f347d7-5558-4802-a6d1-58e658e6b027"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:54:59.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2336" for this suite.

• [SLOW TEST:7.197 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":297,"skipped":5297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:54:59.362: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Mar  2 20:54:59.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9545 create -f -'
Mar  2 20:54:59.951: INFO: stderr: ""
Mar  2 20:54:59.952: INFO: stdout: "pod/pause created\n"
Mar  2 20:54:59.952: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  2 20:54:59.952: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9545" to be "running and ready"
Mar  2 20:54:59.996: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 44.406005ms
Mar  2 20:55:02.036: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.084388369s
Mar  2 20:55:02.036: INFO: Pod "pause" satisfied condition "running and ready"
Mar  2 20:55:02.036: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  2 20:55:02.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9545 label pods pause testing-label=testing-label-value'
Mar  2 20:55:02.136: INFO: stderr: ""
Mar  2 20:55:02.136: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  2 20:55:02.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9545 get pod pause -L testing-label'
Mar  2 20:55:02.209: INFO: stderr: ""
Mar  2 20:55:02.209: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  2 20:55:02.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9545 label pods pause testing-label-'
Mar  2 20:55:02.347: INFO: stderr: ""
Mar  2 20:55:02.347: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  2 20:55:02.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9545 get pod pause -L testing-label'
Mar  2 20:55:02.417: INFO: stderr: ""
Mar  2 20:55:02.417: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Mar  2 20:55:02.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9545 delete --grace-period=0 --force -f -'
Mar  2 20:55:02.542: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 20:55:02.542: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  2 20:55:02.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9545 get rc,svc -l name=pause --no-headers'
Mar  2 20:55:02.631: INFO: stderr: "No resources found in kubectl-9545 namespace.\n"
Mar  2 20:55:02.631: INFO: stdout: ""
Mar  2 20:55:02.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-9545 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 20:55:02.697: INFO: stderr: ""
Mar  2 20:55:02.697: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:55:02.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9545" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":298,"skipped":5323,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:55:02.734: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1337
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-1337/secret-test-36983123-069b-429e-a5ed-0761ac207fc1
STEP: Creating a pod to test consume secrets
Mar  2 20:55:03.055: INFO: Waiting up to 5m0s for pod "pod-configmaps-8d9b79e0-5196-49ee-b2ef-26eaca91d51c" in namespace "secrets-1337" to be "Succeeded or Failed"
Mar  2 20:55:03.069: INFO: Pod "pod-configmaps-8d9b79e0-5196-49ee-b2ef-26eaca91d51c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.386316ms
Mar  2 20:55:05.086: INFO: Pod "pod-configmaps-8d9b79e0-5196-49ee-b2ef-26eaca91d51c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031162279s
STEP: Saw pod success
Mar  2 20:55:05.086: INFO: Pod "pod-configmaps-8d9b79e0-5196-49ee-b2ef-26eaca91d51c" satisfied condition "Succeeded or Failed"
Mar  2 20:55:05.102: INFO: Trying to get logs from node 10.245.0.4 pod pod-configmaps-8d9b79e0-5196-49ee-b2ef-26eaca91d51c container env-test: <nil>
STEP: delete the pod
Mar  2 20:55:05.172: INFO: Waiting for pod pod-configmaps-8d9b79e0-5196-49ee-b2ef-26eaca91d51c to disappear
Mar  2 20:55:05.204: INFO: Pod pod-configmaps-8d9b79e0-5196-49ee-b2ef-26eaca91d51c no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:55:05.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1337" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5350,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:55:05.242: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1018
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-00df9d37-8584-4ae4-986f-f4942259f596
STEP: Creating a pod to test consume secrets
Mar  2 20:55:05.524: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-11ca4e5b-95cd-4ce4-a908-c0eeba30dffa" in namespace "projected-1018" to be "Succeeded or Failed"
Mar  2 20:55:05.535: INFO: Pod "pod-projected-secrets-11ca4e5b-95cd-4ce4-a908-c0eeba30dffa": Phase="Pending", Reason="", readiness=false. Elapsed: 11.593684ms
Mar  2 20:55:07.557: INFO: Pod "pod-projected-secrets-11ca4e5b-95cd-4ce4-a908-c0eeba30dffa": Phase="Running", Reason="", readiness=true. Elapsed: 2.033053302s
Mar  2 20:55:09.580: INFO: Pod "pod-projected-secrets-11ca4e5b-95cd-4ce4-a908-c0eeba30dffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056543611s
STEP: Saw pod success
Mar  2 20:55:09.580: INFO: Pod "pod-projected-secrets-11ca4e5b-95cd-4ce4-a908-c0eeba30dffa" satisfied condition "Succeeded or Failed"
Mar  2 20:55:09.592: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-secrets-11ca4e5b-95cd-4ce4-a908-c0eeba30dffa container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 20:55:09.696: INFO: Waiting for pod pod-projected-secrets-11ca4e5b-95cd-4ce4-a908-c0eeba30dffa to disappear
Mar  2 20:55:09.738: INFO: Pod pod-projected-secrets-11ca4e5b-95cd-4ce4-a908-c0eeba30dffa no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:55:09.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1018" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":300,"skipped":5356,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:55:09.774: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2991
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 20:55:10.068: INFO: Waiting up to 5m0s for pod "busybox-user-65534-086575fd-a31d-42a1-993b-cf6a384dfe2c" in namespace "security-context-test-2991" to be "Succeeded or Failed"
Mar  2 20:55:10.098: INFO: Pod "busybox-user-65534-086575fd-a31d-42a1-993b-cf6a384dfe2c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.658903ms
Mar  2 20:55:12.145: INFO: Pod "busybox-user-65534-086575fd-a31d-42a1-993b-cf6a384dfe2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077020583s
Mar  2 20:55:12.145: INFO: Pod "busybox-user-65534-086575fd-a31d-42a1-993b-cf6a384dfe2c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:55:12.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2991" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":301,"skipped":5396,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:55:12.181: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3315.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3315.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3315.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3315.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3315.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3315.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3315.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3315.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3315.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3315.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3315.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3315.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3315.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3315.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 20:55:16.621: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:16.637: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:16.710: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:16.745: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:16.763: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:16.795: INFO: Lookups using dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326 failed for: [wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3315.svc.cluster.local jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local]

Mar  2 20:55:21.846: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:21.869: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:21.994: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:22.012: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:22.038: INFO: Lookups using dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326 failed for: [wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local]

Mar  2 20:55:26.842: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:26.857: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:26.935: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:26.957: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:26.985: INFO: Lookups using dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326 failed for: [wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local]

Mar  2 20:55:31.853: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:31.867: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:31.947: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:31.962: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:31.991: INFO: Lookups using dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326 failed for: [wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local]

Mar  2 20:55:36.859: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:36.873: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:36.945: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:36.961: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:36.991: INFO: Lookups using dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326 failed for: [wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local]

Mar  2 20:55:41.840: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:41.858: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:41.938: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:41.955: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local from pod dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326: the server could not find the requested resource (get pods dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326)
Mar  2 20:55:41.986: INFO: Lookups using dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326 failed for: [wheezy_udp@dns-test-service-2.dns-3315.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3315.svc.cluster.local jessie_udp@dns-test-service-2.dns-3315.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3315.svc.cluster.local]

Mar  2 20:55:47.009: INFO: DNS probes using dns-3315/dns-test-10d2b40d-e5fa-405e-bcc2-e6baa1653326 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:55:47.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3315" for this suite.

• [SLOW TEST:34.987 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":302,"skipped":5409,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:55:47.168: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-52
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 20:55:48.009: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 20:55:50.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851348, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851348, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851348, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851348, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 20:55:53.095: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:56:05.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-52" for this suite.
STEP: Destroying namespace "webhook-52-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.565 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":303,"skipped":5413,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:56:05.735: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5549
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-djxc
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 20:56:06.086: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-djxc" in namespace "subpath-5549" to be "Succeeded or Failed"
Mar  2 20:56:06.094: INFO: Pod "pod-subpath-test-downwardapi-djxc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.604613ms
Mar  2 20:56:08.110: INFO: Pod "pod-subpath-test-downwardapi-djxc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024111553s
Mar  2 20:56:10.144: INFO: Pod "pod-subpath-test-downwardapi-djxc": Phase="Running", Reason="", readiness=true. Elapsed: 4.058286981s
Mar  2 20:56:12.165: INFO: Pod "pod-subpath-test-downwardapi-djxc": Phase="Running", Reason="", readiness=true. Elapsed: 6.079010402s
Mar  2 20:56:14.191: INFO: Pod "pod-subpath-test-downwardapi-djxc": Phase="Running", Reason="", readiness=true. Elapsed: 8.105880651s
Mar  2 20:56:16.222: INFO: Pod "pod-subpath-test-downwardapi-djxc": Phase="Running", Reason="", readiness=true. Elapsed: 10.136680115s
Mar  2 20:56:18.236: INFO: Pod "pod-subpath-test-downwardapi-djxc": Phase="Running", Reason="", readiness=true. Elapsed: 12.150736032s
Mar  2 20:56:20.264: INFO: Pod "pod-subpath-test-downwardapi-djxc": Phase="Running", Reason="", readiness=true. Elapsed: 14.178095759s
Mar  2 20:56:22.313: INFO: Pod "pod-subpath-test-downwardapi-djxc": Phase="Running", Reason="", readiness=true. Elapsed: 16.227744071s
Mar  2 20:56:24.340: INFO: Pod "pod-subpath-test-downwardapi-djxc": Phase="Running", Reason="", readiness=true. Elapsed: 18.254579743s
Mar  2 20:56:26.364: INFO: Pod "pod-subpath-test-downwardapi-djxc": Phase="Running", Reason="", readiness=true. Elapsed: 20.278678512s
Mar  2 20:56:28.387: INFO: Pod "pod-subpath-test-downwardapi-djxc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.301728384s
STEP: Saw pod success
Mar  2 20:56:28.387: INFO: Pod "pod-subpath-test-downwardapi-djxc" satisfied condition "Succeeded or Failed"
Mar  2 20:56:28.399: INFO: Trying to get logs from node 10.245.0.4 pod pod-subpath-test-downwardapi-djxc container test-container-subpath-downwardapi-djxc: <nil>
STEP: delete the pod
Mar  2 20:56:28.602: INFO: Waiting for pod pod-subpath-test-downwardapi-djxc to disappear
Mar  2 20:56:28.612: INFO: Pod pod-subpath-test-downwardapi-djxc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-djxc
Mar  2 20:56:28.612: INFO: Deleting pod "pod-subpath-test-downwardapi-djxc" in namespace "subpath-5549"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:56:28.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5549" for this suite.

• [SLOW TEST:22.932 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":304,"skipped":5438,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:56:28.667: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4044
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:56:28.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4044" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":305,"skipped":5449,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:56:28.995: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-175
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 20:56:29.335: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar  2 20:56:29.403: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 20:56:29.468: INFO: waiting for watch events with expected annotations
Mar  2 20:56:29.468: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:56:29.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-175" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":306,"skipped":5478,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:56:29.747: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-4b99a914-a9bc-4554-b496-4b8875f1fdef
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:56:29.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1357" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":307,"skipped":5481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:56:30.020: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6020
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6020, will wait for the garbage collector to delete the pods
Mar  2 20:56:34.394: INFO: Deleting Job.batch foo took: 47.598672ms
Mar  2 20:56:34.494: INFO: Terminating Job.batch foo pods took: 100.548905ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:57:06.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6020" for this suite.

• [SLOW TEST:36.557 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":308,"skipped":5521,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:57:06.578: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7346
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar  2 20:57:09.466: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7346 pod-service-account-1d1a68eb-d93b-499f-8bea-1dc24f6498a4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar  2 20:57:09.969: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7346 pod-service-account-1d1a68eb-d93b-499f-8bea-1dc24f6498a4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar  2 20:57:10.263: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7346 pod-service-account-1d1a68eb-d93b-499f-8bea-1dc24f6498a4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:57:10.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7346" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":309,"skipped":5555,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:57:10.567: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-912
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-273b67c3-cb6a-4457-8041-7cfea7daad08
STEP: Creating a pod to test consume configMaps
Mar  2 20:57:10.872: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-edb2feb1-e767-4929-baca-9782b51e49c7" in namespace "projected-912" to be "Succeeded or Failed"
Mar  2 20:57:10.884: INFO: Pod "pod-projected-configmaps-edb2feb1-e767-4929-baca-9782b51e49c7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.14891ms
Mar  2 20:57:12.897: INFO: Pod "pod-projected-configmaps-edb2feb1-e767-4929-baca-9782b51e49c7": Phase="Running", Reason="", readiness=true. Elapsed: 2.024981694s
Mar  2 20:57:14.921: INFO: Pod "pod-projected-configmaps-edb2feb1-e767-4929-baca-9782b51e49c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048650118s
STEP: Saw pod success
Mar  2 20:57:14.921: INFO: Pod "pod-projected-configmaps-edb2feb1-e767-4929-baca-9782b51e49c7" satisfied condition "Succeeded or Failed"
Mar  2 20:57:14.934: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-configmaps-edb2feb1-e767-4929-baca-9782b51e49c7 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 20:57:14.993: INFO: Waiting for pod pod-projected-configmaps-edb2feb1-e767-4929-baca-9782b51e49c7 to disappear
Mar  2 20:57:15.005: INFO: Pod pod-projected-configmaps-edb2feb1-e767-4929-baca-9782b51e49c7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:57:15.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-912" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":5557,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:57:15.056: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  2 20:57:15.321: INFO: Waiting up to 5m0s for pod "pod-c67b6650-4c00-44fe-9567-845ab23d7e22" in namespace "emptydir-5326" to be "Succeeded or Failed"
Mar  2 20:57:15.333: INFO: Pod "pod-c67b6650-4c00-44fe-9567-845ab23d7e22": Phase="Pending", Reason="", readiness=false. Elapsed: 11.637589ms
Mar  2 20:57:17.356: INFO: Pod "pod-c67b6650-4c00-44fe-9567-845ab23d7e22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035140906s
Mar  2 20:57:19.381: INFO: Pod "pod-c67b6650-4c00-44fe-9567-845ab23d7e22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060041506s
STEP: Saw pod success
Mar  2 20:57:19.381: INFO: Pod "pod-c67b6650-4c00-44fe-9567-845ab23d7e22" satisfied condition "Succeeded or Failed"
Mar  2 20:57:19.394: INFO: Trying to get logs from node 10.245.0.4 pod pod-c67b6650-4c00-44fe-9567-845ab23d7e22 container test-container: <nil>
STEP: delete the pod
Mar  2 20:57:19.454: INFO: Waiting for pod pod-c67b6650-4c00-44fe-9567-845ab23d7e22 to disappear
Mar  2 20:57:19.466: INFO: Pod pod-c67b6650-4c00-44fe-9567-845ab23d7e22 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:57:19.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5326" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":311,"skipped":5565,"failed":0}
SSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:57:19.511: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-8535
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:59:01.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8535" for this suite.

• [SLOW TEST:102.458 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":312,"skipped":5570,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:59:01.971: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3071
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-3071/configmap-test-e4ceee65-e1b5-451c-b2a3-ab025151bfe1
STEP: Creating a pod to test consume configMaps
Mar  2 20:59:02.332: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb4b1f43-4756-4c29-8b93-9e4e36c2ee88" in namespace "configmap-3071" to be "Succeeded or Failed"
Mar  2 20:59:02.348: INFO: Pod "pod-configmaps-bb4b1f43-4756-4c29-8b93-9e4e36c2ee88": Phase="Pending", Reason="", readiness=false. Elapsed: 16.424605ms
Mar  2 20:59:04.370: INFO: Pod "pod-configmaps-bb4b1f43-4756-4c29-8b93-9e4e36c2ee88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038813283s
Mar  2 20:59:06.392: INFO: Pod "pod-configmaps-bb4b1f43-4756-4c29-8b93-9e4e36c2ee88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060455653s
STEP: Saw pod success
Mar  2 20:59:06.392: INFO: Pod "pod-configmaps-bb4b1f43-4756-4c29-8b93-9e4e36c2ee88" satisfied condition "Succeeded or Failed"
Mar  2 20:59:06.403: INFO: Trying to get logs from node 10.245.0.4 pod pod-configmaps-bb4b1f43-4756-4c29-8b93-9e4e36c2ee88 container env-test: <nil>
STEP: delete the pod
Mar  2 20:59:06.555: INFO: Waiting for pod pod-configmaps-bb4b1f43-4756-4c29-8b93-9e4e36c2ee88 to disappear
Mar  2 20:59:06.567: INFO: Pod pod-configmaps-bb4b1f43-4756-4c29-8b93-9e4e36c2ee88 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:59:06.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3071" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":313,"skipped":5573,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:59:06.614: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Mar  2 20:59:06.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-5622 api-versions'
Mar  2 20:59:06.921: INFO: stderr: ""
Mar  2 20:59:06.921: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:59:06.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5622" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":314,"skipped":5600,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:59:06.963: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4071
STEP: creating service affinity-clusterip in namespace services-4071
STEP: creating replication controller affinity-clusterip in namespace services-4071
I0302 20:59:07.234011      21 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-4071, replica count: 3
I0302 20:59:10.285441      21 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 20:59:10.325: INFO: Creating new exec pod
Mar  2 20:59:15.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4071 exec execpod-affinityvx6q9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Mar  2 20:59:15.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar  2 20:59:15.612: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 20:59:15.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4071 exec execpod-affinityvx6q9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.136.166 80'
Mar  2 20:59:15.863: INFO: stderr: "+ nc -v -t -w 2 172.21.136.166 80\n+ echo hostName\nConnection to 172.21.136.166 80 port [tcp/http] succeeded!\n"
Mar  2 20:59:15.863: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 20:59:15.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=services-4071 exec execpod-affinityvx6q9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.136.166:80/ ; done'
Mar  2 20:59:16.129: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.136.166:80/\n"
Mar  2 20:59:16.129: INFO: stdout: "\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999\naffinity-clusterip-bv999"
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Received response from host: affinity-clusterip-bv999
Mar  2 20:59:16.130: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4071, will wait for the garbage collector to delete the pods
Mar  2 20:59:16.307: INFO: Deleting ReplicationController affinity-clusterip took: 19.50857ms
Mar  2 20:59:16.408: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.877988ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:59:19.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4071" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.181 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":315,"skipped":5605,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:59:19.144: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar  2 20:59:24.038: INFO: Successfully updated pod "adopt-release--1-25sxp"
STEP: Checking that the Job readopts the Pod
Mar  2 20:59:24.038: INFO: Waiting up to 15m0s for pod "adopt-release--1-25sxp" in namespace "job-6559" to be "adopted"
Mar  2 20:59:24.050: INFO: Pod "adopt-release--1-25sxp": Phase="Running", Reason="", readiness=true. Elapsed: 11.890695ms
Mar  2 20:59:26.119: INFO: Pod "adopt-release--1-25sxp": Phase="Running", Reason="", readiness=true. Elapsed: 2.081019696s
Mar  2 20:59:26.119: INFO: Pod "adopt-release--1-25sxp" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar  2 20:59:26.690: INFO: Successfully updated pod "adopt-release--1-25sxp"
STEP: Checking that the Job releases the Pod
Mar  2 20:59:26.690: INFO: Waiting up to 15m0s for pod "adopt-release--1-25sxp" in namespace "job-6559" to be "released"
Mar  2 20:59:26.755: INFO: Pod "adopt-release--1-25sxp": Phase="Running", Reason="", readiness=true. Elapsed: 64.625353ms
Mar  2 20:59:26.755: INFO: Pod "adopt-release--1-25sxp" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:59:26.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6559" for this suite.

• [SLOW TEST:7.732 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":316,"skipped":5623,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:59:26.877: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:59:27.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-273" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":317,"skipped":5640,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:59:27.405: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-22n9
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 20:59:27.722: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-22n9" in namespace "subpath-8938" to be "Succeeded or Failed"
Mar  2 20:59:27.735: INFO: Pod "pod-subpath-test-projected-22n9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.580454ms
Mar  2 20:59:29.758: INFO: Pod "pod-subpath-test-projected-22n9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036639223s
Mar  2 20:59:31.779: INFO: Pod "pod-subpath-test-projected-22n9": Phase="Running", Reason="", readiness=true. Elapsed: 4.057575162s
Mar  2 20:59:33.794: INFO: Pod "pod-subpath-test-projected-22n9": Phase="Running", Reason="", readiness=true. Elapsed: 6.072223034s
Mar  2 20:59:35.825: INFO: Pod "pod-subpath-test-projected-22n9": Phase="Running", Reason="", readiness=true. Elapsed: 8.103638266s
Mar  2 20:59:37.842: INFO: Pod "pod-subpath-test-projected-22n9": Phase="Running", Reason="", readiness=true. Elapsed: 10.120556249s
Mar  2 20:59:39.894: INFO: Pod "pod-subpath-test-projected-22n9": Phase="Running", Reason="", readiness=true. Elapsed: 12.172333204s
Mar  2 20:59:41.917: INFO: Pod "pod-subpath-test-projected-22n9": Phase="Running", Reason="", readiness=true. Elapsed: 14.195303406s
Mar  2 20:59:43.965: INFO: Pod "pod-subpath-test-projected-22n9": Phase="Running", Reason="", readiness=true. Elapsed: 16.24275954s
Mar  2 20:59:45.992: INFO: Pod "pod-subpath-test-projected-22n9": Phase="Running", Reason="", readiness=true. Elapsed: 18.270516648s
Mar  2 20:59:48.026: INFO: Pod "pod-subpath-test-projected-22n9": Phase="Running", Reason="", readiness=true. Elapsed: 20.304561372s
Mar  2 20:59:50.062: INFO: Pod "pod-subpath-test-projected-22n9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.340093258s
STEP: Saw pod success
Mar  2 20:59:50.062: INFO: Pod "pod-subpath-test-projected-22n9" satisfied condition "Succeeded or Failed"
Mar  2 20:59:50.074: INFO: Trying to get logs from node 10.245.0.6 pod pod-subpath-test-projected-22n9 container test-container-subpath-projected-22n9: <nil>
STEP: delete the pod
Mar  2 20:59:50.297: INFO: Waiting for pod pod-subpath-test-projected-22n9 to disappear
Mar  2 20:59:50.332: INFO: Pod pod-subpath-test-projected-22n9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-22n9
Mar  2 20:59:50.332: INFO: Deleting pod "pod-subpath-test-projected-22n9" in namespace "subpath-8938"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:59:50.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8938" for this suite.

• [SLOW TEST:22.999 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":318,"skipped":5648,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:59:50.404: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1426
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 20:59:51.178: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  2 20:59:53.269: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851591, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851591, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851591, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63781851591, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 20:59:56.324: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar  2 20:59:58.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=webhook-1426 attach --namespace=webhook-1426 to-be-attached-pod -i -c=container1'
Mar  2 20:59:58.640: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 20:59:58.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1426" for this suite.
STEP: Destroying namespace "webhook-1426-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.446 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":319,"skipped":5650,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 20:59:58.851: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-308
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Mar  2 20:59:59.138: INFO: Found 0 stateful pods, waiting for 3
Mar  2 21:00:09.177: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 21:00:09.177: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 21:00:09.177: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Mar  2 21:00:09.269: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  2 21:00:19.402: INFO: Updating stateful set ss2
Mar  2 21:00:19.461: INFO: Waiting for Pod statefulset-308/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Mar  2 21:00:29.711: INFO: Found 2 stateful pods, waiting for 3
Mar  2 21:00:39.750: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 21:00:39.750: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 21:00:39.750: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  2 21:00:39.831: INFO: Updating stateful set ss2
Mar  2 21:00:39.861: INFO: Waiting for Pod statefulset-308/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Mar  2 21:00:49.959: INFO: Updating stateful set ss2
Mar  2 21:00:49.987: INFO: Waiting for StatefulSet statefulset-308/ss2 to complete update
Mar  2 21:00:49.987: INFO: Waiting for Pod statefulset-308/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Mar  2 21:01:00.046: INFO: Deleting all statefulset in ns statefulset-308
Mar  2 21:01:00.063: INFO: Scaling statefulset ss2 to 0
Mar  2 21:01:10.154: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 21:01:10.166: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:01:10.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-308" for this suite.

• [SLOW TEST:71.425 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":320,"skipped":5669,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:01:10.276: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 21:01:10.614: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c2593e9-5d0a-4366-9049-b693e73b9ff3" in namespace "downward-api-6962" to be "Succeeded or Failed"
Mar  2 21:01:10.626: INFO: Pod "downwardapi-volume-5c2593e9-5d0a-4366-9049-b693e73b9ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.441839ms
Mar  2 21:01:12.651: INFO: Pod "downwardapi-volume-5c2593e9-5d0a-4366-9049-b693e73b9ff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03704074s
Mar  2 21:01:14.670: INFO: Pod "downwardapi-volume-5c2593e9-5d0a-4366-9049-b693e73b9ff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055733562s
STEP: Saw pod success
Mar  2 21:01:14.670: INFO: Pod "downwardapi-volume-5c2593e9-5d0a-4366-9049-b693e73b9ff3" satisfied condition "Succeeded or Failed"
Mar  2 21:01:14.683: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-5c2593e9-5d0a-4366-9049-b693e73b9ff3 container client-container: <nil>
STEP: delete the pod
Mar  2 21:01:14.809: INFO: Waiting for pod downwardapi-volume-5c2593e9-5d0a-4366-9049-b693e73b9ff3 to disappear
Mar  2 21:01:14.820: INFO: Pod downwardapi-volume-5c2593e9-5d0a-4366-9049-b693e73b9ff3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:01:14.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6962" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":321,"skipped":5672,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:01:14.859: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-536
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  2 21:01:15.125: INFO: Waiting up to 5m0s for pod "pod-18377998-69c1-4ca1-9857-06d9d9eb50d8" in namespace "emptydir-536" to be "Succeeded or Failed"
Mar  2 21:01:15.138: INFO: Pod "pod-18377998-69c1-4ca1-9857-06d9d9eb50d8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.140427ms
Mar  2 21:01:17.160: INFO: Pod "pod-18377998-69c1-4ca1-9857-06d9d9eb50d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035767307s
STEP: Saw pod success
Mar  2 21:01:17.160: INFO: Pod "pod-18377998-69c1-4ca1-9857-06d9d9eb50d8" satisfied condition "Succeeded or Failed"
Mar  2 21:01:17.171: INFO: Trying to get logs from node 10.245.0.4 pod pod-18377998-69c1-4ca1-9857-06d9d9eb50d8 container test-container: <nil>
STEP: delete the pod
Mar  2 21:01:17.232: INFO: Waiting for pod pod-18377998-69c1-4ca1-9857-06d9d9eb50d8 to disappear
Mar  2 21:01:17.243: INFO: Pod pod-18377998-69c1-4ca1-9857-06d9d9eb50d8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:01:17.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-536" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":322,"skipped":5679,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:01:17.294: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2562
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:01:17.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2562" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":323,"skipped":5695,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:01:17.619: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4642
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 21:01:17.939: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 21:01:22.973: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Mar  2 21:01:23.046: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Mar  2 21:01:23.101: INFO: observed ReplicaSet test-rs in namespace replicaset-4642 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 21:01:23.130: INFO: observed ReplicaSet test-rs in namespace replicaset-4642 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 21:01:23.173: INFO: observed ReplicaSet test-rs in namespace replicaset-4642 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 21:01:23.188: INFO: observed ReplicaSet test-rs in namespace replicaset-4642 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 21:01:25.095: INFO: observed ReplicaSet test-rs in namespace replicaset-4642 with ReadyReplicas 2, AvailableReplicas 2
Mar  2 21:01:25.192: INFO: observed Replicaset test-rs in namespace replicaset-4642 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:01:25.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4642" for this suite.

• [SLOW TEST:7.624 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":324,"skipped":5716,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:01:25.243: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8788
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  2 21:01:26.216: INFO: Pod name wrapped-volume-race-371ac341-6a66-473f-8ee3-a8bdfd6d8b80: Found 0 pods out of 5
Mar  2 21:01:31.335: INFO: Pod name wrapped-volume-race-371ac341-6a66-473f-8ee3-a8bdfd6d8b80: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-371ac341-6a66-473f-8ee3-a8bdfd6d8b80 in namespace emptydir-wrapper-8788, will wait for the garbage collector to delete the pods
Mar  2 21:01:31.502: INFO: Deleting ReplicationController wrapped-volume-race-371ac341-6a66-473f-8ee3-a8bdfd6d8b80 took: 18.717902ms
Mar  2 21:01:31.603: INFO: Terminating ReplicationController wrapped-volume-race-371ac341-6a66-473f-8ee3-a8bdfd6d8b80 pods took: 100.701787ms
STEP: Creating RC which spawns configmap-volume pods
Mar  2 21:01:34.574: INFO: Pod name wrapped-volume-race-21210ead-2ae5-4c15-9ec1-b6bfc6b1412a: Found 1 pods out of 5
Mar  2 21:01:39.628: INFO: Pod name wrapped-volume-race-21210ead-2ae5-4c15-9ec1-b6bfc6b1412a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-21210ead-2ae5-4c15-9ec1-b6bfc6b1412a in namespace emptydir-wrapper-8788, will wait for the garbage collector to delete the pods
Mar  2 21:01:39.815: INFO: Deleting ReplicationController wrapped-volume-race-21210ead-2ae5-4c15-9ec1-b6bfc6b1412a took: 47.4119ms
Mar  2 21:01:39.915: INFO: Terminating ReplicationController wrapped-volume-race-21210ead-2ae5-4c15-9ec1-b6bfc6b1412a pods took: 100.137253ms
STEP: Creating RC which spawns configmap-volume pods
Mar  2 21:01:42.679: INFO: Pod name wrapped-volume-race-73438b22-d2fe-4caf-b008-a187f4f59955: Found 0 pods out of 5
Mar  2 21:01:47.736: INFO: Pod name wrapped-volume-race-73438b22-d2fe-4caf-b008-a187f4f59955: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-73438b22-d2fe-4caf-b008-a187f4f59955 in namespace emptydir-wrapper-8788, will wait for the garbage collector to delete the pods
Mar  2 21:01:47.897: INFO: Deleting ReplicationController wrapped-volume-race-73438b22-d2fe-4caf-b008-a187f4f59955 took: 20.522917ms
Mar  2 21:01:48.000: INFO: Terminating ReplicationController wrapped-volume-race-73438b22-d2fe-4caf-b008-a187f4f59955 pods took: 103.013323ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:01:52.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8788" for this suite.

• [SLOW TEST:27.101 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":325,"skipped":5724,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:01:52.344: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4623
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 21:01:52.692: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 21:01:56.726: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Mar  2 21:01:56.801: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4623  3ea7a1ae-2f25-4ba4-bd11-064eb7f8a3f2 47272 1 2022-03-02 21:01:56 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-03-02 21:01:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060f5ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 21:01:56.815: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-4623  9a7ca679-a3c7-4be0-94dd-918815d8e4ef 47274 1 2022-03-02 21:01:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 3ea7a1ae-2f25-4ba4-bd11-064eb7f8a3f2 0xc002804167 0xc002804168}] []  [{kube-controller-manager Update apps/v1 2022-03-02 21:01:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ea7a1ae-2f25-4ba4-bd11-064eb7f8a3f2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028041f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 21:01:56.815: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  2 21:01:56.815: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4623  f2ebaea6-93f2-460b-8cf0-1b911e2cc8c3 47273 1 2022-03-02 21:01:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 3ea7a1ae-2f25-4ba4-bd11-064eb7f8a3f2 0xc002804037 0xc002804038}] []  [{e2e.test Update apps/v1 2022-03-02 21:01:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-03-02 21:01:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-03-02 21:01:56 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"3ea7a1ae-2f25-4ba4-bd11-064eb7f8a3f2\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0028040f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 21:01:56.829: INFO: Pod "test-cleanup-controller-7dcw5" is available:
&Pod{ObjectMeta:{test-cleanup-controller-7dcw5 test-cleanup-controller- deployment-4623  2d40596f-e59f-4951-a925-1bf4c6a3831a 47268 0 2022-03-02 21:01:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:dc58de69a2c246a663f294bb7b6236e21f8a7d7557c607a9591624f9f9fa236b cni.projectcalico.org/podIP:172.17.100.168/32 cni.projectcalico.org/podIPs:172.17.100.168/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller f2ebaea6-93f2-460b-8cf0-1b911e2cc8c3 0xc0046e3d07 0xc0046e3d08}] []  [{kube-controller-manager Update v1 2022-03-02 21:01:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2ebaea6-93f2-460b-8cf0-1b911e2cc8c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-03-02 21:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-03-02 21:01:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.100.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8gqqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8gqqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.245.0.4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 21:01:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 21:01:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 21:01:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-02 21:01:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.245.0.4,PodIP:172.17.100.168,StartTime:2022-03-02 21:01:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-02 21:01:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://9821c6ece2572b3c8f2f0c4bcc028b30264f7b854d7f755aa4173c4e9d8cde99,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.100.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 21:01:56.829: INFO: Pod "test-cleanup-deployment-5b4d99b59b-8rpht" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-8rpht test-cleanup-deployment-5b4d99b59b- deployment-4623  5cc1cff3-06b1-403f-846b-950b673bb6ba 47277 0 2022-03-02 21:01:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b 9a7ca679-a3c7-4be0-94dd-918815d8e4ef 0xc0046e3f17 0xc0046e3f18}] []  [{kube-controller-manager Update v1 2022-03-02 21:01:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a7ca679-a3c7-4be0-94dd-918815d8e4ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqbmj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqbmj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:01:56.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4623" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":326,"skipped":5739,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:01:56.902: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 21:01:57.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d4c0c9c-3f44-41f5-b94d-ceb3f98247bb" in namespace "downward-api-4764" to be "Succeeded or Failed"
Mar  2 21:01:57.171: INFO: Pod "downwardapi-volume-0d4c0c9c-3f44-41f5-b94d-ceb3f98247bb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.583427ms
Mar  2 21:01:59.190: INFO: Pod "downwardapi-volume-0d4c0c9c-3f44-41f5-b94d-ceb3f98247bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029719293s
Mar  2 21:02:01.217: INFO: Pod "downwardapi-volume-0d4c0c9c-3f44-41f5-b94d-ceb3f98247bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05673276s
STEP: Saw pod success
Mar  2 21:02:01.217: INFO: Pod "downwardapi-volume-0d4c0c9c-3f44-41f5-b94d-ceb3f98247bb" satisfied condition "Succeeded or Failed"
Mar  2 21:02:01.229: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-0d4c0c9c-3f44-41f5-b94d-ceb3f98247bb container client-container: <nil>
STEP: delete the pod
Mar  2 21:02:01.314: INFO: Waiting for pod downwardapi-volume-0d4c0c9c-3f44-41f5-b94d-ceb3f98247bb to disappear
Mar  2 21:02:01.326: INFO: Pod downwardapi-volume-0d4c0c9c-3f44-41f5-b94d-ceb3f98247bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:02:01.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4764" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":5775,"failed":0}
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:02:01.386: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 21:02:04.794: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:02:04.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6704" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":328,"skipped":5776,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:02:04.887: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 21:02:07.172: INFO: Deleting pod "var-expansion-9a71e38b-c0c0-4738-b925-d7fdd577d55f" in namespace "var-expansion-4679"
Mar  2 21:02:07.194: INFO: Wait up to 5m0s for pod "var-expansion-9a71e38b-c0c0-4738-b925-d7fdd577d55f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:02:11.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4679" for this suite.

• [SLOW TEST:6.377 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":329,"skipped":5792,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:02:11.264: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9106
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  2 21:02:11.565: INFO: Waiting up to 5m0s for pod "pod-678c015a-86ca-4dd3-962f-39d59b15995f" in namespace "emptydir-9106" to be "Succeeded or Failed"
Mar  2 21:02:11.580: INFO: Pod "pod-678c015a-86ca-4dd3-962f-39d59b15995f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.977515ms
Mar  2 21:02:13.608: INFO: Pod "pod-678c015a-86ca-4dd3-962f-39d59b15995f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043328928s
STEP: Saw pod success
Mar  2 21:02:13.608: INFO: Pod "pod-678c015a-86ca-4dd3-962f-39d59b15995f" satisfied condition "Succeeded or Failed"
Mar  2 21:02:13.619: INFO: Trying to get logs from node 10.245.0.4 pod pod-678c015a-86ca-4dd3-962f-39d59b15995f container test-container: <nil>
STEP: delete the pod
Mar  2 21:02:13.667: INFO: Waiting for pod pod-678c015a-86ca-4dd3-962f-39d59b15995f to disappear
Mar  2 21:02:13.677: INFO: Pod pod-678c015a-86ca-4dd3-962f-39d59b15995f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:02:13.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9106" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":330,"skipped":5807,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:02:13.723: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:03:14.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-340" for this suite.

• [SLOW TEST:60.332 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":331,"skipped":5811,"failed":0}
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:03:14.055: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8187
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-cd982708-aec0-4f11-9057-7a0f334830b8
STEP: Creating configMap with name cm-test-opt-upd-24cef9b2-31f7-424a-a3a9-335b99531762
STEP: Creating the pod
Mar  2 21:03:14.378: INFO: The status of Pod pod-configmaps-6c54a84f-eafe-4145-82c7-f481abcda875 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 21:03:16.398: INFO: The status of Pod pod-configmaps-6c54a84f-eafe-4145-82c7-f481abcda875 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 21:03:18.406: INFO: The status of Pod pod-configmaps-6c54a84f-eafe-4145-82c7-f481abcda875 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-cd982708-aec0-4f11-9057-7a0f334830b8
STEP: Updating configmap cm-test-opt-upd-24cef9b2-31f7-424a-a3a9-335b99531762
STEP: Creating configMap with name cm-test-opt-create-de49e592-46b6-4027-87e4-6815612edccb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:04:39.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8187" for this suite.

• [SLOW TEST:85.945 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":332,"skipped":5811,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:04:40.001: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar  2 21:04:40.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eafd1af4-35d0-4dfe-9c32-456a6505c095" in namespace "downward-api-6819" to be "Succeeded or Failed"
Mar  2 21:04:40.331: INFO: Pod "downwardapi-volume-eafd1af4-35d0-4dfe-9c32-456a6505c095": Phase="Pending", Reason="", readiness=false. Elapsed: 11.293865ms
Mar  2 21:04:42.369: INFO: Pod "downwardapi-volume-eafd1af4-35d0-4dfe-9c32-456a6505c095": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049336646s
Mar  2 21:04:44.393: INFO: Pod "downwardapi-volume-eafd1af4-35d0-4dfe-9c32-456a6505c095": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07319822s
STEP: Saw pod success
Mar  2 21:04:44.393: INFO: Pod "downwardapi-volume-eafd1af4-35d0-4dfe-9c32-456a6505c095" satisfied condition "Succeeded or Failed"
Mar  2 21:04:44.408: INFO: Trying to get logs from node 10.245.0.4 pod downwardapi-volume-eafd1af4-35d0-4dfe-9c32-456a6505c095 container client-container: <nil>
STEP: delete the pod
Mar  2 21:04:44.490: INFO: Waiting for pod downwardapi-volume-eafd1af4-35d0-4dfe-9c32-456a6505c095 to disappear
Mar  2 21:04:44.502: INFO: Pod downwardapi-volume-eafd1af4-35d0-4dfe-9c32-456a6505c095 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:04:44.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6819" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":333,"skipped":5831,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:04:44.543: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3244
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Mar  2 21:04:44.853: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-3244 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:04:44.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3244" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":334,"skipped":5850,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:04:44.971: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Mar  2 21:04:45.301: INFO: Waiting up to 5m0s for pod "var-expansion-8d64a8a5-c780-4848-a952-0f557a82dd80" in namespace "var-expansion-523" to be "Succeeded or Failed"
Mar  2 21:04:45.324: INFO: Pod "var-expansion-8d64a8a5-c780-4848-a952-0f557a82dd80": Phase="Pending", Reason="", readiness=false. Elapsed: 23.138495ms
Mar  2 21:04:47.348: INFO: Pod "var-expansion-8d64a8a5-c780-4848-a952-0f557a82dd80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046941144s
Mar  2 21:04:49.370: INFO: Pod "var-expansion-8d64a8a5-c780-4848-a952-0f557a82dd80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068290794s
STEP: Saw pod success
Mar  2 21:04:49.370: INFO: Pod "var-expansion-8d64a8a5-c780-4848-a952-0f557a82dd80" satisfied condition "Succeeded or Failed"
Mar  2 21:04:49.382: INFO: Trying to get logs from node 10.245.0.4 pod var-expansion-8d64a8a5-c780-4848-a952-0f557a82dd80 container dapi-container: <nil>
STEP: delete the pod
Mar  2 21:04:49.469: INFO: Waiting for pod var-expansion-8d64a8a5-c780-4848-a952-0f557a82dd80 to disappear
Mar  2 21:04:49.479: INFO: Pod var-expansion-8d64a8a5-c780-4848-a952-0f557a82dd80 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:04:49.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-523" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":335,"skipped":5891,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:04:49.558: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2981
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-803a909e-30b1-46e1-aa94-64587214ea35
STEP: Creating a pod to test consume configMaps
Mar  2 21:04:49.916: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-28c8ee91-4b44-4b57-9256-5b16f3f66648" in namespace "projected-2981" to be "Succeeded or Failed"
Mar  2 21:04:49.930: INFO: Pod "pod-projected-configmaps-28c8ee91-4b44-4b57-9256-5b16f3f66648": Phase="Pending", Reason="", readiness=false. Elapsed: 13.800188ms
Mar  2 21:04:51.951: INFO: Pod "pod-projected-configmaps-28c8ee91-4b44-4b57-9256-5b16f3f66648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034947935s
STEP: Saw pod success
Mar  2 21:04:51.951: INFO: Pod "pod-projected-configmaps-28c8ee91-4b44-4b57-9256-5b16f3f66648" satisfied condition "Succeeded or Failed"
Mar  2 21:04:51.981: INFO: Trying to get logs from node 10.245.0.4 pod pod-projected-configmaps-28c8ee91-4b44-4b57-9256-5b16f3f66648 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 21:04:52.048: INFO: Waiting for pod pod-projected-configmaps-28c8ee91-4b44-4b57-9256-5b16f3f66648 to disappear
Mar  2 21:04:52.061: INFO: Pod pod-projected-configmaps-28c8ee91-4b44-4b57-9256-5b16f3f66648 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:04:52.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2981" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":5894,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:04:52.104: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-3755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  2 21:04:52.454: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 21:05:52.580: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Mar  2 21:05:52.653: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar  2 21:05:52.670: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar  2 21:05:52.708: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar  2 21:05:52.726: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar  2 21:05:52.850: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar  2 21:05:52.867: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:06:09.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3755" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:77.197 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":337,"skipped":5915,"failed":0}
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:06:09.301: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-4543
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Mar  2 21:06:09.603: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Mar  2 21:06:09.671: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:06:09.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4543" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":338,"skipped":5915,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:06:09.797: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Mar  2 21:06:12.162: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1856 PodName:var-expansion-f2071204-97b3-4ae9-93d4-3899a930b905 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:06:12.162: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: test for file in mounted path
Mar  2 21:06:12.450: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1856 PodName:var-expansion-f2071204-97b3-4ae9-93d4-3899a930b905 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 21:06:12.450: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: updating the annotation value
Mar  2 21:06:13.199: INFO: Successfully updated pod "var-expansion-f2071204-97b3-4ae9-93d4-3899a930b905"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Mar  2 21:06:13.211: INFO: Deleting pod "var-expansion-f2071204-97b3-4ae9-93d4-3899a930b905" in namespace "var-expansion-1856"
Mar  2 21:06:13.232: INFO: Wait up to 5m0s for pod "var-expansion-f2071204-97b3-4ae9-93d4-3899a930b905" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:06:47.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1856" for this suite.

• [SLOW TEST:37.510 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":339,"skipped":5959,"failed":0}
SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:06:47.308: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-6349
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 21:06:47.580: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6349
I0302 21:06:47.617637      21 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6349, replica count: 1
I0302 21:06:48.669090      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:06:49.670070      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 21:06:50.670316      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 21:06:50.812: INFO: Created: latency-svc-rk52v
Mar  2 21:06:50.834: INFO: Got endpoints: latency-svc-rk52v [62.918606ms]
Mar  2 21:06:50.873: INFO: Created: latency-svc-4m6nk
Mar  2 21:06:50.883: INFO: Got endpoints: latency-svc-4m6nk [48.346785ms]
Mar  2 21:06:50.888: INFO: Created: latency-svc-mbwdp
Mar  2 21:06:50.899: INFO: Got endpoints: latency-svc-mbwdp [62.282654ms]
Mar  2 21:06:50.905: INFO: Created: latency-svc-xp2rz
Mar  2 21:06:50.916: INFO: Got endpoints: latency-svc-xp2rz [80.53311ms]
Mar  2 21:06:50.923: INFO: Created: latency-svc-jksf2
Mar  2 21:06:50.935: INFO: Created: latency-svc-p5qwh
Mar  2 21:06:50.937: INFO: Got endpoints: latency-svc-jksf2 [102.536955ms]
Mar  2 21:06:50.946: INFO: Got endpoints: latency-svc-p5qwh [110.883459ms]
Mar  2 21:06:50.949: INFO: Created: latency-svc-bk24z
Mar  2 21:06:50.958: INFO: Got endpoints: latency-svc-bk24z [121.83568ms]
Mar  2 21:06:50.965: INFO: Created: latency-svc-hqk99
Mar  2 21:06:50.974: INFO: Got endpoints: latency-svc-hqk99 [138.372065ms]
Mar  2 21:06:50.979: INFO: Created: latency-svc-zd24d
Mar  2 21:06:50.990: INFO: Got endpoints: latency-svc-zd24d [153.253709ms]
Mar  2 21:06:50.997: INFO: Created: latency-svc-8b5t5
Mar  2 21:06:51.009: INFO: Got endpoints: latency-svc-8b5t5 [173.36358ms]
Mar  2 21:06:51.014: INFO: Created: latency-svc-lwwcf
Mar  2 21:06:51.025: INFO: Got endpoints: latency-svc-lwwcf [189.20507ms]
Mar  2 21:06:51.027: INFO: Created: latency-svc-kwwr6
Mar  2 21:06:51.038: INFO: Got endpoints: latency-svc-kwwr6 [201.459073ms]
Mar  2 21:06:51.042: INFO: Created: latency-svc-c6z47
Mar  2 21:06:51.052: INFO: Got endpoints: latency-svc-c6z47 [216.184671ms]
Mar  2 21:06:51.059: INFO: Created: latency-svc-2nj64
Mar  2 21:06:51.073: INFO: Got endpoints: latency-svc-2nj64 [236.484436ms]
Mar  2 21:06:51.075: INFO: Created: latency-svc-44x7d
Mar  2 21:06:51.086: INFO: Got endpoints: latency-svc-44x7d [249.686606ms]
Mar  2 21:06:51.089: INFO: Created: latency-svc-9z8qj
Mar  2 21:06:51.115: INFO: Got endpoints: latency-svc-9z8qj [277.912605ms]
Mar  2 21:06:51.116: INFO: Created: latency-svc-j4brr
Mar  2 21:06:51.127: INFO: Got endpoints: latency-svc-j4brr [244.237673ms]
Mar  2 21:06:51.130: INFO: Created: latency-svc-ksl46
Mar  2 21:06:51.148: INFO: Created: latency-svc-qxt6m
Mar  2 21:06:51.153: INFO: Got endpoints: latency-svc-ksl46 [253.600774ms]
Mar  2 21:06:51.171: INFO: Got endpoints: latency-svc-qxt6m [254.764387ms]
Mar  2 21:06:51.176: INFO: Created: latency-svc-hqtzr
Mar  2 21:06:51.187: INFO: Got endpoints: latency-svc-hqtzr [250.249006ms]
Mar  2 21:06:51.188: INFO: Created: latency-svc-gvjjh
Mar  2 21:06:51.200: INFO: Got endpoints: latency-svc-gvjjh [253.254764ms]
Mar  2 21:06:51.234: INFO: Created: latency-svc-j9xvg
Mar  2 21:06:51.234: INFO: Created: latency-svc-ch7qd
Mar  2 21:06:51.235: INFO: Created: latency-svc-gdbml
Mar  2 21:06:51.247: INFO: Created: latency-svc-l5cm4
Mar  2 21:06:51.247: INFO: Got endpoints: latency-svc-j9xvg [272.939006ms]
Mar  2 21:06:51.247: INFO: Got endpoints: latency-svc-gdbml [257.345726ms]
Mar  2 21:06:51.247: INFO: Got endpoints: latency-svc-ch7qd [289.778171ms]
Mar  2 21:06:51.257: INFO: Got endpoints: latency-svc-l5cm4 [247.53172ms]
Mar  2 21:06:51.262: INFO: Created: latency-svc-kkgnr
Mar  2 21:06:51.271: INFO: Got endpoints: latency-svc-kkgnr [246.309516ms]
Mar  2 21:06:51.274: INFO: Created: latency-svc-5zvkl
Mar  2 21:06:51.286: INFO: Got endpoints: latency-svc-5zvkl [247.964178ms]
Mar  2 21:06:51.295: INFO: Created: latency-svc-l4q2g
Mar  2 21:06:51.323: INFO: Got endpoints: latency-svc-l4q2g [270.969561ms]
Mar  2 21:06:51.330: INFO: Created: latency-svc-5g89t
Mar  2 21:06:51.365: INFO: Got endpoints: latency-svc-5g89t [291.018148ms]
Mar  2 21:06:51.366: INFO: Created: latency-svc-l7rn7
Mar  2 21:06:51.377: INFO: Got endpoints: latency-svc-l7rn7 [289.944637ms]
Mar  2 21:06:51.404: INFO: Created: latency-svc-l8vh7
Mar  2 21:06:51.422: INFO: Created: latency-svc-xdc7l
Mar  2 21:06:51.422: INFO: Got endpoints: latency-svc-l8vh7 [307.201131ms]
Mar  2 21:06:51.431: INFO: Got endpoints: latency-svc-xdc7l [303.081004ms]
Mar  2 21:06:51.436: INFO: Created: latency-svc-x548s
Mar  2 21:06:51.449: INFO: Got endpoints: latency-svc-x548s [296.265343ms]
Mar  2 21:06:51.451: INFO: Created: latency-svc-scnb5
Mar  2 21:06:51.462: INFO: Got endpoints: latency-svc-scnb5 [291.091035ms]
Mar  2 21:06:51.475: INFO: Created: latency-svc-hjs6s
Mar  2 21:06:51.499: INFO: Got endpoints: latency-svc-hjs6s [311.61885ms]
Mar  2 21:06:51.509: INFO: Created: latency-svc-9wzf6
Mar  2 21:06:51.523: INFO: Got endpoints: latency-svc-9wzf6 [323.053558ms]
Mar  2 21:06:51.529: INFO: Created: latency-svc-fhzcp
Mar  2 21:06:51.542: INFO: Got endpoints: latency-svc-fhzcp [294.556819ms]
Mar  2 21:06:51.556: INFO: Created: latency-svc-cx8hs
Mar  2 21:06:51.575: INFO: Got endpoints: latency-svc-cx8hs [327.188278ms]
Mar  2 21:06:51.579: INFO: Created: latency-svc-hdtr5
Mar  2 21:06:51.596: INFO: Got endpoints: latency-svc-hdtr5 [348.276959ms]
Mar  2 21:06:51.597: INFO: Created: latency-svc-mxnsz
Mar  2 21:06:51.609: INFO: Got endpoints: latency-svc-mxnsz [351.690787ms]
Mar  2 21:06:51.615: INFO: Created: latency-svc-c4kz7
Mar  2 21:06:51.632: INFO: Got endpoints: latency-svc-c4kz7 [360.652485ms]
Mar  2 21:06:51.634: INFO: Created: latency-svc-b8g2v
Mar  2 21:06:51.645: INFO: Got endpoints: latency-svc-b8g2v [358.366787ms]
Mar  2 21:06:51.666: INFO: Created: latency-svc-wv7mw
Mar  2 21:06:51.677: INFO: Got endpoints: latency-svc-wv7mw [353.883608ms]
Mar  2 21:06:51.682: INFO: Created: latency-svc-gxzb5
Mar  2 21:06:51.692: INFO: Got endpoints: latency-svc-gxzb5 [326.803567ms]
Mar  2 21:06:51.696: INFO: Created: latency-svc-wbkmr
Mar  2 21:06:51.710: INFO: Got endpoints: latency-svc-wbkmr [332.979533ms]
Mar  2 21:06:51.712: INFO: Created: latency-svc-rbwpq
Mar  2 21:06:51.726: INFO: Created: latency-svc-6xvk2
Mar  2 21:06:51.738: INFO: Got endpoints: latency-svc-rbwpq [315.615407ms]
Mar  2 21:06:51.743: INFO: Got endpoints: latency-svc-6xvk2 [312.229347ms]
Mar  2 21:06:51.743: INFO: Created: latency-svc-4l58p
Mar  2 21:06:51.759: INFO: Got endpoints: latency-svc-4l58p [309.340071ms]
Mar  2 21:06:51.761: INFO: Created: latency-svc-84tmn
Mar  2 21:06:51.770: INFO: Got endpoints: latency-svc-84tmn [308.173404ms]
Mar  2 21:06:51.775: INFO: Created: latency-svc-jxhtz
Mar  2 21:06:51.784: INFO: Got endpoints: latency-svc-jxhtz [284.990936ms]
Mar  2 21:06:51.788: INFO: Created: latency-svc-247fz
Mar  2 21:06:51.801: INFO: Got endpoints: latency-svc-247fz [278.39316ms]
Mar  2 21:06:51.809: INFO: Created: latency-svc-7fgzz
Mar  2 21:06:51.830: INFO: Got endpoints: latency-svc-7fgzz [288.337873ms]
Mar  2 21:06:51.834: INFO: Created: latency-svc-q6tgv
Mar  2 21:06:51.849: INFO: Created: latency-svc-8xjnp
Mar  2 21:06:51.849: INFO: Got endpoints: latency-svc-q6tgv [274.482949ms]
Mar  2 21:06:51.860: INFO: Got endpoints: latency-svc-8xjnp [263.74213ms]
Mar  2 21:06:51.867: INFO: Created: latency-svc-8zqlz
Mar  2 21:06:51.878: INFO: Got endpoints: latency-svc-8zqlz [269.073557ms]
Mar  2 21:06:51.880: INFO: Created: latency-svc-xjk85
Mar  2 21:06:51.890: INFO: Got endpoints: latency-svc-xjk85 [258.306926ms]
Mar  2 21:06:51.896: INFO: Created: latency-svc-j965j
Mar  2 21:06:51.907: INFO: Got endpoints: latency-svc-j965j [261.671797ms]
Mar  2 21:06:51.913: INFO: Created: latency-svc-r7ck4
Mar  2 21:06:51.923: INFO: Got endpoints: latency-svc-r7ck4 [245.771977ms]
Mar  2 21:06:51.929: INFO: Created: latency-svc-xrm5s
Mar  2 21:06:51.942: INFO: Created: latency-svc-g56rw
Mar  2 21:06:51.943: INFO: Got endpoints: latency-svc-xrm5s [250.919771ms]
Mar  2 21:06:51.955: INFO: Got endpoints: latency-svc-g56rw [245.083994ms]
Mar  2 21:06:51.958: INFO: Created: latency-svc-ndg2v
Mar  2 21:06:51.970: INFO: Got endpoints: latency-svc-ndg2v [231.815548ms]
Mar  2 21:06:51.976: INFO: Created: latency-svc-7bd7v
Mar  2 21:06:51.990: INFO: Got endpoints: latency-svc-7bd7v [247.075338ms]
Mar  2 21:06:51.992: INFO: Created: latency-svc-ljg7s
Mar  2 21:06:52.006: INFO: Got endpoints: latency-svc-ljg7s [246.762464ms]
Mar  2 21:06:52.007: INFO: Created: latency-svc-6spcc
Mar  2 21:06:52.016: INFO: Got endpoints: latency-svc-6spcc [246.081049ms]
Mar  2 21:06:52.023: INFO: Created: latency-svc-pvmzr
Mar  2 21:06:52.032: INFO: Got endpoints: latency-svc-pvmzr [247.218799ms]
Mar  2 21:06:52.038: INFO: Created: latency-svc-tgspn
Mar  2 21:06:52.054: INFO: Got endpoints: latency-svc-tgspn [252.255538ms]
Mar  2 21:06:52.058: INFO: Created: latency-svc-csbxd
Mar  2 21:06:52.073: INFO: Got endpoints: latency-svc-csbxd [243.031751ms]
Mar  2 21:06:52.074: INFO: Created: latency-svc-6wnsp
Mar  2 21:06:52.101: INFO: Created: latency-svc-hjlvc
Mar  2 21:06:52.101: INFO: Got endpoints: latency-svc-6wnsp [252.080713ms]
Mar  2 21:06:52.105: INFO: Got endpoints: latency-svc-hjlvc [245.076432ms]
Mar  2 21:06:52.108: INFO: Created: latency-svc-v625q
Mar  2 21:06:52.127: INFO: Got endpoints: latency-svc-v625q [248.843421ms]
Mar  2 21:06:52.129: INFO: Created: latency-svc-ctb9f
Mar  2 21:06:52.143: INFO: Got endpoints: latency-svc-ctb9f [252.166127ms]
Mar  2 21:06:52.143: INFO: Created: latency-svc-47w9v
Mar  2 21:06:52.155: INFO: Got endpoints: latency-svc-47w9v [248.058897ms]
Mar  2 21:06:52.162: INFO: Created: latency-svc-vkj47
Mar  2 21:06:52.172: INFO: Got endpoints: latency-svc-vkj47 [249.070083ms]
Mar  2 21:06:52.178: INFO: Created: latency-svc-f6jbc
Mar  2 21:06:52.216: INFO: Got endpoints: latency-svc-f6jbc [273.060881ms]
Mar  2 21:06:52.222: INFO: Created: latency-svc-8z5zr
Mar  2 21:06:52.233: INFO: Got endpoints: latency-svc-8z5zr [278.019064ms]
Mar  2 21:06:52.235: INFO: Created: latency-svc-fw6wm
Mar  2 21:06:52.250: INFO: Created: latency-svc-pz7s6
Mar  2 21:06:52.260: INFO: Got endpoints: latency-svc-fw6wm [290.12723ms]
Mar  2 21:06:52.262: INFO: Got endpoints: latency-svc-pz7s6 [271.449852ms]
Mar  2 21:06:52.270: INFO: Created: latency-svc-n2k5t
Mar  2 21:06:52.279: INFO: Got endpoints: latency-svc-n2k5t [273.530972ms]
Mar  2 21:06:52.286: INFO: Created: latency-svc-7mxn6
Mar  2 21:06:52.297: INFO: Got endpoints: latency-svc-7mxn6 [280.358106ms]
Mar  2 21:06:52.303: INFO: Created: latency-svc-5bdg6
Mar  2 21:06:52.313: INFO: Got endpoints: latency-svc-5bdg6 [281.309641ms]
Mar  2 21:06:52.321: INFO: Created: latency-svc-rsq4d
Mar  2 21:06:52.333: INFO: Got endpoints: latency-svc-rsq4d [279.553291ms]
Mar  2 21:06:52.341: INFO: Created: latency-svc-6cf24
Mar  2 21:06:52.355: INFO: Got endpoints: latency-svc-6cf24 [281.280311ms]
Mar  2 21:06:52.365: INFO: Created: latency-svc-4rp98
Mar  2 21:06:52.387: INFO: Got endpoints: latency-svc-4rp98 [285.385766ms]
Mar  2 21:06:52.394: INFO: Created: latency-svc-l9c5x
Mar  2 21:06:52.406: INFO: Got endpoints: latency-svc-l9c5x [301.463734ms]
Mar  2 21:06:52.435: INFO: Created: latency-svc-sc8dm
Mar  2 21:06:52.457: INFO: Got endpoints: latency-svc-sc8dm [329.553809ms]
Mar  2 21:06:52.485: INFO: Created: latency-svc-ghpl5
Mar  2 21:06:52.496: INFO: Created: latency-svc-vtzqd
Mar  2 21:06:52.501: INFO: Got endpoints: latency-svc-ghpl5 [358.826596ms]
Mar  2 21:06:52.585: INFO: Got endpoints: latency-svc-vtzqd [430.245466ms]
Mar  2 21:06:52.601: INFO: Created: latency-svc-rx976
Mar  2 21:06:52.637: INFO: Created: latency-svc-twxd6
Mar  2 21:06:52.638: INFO: Got endpoints: latency-svc-rx976 [466.066963ms]
Mar  2 21:06:52.695: INFO: Created: latency-svc-qwmz2
Mar  2 21:06:52.696: INFO: Got endpoints: latency-svc-twxd6 [194.863635ms]
Mar  2 21:06:52.715: INFO: Created: latency-svc-h7c6g
Mar  2 21:06:52.715: INFO: Got endpoints: latency-svc-qwmz2 [498.841385ms]
Mar  2 21:06:52.722: INFO: Got endpoints: latency-svc-h7c6g [488.976329ms]
Mar  2 21:06:52.725: INFO: Created: latency-svc-h4sm8
Mar  2 21:06:52.736: INFO: Got endpoints: latency-svc-h4sm8 [475.522364ms]
Mar  2 21:06:52.739: INFO: Created: latency-svc-qsdcx
Mar  2 21:06:52.751: INFO: Got endpoints: latency-svc-qsdcx [488.869551ms]
Mar  2 21:06:52.785: INFO: Created: latency-svc-86lgv
Mar  2 21:06:52.793: INFO: Created: latency-svc-pnng8
Mar  2 21:06:52.806: INFO: Got endpoints: latency-svc-86lgv [526.714394ms]
Mar  2 21:06:52.808: INFO: Got endpoints: latency-svc-pnng8 [510.913447ms]
Mar  2 21:06:52.814: INFO: Created: latency-svc-nb6st
Mar  2 21:06:52.821: INFO: Created: latency-svc-mzmk6
Mar  2 21:06:52.827: INFO: Got endpoints: latency-svc-nb6st [513.229529ms]
Mar  2 21:06:52.832: INFO: Got endpoints: latency-svc-mzmk6 [498.850983ms]
Mar  2 21:06:52.836: INFO: Created: latency-svc-hkfhc
Mar  2 21:06:52.850: INFO: Got endpoints: latency-svc-hkfhc [495.772042ms]
Mar  2 21:06:52.863: INFO: Created: latency-svc-rx5bf
Mar  2 21:06:52.900: INFO: Got endpoints: latency-svc-rx5bf [512.008695ms]
Mar  2 21:06:52.907: INFO: Created: latency-svc-g7krl
Mar  2 21:06:52.951: INFO: Got endpoints: latency-svc-g7krl [544.649705ms]
Mar  2 21:06:52.952: INFO: Created: latency-svc-rljmk
Mar  2 21:06:52.962: INFO: Got endpoints: latency-svc-rljmk [505.692372ms]
Mar  2 21:06:52.968: INFO: Created: latency-svc-pmzc5
Mar  2 21:06:52.994: INFO: Created: latency-svc-8l4d6
Mar  2 21:06:52.996: INFO: Got endpoints: latency-svc-pmzc5 [410.171987ms]
Mar  2 21:06:53.004: INFO: Got endpoints: latency-svc-8l4d6 [365.960689ms]
Mar  2 21:06:53.012: INFO: Created: latency-svc-dvhvq
Mar  2 21:06:53.022: INFO: Got endpoints: latency-svc-dvhvq [325.567857ms]
Mar  2 21:06:53.025: INFO: Created: latency-svc-864c5
Mar  2 21:06:53.033: INFO: Got endpoints: latency-svc-864c5 [318.567356ms]
Mar  2 21:06:53.039: INFO: Created: latency-svc-f55qk
Mar  2 21:06:53.081: INFO: Got endpoints: latency-svc-f55qk [345.707062ms]
Mar  2 21:06:53.097: INFO: Created: latency-svc-vbdwt
Mar  2 21:06:53.110: INFO: Got endpoints: latency-svc-vbdwt [387.204459ms]
Mar  2 21:06:53.115: INFO: Created: latency-svc-wqh2v
Mar  2 21:06:53.128: INFO: Got endpoints: latency-svc-wqh2v [376.638154ms]
Mar  2 21:06:53.132: INFO: Created: latency-svc-zkjd6
Mar  2 21:06:53.150: INFO: Got endpoints: latency-svc-zkjd6 [343.859255ms]
Mar  2 21:06:53.165: INFO: Created: latency-svc-t6wgq
Mar  2 21:06:53.179: INFO: Got endpoints: latency-svc-t6wgq [371.127426ms]
Mar  2 21:06:53.188: INFO: Created: latency-svc-xshdz
Mar  2 21:06:53.207: INFO: Got endpoints: latency-svc-xshdz [379.707033ms]
Mar  2 21:06:53.214: INFO: Created: latency-svc-9zsjt
Mar  2 21:06:53.256: INFO: Got endpoints: latency-svc-9zsjt [423.734541ms]
Mar  2 21:06:53.257: INFO: Created: latency-svc-t668h
Mar  2 21:06:53.271: INFO: Got endpoints: latency-svc-t668h [420.594921ms]
Mar  2 21:06:53.274: INFO: Created: latency-svc-wspb8
Mar  2 21:06:53.286: INFO: Got endpoints: latency-svc-wspb8 [386.721723ms]
Mar  2 21:06:53.323: INFO: Created: latency-svc-7lhhq
Mar  2 21:06:53.349: INFO: Got endpoints: latency-svc-7lhhq [398.126616ms]
Mar  2 21:06:53.355: INFO: Created: latency-svc-z25nw
Mar  2 21:06:53.367: INFO: Got endpoints: latency-svc-z25nw [398.555994ms]
Mar  2 21:06:53.369: INFO: Created: latency-svc-mwljc
Mar  2 21:06:53.381: INFO: Got endpoints: latency-svc-mwljc [385.304226ms]
Mar  2 21:06:53.386: INFO: Created: latency-svc-ck9ts
Mar  2 21:06:53.425: INFO: Got endpoints: latency-svc-ck9ts [419.595624ms]
Mar  2 21:06:53.434: INFO: Created: latency-svc-89gbc
Mar  2 21:06:53.450: INFO: Got endpoints: latency-svc-89gbc [427.859479ms]
Mar  2 21:06:53.480: INFO: Created: latency-svc-m27gv
Mar  2 21:06:53.494: INFO: Got endpoints: latency-svc-m27gv [460.373437ms]
Mar  2 21:06:53.496: INFO: Created: latency-svc-tghzz
Mar  2 21:06:53.510: INFO: Created: latency-svc-mv6lg
Mar  2 21:06:53.511: INFO: Got endpoints: latency-svc-tghzz [429.790624ms]
Mar  2 21:06:53.526: INFO: Created: latency-svc-ql72t
Mar  2 21:06:53.539: INFO: Got endpoints: latency-svc-mv6lg [429.416246ms]
Mar  2 21:06:53.540: INFO: Got endpoints: latency-svc-ql72t [412.199419ms]
Mar  2 21:06:53.557: INFO: Created: latency-svc-qv96r
Mar  2 21:06:53.573: INFO: Got endpoints: latency-svc-qv96r [422.957852ms]
Mar  2 21:06:53.577: INFO: Created: latency-svc-5p8nq
Mar  2 21:06:53.595: INFO: Got endpoints: latency-svc-5p8nq [415.217326ms]
Mar  2 21:06:53.599: INFO: Created: latency-svc-nmwks
Mar  2 21:06:53.609: INFO: Got endpoints: latency-svc-nmwks [402.21217ms]
Mar  2 21:06:53.618: INFO: Created: latency-svc-zs2mf
Mar  2 21:06:53.633: INFO: Got endpoints: latency-svc-zs2mf [376.576475ms]
Mar  2 21:06:53.634: INFO: Created: latency-svc-bkstd
Mar  2 21:06:53.651: INFO: Got endpoints: latency-svc-bkstd [379.365857ms]
Mar  2 21:06:53.657: INFO: Created: latency-svc-l92mg
Mar  2 21:06:53.669: INFO: Got endpoints: latency-svc-l92mg [382.842494ms]
Mar  2 21:06:53.679: INFO: Created: latency-svc-sghc6
Mar  2 21:06:53.692: INFO: Got endpoints: latency-svc-sghc6 [342.963586ms]
Mar  2 21:06:53.705: INFO: Created: latency-svc-xss56
Mar  2 21:06:53.716: INFO: Got endpoints: latency-svc-xss56 [348.91742ms]
Mar  2 21:06:53.717: INFO: Created: latency-svc-8wvtt
Mar  2 21:06:53.727: INFO: Got endpoints: latency-svc-8wvtt [346.014478ms]
Mar  2 21:06:53.732: INFO: Created: latency-svc-7vps2
Mar  2 21:06:53.748: INFO: Got endpoints: latency-svc-7vps2 [322.95735ms]
Mar  2 21:06:53.750: INFO: Created: latency-svc-99r8g
Mar  2 21:06:53.760: INFO: Got endpoints: latency-svc-99r8g [310.524998ms]
Mar  2 21:06:53.769: INFO: Created: latency-svc-gtm9w
Mar  2 21:06:53.777: INFO: Got endpoints: latency-svc-gtm9w [283.432931ms]
Mar  2 21:06:53.778: INFO: Created: latency-svc-cd8vn
Mar  2 21:06:53.789: INFO: Got endpoints: latency-svc-cd8vn [278.209259ms]
Mar  2 21:06:53.803: INFO: Created: latency-svc-79lth
Mar  2 21:06:53.835: INFO: Got endpoints: latency-svc-79lth [295.487169ms]
Mar  2 21:06:53.838: INFO: Created: latency-svc-l8k95
Mar  2 21:06:53.854: INFO: Got endpoints: latency-svc-l8k95 [313.678542ms]
Mar  2 21:06:53.858: INFO: Created: latency-svc-6hln8
Mar  2 21:06:53.869: INFO: Got endpoints: latency-svc-6hln8 [295.890957ms]
Mar  2 21:06:53.874: INFO: Created: latency-svc-mrnz2
Mar  2 21:06:53.885: INFO: Got endpoints: latency-svc-mrnz2 [290.776182ms]
Mar  2 21:06:53.893: INFO: Created: latency-svc-d9dxz
Mar  2 21:06:53.907: INFO: Got endpoints: latency-svc-d9dxz [298.417464ms]
Mar  2 21:06:53.913: INFO: Created: latency-svc-6dhgn
Mar  2 21:06:53.925: INFO: Got endpoints: latency-svc-6dhgn [292.287896ms]
Mar  2 21:06:53.927: INFO: Created: latency-svc-wngkp
Mar  2 21:06:53.942: INFO: Got endpoints: latency-svc-wngkp [291.310251ms]
Mar  2 21:06:53.945: INFO: Created: latency-svc-ws7fn
Mar  2 21:06:53.957: INFO: Got endpoints: latency-svc-ws7fn [287.865217ms]
Mar  2 21:06:53.965: INFO: Created: latency-svc-wg9vc
Mar  2 21:06:53.974: INFO: Got endpoints: latency-svc-wg9vc [281.180405ms]
Mar  2 21:06:53.977: INFO: Created: latency-svc-xn8vb
Mar  2 21:06:53.990: INFO: Created: latency-svc-kz77r
Mar  2 21:06:53.999: INFO: Got endpoints: latency-svc-xn8vb [282.737585ms]
Mar  2 21:06:54.002: INFO: Got endpoints: latency-svc-kz77r [274.090883ms]
Mar  2 21:06:54.024: INFO: Created: latency-svc-2d7bs
Mar  2 21:06:54.024: INFO: Created: latency-svc-jg4fm
Mar  2 21:06:54.036: INFO: Got endpoints: latency-svc-jg4fm [275.052634ms]
Mar  2 21:06:54.038: INFO: Got endpoints: latency-svc-2d7bs [289.807419ms]
Mar  2 21:06:54.040: INFO: Created: latency-svc-qdths
Mar  2 21:06:54.052: INFO: Got endpoints: latency-svc-qdths [274.598759ms]
Mar  2 21:06:54.054: INFO: Created: latency-svc-fbjs6
Mar  2 21:06:54.067: INFO: Got endpoints: latency-svc-fbjs6 [277.678531ms]
Mar  2 21:06:54.070: INFO: Created: latency-svc-959bl
Mar  2 21:06:54.094: INFO: Got endpoints: latency-svc-959bl [259.214067ms]
Mar  2 21:06:54.095: INFO: Created: latency-svc-6lv75
Mar  2 21:06:54.105: INFO: Got endpoints: latency-svc-6lv75 [251.016435ms]
Mar  2 21:06:54.125: INFO: Created: latency-svc-qjk72
Mar  2 21:06:54.151: INFO: Got endpoints: latency-svc-qjk72 [282.076837ms]
Mar  2 21:06:54.155: INFO: Created: latency-svc-g7lr4
Mar  2 21:06:54.167: INFO: Got endpoints: latency-svc-g7lr4 [281.27933ms]
Mar  2 21:06:54.174: INFO: Created: latency-svc-z7mz2
Mar  2 21:06:54.186: INFO: Got endpoints: latency-svc-z7mz2 [278.672281ms]
Mar  2 21:06:54.196: INFO: Created: latency-svc-6lxkj
Mar  2 21:06:54.210: INFO: Got endpoints: latency-svc-6lxkj [284.454506ms]
Mar  2 21:06:54.212: INFO: Created: latency-svc-7wc6z
Mar  2 21:06:54.228: INFO: Got endpoints: latency-svc-7wc6z [285.486893ms]
Mar  2 21:06:54.233: INFO: Created: latency-svc-hd5qd
Mar  2 21:06:54.249: INFO: Got endpoints: latency-svc-hd5qd [291.649372ms]
Mar  2 21:06:54.252: INFO: Created: latency-svc-9l66z
Mar  2 21:06:54.270: INFO: Created: latency-svc-8gk9b
Mar  2 21:06:54.271: INFO: Got endpoints: latency-svc-9l66z [296.880113ms]
Mar  2 21:06:54.281: INFO: Got endpoints: latency-svc-8gk9b [282.194681ms]
Mar  2 21:06:54.286: INFO: Created: latency-svc-9xl5v
Mar  2 21:06:54.296: INFO: Got endpoints: latency-svc-9xl5v [294.517218ms]
Mar  2 21:06:54.298: INFO: Created: latency-svc-6thgl
Mar  2 21:06:54.308: INFO: Got endpoints: latency-svc-6thgl [271.726691ms]
Mar  2 21:06:54.315: INFO: Created: latency-svc-n579m
Mar  2 21:06:54.328: INFO: Got endpoints: latency-svc-n579m [290.187776ms]
Mar  2 21:06:54.334: INFO: Created: latency-svc-klqnb
Mar  2 21:06:54.365: INFO: Got endpoints: latency-svc-klqnb [312.963158ms]
Mar  2 21:06:54.369: INFO: Created: latency-svc-gbvrq
Mar  2 21:06:54.393: INFO: Got endpoints: latency-svc-gbvrq [325.028211ms]
Mar  2 21:06:54.397: INFO: Created: latency-svc-p4cxl
Mar  2 21:06:54.415: INFO: Created: latency-svc-748mq
Mar  2 21:06:54.422: INFO: Got endpoints: latency-svc-p4cxl [327.740055ms]
Mar  2 21:06:54.431: INFO: Got endpoints: latency-svc-748mq [325.835416ms]
Mar  2 21:06:54.435: INFO: Created: latency-svc-fvfpx
Mar  2 21:06:54.449: INFO: Got endpoints: latency-svc-fvfpx [298.121373ms]
Mar  2 21:06:54.451: INFO: Created: latency-svc-lq4qh
Mar  2 21:06:54.466: INFO: Created: latency-svc-tcrqb
Mar  2 21:06:54.470: INFO: Got endpoints: latency-svc-lq4qh [301.724266ms]
Mar  2 21:06:54.477: INFO: Got endpoints: latency-svc-tcrqb [290.773536ms]
Mar  2 21:06:54.483: INFO: Created: latency-svc-5tgm4
Mar  2 21:06:54.493: INFO: Got endpoints: latency-svc-5tgm4 [283.30703ms]
Mar  2 21:06:54.500: INFO: Created: latency-svc-9zw7s
Mar  2 21:06:54.511: INFO: Got endpoints: latency-svc-9zw7s [283.714942ms]
Mar  2 21:06:54.517: INFO: Created: latency-svc-95hld
Mar  2 21:06:54.529: INFO: Got endpoints: latency-svc-95hld [279.691811ms]
Mar  2 21:06:54.531: INFO: Created: latency-svc-r256k
Mar  2 21:06:54.545: INFO: Got endpoints: latency-svc-r256k [274.20791ms]
Mar  2 21:06:54.547: INFO: Created: latency-svc-rfmkf
Mar  2 21:06:54.557: INFO: Got endpoints: latency-svc-rfmkf [276.172004ms]
Mar  2 21:06:54.563: INFO: Created: latency-svc-9lj92
Mar  2 21:06:54.574: INFO: Got endpoints: latency-svc-9lj92 [277.737851ms]
Mar  2 21:06:54.576: INFO: Created: latency-svc-4jxb5
Mar  2 21:06:54.586: INFO: Got endpoints: latency-svc-4jxb5 [278.00304ms]
Mar  2 21:06:54.597: INFO: Created: latency-svc-mlrtv
Mar  2 21:06:54.628: INFO: Got endpoints: latency-svc-mlrtv [299.574153ms]
Mar  2 21:06:54.632: INFO: Created: latency-svc-8bkzx
Mar  2 21:06:54.645: INFO: Got endpoints: latency-svc-8bkzx [279.822987ms]
Mar  2 21:06:54.680: INFO: Created: latency-svc-7g2v6
Mar  2 21:06:54.715: INFO: Got endpoints: latency-svc-7g2v6 [321.413679ms]
Mar  2 21:06:54.718: INFO: Created: latency-svc-vzpxx
Mar  2 21:06:54.729: INFO: Got endpoints: latency-svc-vzpxx [307.089737ms]
Mar  2 21:06:54.740: INFO: Created: latency-svc-m5g2k
Mar  2 21:06:54.754: INFO: Got endpoints: latency-svc-m5g2k [323.284208ms]
Mar  2 21:06:54.789: INFO: Created: latency-svc-n9k9d
Mar  2 21:06:54.810: INFO: Got endpoints: latency-svc-n9k9d [360.7218ms]
Mar  2 21:06:54.812: INFO: Created: latency-svc-h9kk2
Mar  2 21:06:54.830: INFO: Got endpoints: latency-svc-h9kk2 [360.172085ms]
Mar  2 21:06:54.837: INFO: Created: latency-svc-brtnw
Mar  2 21:06:54.851: INFO: Got endpoints: latency-svc-brtnw [373.749989ms]
Mar  2 21:06:54.851: INFO: Created: latency-svc-zdd2r
Mar  2 21:06:54.860: INFO: Got endpoints: latency-svc-zdd2r [366.541011ms]
Mar  2 21:06:54.865: INFO: Created: latency-svc-2zlfq
Mar  2 21:06:54.875: INFO: Got endpoints: latency-svc-2zlfq [363.87367ms]
Mar  2 21:06:54.879: INFO: Created: latency-svc-c79l7
Mar  2 21:06:54.891: INFO: Got endpoints: latency-svc-c79l7 [362.638528ms]
Mar  2 21:06:54.893: INFO: Created: latency-svc-bj9ss
Mar  2 21:06:54.911: INFO: Got endpoints: latency-svc-bj9ss [365.893529ms]
Mar  2 21:06:54.911: INFO: Created: latency-svc-v6tkh
Mar  2 21:06:54.924: INFO: Got endpoints: latency-svc-v6tkh [366.269235ms]
Mar  2 21:06:54.926: INFO: Created: latency-svc-x4pd6
Mar  2 21:06:54.935: INFO: Got endpoints: latency-svc-x4pd6 [360.72793ms]
Mar  2 21:06:54.941: INFO: Created: latency-svc-5qm5c
Mar  2 21:06:54.964: INFO: Created: latency-svc-d8976
Mar  2 21:06:54.978: INFO: Created: latency-svc-t2ssq
Mar  2 21:06:54.983: INFO: Got endpoints: latency-svc-5qm5c [397.005979ms]
Mar  2 21:06:54.983: INFO: Got endpoints: latency-svc-d8976 [353.952551ms]
Mar  2 21:06:54.989: INFO: Got endpoints: latency-svc-t2ssq [343.950088ms]
Mar  2 21:06:54.997: INFO: Created: latency-svc-9jqjw
Mar  2 21:06:55.011: INFO: Got endpoints: latency-svc-9jqjw [296.58935ms]
Mar  2 21:06:55.013: INFO: Created: latency-svc-zrpmk
Mar  2 21:06:55.022: INFO: Got endpoints: latency-svc-zrpmk [292.845303ms]
Mar  2 21:06:55.028: INFO: Created: latency-svc-cr5s9
Mar  2 21:06:55.046: INFO: Created: latency-svc-xc74f
Mar  2 21:06:55.047: INFO: Got endpoints: latency-svc-cr5s9 [292.440724ms]
Mar  2 21:06:55.072: INFO: Got endpoints: latency-svc-xc74f [262.585723ms]
Mar  2 21:06:55.076: INFO: Created: latency-svc-ch2md
Mar  2 21:06:55.087: INFO: Got endpoints: latency-svc-ch2md [257.160953ms]
Mar  2 21:06:55.090: INFO: Created: latency-svc-qdnxp
Mar  2 21:06:55.101: INFO: Got endpoints: latency-svc-qdnxp [249.977634ms]
Mar  2 21:06:55.101: INFO: Latencies: [48.346785ms 62.282654ms 80.53311ms 102.536955ms 110.883459ms 121.83568ms 138.372065ms 153.253709ms 173.36358ms 189.20507ms 194.863635ms 201.459073ms 216.184671ms 231.815548ms 236.484436ms 243.031751ms 244.237673ms 245.076432ms 245.083994ms 245.771977ms 246.081049ms 246.309516ms 246.762464ms 247.075338ms 247.218799ms 247.53172ms 247.964178ms 248.058897ms 248.843421ms 249.070083ms 249.686606ms 249.977634ms 250.249006ms 250.919771ms 251.016435ms 252.080713ms 252.166127ms 252.255538ms 253.254764ms 253.600774ms 254.764387ms 257.160953ms 257.345726ms 258.306926ms 259.214067ms 261.671797ms 262.585723ms 263.74213ms 269.073557ms 270.969561ms 271.449852ms 271.726691ms 272.939006ms 273.060881ms 273.530972ms 274.090883ms 274.20791ms 274.482949ms 274.598759ms 275.052634ms 276.172004ms 277.678531ms 277.737851ms 277.912605ms 278.00304ms 278.019064ms 278.209259ms 278.39316ms 278.672281ms 279.553291ms 279.691811ms 279.822987ms 280.358106ms 281.180405ms 281.27933ms 281.280311ms 281.309641ms 282.076837ms 282.194681ms 282.737585ms 283.30703ms 283.432931ms 283.714942ms 284.454506ms 284.990936ms 285.385766ms 285.486893ms 287.865217ms 288.337873ms 289.778171ms 289.807419ms 289.944637ms 290.12723ms 290.187776ms 290.773536ms 290.776182ms 291.018148ms 291.091035ms 291.310251ms 291.649372ms 292.287896ms 292.440724ms 292.845303ms 294.517218ms 294.556819ms 295.487169ms 295.890957ms 296.265343ms 296.58935ms 296.880113ms 298.121373ms 298.417464ms 299.574153ms 301.463734ms 301.724266ms 303.081004ms 307.089737ms 307.201131ms 308.173404ms 309.340071ms 310.524998ms 311.61885ms 312.229347ms 312.963158ms 313.678542ms 315.615407ms 318.567356ms 321.413679ms 322.95735ms 323.053558ms 323.284208ms 325.028211ms 325.567857ms 325.835416ms 326.803567ms 327.188278ms 327.740055ms 329.553809ms 332.979533ms 342.963586ms 343.859255ms 343.950088ms 345.707062ms 346.014478ms 348.276959ms 348.91742ms 351.690787ms 353.883608ms 353.952551ms 358.366787ms 358.826596ms 360.172085ms 360.652485ms 360.7218ms 360.72793ms 362.638528ms 363.87367ms 365.893529ms 365.960689ms 366.269235ms 366.541011ms 371.127426ms 373.749989ms 376.576475ms 376.638154ms 379.365857ms 379.707033ms 382.842494ms 385.304226ms 386.721723ms 387.204459ms 397.005979ms 398.126616ms 398.555994ms 402.21217ms 410.171987ms 412.199419ms 415.217326ms 419.595624ms 420.594921ms 422.957852ms 423.734541ms 427.859479ms 429.416246ms 429.790624ms 430.245466ms 460.373437ms 466.066963ms 475.522364ms 488.869551ms 488.976329ms 495.772042ms 498.841385ms 498.850983ms 505.692372ms 510.913447ms 512.008695ms 513.229529ms 526.714394ms 544.649705ms]
Mar  2 21:06:55.101: INFO: 50 %ile: 292.287896ms
Mar  2 21:06:55.101: INFO: 90 %ile: 422.957852ms
Mar  2 21:06:55.101: INFO: 99 %ile: 526.714394ms
Mar  2 21:06:55.101: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:06:55.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6349" for this suite.

• [SLOW TEST:7.833 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":340,"skipped":5961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:06:55.141: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3726
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 21:06:55.409: INFO: The status of Pod busybox-readonly-fs478e52b0-84aa-476a-8fb3-5fd5690f2ac6 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 21:06:57.428: INFO: The status of Pod busybox-readonly-fs478e52b0-84aa-476a-8fb3-5fd5690f2ac6 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:06:57.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3726" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":341,"skipped":5992,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:06:57.609: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1619
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1619
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 21:06:57.886: INFO: Found 0 stateful pods, waiting for 1
Mar  2 21:07:07.907: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Mar  2 21:07:07.975: INFO: Found 1 stateful pods, waiting for 2
Mar  2 21:07:18.001: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 21:07:18.001: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Mar  2 21:07:18.105: INFO: Deleting all statefulset in ns statefulset-1619
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:07:18.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1619" for this suite.

• [SLOW TEST:20.573 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":342,"skipped":5999,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:07:18.183: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1853
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 21:07:18.462: INFO: The status of Pod test-webserver-bf0b3b49-8ae7-4f50-9e0a-edba2c246750 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 21:07:20.489: INFO: The status of Pod test-webserver-bf0b3b49-8ae7-4f50-9e0a-edba2c246750 is Running (Ready = false)
Mar  2 21:07:22.486: INFO: The status of Pod test-webserver-bf0b3b49-8ae7-4f50-9e0a-edba2c246750 is Running (Ready = false)
Mar  2 21:07:24.487: INFO: The status of Pod test-webserver-bf0b3b49-8ae7-4f50-9e0a-edba2c246750 is Running (Ready = false)
Mar  2 21:07:26.479: INFO: The status of Pod test-webserver-bf0b3b49-8ae7-4f50-9e0a-edba2c246750 is Running (Ready = false)
Mar  2 21:07:28.491: INFO: The status of Pod test-webserver-bf0b3b49-8ae7-4f50-9e0a-edba2c246750 is Running (Ready = false)
Mar  2 21:07:30.483: INFO: The status of Pod test-webserver-bf0b3b49-8ae7-4f50-9e0a-edba2c246750 is Running (Ready = false)
Mar  2 21:07:32.486: INFO: The status of Pod test-webserver-bf0b3b49-8ae7-4f50-9e0a-edba2c246750 is Running (Ready = false)
Mar  2 21:07:34.485: INFO: The status of Pod test-webserver-bf0b3b49-8ae7-4f50-9e0a-edba2c246750 is Running (Ready = false)
Mar  2 21:07:36.480: INFO: The status of Pod test-webserver-bf0b3b49-8ae7-4f50-9e0a-edba2c246750 is Running (Ready = false)
Mar  2 21:07:38.486: INFO: The status of Pod test-webserver-bf0b3b49-8ae7-4f50-9e0a-edba2c246750 is Running (Ready = false)
Mar  2 21:07:40.485: INFO: The status of Pod test-webserver-bf0b3b49-8ae7-4f50-9e0a-edba2c246750 is Running (Ready = true)
Mar  2 21:07:40.495: INFO: Container started at 2022-03-02 21:07:19 +0000 UTC, pod became ready at 2022-03-02 21:07:38 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:07:40.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1853" for this suite.

• [SLOW TEST:22.362 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":343,"skipped":6002,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:07:40.545: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6597
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 21:07:43.904: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:07:43.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6597" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":344,"skipped":6008,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:07:44.053: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 21:07:44.339: INFO: The status of Pod server-envvars-74cba950-4d89-4e0c-bbd6-9fbd1180ca04 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 21:07:46.361: INFO: The status of Pod server-envvars-74cba950-4d89-4e0c-bbd6-9fbd1180ca04 is Running (Ready = true)
Mar  2 21:07:46.493: INFO: Waiting up to 5m0s for pod "client-envvars-0374a5b8-3d54-4e91-a9cc-937af1723f98" in namespace "pods-1482" to be "Succeeded or Failed"
Mar  2 21:07:46.519: INFO: Pod "client-envvars-0374a5b8-3d54-4e91-a9cc-937af1723f98": Phase="Pending", Reason="", readiness=false. Elapsed: 26.111056ms
Mar  2 21:07:48.543: INFO: Pod "client-envvars-0374a5b8-3d54-4e91-a9cc-937af1723f98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049573117s
Mar  2 21:07:50.566: INFO: Pod "client-envvars-0374a5b8-3d54-4e91-a9cc-937af1723f98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072644553s
STEP: Saw pod success
Mar  2 21:07:50.566: INFO: Pod "client-envvars-0374a5b8-3d54-4e91-a9cc-937af1723f98" satisfied condition "Succeeded or Failed"
Mar  2 21:07:50.577: INFO: Trying to get logs from node 10.245.0.4 pod client-envvars-0374a5b8-3d54-4e91-a9cc-937af1723f98 container env3cont: <nil>
STEP: delete the pod
Mar  2 21:07:50.701: INFO: Waiting for pod client-envvars-0374a5b8-3d54-4e91-a9cc-937af1723f98 to disappear
Mar  2 21:07:50.713: INFO: Pod client-envvars-0374a5b8-3d54-4e91-a9cc-937af1723f98 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:07:50.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1482" for this suite.

• [SLOW TEST:6.708 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":345,"skipped":6018,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar  2 21:07:50.763: INFO: >>> kubeConfig: /tmp/kubeconfig-623047118
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1451
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar  2 21:07:50.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-623047118 --namespace=kubectl-1451 version'
Mar  2 21:07:51.065: INFO: stderr: ""
Mar  2 21:07:51.065: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.7\", GitCommit:\"b56e432f2191419647a6a13b9f5867801850f969\", GitTreeState:\"clean\", BuildDate:\"2022-02-16T11:50:27Z\", GoVersion:\"go1.16.14\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.7+IKS\", GitCommit:\"1976f8d09f3bc8d5eb229492c4475915fa212357\", GitTreeState:\"clean\", BuildDate:\"2022-02-18T13:16:39Z\", GoVersion:\"go1.16.14\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar  2 21:07:51.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1451" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":346,"skipped":6044,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSMar  2 21:07:51.103: INFO: Running AfterSuite actions on all nodes
Mar  2 21:07:51.103: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Mar  2 21:07:51.103: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Mar  2 21:07:51.103: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Mar  2 21:07:51.103: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Mar  2 21:07:51.103: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Mar  2 21:07:51.103: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Mar  2 21:07:51.103: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Mar  2 21:07:51.103: INFO: Running AfterSuite actions on node 1
Mar  2 21:07:51.103: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6088,"failed":0}

Ran 346 of 6434 Specs in 6379.367 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6088 Skipped
PASS

Ginkgo ran 1 suite in 1h46m21.061458697s
Test Suite Passed
