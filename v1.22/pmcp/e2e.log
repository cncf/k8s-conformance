I0607 13:33:06.151021      23 e2e.go:129] Starting e2e run "a63240b1-8262-47d7-ba40-543c78f25b7a" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1654608786 - Will randomize all specs
Will run 346 of 6434 specs

Jun  7 13:33:07.715: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:33:07.718: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0607 13:33:07.717933      23 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jun  7 13:33:07.749: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun  7 13:33:07.796: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun  7 13:33:07.797: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jun  7 13:33:07.797: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun  7 13:33:07.808: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun  7 13:33:07.808: INFO: e2e test version: v1.22.7
Jun  7 13:33:07.809: INFO: kube-apiserver version: v1.22.7
Jun  7 13:33:07.809: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:33:07.813: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:33:07.814: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
W0607 13:33:07.851823      23 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jun  7 13:33:07.852: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jun  7 13:33:07.858: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-9c1abcf8-88f4-4e54-8c52-e98dff77ba26
STEP: Creating secret with name s-test-opt-upd-7e90d3e4-bb2b-4428-bd89-51c635b7a824
STEP: Creating the pod
Jun  7 13:33:07.878: INFO: The status of Pod pod-secrets-f762adbc-97ed-4e0d-aca1-bd1c935bb034 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:33:09.888: INFO: The status of Pod pod-secrets-f762adbc-97ed-4e0d-aca1-bd1c935bb034 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:33:11.887: INFO: The status of Pod pod-secrets-f762adbc-97ed-4e0d-aca1-bd1c935bb034 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:33:13.888: INFO: The status of Pod pod-secrets-f762adbc-97ed-4e0d-aca1-bd1c935bb034 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-9c1abcf8-88f4-4e54-8c52-e98dff77ba26
STEP: Updating secret s-test-opt-upd-7e90d3e4-bb2b-4428-bd89-51c635b7a824
STEP: Creating secret with name s-test-opt-create-08860f4a-3358-4fdf-86ce-9af918b8dcbd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:33:15.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5935" for this suite.

• [SLOW TEST:8.188 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":1,"skipped":16,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:33:16.003: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 13:33:16.479: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun  7 13:33:18.500: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790205596, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790205596, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790205596, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790205596, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 13:33:20.512: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790205596, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790205596, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790205596, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790205596, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 13:33:22.511: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790205596, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790205596, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790205596, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790205596, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 13:33:25.535: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun  7 13:33:27.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=webhook-2931 attach --namespace=webhook-2931 to-be-attached-pod -i -c=container1'
Jun  7 13:33:27.713: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:33:27.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2931" for this suite.
STEP: Destroying namespace "webhook-2931-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.778 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":2,"skipped":23,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:33:27.781: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2560
STEP: creating service affinity-clusterip in namespace services-2560
STEP: creating replication controller affinity-clusterip in namespace services-2560
I0607 13:33:27.862107      23 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-2560, replica count: 3
I0607 13:33:30.915191      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0607 13:33:33.915658      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  7 13:33:33.927: INFO: Creating new exec pod
Jun  7 13:33:36.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-2560 exec execpod-affinity48d9r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jun  7 13:33:37.206: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun  7 13:33:37.206: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 13:33:37.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-2560 exec execpod-affinity48d9r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.147.2 80'
Jun  7 13:33:37.413: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.147.2 80\nConnection to 10.106.147.2 80 port [tcp/http] succeeded!\n"
Jun  7 13:33:37.413: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 13:33:37.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-2560 exec execpod-affinity48d9r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.147.2:80/ ; done'
Jun  7 13:33:37.718: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.147.2:80/\n"
Jun  7 13:33:37.718: INFO: stdout: "\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd\naffinity-clusterip-c7tmd"
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Received response from host: affinity-clusterip-c7tmd
Jun  7 13:33:37.718: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2560, will wait for the garbage collector to delete the pods
Jun  7 13:33:37.806: INFO: Deleting ReplicationController affinity-clusterip took: 9.683999ms
Jun  7 13:33:37.908: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.152591ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:33:40.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2560" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.472 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":3,"skipped":51,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:33:40.253: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-f7ffdca5-b775-4a37-9179-cd0da22af6a1
STEP: Creating configMap with name cm-test-opt-upd-a5b9138b-a251-4d28-8e37-a81bb2be9bf8
STEP: Creating the pod
Jun  7 13:33:40.311: INFO: The status of Pod pod-configmaps-2001246d-feff-45dd-b60b-7f18aae74631 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:33:42.320: INFO: The status of Pod pod-configmaps-2001246d-feff-45dd-b60b-7f18aae74631 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-f7ffdca5-b775-4a37-9179-cd0da22af6a1
STEP: Updating configmap cm-test-opt-upd-a5b9138b-a251-4d28-8e37-a81bb2be9bf8
STEP: Creating configMap with name cm-test-opt-create-be41e4c2-fb58-47ad-981a-ef5f5f0cb2c1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:33:44.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6630" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":60,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:33:44.423: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7621
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7621
STEP: creating replication controller externalsvc in namespace services-7621
I0607 13:33:44.492711      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7621, replica count: 2
I0607 13:33:47.544133      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0607 13:33:50.545443      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun  7 13:33:50.588: INFO: Creating new exec pod
Jun  7 13:33:52.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-7621 exec execpodnv5bn -- /bin/sh -x -c nslookup clusterip-service.services-7621.svc.cluster.local'
Jun  7 13:33:52.805: INFO: stderr: "+ nslookup clusterip-service.services-7621.svc.cluster.local\n"
Jun  7 13:33:52.805: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-7621.svc.cluster.local\tcanonical name = externalsvc.services-7621.svc.cluster.local.\nName:\texternalsvc.services-7621.svc.cluster.local\nAddress: 10.108.253.128\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7621, will wait for the garbage collector to delete the pods
Jun  7 13:33:52.869: INFO: Deleting ReplicationController externalsvc took: 8.969625ms
Jun  7 13:33:52.970: INFO: Terminating ReplicationController externalsvc pods took: 100.89994ms
Jun  7 13:33:55.198: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:33:55.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7621" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.793 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":5,"skipped":70,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:33:55.217: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 13:33:55.531: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 13:33:58.566: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:33:58.579: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5630-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:34:01.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4805" for this suite.
STEP: Destroying namespace "webhook-4805-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.572 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":6,"skipped":74,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:34:01.789: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-0a9c2470-f074-4f0e-a9fe-bd58037ebe23
Jun  7 13:34:01.830: INFO: Pod name my-hostname-basic-0a9c2470-f074-4f0e-a9fe-bd58037ebe23: Found 0 pods out of 1
Jun  7 13:34:06.835: INFO: Pod name my-hostname-basic-0a9c2470-f074-4f0e-a9fe-bd58037ebe23: Found 1 pods out of 1
Jun  7 13:34:06.835: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0a9c2470-f074-4f0e-a9fe-bd58037ebe23" are running
Jun  7 13:34:06.840: INFO: Pod "my-hostname-basic-0a9c2470-f074-4f0e-a9fe-bd58037ebe23-7c7t2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-07 13:34:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-07 13:34:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-07 13:34:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-07 13:34:01 +0000 UTC Reason: Message:}])
Jun  7 13:34:06.840: INFO: Trying to dial the pod
Jun  7 13:34:11.855: INFO: Controller my-hostname-basic-0a9c2470-f074-4f0e-a9fe-bd58037ebe23: Got expected result from replica 1 [my-hostname-basic-0a9c2470-f074-4f0e-a9fe-bd58037ebe23-7c7t2]: "my-hostname-basic-0a9c2470-f074-4f0e-a9fe-bd58037ebe23-7c7t2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:34:11.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7656" for this suite.

• [SLOW TEST:10.082 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":7,"skipped":87,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:34:11.871: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jun  7 13:34:11.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-701 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun  7 13:34:11.991: INFO: stderr: ""
Jun  7 13:34:11.991: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jun  7 13:34:11.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-701 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Jun  7 13:34:13.175: INFO: stderr: ""
Jun  7 13:34:13.175: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jun  7 13:34:13.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-701 delete pods e2e-test-httpd-pod'
Jun  7 13:34:19.599: INFO: stderr: ""
Jun  7 13:34:19.599: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:34:19.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-701" for this suite.

• [SLOW TEST:7.739 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:913
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":8,"skipped":93,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:34:19.612: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-625e35b5-95df-49d5-8e12-989d3ab3f1a9 in namespace container-probe-3174
Jun  7 13:34:21.666: INFO: Started pod liveness-625e35b5-95df-49d5-8e12-989d3ab3f1a9 in namespace container-probe-3174
STEP: checking the pod's current state and verifying that restartCount is present
Jun  7 13:34:21.672: INFO: Initial restart count of pod liveness-625e35b5-95df-49d5-8e12-989d3ab3f1a9 is 0
Jun  7 13:34:41.777: INFO: Restart count of pod container-probe-3174/liveness-625e35b5-95df-49d5-8e12-989d3ab3f1a9 is now 1 (20.105394833s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:34:41.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3174" for this suite.

• [SLOW TEST:22.196 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":9,"skipped":169,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:34:41.808: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 13:34:42.560: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 13:34:45.595: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:34:45.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1438" for this suite.
STEP: Destroying namespace "webhook-1438-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":10,"skipped":183,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:34:45.838: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:34:45.871: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  7 13:34:51.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-9634 --namespace=crd-publish-openapi-9634 create -f -'
Jun  7 13:34:52.015: INFO: stderr: ""
Jun  7 13:34:52.015: INFO: stdout: "e2e-test-crd-publish-openapi-4331-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  7 13:34:52.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-9634 --namespace=crd-publish-openapi-9634 delete e2e-test-crd-publish-openapi-4331-crds test-cr'
Jun  7 13:34:52.133: INFO: stderr: ""
Jun  7 13:34:52.133: INFO: stdout: "e2e-test-crd-publish-openapi-4331-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun  7 13:34:52.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-9634 --namespace=crd-publish-openapi-9634 apply -f -'
Jun  7 13:34:52.956: INFO: stderr: ""
Jun  7 13:34:52.956: INFO: stdout: "e2e-test-crd-publish-openapi-4331-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  7 13:34:52.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-9634 --namespace=crd-publish-openapi-9634 delete e2e-test-crd-publish-openapi-4331-crds test-cr'
Jun  7 13:34:53.085: INFO: stderr: ""
Jun  7 13:34:53.085: INFO: stdout: "e2e-test-crd-publish-openapi-4331-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun  7 13:34:53.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-9634 explain e2e-test-crd-publish-openapi-4331-crds'
Jun  7 13:34:53.812: INFO: stderr: ""
Jun  7 13:34:53.812: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4331-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:34:58.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9634" for this suite.

• [SLOW TEST:13.187 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":11,"skipped":190,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:34:59.024: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-414842a8-7e3f-40b6-8f61-768b8ac1db4c
STEP: Creating a pod to test consume secrets
Jun  7 13:34:59.085: INFO: Waiting up to 5m0s for pod "pod-secrets-179f9d8b-1718-43c1-b311-3bf321b381a3" in namespace "secrets-2656" to be "Succeeded or Failed"
Jun  7 13:34:59.088: INFO: Pod "pod-secrets-179f9d8b-1718-43c1-b311-3bf321b381a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.666166ms
Jun  7 13:35:01.101: INFO: Pod "pod-secrets-179f9d8b-1718-43c1-b311-3bf321b381a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015836326s
STEP: Saw pod success
Jun  7 13:35:01.101: INFO: Pod "pod-secrets-179f9d8b-1718-43c1-b311-3bf321b381a3" satisfied condition "Succeeded or Failed"
Jun  7 13:35:01.106: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-secrets-179f9d8b-1718-43c1-b311-3bf321b381a3 container secret-volume-test: <nil>
STEP: delete the pod
Jun  7 13:35:01.141: INFO: Waiting for pod pod-secrets-179f9d8b-1718-43c1-b311-3bf321b381a3 to disappear
Jun  7 13:35:01.145: INFO: Pod pod-secrets-179f9d8b-1718-43c1-b311-3bf321b381a3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:35:01.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2656" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":12,"skipped":194,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:35:01.162: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Jun  7 13:35:01.221: INFO: Found Service test-service-8cjbz in namespace services-1044 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jun  7 13:35:01.221: INFO: Service test-service-8cjbz created
STEP: Getting /status
Jun  7 13:35:01.226: INFO: Service test-service-8cjbz has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jun  7 13:35:01.237: INFO: observed Service test-service-8cjbz in namespace services-1044 with annotations: map[] & LoadBalancer: {[]}
Jun  7 13:35:01.237: INFO: Found Service test-service-8cjbz in namespace services-1044 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jun  7 13:35:01.237: INFO: Service test-service-8cjbz has service status patched
STEP: updating the ServiceStatus
Jun  7 13:35:01.246: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jun  7 13:35:01.248: INFO: Observed Service test-service-8cjbz in namespace services-1044 with annotations: map[] & Conditions: {[]}
Jun  7 13:35:01.248: INFO: Observed event: &Service{ObjectMeta:{test-service-8cjbz  services-1044  582ee1d8-fd5c-4049-be50-80eb421c0363 33557443 0 2022-06-07 13:35:01 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-06-07 13:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-06-07 13:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.222.9,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.222.9],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jun  7 13:35:01.248: INFO: Found Service test-service-8cjbz in namespace services-1044 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  7 13:35:01.248: INFO: Service test-service-8cjbz has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jun  7 13:35:01.257: INFO: observed Service test-service-8cjbz in namespace services-1044 with labels: map[test-service-static:true]
Jun  7 13:35:01.257: INFO: observed Service test-service-8cjbz in namespace services-1044 with labels: map[test-service-static:true]
Jun  7 13:35:01.257: INFO: observed Service test-service-8cjbz in namespace services-1044 with labels: map[test-service-static:true]
Jun  7 13:35:01.257: INFO: Found Service test-service-8cjbz in namespace services-1044 with labels: map[test-service:patched test-service-static:true]
Jun  7 13:35:01.257: INFO: Service test-service-8cjbz patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jun  7 13:35:01.272: INFO: Observed event: ADDED
Jun  7 13:35:01.272: INFO: Observed event: MODIFIED
Jun  7 13:35:01.272: INFO: Observed event: MODIFIED
Jun  7 13:35:01.272: INFO: Observed event: MODIFIED
Jun  7 13:35:01.272: INFO: Found Service test-service-8cjbz in namespace services-1044 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jun  7 13:35:01.272: INFO: Service test-service-8cjbz deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:35:01.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1044" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":13,"skipped":210,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:35:01.283: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:35:01.324: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun  7 13:35:06.339: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  7 13:35:06.340: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  7 13:35:06.369: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6905  b23cd332-2966-43a4-b4a7-15f257950ef0 33557509 1 2022-06-07 13:35:06 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-06-07 13:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a7a86b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun  7 13:35:06.374: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-6905  ecc740c5-8895-4c21-a1e8-6a1484cc099f 33557511 1 2022-06-07 13:35:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b23cd332-2966-43a4-b4a7-15f257950ef0 0xc00a779457 0xc00a779458}] []  [{kube-controller-manager Update apps/v1 2022-06-07 13:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b23cd332-2966-43a4-b4a7-15f257950ef0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a7794e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  7 13:35:06.374: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun  7 13:35:06.374: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6905  60f6cf41-e788-4b1c-938f-82cfa6a3be14 33557510 1 2022-06-07 13:35:01 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment b23cd332-2966-43a4-b4a7-15f257950ef0 0xc00a779327 0xc00a779328}] []  [{e2e.test Update apps/v1 2022-06-07 13:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 13:35:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-06-07 13:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"b23cd332-2966-43a4-b4a7-15f257950ef0\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00a7793e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  7 13:35:06.384: INFO: Pod "test-cleanup-controller-kl2lq" is available:
&Pod{ObjectMeta:{test-cleanup-controller-kl2lq test-cleanup-controller- deployment-6905  55f856ba-c32b-4af3-a4db-ec97dbcc0a1b 33557480 0 2022-06-07 13:35:01 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:944a966534eb9bfe83ed88ac71fad4f4d0aa7f42889304984184f0a1e7680c71 cni.projectcalico.org/podIP:192.168.39.232/32 cni.projectcalico.org/podIPs:192.168.39.232/32] [{apps/v1 ReplicaSet test-cleanup-controller 60f6cf41-e788-4b1c-938f-82cfa6a3be14 0xc00a779df7 0xc00a779df8}] []  [{calico Update v1 2022-06-07 13:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 13:35:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60f6cf41-e788-4b1c-938f-82cfa6a3be14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 13:35:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.39.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hs97w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hs97w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:35:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:35:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:35:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:35:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.14,PodIP:192.168.39.232,StartTime:2022-06-07 13:35:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 13:35:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://77a8ae22ae5ab7082da1d6f0b4b27e6406a44092e326ac4027e3207ae1e6bf2b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.39.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 13:35:06.387: INFO: Pod "test-cleanup-deployment-5b4d99b59b-gnwpl" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-gnwpl test-cleanup-deployment-5b4d99b59b- deployment-6905  6fe5d181-3d77-4141-8a79-151ff2a614a3 33557513 0 2022-06-07 13:35:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b ecc740c5-8895-4c21-a1e8-6a1484cc099f 0xc00a7d61d7 0xc00a7d61d8}] []  [{kube-controller-manager Update v1 2022-06-07 13:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ecc740c5-8895-4c21-a1e8-6a1484cc099f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dvgqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dvgqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:35:06.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6905" for this suite.

• [SLOW TEST:5.116 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":14,"skipped":212,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:35:06.400: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun  7 13:35:06.448: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1098  07a2287f-f0c8-4a2a-b766-0f8cc6b56ab3 33557533 0 2022-06-07 13:35:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-07 13:35:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 13:35:06.448: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1098  07a2287f-f0c8-4a2a-b766-0f8cc6b56ab3 33557534 0 2022-06-07 13:35:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-07 13:35:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 13:35:06.448: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1098  07a2287f-f0c8-4a2a-b766-0f8cc6b56ab3 33557535 0 2022-06-07 13:35:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-07 13:35:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun  7 13:35:16.501: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1098  07a2287f-f0c8-4a2a-b766-0f8cc6b56ab3 33557629 0 2022-06-07 13:35:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-07 13:35:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 13:35:16.502: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1098  07a2287f-f0c8-4a2a-b766-0f8cc6b56ab3 33557630 0 2022-06-07 13:35:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-07 13:35:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 13:35:16.502: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1098  07a2287f-f0c8-4a2a-b766-0f8cc6b56ab3 33557631 0 2022-06-07 13:35:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-07 13:35:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:35:16.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1098" for this suite.

• [SLOW TEST:10.119 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":15,"skipped":215,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:35:16.519: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:35:32.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1342" for this suite.

• [SLOW TEST:16.272 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":16,"skipped":228,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:35:32.792: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 13:35:33.120: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 13:35:36.172: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:35:36.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3775" for this suite.
STEP: Destroying namespace "webhook-3775-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":17,"skipped":249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:35:36.416: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-b7be66bc-8280-43b2-87f4-c341bfdd844a in namespace container-probe-291
Jun  7 13:35:40.471: INFO: Started pod busybox-b7be66bc-8280-43b2-87f4-c341bfdd844a in namespace container-probe-291
STEP: checking the pod's current state and verifying that restartCount is present
Jun  7 13:35:40.475: INFO: Initial restart count of pod busybox-b7be66bc-8280-43b2-87f4-c341bfdd844a is 0
Jun  7 13:36:28.776: INFO: Restart count of pod container-probe-291/busybox-b7be66bc-8280-43b2-87f4-c341bfdd844a is now 1 (48.300755241s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:36:28.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-291" for this suite.

• [SLOW TEST:52.391 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":18,"skipped":271,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:36:28.810: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:36:39.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4890" for this suite.

• [SLOW TEST:11.139 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":19,"skipped":271,"failed":0}
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:36:39.949: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  7 13:36:40.045: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:40.045: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:40.045: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:40.049: INFO: Number of nodes with available pods: 0
Jun  7 13:36:40.049: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 13:36:41.062: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:41.063: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:41.063: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:41.068: INFO: Number of nodes with available pods: 0
Jun  7 13:36:41.068: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 13:36:42.064: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:42.065: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:42.065: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:42.071: INFO: Number of nodes with available pods: 1
Jun  7 13:36:42.071: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 13:36:43.060: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:43.060: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:43.060: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:43.066: INFO: Number of nodes with available pods: 1
Jun  7 13:36:43.066: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 13:36:44.063: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:44.063: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:44.063: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:44.067: INFO: Number of nodes with available pods: 1
Jun  7 13:36:44.067: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 13:36:45.064: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:45.065: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:45.065: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:45.069: INFO: Number of nodes with available pods: 2
Jun  7 13:36:45.069: INFO: Node proact-prod01-wk003 is running more than one daemon pod
Jun  7 13:36:46.063: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:46.063: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:46.064: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:36:46.069: INFO: Number of nodes with available pods: 4
Jun  7 13:36:46.069: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Getting /status
Jun  7 13:36:46.081: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jun  7 13:36:46.093: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jun  7 13:36:46.096: INFO: Observed &DaemonSet event: ADDED
Jun  7 13:36:46.096: INFO: Observed &DaemonSet event: MODIFIED
Jun  7 13:36:46.096: INFO: Observed &DaemonSet event: MODIFIED
Jun  7 13:36:46.097: INFO: Observed &DaemonSet event: MODIFIED
Jun  7 13:36:46.097: INFO: Observed &DaemonSet event: MODIFIED
Jun  7 13:36:46.098: INFO: Observed &DaemonSet event: MODIFIED
Jun  7 13:36:46.098: INFO: Found daemon set daemon-set in namespace daemonsets-4899 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  7 13:36:46.098: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jun  7 13:36:46.109: INFO: Observed &DaemonSet event: ADDED
Jun  7 13:36:46.110: INFO: Observed &DaemonSet event: MODIFIED
Jun  7 13:36:46.110: INFO: Observed &DaemonSet event: MODIFIED
Jun  7 13:36:46.110: INFO: Observed &DaemonSet event: MODIFIED
Jun  7 13:36:46.110: INFO: Observed &DaemonSet event: MODIFIED
Jun  7 13:36:46.110: INFO: Observed &DaemonSet event: MODIFIED
Jun  7 13:36:46.110: INFO: Observed daemon set daemon-set in namespace daemonsets-4899 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  7 13:36:46.110: INFO: Observed &DaemonSet event: MODIFIED
Jun  7 13:36:46.110: INFO: Found daemon set daemon-set in namespace daemonsets-4899 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jun  7 13:36:46.110: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4899, will wait for the garbage collector to delete the pods
Jun  7 13:36:46.178: INFO: Deleting DaemonSet.extensions daemon-set took: 9.019803ms
Jun  7 13:36:46.279: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.191692ms
Jun  7 13:36:48.886: INFO: Number of nodes with available pods: 0
Jun  7 13:36:48.886: INFO: Number of running nodes: 0, number of available pods: 0
Jun  7 13:36:48.891: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33558407"},"items":null}

Jun  7 13:36:48.894: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33558407"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:36:48.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4899" for this suite.

• [SLOW TEST:8.975 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":20,"skipped":271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:36:48.925: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-ff70bb83-771f-466c-900f-712db39a8ad2
STEP: Creating a pod to test consume configMaps
Jun  7 13:36:48.975: INFO: Waiting up to 5m0s for pod "pod-configmaps-954a1d8b-493f-4a2b-b634-7149f51a7659" in namespace "configmap-400" to be "Succeeded or Failed"
Jun  7 13:36:48.978: INFO: Pod "pod-configmaps-954a1d8b-493f-4a2b-b634-7149f51a7659": Phase="Pending", Reason="", readiness=false. Elapsed: 3.127508ms
Jun  7 13:36:50.990: INFO: Pod "pod-configmaps-954a1d8b-493f-4a2b-b634-7149f51a7659": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0149044s
STEP: Saw pod success
Jun  7 13:36:50.990: INFO: Pod "pod-configmaps-954a1d8b-493f-4a2b-b634-7149f51a7659" satisfied condition "Succeeded or Failed"
Jun  7 13:36:50.994: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-configmaps-954a1d8b-493f-4a2b-b634-7149f51a7659 container agnhost-container: <nil>
STEP: delete the pod
Jun  7 13:36:51.018: INFO: Waiting for pod pod-configmaps-954a1d8b-493f-4a2b-b634-7149f51a7659 to disappear
Jun  7 13:36:51.023: INFO: Pod pod-configmaps-954a1d8b-493f-4a2b-b634-7149f51a7659 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:36:51.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-400" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":295,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:36:51.042: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:36:51.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5590" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":22,"skipped":316,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:36:51.156: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun  7 13:36:53.737: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1226 pod-service-account-e77a8e10-2c2a-44f7-a1d1-9e67eccff3b7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun  7 13:36:53.946: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1226 pod-service-account-e77a8e10-2c2a-44f7-a1d1-9e67eccff3b7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun  7 13:36:54.126: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1226 pod-service-account-e77a8e10-2c2a-44f7-a1d1-9e67eccff3b7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:36:54.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1226" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":23,"skipped":352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:36:54.319: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun  7 13:37:04.468: INFO: The status of Pod kube-controller-manager-proact-prod01-cp003 is Running (Ready = true)
Jun  7 13:37:04.526: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun  7 13:37:04.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wc44" in namespace "gc-4692"
Jun  7 13:37:04.539: INFO: Deleting pod "simpletest-rc-to-be-deleted-b4grf" in namespace "gc-4692"
Jun  7 13:37:04.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-dt9tt" in namespace "gc-4692"
Jun  7 13:37:04.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9kff" in namespace "gc-4692"
Jun  7 13:37:04.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-l9x8n" in namespace "gc-4692"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:04.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4692" for this suite.

• [SLOW TEST:10.272 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":24,"skipped":374,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:04.591: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:37:06.657: INFO: Deleting pod "var-expansion-c39acde9-e6ed-476f-a29f-c1d92b67f45f" in namespace "var-expansion-3130"
Jun  7 13:37:06.667: INFO: Wait up to 5m0s for pod "var-expansion-c39acde9-e6ed-476f-a29f-c1d92b67f45f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:08.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3130" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":25,"skipped":383,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:08.696: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 13:37:08.745: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c842679-da11-47da-9ff6-919d41d2e69b" in namespace "projected-2096" to be "Succeeded or Failed"
Jun  7 13:37:08.749: INFO: Pod "downwardapi-volume-8c842679-da11-47da-9ff6-919d41d2e69b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.74839ms
Jun  7 13:37:10.758: INFO: Pod "downwardapi-volume-8c842679-da11-47da-9ff6-919d41d2e69b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012175779s
STEP: Saw pod success
Jun  7 13:37:10.758: INFO: Pod "downwardapi-volume-8c842679-da11-47da-9ff6-919d41d2e69b" satisfied condition "Succeeded or Failed"
Jun  7 13:37:10.761: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-8c842679-da11-47da-9ff6-919d41d2e69b container client-container: <nil>
STEP: delete the pod
Jun  7 13:37:10.781: INFO: Waiting for pod downwardapi-volume-8c842679-da11-47da-9ff6-919d41d2e69b to disappear
Jun  7 13:37:10.783: INFO: Pod downwardapi-volume-8c842679-da11-47da-9ff6-919d41d2e69b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:10.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2096" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":26,"skipped":387,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:10.801: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:16.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4483" for this suite.

• [SLOW TEST:5.761 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":27,"skipped":401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:16.563: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-1793
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1793 to expose endpoints map[]
Jun  7 13:37:16.635: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jun  7 13:37:17.647: INFO: successfully validated that service endpoint-test2 in namespace services-1793 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1793
Jun  7 13:37:17.664: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:37:19.677: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1793 to expose endpoints map[pod1:[80]]
Jun  7 13:37:19.695: INFO: successfully validated that service endpoint-test2 in namespace services-1793 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jun  7 13:37:19.695: INFO: Creating new exec pod
Jun  7 13:37:22.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-1793 exec execpodgzzd8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun  7 13:37:22.936: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun  7 13:37:22.936: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 13:37:22.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-1793 exec execpodgzzd8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.107.151 80'
Jun  7 13:37:23.124: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.107.151 80\nConnection to 10.107.107.151 80 port [tcp/http] succeeded!\n"
Jun  7 13:37:23.124: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-1793
Jun  7 13:37:23.144: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:37:25.152: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1793 to expose endpoints map[pod1:[80] pod2:[80]]
Jun  7 13:37:25.173: INFO: successfully validated that service endpoint-test2 in namespace services-1793 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jun  7 13:37:26.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-1793 exec execpodgzzd8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun  7 13:37:26.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun  7 13:37:26.362: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 13:37:26.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-1793 exec execpodgzzd8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.107.151 80'
Jun  7 13:37:26.571: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.107.151 80\nConnection to 10.107.107.151 80 port [tcp/http] succeeded!\n"
Jun  7 13:37:26.571: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1793
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1793 to expose endpoints map[pod2:[80]]
Jun  7 13:37:27.631: INFO: successfully validated that service endpoint-test2 in namespace services-1793 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jun  7 13:37:28.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-1793 exec execpodgzzd8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun  7 13:37:28.833: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun  7 13:37:28.833: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 13:37:28.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-1793 exec execpodgzzd8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.107.151 80'
Jun  7 13:37:29.024: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.107.151 80\nConnection to 10.107.107.151 80 port [tcp/http] succeeded!\n"
Jun  7 13:37:29.024: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-1793
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1793 to expose endpoints map[]
Jun  7 13:37:30.076: INFO: successfully validated that service endpoint-test2 in namespace services-1793 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:30.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1793" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.560 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":28,"skipped":426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:30.123: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:30.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4854" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":29,"skipped":463,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:30.208: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  7 13:37:30.264: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun  7 13:37:30.268: INFO: starting watch
STEP: patching
STEP: updating
Jun  7 13:37:30.287: INFO: waiting for watch events with expected annotations
Jun  7 13:37:30.287: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:30.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9041" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":30,"skipped":494,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:30.331: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:30.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3510" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":31,"skipped":502,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:30.402: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:30.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4226" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":32,"skipped":512,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:30.451: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9497
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9497
I0607 13:37:30.522179      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9497, replica count: 2
I0607 13:37:33.573137      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  7 13:37:33.573: INFO: Creating new exec pod
Jun  7 13:37:36.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-9497 exec execpodllmbd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun  7 13:37:36.824: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  7 13:37:36.824: INFO: stdout: ""
Jun  7 13:37:37.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-9497 exec execpodllmbd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun  7 13:37:38.032: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  7 13:37:38.032: INFO: stdout: ""
Jun  7 13:37:38.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-9497 exec execpodllmbd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun  7 13:37:39.050: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  7 13:37:39.050: INFO: stdout: "externalname-service-g5bzk"
Jun  7 13:37:39.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-9497 exec execpodllmbd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.34.27 80'
Jun  7 13:37:39.239: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.34.27 80\nConnection to 10.109.34.27 80 port [tcp/http] succeeded!\n"
Jun  7 13:37:39.239: INFO: stdout: "externalname-service-8xttz"
Jun  7 13:37:39.239: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:39.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9497" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.835 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":33,"skipped":532,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:39.287: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9581.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9581.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  7 13:37:47.388: INFO: DNS probes using dns-9581/dns-test-b130bc21-8aa5-4013-99cd-48c93fe1eec0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:47.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9581" for this suite.

• [SLOW TEST:8.131 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":34,"skipped":543,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:47.419: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:37:47.465: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  7 13:37:52.480: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jun  7 13:37:52.503: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jun  7 13:37:52.530: INFO: observed ReplicaSet test-rs in namespace replicaset-7267 with ReadyReplicas 1, AvailableReplicas 1
Jun  7 13:37:52.540: INFO: observed ReplicaSet test-rs in namespace replicaset-7267 with ReadyReplicas 1, AvailableReplicas 1
Jun  7 13:37:52.544: INFO: observed ReplicaSet test-rs in namespace replicaset-7267 with ReadyReplicas 1, AvailableReplicas 1
Jun  7 13:37:54.103: INFO: observed ReplicaSet test-rs in namespace replicaset-7267 with ReadyReplicas 2, AvailableReplicas 2
Jun  7 13:37:54.235: INFO: observed Replicaset test-rs in namespace replicaset-7267 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:54.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7267" for this suite.

• [SLOW TEST:6.839 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":35,"skipped":553,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:54.258: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 13:37:54.613: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 13:37:57.675: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:57.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3954" for this suite.
STEP: Destroying namespace "webhook-3954-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":36,"skipped":568,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:57.830: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 13:37:57.874: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e61d7048-12ed-4569-b36c-298a4827a494" in namespace "projected-4294" to be "Succeeded or Failed"
Jun  7 13:37:57.876: INFO: Pod "downwardapi-volume-e61d7048-12ed-4569-b36c-298a4827a494": Phase="Pending", Reason="", readiness=false. Elapsed: 1.949121ms
Jun  7 13:37:59.879: INFO: Pod "downwardapi-volume-e61d7048-12ed-4569-b36c-298a4827a494": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005130704s
STEP: Saw pod success
Jun  7 13:37:59.879: INFO: Pod "downwardapi-volume-e61d7048-12ed-4569-b36c-298a4827a494" satisfied condition "Succeeded or Failed"
Jun  7 13:37:59.882: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-e61d7048-12ed-4569-b36c-298a4827a494 container client-container: <nil>
STEP: delete the pod
Jun  7 13:37:59.896: INFO: Waiting for pod downwardapi-volume-e61d7048-12ed-4569-b36c-298a4827a494 to disappear
Jun  7 13:37:59.897: INFO: Pod downwardapi-volume-e61d7048-12ed-4569-b36c-298a4827a494 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:37:59.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4294" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":37,"skipped":573,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:37:59.905: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Jun  7 13:37:59.936: INFO: Waiting up to 5m0s for pod "var-expansion-20c2dc7c-3667-410c-9dec-e0ce0bac798b" in namespace "var-expansion-6795" to be "Succeeded or Failed"
Jun  7 13:37:59.940: INFO: Pod "var-expansion-20c2dc7c-3667-410c-9dec-e0ce0bac798b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.275124ms
Jun  7 13:38:01.947: INFO: Pod "var-expansion-20c2dc7c-3667-410c-9dec-e0ce0bac798b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010883748s
STEP: Saw pod success
Jun  7 13:38:01.947: INFO: Pod "var-expansion-20c2dc7c-3667-410c-9dec-e0ce0bac798b" satisfied condition "Succeeded or Failed"
Jun  7 13:38:01.952: INFO: Trying to get logs from node proact-prod01-wk002 pod var-expansion-20c2dc7c-3667-410c-9dec-e0ce0bac798b container dapi-container: <nil>
STEP: delete the pod
Jun  7 13:38:01.979: INFO: Waiting for pod var-expansion-20c2dc7c-3667-410c-9dec-e0ce0bac798b to disappear
Jun  7 13:38:01.983: INFO: Pod var-expansion-20c2dc7c-3667-410c-9dec-e0ce0bac798b no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:38:01.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6795" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":611,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:38:01.996: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:38:02.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3875" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":39,"skipped":628,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:38:02.059: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jun  7 13:38:03.150: INFO: The status of Pod kube-controller-manager-proact-prod01-cp003 is Running (Ready = true)
Jun  7 13:38:03.188: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:38:03.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6236" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":40,"skipped":640,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:38:03.200: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Jun  7 13:38:03.241: INFO: Waiting up to 5m0s for pod "pod-9ddb0499-1573-493d-a966-5fc2f6e03dbf" in namespace "emptydir-3051" to be "Succeeded or Failed"
Jun  7 13:38:03.244: INFO: Pod "pod-9ddb0499-1573-493d-a966-5fc2f6e03dbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.556694ms
Jun  7 13:38:05.251: INFO: Pod "pod-9ddb0499-1573-493d-a966-5fc2f6e03dbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009383064s
STEP: Saw pod success
Jun  7 13:38:05.251: INFO: Pod "pod-9ddb0499-1573-493d-a966-5fc2f6e03dbf" satisfied condition "Succeeded or Failed"
Jun  7 13:38:05.255: INFO: Trying to get logs from node proact-prod01-wk004 pod pod-9ddb0499-1573-493d-a966-5fc2f6e03dbf container test-container: <nil>
STEP: delete the pod
Jun  7 13:38:05.283: INFO: Waiting for pod pod-9ddb0499-1573-493d-a966-5fc2f6e03dbf to disappear
Jun  7 13:38:05.286: INFO: Pod pod-9ddb0499-1573-493d-a966-5fc2f6e03dbf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:38:05.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3051" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":41,"skipped":648,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:38:05.301: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun  7 13:38:05.329: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  7 13:38:05.338: INFO: Waiting for terminating namespaces to be deleted...
Jun  7 13:38:05.341: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk001 before test
Jun  7 13:38:05.359: INFO: argocd-application-controller-0 from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container application-controller ready: true, restart count 0
Jun  7 13:38:05.359: INFO: calico-node-mpbx2 from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container calico-node ready: true, restart count 0
Jun  7 13:38:05.359: INFO: mypostgres-7-0 from default started at 2022-05-24 07:06:43 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:38:05.359: INFO: 	Container mypostgres-7 ready: true, restart count 0
Jun  7 13:38:05.359: INFO: reviews-v1-545db77b95-z2hm8 from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:38:05.359: INFO: 	Container reviews ready: true, restart count 0
Jun  7 13:38:05.359: INFO: foobar-75685968d4-575v5 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.359: INFO: foobar-75685968d4-d24zc from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.359: INFO: foobar-75685968d4-s72dw from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.359: INFO: foobar-75685968d4-scvj7 from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.359: INFO: foobar-75685968d4-wtsqh from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.359: INFO: mypod from foobar2 started at 2022-04-28 07:54:48 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container myfrontend ready: true, restart count 0
Jun  7 13:38:05.359: INFO: nginx-deployment-66b6c48dd5-dbfcx from foobar2 started at 2022-04-28 08:35:34 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.359: INFO: nginx-deployment-66b6c48dd5-pfdqw from foobar2 started at 2022-04-28 07:46:04 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.359: INFO: nginx-deployment2-d5845d7c8-vfthc from foobar2 started at 2022-04-28 07:49:41 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container nginx2 ready: true, restart count 0
Jun  7 13:38:05.359: INFO: gitea-0 from gitea started at 2022-03-30 07:25:54 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container gitea ready: true, restart count 0
Jun  7 13:38:05.359: INFO: gitea-postgresql-0 from gitea started at 2022-03-30 07:25:56 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container gitea-postgresql ready: true, restart count 1
Jun  7 13:38:05.359: INFO: ingress-nginx-controller-848878cd85-9wqg9 from ingress-nginx started at 2022-06-03 07:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container controller ready: true, restart count 0
Jun  7 13:38:05.359: INFO: k8s-status-c44b9cb68-pcnd4 from k8s-status started at 2022-04-14 07:03:09 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container k8s-status ready: true, restart count 0
Jun  7 13:38:05.359: INFO: kube-proxy-4c7bj from kube-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  7 13:38:05.359: INFO: loki-fluent-bit-loki-gqzll from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 13:38:05.359: INFO: prometheus-alertmanager-6c845bbb9c-r6krx from monitoring started at 2022-05-23 17:37:20 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jun  7 13:38:05.359: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jun  7 13:38:05.359: INFO: prometheus-kube-state-metrics-6c44ff7fb6-9zx9s from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  7 13:38:05.359: INFO: prometheus-node-exporter-z6jpz from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 13:38:05.359: INFO: prometheus-pushgateway-86679dcf68-qk8x2 from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Jun  7 13:38:05.359: INFO: sonobuoy from sonobuoy started at 2022-06-07 13:32:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  7 13:38:05.359: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-525dj from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 13:38:05.359: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 13:38:05.359: INFO: trident-csi-kqw7h from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 13:38:05.359: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 13:38:05.359: INFO: trident-operator-56d66bb95b-255bx from trident started at 2022-03-18 12:31:45 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.359: INFO: 	Container trident-operator ready: true, restart count 0
Jun  7 13:38:05.359: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk002 before test
Jun  7 13:38:05.381: INFO: argocd-repo-server-5547b66bd9-4k6nx from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container repo-server ready: true, restart count 0
Jun  7 13:38:05.381: INFO: calico-node-sv82j from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container calico-node ready: true, restart count 1
Jun  7 13:38:05.381: INFO: mypostgres-9-0 from default started at 2022-05-24 07:39:52 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:38:05.381: INFO: 	Container mypostgres-9 ready: true, restart count 0
Jun  7 13:38:05.381: INFO: foobar-75685968d4-7fjn9 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.381: INFO: foobar-75685968d4-cf67j from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.381: INFO: foobar-75685968d4-fkrqq from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.381: INFO: foobar-75685968d4-tcxbp from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.381: INFO: foobar-75685968d4-wdtwz from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.381: INFO: nginx-deployment-66b6c48dd5-9fntt from foobar2 started at 2022-04-28 07:46:05 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.381: INFO: nginx-deployment2-d5845d7c8-nfwh4 from foobar2 started at 2022-04-28 07:49:43 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container nginx2 ready: true, restart count 0
Jun  7 13:38:05.381: INFO: simpletest.deployment-b7f68f5b-kktmr from gc-6236 started at 2022-06-07 13:38:02 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.381: INFO: ingress-nginx-controller-848878cd85-5clcc from ingress-nginx started at 2022-06-03 07:40:39 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container controller ready: true, restart count 0
Jun  7 13:38:05.381: INFO: grafana-6c5dc6df7c-96sx8 from istio-system started at 2022-03-16 15:11:52 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container grafana ready: true, restart count 0
Jun  7 13:38:05.381: INFO: istio-egressgateway-66fdd867f4-dvbn9 from istio-system started at 2022-03-16 15:07:19 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:38:05.381: INFO: istio-ingressgateway-77968dbd74-94rjb from istio-system started at 2022-03-16 15:07:19 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:38:05.381: INFO: istiod-699b647f8b-6ngj6 from istio-system started at 2022-03-16 15:07:16 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container discovery ready: true, restart count 0
Jun  7 13:38:05.381: INFO: jaeger-9dd685668-nwlbs from istio-system started at 2022-03-16 15:11:53 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container jaeger ready: true, restart count 0
Jun  7 13:38:05.381: INFO: kiali-699f98c497-tfr8m from istio-system started at 2022-03-16 15:11:53 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container kiali ready: true, restart count 0
Jun  7 13:38:05.381: INFO: prometheus-699b7cc575-6dmwg from istio-system started at 2022-03-16 15:11:53 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container prometheus-server ready: true, restart count 0
Jun  7 13:38:05.381: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun  7 13:38:05.381: INFO: po from jlarson started at 2022-05-12 06:48:11 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container po ready: false, restart count 0
Jun  7 13:38:05.381: INFO: foo-bf9cd6fb5-9zkzm from jltest started at 2022-05-10 08:20:21 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.381: INFO: kube-proxy-8sbg5 from kube-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container kube-proxy ready: true, restart count 1
Jun  7 13:38:05.381: INFO: kubegres-controller-manager-755b4c48f6-v7gsj from kubegres-system started at 2022-04-22 13:31:50 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  7 13:38:05.381: INFO: 	Container manager ready: true, restart count 0
Jun  7 13:38:05.381: INFO: loki-fluent-bit-loki-mzmsh from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 13:38:05.381: INFO: prometheus-node-exporter-flct5 from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 13:38:05.381: INFO: prometheus-server-869789c557-pc7rv from monitoring started at 2022-05-23 17:37:15 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container prometheus-server ready: true, restart count 0
Jun  7 13:38:05.381: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun  7 13:38:05.381: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-7w99f from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 13:38:05.381: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 13:38:05.381: INFO: trident-csi-b4p6l from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 13:38:05.381: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 13:38:05.381: INFO: trident-csi-fcff9fbb6-6wwnd from trident started at 2022-03-18 12:31:59 +0000 UTC (6 container statuses recorded)
Jun  7 13:38:05.381: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  7 13:38:05.381: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  7 13:38:05.381: INFO: 	Container csi-resizer ready: true, restart count 0
Jun  7 13:38:05.381: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun  7 13:38:05.381: INFO: 	Container trident-autosupport ready: true, restart count 0
Jun  7 13:38:05.381: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 13:38:05.381: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk003 before test
Jun  7 13:38:05.400: INFO: argocd-applicationset-controller-84bc7544cd-7zlhg from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container applicationset-controller ready: true, restart count 0
Jun  7 13:38:05.400: INFO: argocd-dex-server-6b5ffb8f5c-r5sf2 from argocd started at 2022-03-31 06:26:27 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container dex-server ready: true, restart count 0
Jun  7 13:38:05.400: INFO: calico-node-rmwmb from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container calico-node ready: true, restart count 0
Jun  7 13:38:05.400: INFO: calico-typha-54559758b4-6gmlw from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container calico-typha ready: true, restart count 0
Jun  7 13:38:05.400: INFO: cert-manager-6bbf595697-lr27t from cert-manager started at 2022-06-03 08:46:47 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container cert-manager ready: true, restart count 0
Jun  7 13:38:05.400: INFO: cert-manager-cainjector-6bc9d758b-vh4wl from cert-manager started at 2022-06-03 08:46:47 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container cert-manager ready: true, restart count 0
Jun  7 13:38:05.400: INFO: productpage-v1-6b746f74dc-xrnvw from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:38:05.400: INFO: 	Container productpage ready: true, restart count 0
Jun  7 13:38:05.400: INFO: ratings-v1-b6994bb9-vndct from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:38:05.400: INFO: 	Container ratings ready: true, restart count 0
Jun  7 13:38:05.400: INFO: reviews-v3-84779c7bbc-p4hjg from default started at 2022-04-22 12:33:02 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:38:05.400: INFO: 	Container reviews ready: true, restart count 0
Jun  7 13:38:05.400: INFO: dex-7b7c86db7-hcrrd from dex started at 2022-06-03 07:21:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container dex ready: true, restart count 0
Jun  7 13:38:05.400: INFO: dex-authenticator-dex-k8s-authenticator-5795dc9755-fgflk from dex started at 2022-03-22 15:07:54 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container dex-k8s-authenticator ready: true, restart count 0
Jun  7 13:38:05.400: INFO: foobar-75685968d4-7ktm8 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.400: INFO: foobar-75685968d4-cn6ww from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.400: INFO: foobar-75685968d4-fldfg from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.400: INFO: foobar-75685968d4-mjvcq from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.400: INFO: foobar-75685968d4-wk7h8 from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.400: INFO: nginx-deployment-66b6c48dd5-kwzdn from foobar2 started at 2022-04-28 08:35:34 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.400: INFO: simple from foobar2 started at 2022-04-27 08:47:49 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container simple ready: true, restart count 0
Jun  7 13:38:05.400: INFO: simpletest.deployment-b7f68f5b-zxdwm from gc-6236 started at 2022-06-07 13:38:02 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.400: INFO: k8s-status-c44b9cb68-dx6r8 from k8s-status started at 2022-04-14 07:03:09 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container k8s-status ready: true, restart count 0
Jun  7 13:38:05.400: INFO: kube-proxy-wx7st from kube-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  7 13:38:05.400: INFO: loki-0 from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container loki ready: true, restart count 0
Jun  7 13:38:05.400: INFO: loki-fluent-bit-loki-ss7rt from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 13:38:05.400: INFO: prometheus-node-exporter-rz8ws from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 13:38:05.400: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-jxr44 from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 13:38:05.400: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 13:38:05.400: INFO: trident-csi-7wfz2 from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.400: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 13:38:05.400: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 13:38:05.400: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk004 before test
Jun  7 13:38:05.418: INFO: argocd-notifications-controller-6dd95488b4-l4t7z from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.418: INFO: 	Container notifications-controller ready: true, restart count 0
Jun  7 13:38:05.418: INFO: argocd-redis-57bcc665bf-lpcsm from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.418: INFO: 	Container argocd-redis ready: true, restart count 0
Jun  7 13:38:05.418: INFO: argocd-server-7b98b7446b-n7nmr from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.418: INFO: 	Container server ready: true, restart count 0
Jun  7 13:38:05.418: INFO: calico-node-gmd98 from calico-system started at 2022-03-10 15:32:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.418: INFO: 	Container calico-node ready: true, restart count 0
Jun  7 13:38:05.418: INFO: cert-manager-webhook-586d45d5ff-pftz7 from cert-manager started at 2022-06-03 08:46:47 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.418: INFO: 	Container cert-manager ready: true, restart count 0
Jun  7 13:38:05.418: INFO: details-v1-79f774bdb9-pxpbj from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.418: INFO: 	Container details ready: true, restart count 0
Jun  7 13:38:05.418: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:38:05.418: INFO: mypostgres-6-0 from default started at 2022-05-16 17:57:42 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:38:05.419: INFO: 	Container mypostgres-6 ready: true, restart count 0
Jun  7 13:38:05.419: INFO: reviews-v2-7bf8c9648f-q86cj from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:38:05.419: INFO: 	Container reviews ready: true, restart count 0
Jun  7 13:38:05.419: INFO: oauth2-oauth2-proxy-85748949c4-rlj6t from dex started at 2022-06-03 07:21:23 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container oauth2-proxy ready: true, restart count 0
Jun  7 13:38:05.419: INFO: foobar-75685968d4-8k5dq from foobar started at 2022-03-25 12:14:06 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.419: INFO: foobar-75685968d4-98jz8 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.419: INFO: foobar-75685968d4-qj6bn from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.419: INFO: foobar-75685968d4-skd4f from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.419: INFO: foobar-75685968d4-zgz7t from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.419: INFO: nginx-deployment-66b6c48dd5-psjh5 from foobar2 started at 2022-04-28 07:46:07 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:38:05.419: INFO: nginx-deployment2-d5845d7c8-xhm74 from foobar2 started at 2022-04-28 07:49:45 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container nginx2 ready: true, restart count 0
Jun  7 13:38:05.419: INFO: gitea-memcached-7b44bc5f74-6vssj from gitea started at 2022-03-30 07:25:52 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container memcached ready: true, restart count 0
Jun  7 13:38:05.419: INFO: k8s-status-c44b9cb68-f4rv9 from k8s-status started at 2022-04-14 07:03:09 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container k8s-status ready: true, restart count 0
Jun  7 13:38:05.419: INFO: kube-proxy-t5klm from kube-system started at 2022-03-10 15:32:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.419: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  7 13:38:05.419: INFO: grafana-5ff8c85cf8-gv9c2 from loki started at 2022-04-25 05:59:11 +0000 UTC (3 container statuses recorded)
Jun  7 13:38:05.420: INFO: 	Container grafana ready: true, restart count 0
Jun  7 13:38:05.420: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun  7 13:38:05.420: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Jun  7 13:38:05.420: INFO: loki-fluent-bit-loki-mrrh2 from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.420: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 13:38:05.420: INFO: testpod from loki started at 2022-03-28 08:55:49 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.420: INFO: 	Container testpod ready: true, restart count 71
Jun  7 13:38:05.420: INFO: prometheus-node-exporter-5ph5n from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:38:05.420: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 13:38:05.420: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-pxgrv from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 13:38:05.420: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 13:38:05.420: INFO: trident-csi-cscfg from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 13:38:05.420: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 13:38:05.420: INFO: 	Container trident-main ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node proact-prod01-wk001
STEP: verifying the node has the label node proact-prod01-wk002
STEP: verifying the node has the label node proact-prod01-wk003
STEP: verifying the node has the label node proact-prod01-wk004
Jun  7 13:38:05.562: INFO: Pod argocd-application-controller-0 requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.562: INFO: Pod argocd-applicationset-controller-84bc7544cd-7zlhg requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.562: INFO: Pod argocd-dex-server-6b5ffb8f5c-r5sf2 requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.562: INFO: Pod argocd-notifications-controller-6dd95488b4-l4t7z requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.562: INFO: Pod argocd-redis-57bcc665bf-lpcsm requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.562: INFO: Pod argocd-repo-server-5547b66bd9-4k6nx requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.562: INFO: Pod argocd-server-7b98b7446b-n7nmr requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.562: INFO: Pod calico-node-gmd98 requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.562: INFO: Pod calico-node-mpbx2 requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.562: INFO: Pod calico-node-rmwmb requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.562: INFO: Pod calico-node-sv82j requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.562: INFO: Pod calico-typha-54559758b4-6gmlw requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.562: INFO: Pod cert-manager-6bbf595697-lr27t requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.562: INFO: Pod cert-manager-cainjector-6bc9d758b-vh4wl requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.562: INFO: Pod cert-manager-webhook-586d45d5ff-pftz7 requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod details-v1-79f774bdb9-pxpbj requesting resource cpu=100m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod mypostgres-6-0 requesting resource cpu=10m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod mypostgres-7-0 requesting resource cpu=10m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod mypostgres-9-0 requesting resource cpu=10m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod productpage-v1-6b746f74dc-xrnvw requesting resource cpu=100m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod ratings-v1-b6994bb9-vndct requesting resource cpu=100m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod reviews-v1-545db77b95-z2hm8 requesting resource cpu=100m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod reviews-v2-7bf8c9648f-q86cj requesting resource cpu=100m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod reviews-v3-84779c7bbc-p4hjg requesting resource cpu=10m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod dex-7b7c86db7-hcrrd requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod dex-authenticator-dex-k8s-authenticator-5795dc9755-fgflk requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod oauth2-oauth2-proxy-85748949c4-rlj6t requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-575v5 requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-7fjn9 requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-7ktm8 requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-8k5dq requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-98jz8 requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-cf67j requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-cn6ww requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-d24zc requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-fkrqq requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-fldfg requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-mjvcq requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-qj6bn requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-s72dw requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-scvj7 requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-skd4f requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-tcxbp requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-wdtwz requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-wk7h8 requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-wtsqh requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod foobar-75685968d4-zgz7t requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod mypod requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod nginx-deployment-66b6c48dd5-9fntt requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod nginx-deployment-66b6c48dd5-dbfcx requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod nginx-deployment-66b6c48dd5-kwzdn requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod nginx-deployment-66b6c48dd5-pfdqw requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod nginx-deployment-66b6c48dd5-psjh5 requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod nginx-deployment2-d5845d7c8-nfwh4 requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod nginx-deployment2-d5845d7c8-vfthc requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod nginx-deployment2-d5845d7c8-xhm74 requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod simple requesting resource cpu=400m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod simpletest.deployment-b7f68f5b-kktmr requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod simpletest.deployment-b7f68f5b-zxdwm requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod gitea-0 requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod gitea-memcached-7b44bc5f74-6vssj requesting resource cpu=250m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod gitea-postgresql-0 requesting resource cpu=250m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod ingress-nginx-controller-848878cd85-5clcc requesting resource cpu=100m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod ingress-nginx-controller-848878cd85-9wqg9 requesting resource cpu=100m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod grafana-6c5dc6df7c-96sx8 requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod istio-egressgateway-66fdd867f4-dvbn9 requesting resource cpu=10m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod istio-ingressgateway-77968dbd74-94rjb requesting resource cpu=10m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod istiod-699b647f8b-6ngj6 requesting resource cpu=10m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod jaeger-9dd685668-nwlbs requesting resource cpu=10m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod kiali-699f98c497-tfr8m requesting resource cpu=10m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod prometheus-699b7cc575-6dmwg requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod po requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod foo-bf9cd6fb5-9zkzm requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod k8s-status-c44b9cb68-dx6r8 requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod k8s-status-c44b9cb68-f4rv9 requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod k8s-status-c44b9cb68-pcnd4 requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod kube-proxy-4c7bj requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod kube-proxy-8sbg5 requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod kube-proxy-t5klm requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod kube-proxy-wx7st requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod kubegres-controller-manager-755b4c48f6-v7gsj requesting resource cpu=100m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod grafana-5ff8c85cf8-gv9c2 requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod loki-0 requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod loki-fluent-bit-loki-gqzll requesting resource cpu=100m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod loki-fluent-bit-loki-mrrh2 requesting resource cpu=100m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod loki-fluent-bit-loki-mzmsh requesting resource cpu=100m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod loki-fluent-bit-loki-ss7rt requesting resource cpu=100m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod testpod requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod prometheus-alertmanager-6c845bbb9c-r6krx requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod prometheus-kube-state-metrics-6c44ff7fb6-9zx9s requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod prometheus-node-exporter-5ph5n requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod prometheus-node-exporter-flct5 requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod prometheus-node-exporter-rz8ws requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod prometheus-node-exporter-z6jpz requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod prometheus-pushgateway-86679dcf68-qk8x2 requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod prometheus-server-869789c557-pc7rv requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod sonobuoy requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-525dj requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-7w99f requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-jxr44 requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-pxgrv requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod trident-csi-7wfz2 requesting resource cpu=0m on Node proact-prod01-wk003
Jun  7 13:38:05.563: INFO: Pod trident-csi-b4p6l requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod trident-csi-cscfg requesting resource cpu=0m on Node proact-prod01-wk004
Jun  7 13:38:05.563: INFO: Pod trident-csi-fcff9fbb6-6wwnd requesting resource cpu=0m on Node proact-prod01-wk002
Jun  7 13:38:05.563: INFO: Pod trident-csi-kqw7h requesting resource cpu=0m on Node proact-prod01-wk001
Jun  7 13:38:05.563: INFO: Pod trident-operator-56d66bb95b-255bx requesting resource cpu=0m on Node proact-prod01-wk001
STEP: Starting Pods to consume most of the cluster CPU.
Jun  7 13:38:05.563: INFO: Creating a pod which consumes cpu=2408m on Node proact-prod01-wk001
Jun  7 13:38:05.574: INFO: Creating a pod which consumes cpu=2548m on Node proact-prod01-wk002
Jun  7 13:38:05.580: INFO: Creating a pod which consumes cpu=2303m on Node proact-prod01-wk003
Jun  7 13:38:05.587: INFO: Creating a pod which consumes cpu=2408m on Node proact-prod01-wk004
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-152bd8e4-eb52-4cbd-b3c3-45ce96019503.16f65a32657b21fc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5072/filler-pod-152bd8e4-eb52-4cbd-b3c3-45ce96019503 to proact-prod01-wk004]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-152bd8e4-eb52-4cbd-b3c3-45ce96019503.16f65a328d5405c8], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-152bd8e4-eb52-4cbd-b3c3-45ce96019503.16f65a32941bdfb3], Reason = [Created], Message = [Created container filler-pod-152bd8e4-eb52-4cbd-b3c3-45ce96019503]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-152bd8e4-eb52-4cbd-b3c3-45ce96019503.16f65a3295cf010c], Reason = [Started], Message = [Started container filler-pod-152bd8e4-eb52-4cbd-b3c3-45ce96019503]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9668b898-d886-4629-a0b0-bfab6847c313.16f65a32640b58b5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5072/filler-pod-9668b898-d886-4629-a0b0-bfab6847c313 to proact-prod01-wk001]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9668b898-d886-4629-a0b0-bfab6847c313.16f65a328c609114], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9668b898-d886-4629-a0b0-bfab6847c313.16f65a32932c3f8c], Reason = [Created], Message = [Created container filler-pod-9668b898-d886-4629-a0b0-bfab6847c313]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9668b898-d886-4629-a0b0-bfab6847c313.16f65a32956175de], Reason = [Started], Message = [Started container filler-pod-9668b898-d886-4629-a0b0-bfab6847c313]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae09d3e0-4a95-4e70-8a0e-b48cc2b0141a.16f65a3264fad4e2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5072/filler-pod-ae09d3e0-4a95-4e70-8a0e-b48cc2b0141a to proact-prod01-wk003]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae09d3e0-4a95-4e70-8a0e-b48cc2b0141a.16f65a328d342726], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae09d3e0-4a95-4e70-8a0e-b48cc2b0141a.16f65a329453fe9b], Reason = [Created], Message = [Created container filler-pod-ae09d3e0-4a95-4e70-8a0e-b48cc2b0141a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae09d3e0-4a95-4e70-8a0e-b48cc2b0141a.16f65a3295db905d], Reason = [Started], Message = [Started container filler-pod-ae09d3e0-4a95-4e70-8a0e-b48cc2b0141a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b58de872-6d40-4086-b2f5-c83aa69c2280.16f65a3264622528], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5072/filler-pod-b58de872-6d40-4086-b2f5-c83aa69c2280 to proact-prod01-wk002]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b58de872-6d40-4086-b2f5-c83aa69c2280.16f65a328c403584], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.5" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b58de872-6d40-4086-b2f5-c83aa69c2280.16f65a329273ca2a], Reason = [Created], Message = [Created container filler-pod-b58de872-6d40-4086-b2f5-c83aa69c2280]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b58de872-6d40-4086-b2f5-c83aa69c2280.16f65a32941bcedd], Reason = [Started], Message = [Started container filler-pod-b58de872-6d40-4086-b2f5-c83aa69c2280]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16f65a32df243312], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 4 Insufficient cpu.]
STEP: removing the label node off the node proact-prod01-wk003
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node proact-prod01-wk004
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node proact-prod01-wk001
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node proact-prod01-wk002
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:38:08.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5072" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":42,"skipped":659,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:38:08.738: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jun  7 13:38:08.767: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:38:32.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6539" for this suite.

• [SLOW TEST:24.246 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":43,"skipped":664,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:38:32.986: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5771
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  7 13:38:33.021: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  7 13:38:33.058: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:38:35.067: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:38:37.069: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:38:39.065: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:38:41.066: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:38:43.064: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:38:45.069: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun  7 13:38:45.079: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun  7 13:38:45.094: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jun  7 13:38:45.100: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jun  7 13:38:47.142: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jun  7 13:38:47.142: INFO: Going to poll 192.168.56.246 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jun  7 13:38:47.145: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.56.246 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5771 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:38:47.145: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:38:48.258: INFO: Found all 1 expected endpoints: [netserver-0]
Jun  7 13:38:48.258: INFO: Going to poll 192.168.39.201 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jun  7 13:38:48.263: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.39.201 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5771 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:38:48.263: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:38:49.378: INFO: Found all 1 expected endpoints: [netserver-1]
Jun  7 13:38:49.378: INFO: Going to poll 192.168.46.91 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jun  7 13:38:49.387: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.46.91 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5771 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:38:49.387: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:38:50.518: INFO: Found all 1 expected endpoints: [netserver-2]
Jun  7 13:38:50.518: INFO: Going to poll 192.168.7.34 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Jun  7 13:38:50.527: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.7.34 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5771 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:38:50.527: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:38:51.667: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:38:51.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5771" for this suite.

• [SLOW TEST:18.701 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":673,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:38:51.689: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jun  7 13:38:51.750: INFO: The status of Pod annotationupdatea5fd392e-7da8-4641-8e7e-b55fe5e08550 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:38:53.759: INFO: The status of Pod annotationupdatea5fd392e-7da8-4641-8e7e-b55fe5e08550 is Running (Ready = true)
Jun  7 13:38:54.309: INFO: Successfully updated pod "annotationupdatea5fd392e-7da8-4641-8e7e-b55fe5e08550"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:38:56.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1577" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":45,"skipped":694,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:38:56.342: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:38:56.371: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:38:59.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6344" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":46,"skipped":703,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:38:59.522: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:39:15.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3349" for this suite.

• [SLOW TEST:16.233 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":47,"skipped":736,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:39:15.755: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jun  7 13:39:15.787: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  7 13:40:15.894: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:40:15.898: INFO: Starting informer...
STEP: Starting pods...
Jun  7 13:40:16.121: INFO: Pod1 is running on proact-prod01-wk002. Tainting Node
Jun  7 13:40:18.349: INFO: Pod2 is running on proact-prod01-wk002. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun  7 13:40:30.737: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun  7 13:40:44.799: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:40:44.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4619" for this suite.

• [SLOW TEST:89.089 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":48,"skipped":772,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:40:44.845: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Jun  7 13:40:44.891: INFO: Waiting up to 5m0s for pod "client-containers-c48fa1bc-5214-4f69-b18d-cbb5f6fbb1bf" in namespace "containers-2336" to be "Succeeded or Failed"
Jun  7 13:40:44.895: INFO: Pod "client-containers-c48fa1bc-5214-4f69-b18d-cbb5f6fbb1bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.476195ms
Jun  7 13:40:46.902: INFO: Pod "client-containers-c48fa1bc-5214-4f69-b18d-cbb5f6fbb1bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010525294s
STEP: Saw pod success
Jun  7 13:40:46.902: INFO: Pod "client-containers-c48fa1bc-5214-4f69-b18d-cbb5f6fbb1bf" satisfied condition "Succeeded or Failed"
Jun  7 13:40:46.906: INFO: Trying to get logs from node proact-prod01-wk002 pod client-containers-c48fa1bc-5214-4f69-b18d-cbb5f6fbb1bf container agnhost-container: <nil>
STEP: delete the pod
Jun  7 13:40:47.183: INFO: Waiting for pod client-containers-c48fa1bc-5214-4f69-b18d-cbb5f6fbb1bf to disappear
Jun  7 13:40:47.187: INFO: Pod client-containers-c48fa1bc-5214-4f69-b18d-cbb5f6fbb1bf no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:40:47.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2336" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":49,"skipped":784,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:40:47.198: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-37a5845e-ef26-4e2f-bca1-973d9ea21173
STEP: Creating a pod to test consume configMaps
Jun  7 13:40:47.243: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-207ac841-f6a3-4f3c-bf3f-37998599c4a2" in namespace "projected-8797" to be "Succeeded or Failed"
Jun  7 13:40:47.246: INFO: Pod "pod-projected-configmaps-207ac841-f6a3-4f3c-bf3f-37998599c4a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.063221ms
Jun  7 13:40:49.253: INFO: Pod "pod-projected-configmaps-207ac841-f6a3-4f3c-bf3f-37998599c4a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010245814s
Jun  7 13:40:51.260: INFO: Pod "pod-projected-configmaps-207ac841-f6a3-4f3c-bf3f-37998599c4a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017079832s
STEP: Saw pod success
Jun  7 13:40:51.260: INFO: Pod "pod-projected-configmaps-207ac841-f6a3-4f3c-bf3f-37998599c4a2" satisfied condition "Succeeded or Failed"
Jun  7 13:40:51.265: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-configmaps-207ac841-f6a3-4f3c-bf3f-37998599c4a2 container agnhost-container: <nil>
STEP: delete the pod
Jun  7 13:40:51.288: INFO: Waiting for pod pod-projected-configmaps-207ac841-f6a3-4f3c-bf3f-37998599c4a2 to disappear
Jun  7 13:40:51.291: INFO: Pod pod-projected-configmaps-207ac841-f6a3-4f3c-bf3f-37998599c4a2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:40:51.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8797" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":50,"skipped":802,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:40:51.301: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jun  7 13:40:51.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 create -f -'
Jun  7 13:40:52.609: INFO: stderr: ""
Jun  7 13:40:52.609: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  7 13:40:52.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  7 13:40:52.685: INFO: stderr: ""
Jun  7 13:40:52.685: INFO: stdout: "update-demo-nautilus-7crtq update-demo-nautilus-nbmpj "
Jun  7 13:40:52.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods update-demo-nautilus-7crtq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  7 13:40:52.757: INFO: stderr: ""
Jun  7 13:40:52.757: INFO: stdout: ""
Jun  7 13:40:52.757: INFO: update-demo-nautilus-7crtq is created but not running
Jun  7 13:40:57.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  7 13:40:57.839: INFO: stderr: ""
Jun  7 13:40:57.839: INFO: stdout: "update-demo-nautilus-7crtq update-demo-nautilus-nbmpj "
Jun  7 13:40:57.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods update-demo-nautilus-7crtq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  7 13:40:57.914: INFO: stderr: ""
Jun  7 13:40:57.914: INFO: stdout: ""
Jun  7 13:40:57.914: INFO: update-demo-nautilus-7crtq is created but not running
Jun  7 13:41:02.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  7 13:41:03.014: INFO: stderr: ""
Jun  7 13:41:03.014: INFO: stdout: "update-demo-nautilus-7crtq update-demo-nautilus-nbmpj "
Jun  7 13:41:03.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods update-demo-nautilus-7crtq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  7 13:41:03.079: INFO: stderr: ""
Jun  7 13:41:03.079: INFO: stdout: "true"
Jun  7 13:41:03.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods update-demo-nautilus-7crtq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  7 13:41:03.147: INFO: stderr: ""
Jun  7 13:41:03.147: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun  7 13:41:03.147: INFO: validating pod update-demo-nautilus-7crtq
Jun  7 13:41:03.154: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  7 13:41:03.154: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  7 13:41:03.155: INFO: update-demo-nautilus-7crtq is verified up and running
Jun  7 13:41:03.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods update-demo-nautilus-nbmpj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  7 13:41:03.225: INFO: stderr: ""
Jun  7 13:41:03.225: INFO: stdout: "true"
Jun  7 13:41:03.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods update-demo-nautilus-nbmpj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  7 13:41:03.293: INFO: stderr: ""
Jun  7 13:41:03.293: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun  7 13:41:03.293: INFO: validating pod update-demo-nautilus-nbmpj
Jun  7 13:41:03.298: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  7 13:41:03.298: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  7 13:41:03.298: INFO: update-demo-nautilus-nbmpj is verified up and running
STEP: scaling down the replication controller
Jun  7 13:41:03.300: INFO: scanned /root for discovery docs: <nil>
Jun  7 13:41:03.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun  7 13:41:04.393: INFO: stderr: ""
Jun  7 13:41:04.393: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  7 13:41:04.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  7 13:41:04.470: INFO: stderr: ""
Jun  7 13:41:04.470: INFO: stdout: "update-demo-nautilus-7crtq "
Jun  7 13:41:04.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods update-demo-nautilus-7crtq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  7 13:41:04.533: INFO: stderr: ""
Jun  7 13:41:04.533: INFO: stdout: "true"
Jun  7 13:41:04.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods update-demo-nautilus-7crtq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  7 13:41:04.600: INFO: stderr: ""
Jun  7 13:41:04.600: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun  7 13:41:04.600: INFO: validating pod update-demo-nautilus-7crtq
Jun  7 13:41:04.608: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  7 13:41:04.608: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  7 13:41:04.608: INFO: update-demo-nautilus-7crtq is verified up and running
STEP: scaling up the replication controller
Jun  7 13:41:04.610: INFO: scanned /root for discovery docs: <nil>
Jun  7 13:41:04.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun  7 13:41:05.728: INFO: stderr: ""
Jun  7 13:41:05.729: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  7 13:41:05.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  7 13:41:05.810: INFO: stderr: ""
Jun  7 13:41:05.810: INFO: stdout: "update-demo-nautilus-7crtq update-demo-nautilus-xrbsl "
Jun  7 13:41:05.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods update-demo-nautilus-7crtq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  7 13:41:05.890: INFO: stderr: ""
Jun  7 13:41:05.890: INFO: stdout: "true"
Jun  7 13:41:05.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods update-demo-nautilus-7crtq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  7 13:41:05.950: INFO: stderr: ""
Jun  7 13:41:05.950: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun  7 13:41:05.950: INFO: validating pod update-demo-nautilus-7crtq
Jun  7 13:41:05.954: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  7 13:41:05.954: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  7 13:41:05.954: INFO: update-demo-nautilus-7crtq is verified up and running
Jun  7 13:41:05.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods update-demo-nautilus-xrbsl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  7 13:41:06.017: INFO: stderr: ""
Jun  7 13:41:06.017: INFO: stdout: "true"
Jun  7 13:41:06.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods update-demo-nautilus-xrbsl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  7 13:41:06.092: INFO: stderr: ""
Jun  7 13:41:06.092: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun  7 13:41:06.092: INFO: validating pod update-demo-nautilus-xrbsl
Jun  7 13:41:06.099: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  7 13:41:06.099: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  7 13:41:06.099: INFO: update-demo-nautilus-xrbsl is verified up and running
STEP: using delete to clean up resources
Jun  7 13:41:06.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 delete --grace-period=0 --force -f -'
Jun  7 13:41:06.188: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  7 13:41:06.188: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  7 13:41:06.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get rc,svc -l name=update-demo --no-headers'
Jun  7 13:41:06.270: INFO: stderr: "No resources found in kubectl-3229 namespace.\n"
Jun  7 13:41:06.270: INFO: stdout: ""
Jun  7 13:41:06.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3229 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  7 13:41:06.336: INFO: stderr: ""
Jun  7 13:41:06.336: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:06.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3229" for this suite.

• [SLOW TEST:15.051 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":51,"skipped":827,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:06.353: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:41:06.389: INFO: The status of Pod server-envvars-d6f5ecac-36de-4a39-99a0-0c61447ae54a is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:41:08.403: INFO: The status of Pod server-envvars-d6f5ecac-36de-4a39-99a0-0c61447ae54a is Running (Ready = true)
Jun  7 13:41:08.442: INFO: Waiting up to 5m0s for pod "client-envvars-efdfdb96-32bf-45a2-a2cf-835b98a2b8e9" in namespace "pods-7491" to be "Succeeded or Failed"
Jun  7 13:41:08.448: INFO: Pod "client-envvars-efdfdb96-32bf-45a2-a2cf-835b98a2b8e9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.3846ms
Jun  7 13:41:10.455: INFO: Pod "client-envvars-efdfdb96-32bf-45a2-a2cf-835b98a2b8e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013625916s
STEP: Saw pod success
Jun  7 13:41:10.455: INFO: Pod "client-envvars-efdfdb96-32bf-45a2-a2cf-835b98a2b8e9" satisfied condition "Succeeded or Failed"
Jun  7 13:41:10.461: INFO: Trying to get logs from node proact-prod01-wk002 pod client-envvars-efdfdb96-32bf-45a2-a2cf-835b98a2b8e9 container env3cont: <nil>
STEP: delete the pod
Jun  7 13:41:10.484: INFO: Waiting for pod client-envvars-efdfdb96-32bf-45a2-a2cf-835b98a2b8e9 to disappear
Jun  7 13:41:10.489: INFO: Pod client-envvars-efdfdb96-32bf-45a2-a2cf-835b98a2b8e9 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:10.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7491" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":52,"skipped":837,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:10.502: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun  7 13:41:10.542: INFO: Waiting up to 5m0s for pod "pod-c6088e4b-b9f4-4d86-8880-c3416ea48fa9" in namespace "emptydir-1123" to be "Succeeded or Failed"
Jun  7 13:41:10.544: INFO: Pod "pod-c6088e4b-b9f4-4d86-8880-c3416ea48fa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08744ms
Jun  7 13:41:12.551: INFO: Pod "pod-c6088e4b-b9f4-4d86-8880-c3416ea48fa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008405116s
STEP: Saw pod success
Jun  7 13:41:12.551: INFO: Pod "pod-c6088e4b-b9f4-4d86-8880-c3416ea48fa9" satisfied condition "Succeeded or Failed"
Jun  7 13:41:12.557: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-c6088e4b-b9f4-4d86-8880-c3416ea48fa9 container test-container: <nil>
STEP: delete the pod
Jun  7 13:41:12.587: INFO: Waiting for pod pod-c6088e4b-b9f4-4d86-8880-c3416ea48fa9 to disappear
Jun  7 13:41:12.590: INFO: Pod pod-c6088e4b-b9f4-4d86-8880-c3416ea48fa9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:12.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1123" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":53,"skipped":847,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:12.603: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 13:41:12.649: INFO: Waiting up to 5m0s for pod "downwardapi-volume-adef0399-94ad-4ade-aa33-0385ffce3e00" in namespace "downward-api-3000" to be "Succeeded or Failed"
Jun  7 13:41:12.652: INFO: Pod "downwardapi-volume-adef0399-94ad-4ade-aa33-0385ffce3e00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.742209ms
Jun  7 13:41:14.659: INFO: Pod "downwardapi-volume-adef0399-94ad-4ade-aa33-0385ffce3e00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009387591s
STEP: Saw pod success
Jun  7 13:41:14.659: INFO: Pod "downwardapi-volume-adef0399-94ad-4ade-aa33-0385ffce3e00" satisfied condition "Succeeded or Failed"
Jun  7 13:41:14.664: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-adef0399-94ad-4ade-aa33-0385ffce3e00 container client-container: <nil>
STEP: delete the pod
Jun  7 13:41:14.692: INFO: Waiting for pod downwardapi-volume-adef0399-94ad-4ade-aa33-0385ffce3e00 to disappear
Jun  7 13:41:14.695: INFO: Pod downwardapi-volume-adef0399-94ad-4ade-aa33-0385ffce3e00 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:14.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3000" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":54,"skipped":864,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:14.708: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Jun  7 13:41:14.765: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  7 13:41:19.771: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jun  7 13:41:19.774: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jun  7 13:41:19.782: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jun  7 13:41:19.784: INFO: Observed &ReplicaSet event: ADDED
Jun  7 13:41:19.784: INFO: Observed &ReplicaSet event: MODIFIED
Jun  7 13:41:19.784: INFO: Observed &ReplicaSet event: MODIFIED
Jun  7 13:41:19.784: INFO: Observed &ReplicaSet event: MODIFIED
Jun  7 13:41:19.784: INFO: Found replicaset test-rs in namespace replicaset-5278 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  7 13:41:19.784: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jun  7 13:41:19.784: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun  7 13:41:19.790: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jun  7 13:41:19.791: INFO: Observed &ReplicaSet event: ADDED
Jun  7 13:41:19.791: INFO: Observed &ReplicaSet event: MODIFIED
Jun  7 13:41:19.791: INFO: Observed &ReplicaSet event: MODIFIED
Jun  7 13:41:19.791: INFO: Observed &ReplicaSet event: MODIFIED
Jun  7 13:41:19.791: INFO: Observed replicaset test-rs in namespace replicaset-5278 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  7 13:41:19.791: INFO: Observed &ReplicaSet event: MODIFIED
Jun  7 13:41:19.791: INFO: Found replicaset test-rs in namespace replicaset-5278 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jun  7 13:41:19.791: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:19.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5278" for this suite.

• [SLOW TEST:5.092 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":55,"skipped":879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:19.802: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jun  7 13:41:19.887: INFO: Waiting up to 5m0s for pod "downward-api-972c5c44-bc56-4712-8763-f4baf8136cdb" in namespace "downward-api-5830" to be "Succeeded or Failed"
Jun  7 13:41:19.891: INFO: Pod "downward-api-972c5c44-bc56-4712-8763-f4baf8136cdb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.739132ms
Jun  7 13:41:21.902: INFO: Pod "downward-api-972c5c44-bc56-4712-8763-f4baf8136cdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014786882s
STEP: Saw pod success
Jun  7 13:41:21.902: INFO: Pod "downward-api-972c5c44-bc56-4712-8763-f4baf8136cdb" satisfied condition "Succeeded or Failed"
Jun  7 13:41:21.908: INFO: Trying to get logs from node proact-prod01-wk002 pod downward-api-972c5c44-bc56-4712-8763-f4baf8136cdb container dapi-container: <nil>
STEP: delete the pod
Jun  7 13:41:21.937: INFO: Waiting for pod downward-api-972c5c44-bc56-4712-8763-f4baf8136cdb to disappear
Jun  7 13:41:21.940: INFO: Pod downward-api-972c5c44-bc56-4712-8763-f4baf8136cdb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:21.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5830" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":56,"skipped":902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:21.956: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun  7 13:41:21.995: INFO: Waiting up to 5m0s for pod "pod-d034722e-0417-406c-b562-75546c5b5622" in namespace "emptydir-7956" to be "Succeeded or Failed"
Jun  7 13:41:21.998: INFO: Pod "pod-d034722e-0417-406c-b562-75546c5b5622": Phase="Pending", Reason="", readiness=false. Elapsed: 3.212008ms
Jun  7 13:41:24.005: INFO: Pod "pod-d034722e-0417-406c-b562-75546c5b5622": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010299471s
STEP: Saw pod success
Jun  7 13:41:24.005: INFO: Pod "pod-d034722e-0417-406c-b562-75546c5b5622" satisfied condition "Succeeded or Failed"
Jun  7 13:41:24.010: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-d034722e-0417-406c-b562-75546c5b5622 container test-container: <nil>
STEP: delete the pod
Jun  7 13:41:24.044: INFO: Waiting for pod pod-d034722e-0417-406c-b562-75546c5b5622 to disappear
Jun  7 13:41:24.051: INFO: Pod pod-d034722e-0417-406c-b562-75546c5b5622 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:24.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7956" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":57,"skipped":926,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:24.062: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Jun  7 13:41:24.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-40 api-versions'
Jun  7 13:41:24.159: INFO: stderr: ""
Jun  7 13:41:24.159: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nargoproj.io/v1alpha1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndex.coreos.com/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions.istio.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\ninstall.istio.io/v1alpha1\nkubegres.reactive-tech.io/v1\nnetworking.istio.io/v1alpha3\nnetworking.istio.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noperator.tigera.io/v1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsecurity.istio.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntelemetry.istio.io/v1alpha1\ntrident.netapp.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:24.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-40" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":58,"skipped":940,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:24.171: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:24.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5051" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":59,"skipped":953,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:24.212: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6022
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6022
I0607 13:41:24.283818      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6022, replica count: 2
I0607 13:41:27.334958      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  7 13:41:27.335: INFO: Creating new exec pod
Jun  7 13:41:30.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6022 exec execpodc7vwh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun  7 13:41:30.576: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  7 13:41:30.576: INFO: stdout: "externalname-service-rshpd"
Jun  7 13:41:30.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6022 exec execpodc7vwh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.45.213 80'
Jun  7 13:41:30.773: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.45.213 80\nConnection to 10.104.45.213 80 port [tcp/http] succeeded!\n"
Jun  7 13:41:30.773: INFO: stdout: ""
Jun  7 13:41:31.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6022 exec execpodc7vwh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.45.213 80'
Jun  7 13:41:31.951: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.45.213 80\nConnection to 10.104.45.213 80 port [tcp/http] succeeded!\n"
Jun  7 13:41:31.951: INFO: stdout: ""
Jun  7 13:41:32.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6022 exec execpodc7vwh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.45.213 80'
Jun  7 13:41:32.959: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.45.213 80\nConnection to 10.104.45.213 80 port [tcp/http] succeeded!\n"
Jun  7 13:41:32.959: INFO: stdout: "externalname-service-rshpd"
Jun  7 13:41:32.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6022 exec execpodc7vwh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.55.210.13 32365'
Jun  7 13:41:33.167: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.55.210.13 32365\nConnection to 10.55.210.13 32365 port [tcp/*] succeeded!\n"
Jun  7 13:41:33.167: INFO: stdout: "externalname-service-rshpd"
Jun  7 13:41:33.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6022 exec execpodc7vwh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.55.210.14 32365'
Jun  7 13:41:33.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.55.210.14 32365\nConnection to 10.55.210.14 32365 port [tcp/*] succeeded!\n"
Jun  7 13:41:33.361: INFO: stdout: ""
Jun  7 13:41:34.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6022 exec execpodc7vwh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.55.210.14 32365'
Jun  7 13:41:34.580: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.55.210.14 32365\nConnection to 10.55.210.14 32365 port [tcp/*] succeeded!\n"
Jun  7 13:41:34.581: INFO: stdout: "externalname-service-wkzfq"
Jun  7 13:41:34.581: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:34.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6022" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.443 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":60,"skipped":979,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:34.655: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Jun  7 13:41:34.696: INFO: Waiting up to 5m0s for pod "var-expansion-92b19100-c737-4575-96d5-747054544da9" in namespace "var-expansion-860" to be "Succeeded or Failed"
Jun  7 13:41:34.702: INFO: Pod "var-expansion-92b19100-c737-4575-96d5-747054544da9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.710624ms
Jun  7 13:41:36.707: INFO: Pod "var-expansion-92b19100-c737-4575-96d5-747054544da9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010924506s
STEP: Saw pod success
Jun  7 13:41:36.707: INFO: Pod "var-expansion-92b19100-c737-4575-96d5-747054544da9" satisfied condition "Succeeded or Failed"
Jun  7 13:41:36.713: INFO: Trying to get logs from node proact-prod01-wk002 pod var-expansion-92b19100-c737-4575-96d5-747054544da9 container dapi-container: <nil>
STEP: delete the pod
Jun  7 13:41:36.740: INFO: Waiting for pod var-expansion-92b19100-c737-4575-96d5-747054544da9 to disappear
Jun  7 13:41:36.743: INFO: Pod var-expansion-92b19100-c737-4575-96d5-747054544da9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:36.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-860" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":61,"skipped":981,"failed":0}
SS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:36.755: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-64f2172c-1fa4-4ff8-a6cb-bafa5a836d9e
STEP: Creating secret with name secret-projected-all-test-volume-509eb38b-d087-4e34-b344-866df11f19bd
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun  7 13:41:36.802: INFO: Waiting up to 5m0s for pod "projected-volume-35834238-6eeb-4cd2-9239-827cdbcaeab1" in namespace "projected-2494" to be "Succeeded or Failed"
Jun  7 13:41:36.805: INFO: Pod "projected-volume-35834238-6eeb-4cd2-9239-827cdbcaeab1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.607066ms
Jun  7 13:41:38.813: INFO: Pod "projected-volume-35834238-6eeb-4cd2-9239-827cdbcaeab1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010730819s
STEP: Saw pod success
Jun  7 13:41:38.813: INFO: Pod "projected-volume-35834238-6eeb-4cd2-9239-827cdbcaeab1" satisfied condition "Succeeded or Failed"
Jun  7 13:41:38.818: INFO: Trying to get logs from node proact-prod01-wk002 pod projected-volume-35834238-6eeb-4cd2-9239-827cdbcaeab1 container projected-all-volume-test: <nil>
STEP: delete the pod
Jun  7 13:41:38.846: INFO: Waiting for pod projected-volume-35834238-6eeb-4cd2-9239-827cdbcaeab1 to disappear
Jun  7 13:41:38.851: INFO: Pod projected-volume-35834238-6eeb-4cd2-9239-827cdbcaeab1 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:38.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2494" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":62,"skipped":983,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:38.865: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun  7 13:41:38.903: INFO: Waiting up to 5m0s for pod "pod-2858071e-379f-4937-b073-a03185053e2d" in namespace "emptydir-4096" to be "Succeeded or Failed"
Jun  7 13:41:38.905: INFO: Pod "pod-2858071e-379f-4937-b073-a03185053e2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138253ms
Jun  7 13:41:40.909: INFO: Pod "pod-2858071e-379f-4937-b073-a03185053e2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005785363s
STEP: Saw pod success
Jun  7 13:41:40.909: INFO: Pod "pod-2858071e-379f-4937-b073-a03185053e2d" satisfied condition "Succeeded or Failed"
Jun  7 13:41:40.912: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-2858071e-379f-4937-b073-a03185053e2d container test-container: <nil>
STEP: delete the pod
Jun  7 13:41:40.926: INFO: Waiting for pod pod-2858071e-379f-4937-b073-a03185053e2d to disappear
Jun  7 13:41:40.929: INFO: Pod pod-2858071e-379f-4937-b073-a03185053e2d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:41:40.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4096" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":63,"skipped":1004,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:41:40.941: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:42:04.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6667" for this suite.

• [SLOW TEST:23.350 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":64,"skipped":1018,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:42:04.292: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun  7 13:42:04.327: INFO: Waiting up to 5m0s for pod "pod-f15f1eb9-ceff-47cf-b3bd-f15621368ee4" in namespace "emptydir-156" to be "Succeeded or Failed"
Jun  7 13:42:04.329: INFO: Pod "pod-f15f1eb9-ceff-47cf-b3bd-f15621368ee4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.276821ms
Jun  7 13:42:06.335: INFO: Pod "pod-f15f1eb9-ceff-47cf-b3bd-f15621368ee4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008276805s
STEP: Saw pod success
Jun  7 13:42:06.335: INFO: Pod "pod-f15f1eb9-ceff-47cf-b3bd-f15621368ee4" satisfied condition "Succeeded or Failed"
Jun  7 13:42:06.340: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-f15f1eb9-ceff-47cf-b3bd-f15621368ee4 container test-container: <nil>
STEP: delete the pod
Jun  7 13:42:06.359: INFO: Waiting for pod pod-f15f1eb9-ceff-47cf-b3bd-f15621368ee4 to disappear
Jun  7 13:42:06.361: INFO: Pod pod-f15f1eb9-ceff-47cf-b3bd-f15621368ee4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:42:06.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-156" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":65,"skipped":1033,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:42:06.377: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-38e97ee3-2c72-4316-9536-0d146d048af3
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:42:08.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3411" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":66,"skipped":1053,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:42:08.468: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun  7 13:42:08.515: INFO: Waiting up to 5m0s for pod "pod-45e6aafb-00fa-4329-bf10-c71c7c854b0f" in namespace "emptydir-2064" to be "Succeeded or Failed"
Jun  7 13:42:08.518: INFO: Pod "pod-45e6aafb-00fa-4329-bf10-c71c7c854b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.936104ms
Jun  7 13:42:10.524: INFO: Pod "pod-45e6aafb-00fa-4329-bf10-c71c7c854b0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009012755s
STEP: Saw pod success
Jun  7 13:42:10.524: INFO: Pod "pod-45e6aafb-00fa-4329-bf10-c71c7c854b0f" satisfied condition "Succeeded or Failed"
Jun  7 13:42:10.528: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-45e6aafb-00fa-4329-bf10-c71c7c854b0f container test-container: <nil>
STEP: delete the pod
Jun  7 13:42:10.551: INFO: Waiting for pod pod-45e6aafb-00fa-4329-bf10-c71c7c854b0f to disappear
Jun  7 13:42:10.554: INFO: Pod pod-45e6aafb-00fa-4329-bf10-c71c7c854b0f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:42:10.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2064" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":67,"skipped":1060,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:42:10.565: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jun  7 13:42:11.679: INFO: The status of Pod kube-controller-manager-proact-prod01-cp003 is Running (Ready = true)
Jun  7 13:42:11.741: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:42:11.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4367" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":68,"skipped":1098,"failed":0}

------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:42:11.753: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:42:15.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5643" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1098,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:42:15.814: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jun  7 13:42:15.846: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:42:19.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8614" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":70,"skipped":1116,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:42:19.334: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jun  7 13:42:19.393: INFO: The status of Pod labelsupdateba3d07cd-457a-43dc-96be-4db928d90fe0 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:42:21.400: INFO: The status of Pod labelsupdateba3d07cd-457a-43dc-96be-4db928d90fe0 is Running (Ready = true)
Jun  7 13:42:21.948: INFO: Successfully updated pod "labelsupdateba3d07cd-457a-43dc-96be-4db928d90fe0"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:42:25.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2987" for this suite.

• [SLOW TEST:6.678 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":71,"skipped":1128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:42:26.014: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:42:37.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8747" for this suite.

• [SLOW TEST:11.247 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":72,"skipped":1172,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:42:37.261: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-lnf7v in namespace proxy-7293
I0607 13:42:37.323585      23 runners.go:190] Created replication controller with name: proxy-service-lnf7v, namespace: proxy-7293, replica count: 1
I0607 13:42:38.375373      23 runners.go:190] proxy-service-lnf7v Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0607 13:42:39.375579      23 runners.go:190] proxy-service-lnf7v Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0607 13:42:40.376512      23 runners.go:190] proxy-service-lnf7v Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  7 13:42:40.381: INFO: setup took 3.090423036s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun  7 13:42:40.393: INFO: (0) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 11.297467ms)
Jun  7 13:42:40.393: INFO: (0) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 11.028056ms)
Jun  7 13:42:40.393: INFO: (0) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 11.122554ms)
Jun  7 13:42:40.394: INFO: (0) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 11.779712ms)
Jun  7 13:42:40.394: INFO: (0) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 12.180786ms)
Jun  7 13:42:40.394: INFO: (0) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 11.86766ms)
Jun  7 13:42:40.394: INFO: (0) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 11.763272ms)
Jun  7 13:42:40.394: INFO: (0) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 12.10198ms)
Jun  7 13:42:40.394: INFO: (0) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 11.665263ms)
Jun  7 13:42:40.395: INFO: (0) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 12.679634ms)
Jun  7 13:42:40.395: INFO: (0) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 13.631131ms)
Jun  7 13:42:40.400: INFO: (0) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 18.133519ms)
Jun  7 13:42:40.401: INFO: (0) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 18.973181ms)
Jun  7 13:42:40.401: INFO: (0) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 19.358202ms)
Jun  7 13:42:40.402: INFO: (0) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 19.491796ms)
Jun  7 13:42:40.402: INFO: (0) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 20.127767ms)
Jun  7 13:42:40.408: INFO: (1) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 5.746108ms)
Jun  7 13:42:40.408: INFO: (1) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 5.860163ms)
Jun  7 13:42:40.408: INFO: (1) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 6.161417ms)
Jun  7 13:42:40.408: INFO: (1) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 5.907112ms)
Jun  7 13:42:40.408: INFO: (1) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 5.763355ms)
Jun  7 13:42:40.408: INFO: (1) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 5.859311ms)
Jun  7 13:42:40.408: INFO: (1) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 5.963853ms)
Jun  7 13:42:40.408: INFO: (1) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 5.826025ms)
Jun  7 13:42:40.408: INFO: (1) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 5.949804ms)
Jun  7 13:42:40.408: INFO: (1) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 5.941282ms)
Jun  7 13:42:40.411: INFO: (1) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 8.711789ms)
Jun  7 13:42:40.411: INFO: (1) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 8.966562ms)
Jun  7 13:42:40.411: INFO: (1) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 8.992651ms)
Jun  7 13:42:40.411: INFO: (1) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 8.924458ms)
Jun  7 13:42:40.411: INFO: (1) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 8.847142ms)
Jun  7 13:42:40.411: INFO: (1) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 8.76444ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 9.367429ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 9.038901ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 9.07749ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 9.031263ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 8.973604ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 8.959794ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 9.286031ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 9.265093ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 9.240306ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 8.953531ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 8.998132ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 9.440483ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 9.260204ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 9.113214ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 9.397882ms)
Jun  7 13:42:40.420: INFO: (2) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 9.250219ms)
Jun  7 13:42:40.444: INFO: (3) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 23.276463ms)
Jun  7 13:42:40.444: INFO: (3) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 23.370402ms)
Jun  7 13:42:40.448: INFO: (3) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 27.361393ms)
Jun  7 13:42:40.448: INFO: (3) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 27.301062ms)
Jun  7 13:42:40.448: INFO: (3) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 27.370474ms)
Jun  7 13:42:40.448: INFO: (3) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 27.321374ms)
Jun  7 13:42:40.448: INFO: (3) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 27.374518ms)
Jun  7 13:42:40.448: INFO: (3) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 27.294203ms)
Jun  7 13:42:40.448: INFO: (3) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 27.393217ms)
Jun  7 13:42:40.448: INFO: (3) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 27.344326ms)
Jun  7 13:42:40.456: INFO: (3) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 35.591166ms)
Jun  7 13:42:40.460: INFO: (3) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 39.195512ms)
Jun  7 13:42:40.465: INFO: (3) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 44.171613ms)
Jun  7 13:42:40.465: INFO: (3) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 44.232663ms)
Jun  7 13:42:40.465: INFO: (3) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 44.232295ms)
Jun  7 13:42:40.465: INFO: (3) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 44.259464ms)
Jun  7 13:42:40.478: INFO: (4) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 12.644482ms)
Jun  7 13:42:40.484: INFO: (4) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 18.654644ms)
Jun  7 13:42:40.484: INFO: (4) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 18.746639ms)
Jun  7 13:42:40.484: INFO: (4) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 18.499983ms)
Jun  7 13:42:40.484: INFO: (4) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 18.577354ms)
Jun  7 13:42:40.492: INFO: (4) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 26.757528ms)
Jun  7 13:42:40.492: INFO: (4) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 26.533518ms)
Jun  7 13:42:40.492: INFO: (4) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 26.552393ms)
Jun  7 13:42:40.492: INFO: (4) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 26.844898ms)
Jun  7 13:42:40.492: INFO: (4) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 26.891689ms)
Jun  7 13:42:40.500: INFO: (4) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 34.702375ms)
Jun  7 13:42:40.500: INFO: (4) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 34.486671ms)
Jun  7 13:42:40.500: INFO: (4) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 34.967316ms)
Jun  7 13:42:40.500: INFO: (4) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 34.680312ms)
Jun  7 13:42:40.500: INFO: (4) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 34.498731ms)
Jun  7 13:42:40.500: INFO: (4) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 34.669204ms)
Jun  7 13:42:40.515: INFO: (5) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 15.099329ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 19.411648ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 19.61675ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 19.696386ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 19.871153ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 19.696941ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 19.618019ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 19.659182ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 19.896359ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 19.514205ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 19.549094ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 20.001634ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 19.673217ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 20.164361ms)
Jun  7 13:42:40.520: INFO: (5) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 19.969006ms)
Jun  7 13:42:40.524: INFO: (5) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 23.576127ms)
Jun  7 13:42:40.536: INFO: (6) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 11.703641ms)
Jun  7 13:42:40.536: INFO: (6) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 11.904826ms)
Jun  7 13:42:40.540: INFO: (6) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 15.798323ms)
Jun  7 13:42:40.540: INFO: (6) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 15.828076ms)
Jun  7 13:42:40.543: INFO: (6) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 18.745779ms)
Jun  7 13:42:40.543: INFO: (6) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 18.888361ms)
Jun  7 13:42:40.543: INFO: (6) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 18.89869ms)
Jun  7 13:42:40.543: INFO: (6) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 19.201954ms)
Jun  7 13:42:40.543: INFO: (6) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 19.059705ms)
Jun  7 13:42:40.554: INFO: (6) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 29.244711ms)
Jun  7 13:42:40.554: INFO: (6) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 29.19414ms)
Jun  7 13:42:40.554: INFO: (6) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 29.423457ms)
Jun  7 13:42:40.554: INFO: (6) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 29.283374ms)
Jun  7 13:42:40.554: INFO: (6) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 29.210382ms)
Jun  7 13:42:40.554: INFO: (6) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 29.748249ms)
Jun  7 13:42:40.554: INFO: (6) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 29.818156ms)
Jun  7 13:42:40.559: INFO: (7) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 4.270604ms)
Jun  7 13:42:40.561: INFO: (7) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 6.242642ms)
Jun  7 13:42:40.561: INFO: (7) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 6.194378ms)
Jun  7 13:42:40.561: INFO: (7) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 6.236353ms)
Jun  7 13:42:40.561: INFO: (7) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 6.185754ms)
Jun  7 13:42:40.561: INFO: (7) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 6.116842ms)
Jun  7 13:42:40.561: INFO: (7) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 6.167671ms)
Jun  7 13:42:40.564: INFO: (7) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 9.470878ms)
Jun  7 13:42:40.565: INFO: (7) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 10.122754ms)
Jun  7 13:42:40.565: INFO: (7) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 9.964029ms)
Jun  7 13:42:40.565: INFO: (7) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 9.932495ms)
Jun  7 13:42:40.565: INFO: (7) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 10.817322ms)
Jun  7 13:42:40.565: INFO: (7) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 10.939238ms)
Jun  7 13:42:40.565: INFO: (7) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 10.973677ms)
Jun  7 13:42:40.566: INFO: (7) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 11.126176ms)
Jun  7 13:42:40.566: INFO: (7) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 11.718719ms)
Jun  7 13:42:40.576: INFO: (8) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 9.376638ms)
Jun  7 13:42:40.577: INFO: (8) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 9.965316ms)
Jun  7 13:42:40.577: INFO: (8) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 9.817239ms)
Jun  7 13:42:40.577: INFO: (8) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 10.04117ms)
Jun  7 13:42:40.577: INFO: (8) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 10.269841ms)
Jun  7 13:42:40.577: INFO: (8) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 10.037335ms)
Jun  7 13:42:40.577: INFO: (8) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 10.238531ms)
Jun  7 13:42:40.577: INFO: (8) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 10.15164ms)
Jun  7 13:42:40.578: INFO: (8) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 10.872989ms)
Jun  7 13:42:40.578: INFO: (8) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 11.8069ms)
Jun  7 13:42:40.578: INFO: (8) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 11.638399ms)
Jun  7 13:42:40.578: INFO: (8) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 11.621599ms)
Jun  7 13:42:40.579: INFO: (8) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 11.846873ms)
Jun  7 13:42:40.580: INFO: (8) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 13.243062ms)
Jun  7 13:42:40.580: INFO: (8) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 13.800139ms)
Jun  7 13:42:40.581: INFO: (8) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 13.786841ms)
Jun  7 13:42:40.589: INFO: (9) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 8.931523ms)
Jun  7 13:42:40.590: INFO: (9) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 8.298701ms)
Jun  7 13:42:40.590: INFO: (9) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 8.517001ms)
Jun  7 13:42:40.590: INFO: (9) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 9.258897ms)
Jun  7 13:42:40.590: INFO: (9) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 9.594594ms)
Jun  7 13:42:40.590: INFO: (9) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 9.571157ms)
Jun  7 13:42:40.590: INFO: (9) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 9.641106ms)
Jun  7 13:42:40.590: INFO: (9) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 9.341246ms)
Jun  7 13:42:40.590: INFO: (9) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 9.297249ms)
Jun  7 13:42:40.590: INFO: (9) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 9.419553ms)
Jun  7 13:42:40.591: INFO: (9) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 9.661821ms)
Jun  7 13:42:40.592: INFO: (9) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 11.184412ms)
Jun  7 13:42:40.592: INFO: (9) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 11.040903ms)
Jun  7 13:42:40.592: INFO: (9) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 11.145875ms)
Jun  7 13:42:40.592: INFO: (9) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 11.605136ms)
Jun  7 13:42:40.592: INFO: (9) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 11.654646ms)
Jun  7 13:42:40.599: INFO: (10) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 6.54849ms)
Jun  7 13:42:40.599: INFO: (10) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 6.540446ms)
Jun  7 13:42:40.599: INFO: (10) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 6.663991ms)
Jun  7 13:42:40.599: INFO: (10) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 6.844803ms)
Jun  7 13:42:40.599: INFO: (10) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 6.75768ms)
Jun  7 13:42:40.599: INFO: (10) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 6.578615ms)
Jun  7 13:42:40.599: INFO: (10) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 6.589765ms)
Jun  7 13:42:40.600: INFO: (10) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 6.99625ms)
Jun  7 13:42:40.600: INFO: (10) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 7.374508ms)
Jun  7 13:42:40.600: INFO: (10) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 8.077217ms)
Jun  7 13:42:40.600: INFO: (10) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 7.569626ms)
Jun  7 13:42:40.601: INFO: (10) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 8.556958ms)
Jun  7 13:42:40.602: INFO: (10) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 9.643434ms)
Jun  7 13:42:40.602: INFO: (10) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 9.632519ms)
Jun  7 13:42:40.602: INFO: (10) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 9.740273ms)
Jun  7 13:42:40.602: INFO: (10) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 9.778664ms)
Jun  7 13:42:40.605: INFO: (11) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 2.459056ms)
Jun  7 13:42:40.607: INFO: (11) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 4.140928ms)
Jun  7 13:42:40.607: INFO: (11) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 4.384885ms)
Jun  7 13:42:40.607: INFO: (11) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 4.214877ms)
Jun  7 13:42:40.607: INFO: (11) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 4.696081ms)
Jun  7 13:42:40.608: INFO: (11) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 4.984005ms)
Jun  7 13:42:40.608: INFO: (11) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 4.956813ms)
Jun  7 13:42:40.608: INFO: (11) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 4.828018ms)
Jun  7 13:42:40.608: INFO: (11) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 4.897631ms)
Jun  7 13:42:40.608: INFO: (11) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 4.968095ms)
Jun  7 13:42:40.608: INFO: (11) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 5.762227ms)
Jun  7 13:42:40.610: INFO: (11) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 6.586581ms)
Jun  7 13:42:40.610: INFO: (11) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 7.109415ms)
Jun  7 13:42:40.610: INFO: (11) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 7.001296ms)
Jun  7 13:42:40.610: INFO: (11) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 7.158053ms)
Jun  7 13:42:40.610: INFO: (11) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 6.942815ms)
Jun  7 13:42:40.616: INFO: (12) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 6.149506ms)
Jun  7 13:42:40.616: INFO: (12) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 6.451743ms)
Jun  7 13:42:40.617: INFO: (12) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 6.738188ms)
Jun  7 13:42:40.617: INFO: (12) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 6.726361ms)
Jun  7 13:42:40.617: INFO: (12) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 6.546132ms)
Jun  7 13:42:40.617: INFO: (12) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 6.607016ms)
Jun  7 13:42:40.617: INFO: (12) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 6.639425ms)
Jun  7 13:42:40.617: INFO: (12) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 6.941366ms)
Jun  7 13:42:40.617: INFO: (12) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 6.546031ms)
Jun  7 13:42:40.617: INFO: (12) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 6.620905ms)
Jun  7 13:42:40.617: INFO: (12) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 6.893769ms)
Jun  7 13:42:40.617: INFO: (12) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 7.087196ms)
Jun  7 13:42:40.620: INFO: (12) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 9.5337ms)
Jun  7 13:42:40.620: INFO: (12) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 9.389684ms)
Jun  7 13:42:40.620: INFO: (12) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 9.33599ms)
Jun  7 13:42:40.620: INFO: (12) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 9.378402ms)
Jun  7 13:42:40.624: INFO: (13) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 4.309703ms)
Jun  7 13:42:40.626: INFO: (13) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 5.730838ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 7.01659ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 6.92175ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 6.935061ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 6.925119ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 6.99662ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 6.951574ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 6.971665ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 6.994232ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 7.006993ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 7.028596ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 7.160786ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 7.015442ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 7.010029ms)
Jun  7 13:42:40.627: INFO: (13) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 7.132194ms)
Jun  7 13:42:40.630: INFO: (14) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 2.932772ms)
Jun  7 13:42:40.632: INFO: (14) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 5.330509ms)
Jun  7 13:42:40.632: INFO: (14) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 5.22989ms)
Jun  7 13:42:40.632: INFO: (14) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 5.173791ms)
Jun  7 13:42:40.633: INFO: (14) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 5.17504ms)
Jun  7 13:42:40.633: INFO: (14) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 5.139776ms)
Jun  7 13:42:40.633: INFO: (14) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 5.27408ms)
Jun  7 13:42:40.633: INFO: (14) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 5.170629ms)
Jun  7 13:42:40.633: INFO: (14) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 5.196043ms)
Jun  7 13:42:40.633: INFO: (14) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 6.03744ms)
Jun  7 13:42:40.634: INFO: (14) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 6.475548ms)
Jun  7 13:42:40.635: INFO: (14) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 7.371653ms)
Jun  7 13:42:40.635: INFO: (14) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 7.423788ms)
Jun  7 13:42:40.635: INFO: (14) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 7.441538ms)
Jun  7 13:42:40.635: INFO: (14) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 7.497654ms)
Jun  7 13:42:40.635: INFO: (14) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 7.901486ms)
Jun  7 13:42:40.641: INFO: (15) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 5.545493ms)
Jun  7 13:42:40.641: INFO: (15) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 5.406819ms)
Jun  7 13:42:40.641: INFO: (15) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 5.3831ms)
Jun  7 13:42:40.641: INFO: (15) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 5.512949ms)
Jun  7 13:42:40.641: INFO: (15) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 5.550731ms)
Jun  7 13:42:40.641: INFO: (15) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 5.326898ms)
Jun  7 13:42:40.641: INFO: (15) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 5.813461ms)
Jun  7 13:42:40.641: INFO: (15) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 5.402999ms)
Jun  7 13:42:40.641: INFO: (15) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 5.912489ms)
Jun  7 13:42:40.641: INFO: (15) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 5.743427ms)
Jun  7 13:42:40.641: INFO: (15) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 5.565619ms)
Jun  7 13:42:40.642: INFO: (15) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 6.950541ms)
Jun  7 13:42:40.642: INFO: (15) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 6.722069ms)
Jun  7 13:42:40.643: INFO: (15) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 7.25422ms)
Jun  7 13:42:40.643: INFO: (15) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 7.109227ms)
Jun  7 13:42:40.643: INFO: (15) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 7.34506ms)
Jun  7 13:42:40.650: INFO: (16) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 7.109212ms)
Jun  7 13:42:40.650: INFO: (16) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 7.176552ms)
Jun  7 13:42:40.650: INFO: (16) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 7.308186ms)
Jun  7 13:42:40.650: INFO: (16) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 7.297008ms)
Jun  7 13:42:40.651: INFO: (16) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 8.56378ms)
Jun  7 13:42:40.651: INFO: (16) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 8.499377ms)
Jun  7 13:42:40.652: INFO: (16) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 8.544662ms)
Jun  7 13:42:40.652: INFO: (16) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 8.768905ms)
Jun  7 13:42:40.652: INFO: (16) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 8.682859ms)
Jun  7 13:42:40.652: INFO: (16) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 8.776915ms)
Jun  7 13:42:40.652: INFO: (16) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 8.772173ms)
Jun  7 13:42:40.652: INFO: (16) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 8.676717ms)
Jun  7 13:42:40.653: INFO: (16) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 9.666495ms)
Jun  7 13:42:40.653: INFO: (16) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 9.932641ms)
Jun  7 13:42:40.653: INFO: (16) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 10.071898ms)
Jun  7 13:42:40.653: INFO: (16) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 10.21832ms)
Jun  7 13:42:40.661: INFO: (17) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 7.836225ms)
Jun  7 13:42:40.661: INFO: (17) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 7.980349ms)
Jun  7 13:42:40.661: INFO: (17) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 8.190514ms)
Jun  7 13:42:40.661: INFO: (17) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 8.089808ms)
Jun  7 13:42:40.661: INFO: (17) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 7.926583ms)
Jun  7 13:42:40.661: INFO: (17) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 7.99558ms)
Jun  7 13:42:40.661: INFO: (17) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 7.946491ms)
Jun  7 13:42:40.661: INFO: (17) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 8.213058ms)
Jun  7 13:42:40.661: INFO: (17) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 8.084965ms)
Jun  7 13:42:40.661: INFO: (17) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 8.008736ms)
Jun  7 13:42:40.661: INFO: (17) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 7.909516ms)
Jun  7 13:42:40.661: INFO: (17) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 8.225508ms)
Jun  7 13:42:40.662: INFO: (17) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 8.505639ms)
Jun  7 13:42:40.662: INFO: (17) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 8.561044ms)
Jun  7 13:42:40.662: INFO: (17) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 8.507341ms)
Jun  7 13:42:40.662: INFO: (17) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 8.503598ms)
Jun  7 13:42:40.667: INFO: (18) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 5.145018ms)
Jun  7 13:42:40.667: INFO: (18) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 5.075694ms)
Jun  7 13:42:40.667: INFO: (18) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 5.203251ms)
Jun  7 13:42:40.667: INFO: (18) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 5.079755ms)
Jun  7 13:42:40.667: INFO: (18) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 5.27372ms)
Jun  7 13:42:40.667: INFO: (18) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 5.153664ms)
Jun  7 13:42:40.667: INFO: (18) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 5.361234ms)
Jun  7 13:42:40.668: INFO: (18) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 5.245832ms)
Jun  7 13:42:40.668: INFO: (18) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 5.351777ms)
Jun  7 13:42:40.668: INFO: (18) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 5.721745ms)
Jun  7 13:42:40.668: INFO: (18) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 5.902872ms)
Jun  7 13:42:40.669: INFO: (18) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 7.443844ms)
Jun  7 13:42:40.670: INFO: (18) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 7.256919ms)
Jun  7 13:42:40.670: INFO: (18) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 7.299653ms)
Jun  7 13:42:40.670: INFO: (18) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 7.361737ms)
Jun  7 13:42:40.670: INFO: (18) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 7.906167ms)
Jun  7 13:42:40.676: INFO: (19) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 5.377038ms)
Jun  7 13:42:40.676: INFO: (19) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:462/proxy/: tls qux (200; 5.109503ms)
Jun  7 13:42:40.676: INFO: (19) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 5.214976ms)
Jun  7 13:42:40.676: INFO: (19) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb/proxy/rewriteme">test</a> (200; 5.069563ms)
Jun  7 13:42:40.676: INFO: (19) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:162/proxy/: bar (200; 6.156419ms)
Jun  7 13:42:40.676: INFO: (19) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:160/proxy/: foo (200; 5.171332ms)
Jun  7 13:42:40.677: INFO: (19) /api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/http:proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">... (200; 6.370687ms)
Jun  7 13:42:40.678: INFO: (19) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:460/proxy/: tls baz (200; 6.538647ms)
Jun  7 13:42:40.678: INFO: (19) /api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/https:proxy-service-lnf7v-m22kb:443/proxy/tlsrewritem... (200; 6.489012ms)
Jun  7 13:42:40.678: INFO: (19) /api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/: <a href="/api/v1/namespaces/proxy-7293/pods/proxy-service-lnf7v-m22kb:1080/proxy/rewriteme">test<... (200; 6.821603ms)
Jun  7 13:42:40.678: INFO: (19) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname2/proxy/: bar (200; 7.54714ms)
Jun  7 13:42:40.679: INFO: (19) /api/v1/namespaces/proxy-7293/services/http:proxy-service-lnf7v:portname1/proxy/: foo (200; 7.880114ms)
Jun  7 13:42:40.679: INFO: (19) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname2/proxy/: tls qux (200; 8.179112ms)
Jun  7 13:42:40.680: INFO: (19) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname2/proxy/: bar (200; 8.66584ms)
Jun  7 13:42:40.680: INFO: (19) /api/v1/namespaces/proxy-7293/services/https:proxy-service-lnf7v:tlsportname1/proxy/: tls baz (200; 8.930235ms)
Jun  7 13:42:40.680: INFO: (19) /api/v1/namespaces/proxy-7293/services/proxy-service-lnf7v:portname1/proxy/: foo (200; 8.836962ms)
STEP: deleting ReplicationController proxy-service-lnf7v in namespace proxy-7293, will wait for the garbage collector to delete the pods
Jun  7 13:42:40.743: INFO: Deleting ReplicationController proxy-service-lnf7v took: 9.63133ms
Jun  7 13:42:40.844: INFO: Terminating ReplicationController proxy-service-lnf7v pods took: 101.076623ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:42:43.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7293" for this suite.

• [SLOW TEST:6.304 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":73,"skipped":1181,"failed":0}
S
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:42:43.565: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jun  7 13:42:43.593: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:42:46.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8546" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":74,"skipped":1182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:42:46.499: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  7 13:42:46.556: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:46.556: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:46.556: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:46.558: INFO: Number of nodes with available pods: 0
Jun  7 13:42:46.558: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 13:42:47.569: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:47.569: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:47.569: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:47.573: INFO: Number of nodes with available pods: 1
Jun  7 13:42:47.573: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 13:42:48.570: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:48.570: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:48.570: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:48.578: INFO: Number of nodes with available pods: 4
Jun  7 13:42:48.578: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun  7 13:42:48.615: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:48.615: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:48.615: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:48.619: INFO: Number of nodes with available pods: 3
Jun  7 13:42:48.619: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 13:42:49.631: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:49.631: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:49.631: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:49.637: INFO: Number of nodes with available pods: 3
Jun  7 13:42:49.637: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 13:42:50.635: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:50.647: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:50.647: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 13:42:50.653: INFO: Number of nodes with available pods: 4
Jun  7 13:42:50.654: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4414, will wait for the garbage collector to delete the pods
Jun  7 13:42:50.727: INFO: Deleting DaemonSet.extensions daemon-set took: 13.37639ms
Jun  7 13:42:50.828: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.930973ms
Jun  7 13:42:52.734: INFO: Number of nodes with available pods: 0
Jun  7 13:42:52.734: INFO: Number of running nodes: 0, number of available pods: 0
Jun  7 13:42:52.738: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33564442"},"items":null}

Jun  7 13:42:52.742: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33564442"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:42:52.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4414" for this suite.

• [SLOW TEST:6.283 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":75,"skipped":1205,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:42:52.783: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5051
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  7 13:42:52.817: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  7 13:42:52.858: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:42:54.866: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:42:56.868: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:42:58.869: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:43:00.865: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:43:02.868: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:43:04.866: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:43:06.864: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:43:08.868: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:43:10.868: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:43:12.867: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 13:43:14.866: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun  7 13:43:14.876: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun  7 13:43:14.882: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jun  7 13:43:14.889: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jun  7 13:43:16.914: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jun  7 13:43:16.914: INFO: Breadth first check of 192.168.56.215 on host 10.55.210.13...
Jun  7 13:43:16.918: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.39.238:9080/dial?request=hostname&protocol=http&host=192.168.56.215&port=8083&tries=1'] Namespace:pod-network-test-5051 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:43:16.918: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:43:17.033: INFO: Waiting for responses: map[]
Jun  7 13:43:17.033: INFO: reached 192.168.56.215 after 0/1 tries
Jun  7 13:43:17.033: INFO: Breadth first check of 192.168.39.239 on host 10.55.210.14...
Jun  7 13:43:17.039: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.39.238:9080/dial?request=hostname&protocol=http&host=192.168.39.239&port=8083&tries=1'] Namespace:pod-network-test-5051 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:43:17.039: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:43:17.141: INFO: Waiting for responses: map[]
Jun  7 13:43:17.141: INFO: reached 192.168.39.239 after 0/1 tries
Jun  7 13:43:17.141: INFO: Breadth first check of 192.168.46.104 on host 10.55.210.15...
Jun  7 13:43:17.146: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.39.238:9080/dial?request=hostname&protocol=http&host=192.168.46.104&port=8083&tries=1'] Namespace:pod-network-test-5051 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:43:17.146: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:43:17.279: INFO: Waiting for responses: map[]
Jun  7 13:43:17.279: INFO: reached 192.168.46.104 after 0/1 tries
Jun  7 13:43:17.279: INFO: Breadth first check of 192.168.7.60 on host 10.55.210.16...
Jun  7 13:43:17.285: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.39.238:9080/dial?request=hostname&protocol=http&host=192.168.7.60&port=8083&tries=1'] Namespace:pod-network-test-5051 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:43:17.285: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:43:17.412: INFO: Waiting for responses: map[]
Jun  7 13:43:17.413: INFO: reached 192.168.7.60 after 0/1 tries
Jun  7 13:43:17.413: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:43:17.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5051" for this suite.

• [SLOW TEST:24.650 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":76,"skipped":1212,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:43:17.436: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9bb9e1a8-37cb-483b-ae09-94e003983e84
STEP: Creating the pod
Jun  7 13:43:17.505: INFO: The status of Pod pod-projected-configmaps-c7a1ba45-8d69-400d-b5ad-b0e8118fec82 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:43:19.515: INFO: The status of Pod pod-projected-configmaps-c7a1ba45-8d69-400d-b5ad-b0e8118fec82 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-9bb9e1a8-37cb-483b-ae09-94e003983e84
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:43:21.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2240" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":77,"skipped":1225,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:43:21.578: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:149
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:43:21.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8909" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":78,"skipped":1241,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:43:21.637: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-bdacbed6-3ab2-4001-ab0e-1d81dfbcadf9
STEP: Creating a pod to test consume secrets
Jun  7 13:43:21.674: INFO: Waiting up to 5m0s for pod "pod-secrets-7320946f-e83d-4d00-82ac-5feaee3580b2" in namespace "secrets-9461" to be "Succeeded or Failed"
Jun  7 13:43:21.679: INFO: Pod "pod-secrets-7320946f-e83d-4d00-82ac-5feaee3580b2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.196363ms
Jun  7 13:43:23.686: INFO: Pod "pod-secrets-7320946f-e83d-4d00-82ac-5feaee3580b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011420024s
STEP: Saw pod success
Jun  7 13:43:23.686: INFO: Pod "pod-secrets-7320946f-e83d-4d00-82ac-5feaee3580b2" satisfied condition "Succeeded or Failed"
Jun  7 13:43:23.689: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-secrets-7320946f-e83d-4d00-82ac-5feaee3580b2 container secret-volume-test: <nil>
STEP: delete the pod
Jun  7 13:43:23.707: INFO: Waiting for pod pod-secrets-7320946f-e83d-4d00-82ac-5feaee3580b2 to disappear
Jun  7 13:43:23.709: INFO: Pod pod-secrets-7320946f-e83d-4d00-82ac-5feaee3580b2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:43:23.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9461" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":79,"skipped":1275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:43:23.719: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Jun  7 13:43:24.277: INFO: created pod pod-service-account-defaultsa
Jun  7 13:43:24.277: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun  7 13:43:24.285: INFO: created pod pod-service-account-mountsa
Jun  7 13:43:24.285: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun  7 13:43:24.292: INFO: created pod pod-service-account-nomountsa
Jun  7 13:43:24.292: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun  7 13:43:24.300: INFO: created pod pod-service-account-defaultsa-mountspec
Jun  7 13:43:24.300: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun  7 13:43:24.306: INFO: created pod pod-service-account-mountsa-mountspec
Jun  7 13:43:24.306: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun  7 13:43:24.311: INFO: created pod pod-service-account-nomountsa-mountspec
Jun  7 13:43:24.311: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun  7 13:43:24.317: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun  7 13:43:24.317: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun  7 13:43:24.326: INFO: created pod pod-service-account-mountsa-nomountspec
Jun  7 13:43:24.326: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun  7 13:43:24.334: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun  7 13:43:24.334: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:43:24.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1405" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":80,"skipped":1314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:43:24.349: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:43:24.387: INFO: The status of Pod busybox-scheduling-cc915c66-f2c7-444a-af61-cfa763d1620f is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:43:26.392: INFO: The status of Pod busybox-scheduling-cc915c66-f2c7-444a-af61-cfa763d1620f is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:43:28.391: INFO: The status of Pod busybox-scheduling-cc915c66-f2c7-444a-af61-cfa763d1620f is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:43:30.392: INFO: The status of Pod busybox-scheduling-cc915c66-f2c7-444a-af61-cfa763d1620f is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:43:32.391: INFO: The status of Pod busybox-scheduling-cc915c66-f2c7-444a-af61-cfa763d1620f is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:43:32.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1099" for this suite.

• [SLOW TEST:8.064 seconds]
[sig-node] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":81,"skipped":1353,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:43:32.413: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 13:43:32.444: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db9e302f-852a-4b00-b6a1-5f4910eb85a8" in namespace "projected-9680" to be "Succeeded or Failed"
Jun  7 13:43:32.446: INFO: Pod "downwardapi-volume-db9e302f-852a-4b00-b6a1-5f4910eb85a8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.925166ms
Jun  7 13:43:34.451: INFO: Pod "downwardapi-volume-db9e302f-852a-4b00-b6a1-5f4910eb85a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007228905s
Jun  7 13:43:36.459: INFO: Pod "downwardapi-volume-db9e302f-852a-4b00-b6a1-5f4910eb85a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014899974s
Jun  7 13:43:38.470: INFO: Pod "downwardapi-volume-db9e302f-852a-4b00-b6a1-5f4910eb85a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026358337s
Jun  7 13:43:40.485: INFO: Pod "downwardapi-volume-db9e302f-852a-4b00-b6a1-5f4910eb85a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.041182115s
STEP: Saw pod success
Jun  7 13:43:40.485: INFO: Pod "downwardapi-volume-db9e302f-852a-4b00-b6a1-5f4910eb85a8" satisfied condition "Succeeded or Failed"
Jun  7 13:43:40.489: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-db9e302f-852a-4b00-b6a1-5f4910eb85a8 container client-container: <nil>
STEP: delete the pod
Jun  7 13:43:40.515: INFO: Waiting for pod downwardapi-volume-db9e302f-852a-4b00-b6a1-5f4910eb85a8 to disappear
Jun  7 13:43:40.517: INFO: Pod downwardapi-volume-db9e302f-852a-4b00-b6a1-5f4910eb85a8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:43:40.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9680" for this suite.

• [SLOW TEST:8.120 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":82,"skipped":1384,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:43:40.534: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:43:40.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8255" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":83,"skipped":1401,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:43:40.614: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 13:43:40.656: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a3c2ea4-ad0c-4cfc-9e38-59287979c317" in namespace "downward-api-4257" to be "Succeeded or Failed"
Jun  7 13:43:40.659: INFO: Pod "downwardapi-volume-7a3c2ea4-ad0c-4cfc-9e38-59287979c317": Phase="Pending", Reason="", readiness=false. Elapsed: 2.743608ms
Jun  7 13:43:42.666: INFO: Pod "downwardapi-volume-7a3c2ea4-ad0c-4cfc-9e38-59287979c317": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009825489s
Jun  7 13:43:44.674: INFO: Pod "downwardapi-volume-7a3c2ea4-ad0c-4cfc-9e38-59287979c317": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017369703s
STEP: Saw pod success
Jun  7 13:43:44.674: INFO: Pod "downwardapi-volume-7a3c2ea4-ad0c-4cfc-9e38-59287979c317" satisfied condition "Succeeded or Failed"
Jun  7 13:43:44.678: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-7a3c2ea4-ad0c-4cfc-9e38-59287979c317 container client-container: <nil>
STEP: delete the pod
Jun  7 13:43:44.698: INFO: Waiting for pod downwardapi-volume-7a3c2ea4-ad0c-4cfc-9e38-59287979c317 to disappear
Jun  7 13:43:44.703: INFO: Pod downwardapi-volume-7a3c2ea4-ad0c-4cfc-9e38-59287979c317 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:43:44.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4257" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1422,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:43:44.717: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun  7 13:43:44.759: INFO: Waiting up to 5m0s for pod "pod-30ccbcab-59d7-4682-a3fb-7f2d5311ee92" in namespace "emptydir-540" to be "Succeeded or Failed"
Jun  7 13:43:44.766: INFO: Pod "pod-30ccbcab-59d7-4682-a3fb-7f2d5311ee92": Phase="Pending", Reason="", readiness=false. Elapsed: 5.662745ms
Jun  7 13:43:46.772: INFO: Pod "pod-30ccbcab-59d7-4682-a3fb-7f2d5311ee92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012292034s
STEP: Saw pod success
Jun  7 13:43:46.772: INFO: Pod "pod-30ccbcab-59d7-4682-a3fb-7f2d5311ee92" satisfied condition "Succeeded or Failed"
Jun  7 13:43:46.776: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-30ccbcab-59d7-4682-a3fb-7f2d5311ee92 container test-container: <nil>
STEP: delete the pod
Jun  7 13:43:46.797: INFO: Waiting for pod pod-30ccbcab-59d7-4682-a3fb-7f2d5311ee92 to disappear
Jun  7 13:43:46.800: INFO: Pod pod-30ccbcab-59d7-4682-a3fb-7f2d5311ee92 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:43:46.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-540" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":85,"skipped":1425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:43:46.812: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jun  7 13:43:46.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-8008 create -f -'
Jun  7 13:43:48.007: INFO: stderr: ""
Jun  7 13:43:48.007: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun  7 13:43:49.013: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  7 13:43:49.013: INFO: Found 0 / 1
Jun  7 13:43:50.010: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  7 13:43:50.010: INFO: Found 1 / 1
Jun  7 13:43:50.010: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun  7 13:43:50.012: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  7 13:43:50.012: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  7 13:43:50.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-8008 patch pod agnhost-primary-jm442 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun  7 13:43:50.067: INFO: stderr: ""
Jun  7 13:43:50.067: INFO: stdout: "pod/agnhost-primary-jm442 patched\n"
STEP: checking annotations
Jun  7 13:43:50.069: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  7 13:43:50.069: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:43:50.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8008" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":86,"skipped":1461,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:43:50.077: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jun  7 13:43:50.108: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:43:52.112: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jun  7 13:43:52.124: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:43:54.131: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun  7 13:43:54.156: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  7 13:43:54.160: INFO: Pod pod-with-poststart-http-hook still exists
Jun  7 13:43:56.160: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  7 13:43:56.165: INFO: Pod pod-with-poststart-http-hook still exists
Jun  7 13:43:58.160: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  7 13:43:58.166: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:43:58.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6843" for this suite.

• [SLOW TEST:8.102 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1470,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:43:58.180: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jun  7 13:43:58.230: INFO: Waiting up to 5m0s for pod "downward-api-a0cc077b-b7b0-42dd-9fb3-caf49e3c37e5" in namespace "downward-api-9856" to be "Succeeded or Failed"
Jun  7 13:43:58.235: INFO: Pod "downward-api-a0cc077b-b7b0-42dd-9fb3-caf49e3c37e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.567563ms
Jun  7 13:44:00.241: INFO: Pod "downward-api-a0cc077b-b7b0-42dd-9fb3-caf49e3c37e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010538102s
STEP: Saw pod success
Jun  7 13:44:00.241: INFO: Pod "downward-api-a0cc077b-b7b0-42dd-9fb3-caf49e3c37e5" satisfied condition "Succeeded or Failed"
Jun  7 13:44:00.243: INFO: Trying to get logs from node proact-prod01-wk002 pod downward-api-a0cc077b-b7b0-42dd-9fb3-caf49e3c37e5 container dapi-container: <nil>
STEP: delete the pod
Jun  7 13:44:00.260: INFO: Waiting for pod downward-api-a0cc077b-b7b0-42dd-9fb3-caf49e3c37e5 to disappear
Jun  7 13:44:00.262: INFO: Pod downward-api-a0cc077b-b7b0-42dd-9fb3-caf49e3c37e5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:44:00.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9856" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":88,"skipped":1471,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:44:00.272: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-8fbf126c-45e7-4b74-95ed-35b8aad3bb1d
STEP: Creating the pod
Jun  7 13:44:00.316: INFO: The status of Pod pod-configmaps-5178ddd9-5beb-4b36-9247-994088059c38 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:44:02.323: INFO: The status of Pod pod-configmaps-5178ddd9-5beb-4b36-9247-994088059c38 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-8fbf126c-45e7-4b74-95ed-35b8aad3bb1d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:44:04.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9013" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":89,"skipped":1489,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:44:04.389: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:44:15.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2634" for this suite.

• [SLOW TEST:11.110 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":90,"skipped":1495,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:44:15.505: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:44:15.541: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun  7 13:44:17.581: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:44:17.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4389" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":91,"skipped":1545,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:44:17.604: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:44:17.640: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun  7 13:44:17.647: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  7 13:44:22.652: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  7 13:44:22.652: INFO: Creating deployment "test-rolling-update-deployment"
Jun  7 13:44:22.660: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun  7 13:44:22.665: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun  7 13:44:24.672: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun  7 13:44:24.676: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  7 13:44:24.685: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5727  7840b0f9-bcbb-4c27-9bd7-c6b5bd28fd36 33565806 1 2022-06-07 13:44:22 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-06-07 13:44:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 13:44:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00315ef28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-07 13:44:22 +0000 UTC,LastTransitionTime:2022-06-07 13:44:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2022-06-07 13:44:24 +0000 UTC,LastTransitionTime:2022-06-07 13:44:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  7 13:44:24.688: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-5727  8285f602-14fc-4077-a6bf-5dd62743d85a 33565796 1 2022-06-07 13:44:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7840b0f9-bcbb-4c27-9bd7-c6b5bd28fd36 0xc00315f607 0xc00315f608}] []  [{kube-controller-manager Update apps/v1 2022-06-07 13:44:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7840b0f9-bcbb-4c27-9bd7-c6b5bd28fd36\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 13:44:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00315f788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  7 13:44:24.688: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun  7 13:44:24.689: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5727  81be779d-6e83-463d-8aa7-ea9dbfba0c9f 33565805 2 2022-06-07 13:44:17 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7840b0f9-bcbb-4c27-9bd7-c6b5bd28fd36 0xc00315f4d7 0xc00315f4d8}] []  [{e2e.test Update apps/v1 2022-06-07 13:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 13:44:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7840b0f9-bcbb-4c27-9bd7-c6b5bd28fd36\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-06-07 13:44:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00315f598 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  7 13:44:24.692: INFO: Pod "test-rolling-update-deployment-585b757574-2rjvf" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-2rjvf test-rolling-update-deployment-585b757574- deployment-5727  19f6ce5a-92c9-4291-b61b-bdad8837d8bb 33565795 0 2022-06-07 13:44:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/containerID:71ca54762b93e08bdbf1f00d00719745e945a88046eb38608015296fe6f6b863 cni.projectcalico.org/podIP:192.168.39.211/32 cni.projectcalico.org/podIPs:192.168.39.211/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 8285f602-14fc-4077-a6bf-5dd62743d85a 0xc003f39d97 0xc003f39d98}] []  [{kube-controller-manager Update v1 2022-06-07 13:44:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8285f602-14fc-4077-a6bf-5dd62743d85a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-07 13:44:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-07 13:44:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.39.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfhkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfhkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:44:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:44:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:44:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:44:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.14,PodIP:192.168.39.211,StartTime:2022-06-07 13:44:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 13:44:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://34987b59322541e1e7949157776019ec10704cd12ea74210cf8e65e9aaea694c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.39.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:44:24.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5727" for this suite.

• [SLOW TEST:7.098 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":92,"skipped":1556,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:44:24.703: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun  7 13:44:27.264: INFO: Successfully updated pod "adopt-release--1-296wc"
STEP: Checking that the Job readopts the Pod
Jun  7 13:44:27.264: INFO: Waiting up to 15m0s for pod "adopt-release--1-296wc" in namespace "job-3295" to be "adopted"
Jun  7 13:44:27.272: INFO: Pod "adopt-release--1-296wc": Phase="Running", Reason="", readiness=true. Elapsed: 8.266923ms
Jun  7 13:44:29.281: INFO: Pod "adopt-release--1-296wc": Phase="Running", Reason="", readiness=true. Elapsed: 2.017411522s
Jun  7 13:44:29.281: INFO: Pod "adopt-release--1-296wc" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun  7 13:44:29.794: INFO: Successfully updated pod "adopt-release--1-296wc"
STEP: Checking that the Job releases the Pod
Jun  7 13:44:29.794: INFO: Waiting up to 15m0s for pod "adopt-release--1-296wc" in namespace "job-3295" to be "released"
Jun  7 13:44:29.800: INFO: Pod "adopt-release--1-296wc": Phase="Running", Reason="", readiness=true. Elapsed: 6.141667ms
Jun  7 13:44:29.800: INFO: Pod "adopt-release--1-296wc" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:44:29.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3295" for this suite.

• [SLOW TEST:5.109 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":93,"skipped":1571,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:44:29.812: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  7 13:44:31.880: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:44:31.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1680" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":94,"skipped":1576,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:44:31.928: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun  7 13:44:31.967: INFO: Waiting up to 5m0s for pod "pod-7d58408a-b3d2-48be-902a-a8f1ec35abc6" in namespace "emptydir-8708" to be "Succeeded or Failed"
Jun  7 13:44:31.971: INFO: Pod "pod-7d58408a-b3d2-48be-902a-a8f1ec35abc6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.466288ms
Jun  7 13:44:33.977: INFO: Pod "pod-7d58408a-b3d2-48be-902a-a8f1ec35abc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009724911s
STEP: Saw pod success
Jun  7 13:44:33.977: INFO: Pod "pod-7d58408a-b3d2-48be-902a-a8f1ec35abc6" satisfied condition "Succeeded or Failed"
Jun  7 13:44:33.981: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-7d58408a-b3d2-48be-902a-a8f1ec35abc6 container test-container: <nil>
STEP: delete the pod
Jun  7 13:44:34.009: INFO: Waiting for pod pod-7d58408a-b3d2-48be-902a-a8f1ec35abc6 to disappear
Jun  7 13:44:34.013: INFO: Pod pod-7d58408a-b3d2-48be-902a-a8f1ec35abc6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:44:34.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8708" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":95,"skipped":1583,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:44:34.031: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-0e5ce91b-b669-430c-a624-d419eac0d44a
STEP: Creating a pod to test consume secrets
Jun  7 13:44:34.077: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-16e427c6-4e87-4973-9f4f-22296b53ae05" in namespace "projected-412" to be "Succeeded or Failed"
Jun  7 13:44:34.080: INFO: Pod "pod-projected-secrets-16e427c6-4e87-4973-9f4f-22296b53ae05": Phase="Pending", Reason="", readiness=false. Elapsed: 3.67007ms
Jun  7 13:44:36.086: INFO: Pod "pod-projected-secrets-16e427c6-4e87-4973-9f4f-22296b53ae05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009672491s
STEP: Saw pod success
Jun  7 13:44:36.086: INFO: Pod "pod-projected-secrets-16e427c6-4e87-4973-9f4f-22296b53ae05" satisfied condition "Succeeded or Failed"
Jun  7 13:44:36.092: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-secrets-16e427c6-4e87-4973-9f4f-22296b53ae05 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  7 13:44:36.117: INFO: Waiting for pod pod-projected-secrets-16e427c6-4e87-4973-9f4f-22296b53ae05 to disappear
Jun  7 13:44:36.121: INFO: Pod pod-projected-secrets-16e427c6-4e87-4973-9f4f-22296b53ae05 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:44:36.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-412" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":96,"skipped":1596,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:44:36.138: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun  7 13:44:36.174: INFO: Waiting up to 5m0s for pod "pod-25664b45-e413-4835-ad66-535171dbd2f6" in namespace "emptydir-9320" to be "Succeeded or Failed"
Jun  7 13:44:36.176: INFO: Pod "pod-25664b45-e413-4835-ad66-535171dbd2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.924595ms
Jun  7 13:44:38.182: INFO: Pod "pod-25664b45-e413-4835-ad66-535171dbd2f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008518998s
STEP: Saw pod success
Jun  7 13:44:38.183: INFO: Pod "pod-25664b45-e413-4835-ad66-535171dbd2f6" satisfied condition "Succeeded or Failed"
Jun  7 13:44:38.187: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-25664b45-e413-4835-ad66-535171dbd2f6 container test-container: <nil>
STEP: delete the pod
Jun  7 13:44:38.212: INFO: Waiting for pod pod-25664b45-e413-4835-ad66-535171dbd2f6 to disappear
Jun  7 13:44:38.216: INFO: Pod pod-25664b45-e413-4835-ad66-535171dbd2f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:44:38.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9320" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":97,"skipped":1619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:44:38.234: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  7 13:44:40.293: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:44:40.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7883" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":98,"skipped":1688,"failed":0}
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:44:40.327: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jun  7 13:44:40.353: INFO: PodSpec: initContainers in spec.initContainers
Jun  7 13:45:24.325: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a8b83f7a-0969-4f51-be83-dcb85674f9a5", GenerateName:"", Namespace:"init-container-6938", SelfLink:"", UID:"db5f1cb5-721f-43fd-8f49-93ac5b8d38e6", ResourceVersion:"33566443", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63790206280, loc:(*time.Location)(0xa0aaf60)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"353518299"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"98ca0f66fec2e75449c5ea4f182128c367908a77a5886cb067c5f9c551f73d6b", "cni.projectcalico.org/podIP":"192.168.39.199/32", "cni.projectcalico.org/podIPs":"192.168.39.199/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0047b02b8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0047b02d0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0047b02e8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0047b0300), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0047b0318), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0047b0330), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-f7d5t", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003c243a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f7d5t", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f7d5t", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-f7d5t", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003bb4258), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"proact-prod01-wk002", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000250930), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003bb42d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003bb42f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003bb42f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003bb42fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003d20a50), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790206280, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790206280, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790206280, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790206280, loc:(*time.Location)(0xa0aaf60)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.55.210.14", PodIP:"192.168.39.199", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.39.199"}}, StartTime:(*v1.Time)(0xc0047b0360), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000250a80)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000250af0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:244bdbdf4b8d368b5836e9d2c7808a280a73ad72ae321d644e9f220da503218f", ContainerID:"cri-o://dc8f2be0c4b6d48bdc6cb4028ce39af854c0a7833b845205ed978920f9d8149c", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003c24420), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003c24400), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.5", ImageID:"", ContainerID:"", Started:(*bool)(0xc003bb437f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:45:24.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6938" for this suite.

• [SLOW TEST:44.014 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":99,"skipped":1695,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:45:24.342: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-tfjq
STEP: Creating a pod to test atomic-volume-subpath
Jun  7 13:45:24.436: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-tfjq" in namespace "subpath-5754" to be "Succeeded or Failed"
Jun  7 13:45:24.468: INFO: Pod "pod-subpath-test-projected-tfjq": Phase="Pending", Reason="", readiness=false. Elapsed: 32.149228ms
Jun  7 13:45:26.476: INFO: Pod "pod-subpath-test-projected-tfjq": Phase="Running", Reason="", readiness=true. Elapsed: 2.039623696s
Jun  7 13:45:28.485: INFO: Pod "pod-subpath-test-projected-tfjq": Phase="Running", Reason="", readiness=true. Elapsed: 4.04920584s
Jun  7 13:45:30.495: INFO: Pod "pod-subpath-test-projected-tfjq": Phase="Running", Reason="", readiness=true. Elapsed: 6.058931291s
Jun  7 13:45:32.501: INFO: Pod "pod-subpath-test-projected-tfjq": Phase="Running", Reason="", readiness=true. Elapsed: 8.065090779s
Jun  7 13:45:34.509: INFO: Pod "pod-subpath-test-projected-tfjq": Phase="Running", Reason="", readiness=true. Elapsed: 10.072411308s
Jun  7 13:45:36.517: INFO: Pod "pod-subpath-test-projected-tfjq": Phase="Running", Reason="", readiness=true. Elapsed: 12.081014888s
Jun  7 13:45:38.527: INFO: Pod "pod-subpath-test-projected-tfjq": Phase="Running", Reason="", readiness=true. Elapsed: 14.090447495s
Jun  7 13:45:40.536: INFO: Pod "pod-subpath-test-projected-tfjq": Phase="Running", Reason="", readiness=true. Elapsed: 16.100299262s
Jun  7 13:45:42.544: INFO: Pod "pod-subpath-test-projected-tfjq": Phase="Running", Reason="", readiness=true. Elapsed: 18.107450065s
Jun  7 13:45:44.553: INFO: Pod "pod-subpath-test-projected-tfjq": Phase="Running", Reason="", readiness=true. Elapsed: 20.117301457s
Jun  7 13:45:46.560: INFO: Pod "pod-subpath-test-projected-tfjq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.123918965s
STEP: Saw pod success
Jun  7 13:45:46.560: INFO: Pod "pod-subpath-test-projected-tfjq" satisfied condition "Succeeded or Failed"
Jun  7 13:45:46.565: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-subpath-test-projected-tfjq container test-container-subpath-projected-tfjq: <nil>
STEP: delete the pod
Jun  7 13:45:46.590: INFO: Waiting for pod pod-subpath-test-projected-tfjq to disappear
Jun  7 13:45:46.593: INFO: Pod pod-subpath-test-projected-tfjq no longer exists
STEP: Deleting pod pod-subpath-test-projected-tfjq
Jun  7 13:45:46.593: INFO: Deleting pod "pod-subpath-test-projected-tfjq" in namespace "subpath-5754"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:45:46.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5754" for this suite.

• [SLOW TEST:22.265 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":346,"completed":100,"skipped":1695,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:45:46.607: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Jun  7 13:45:46.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-7956 cluster-info'
Jun  7 13:45:46.733: INFO: stderr: ""
Jun  7 13:45:46.733: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:45:46.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7956" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":101,"skipped":1730,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:45:46.745: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-3816/configmap-test-9140d0da-b869-4e87-8fbc-d19576dad50f
STEP: Creating a pod to test consume configMaps
Jun  7 13:45:46.787: INFO: Waiting up to 5m0s for pod "pod-configmaps-27517331-a2e1-4d83-a9cc-93cb2646800f" in namespace "configmap-3816" to be "Succeeded or Failed"
Jun  7 13:45:46.792: INFO: Pod "pod-configmaps-27517331-a2e1-4d83-a9cc-93cb2646800f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.217132ms
Jun  7 13:45:48.797: INFO: Pod "pod-configmaps-27517331-a2e1-4d83-a9cc-93cb2646800f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01022057s
STEP: Saw pod success
Jun  7 13:45:48.797: INFO: Pod "pod-configmaps-27517331-a2e1-4d83-a9cc-93cb2646800f" satisfied condition "Succeeded or Failed"
Jun  7 13:45:48.801: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-configmaps-27517331-a2e1-4d83-a9cc-93cb2646800f container env-test: <nil>
STEP: delete the pod
Jun  7 13:45:48.827: INFO: Waiting for pod pod-configmaps-27517331-a2e1-4d83-a9cc-93cb2646800f to disappear
Jun  7 13:45:48.830: INFO: Pod pod-configmaps-27517331-a2e1-4d83-a9cc-93cb2646800f no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:45:48.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3816" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":102,"skipped":1750,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:45:48.846: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-17125046-d0bc-4bd8-90f9-08388e0b1d4a in namespace container-probe-9299
Jun  7 13:45:50.886: INFO: Started pod liveness-17125046-d0bc-4bd8-90f9-08388e0b1d4a in namespace container-probe-9299
STEP: checking the pod's current state and verifying that restartCount is present
Jun  7 13:45:50.890: INFO: Initial restart count of pod liveness-17125046-d0bc-4bd8-90f9-08388e0b1d4a is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:49:51.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9299" for this suite.

• [SLOW TEST:243.124 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":103,"skipped":1768,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:49:51.970: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 13:49:52.010: INFO: Waiting up to 5m0s for pod "downwardapi-volume-46134ab5-38d6-41c3-a7fe-47920c1ad206" in namespace "downward-api-8531" to be "Succeeded or Failed"
Jun  7 13:49:52.013: INFO: Pod "downwardapi-volume-46134ab5-38d6-41c3-a7fe-47920c1ad206": Phase="Pending", Reason="", readiness=false. Elapsed: 3.196863ms
Jun  7 13:49:54.019: INFO: Pod "downwardapi-volume-46134ab5-38d6-41c3-a7fe-47920c1ad206": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009232122s
STEP: Saw pod success
Jun  7 13:49:54.019: INFO: Pod "downwardapi-volume-46134ab5-38d6-41c3-a7fe-47920c1ad206" satisfied condition "Succeeded or Failed"
Jun  7 13:49:54.024: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-46134ab5-38d6-41c3-a7fe-47920c1ad206 container client-container: <nil>
STEP: delete the pod
Jun  7 13:49:54.051: INFO: Waiting for pod downwardapi-volume-46134ab5-38d6-41c3-a7fe-47920c1ad206 to disappear
Jun  7 13:49:54.055: INFO: Pod downwardapi-volume-46134ab5-38d6-41c3-a7fe-47920c1ad206 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:49:54.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8531" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":104,"skipped":1785,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:49:54.072: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jun  7 13:50:34.183: INFO: The status of Pod kube-controller-manager-proact-prod01-cp003 is Running (Ready = true)
Jun  7 13:50:34.247: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun  7 13:50:34.247: INFO: Deleting pod "simpletest.rc-47r8b" in namespace "gc-6947"
Jun  7 13:50:34.262: INFO: Deleting pod "simpletest.rc-62dfn" in namespace "gc-6947"
Jun  7 13:50:34.274: INFO: Deleting pod "simpletest.rc-7b5gd" in namespace "gc-6947"
Jun  7 13:50:34.288: INFO: Deleting pod "simpletest.rc-d268x" in namespace "gc-6947"
Jun  7 13:50:34.297: INFO: Deleting pod "simpletest.rc-jrdcq" in namespace "gc-6947"
Jun  7 13:50:34.305: INFO: Deleting pod "simpletest.rc-n7xb9" in namespace "gc-6947"
Jun  7 13:50:34.312: INFO: Deleting pod "simpletest.rc-nfz2j" in namespace "gc-6947"
Jun  7 13:50:34.321: INFO: Deleting pod "simpletest.rc-tk7bq" in namespace "gc-6947"
Jun  7 13:50:34.333: INFO: Deleting pod "simpletest.rc-xm877" in namespace "gc-6947"
Jun  7 13:50:34.340: INFO: Deleting pod "simpletest.rc-zcjw9" in namespace "gc-6947"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:50:34.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6947" for this suite.

• [SLOW TEST:40.286 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":105,"skipped":1876,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:50:34.359: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:50:34.389: INFO: Creating deployment "test-recreate-deployment"
Jun  7 13:50:34.396: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun  7 13:50:34.405: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun  7 13:50:36.418: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun  7 13:50:36.422: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun  7 13:50:36.437: INFO: Updating deployment test-recreate-deployment
Jun  7 13:50:36.437: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  7 13:50:36.518: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4990  d10663a5-3805-4d06-af4f-01dd49244bd6 33568456 2 2022-06-07 13:50:34 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-06-07 13:50:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 13:50:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032ac7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-06-07 13:50:36 +0000 UTC,LastTransitionTime:2022-06-07 13:50:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2022-06-07 13:50:36 +0000 UTC,LastTransitionTime:2022-06-07 13:50:34 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun  7 13:50:36.520: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-4990  540e571c-83a0-417b-8d98-025e212f3ac6 33568454 1 2022-06-07 13:50:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d10663a5-3805-4d06-af4f-01dd49244bd6 0xc0029848c0 0xc0029848c1}] []  [{kube-controller-manager Update apps/v1 2022-06-07 13:50:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d10663a5-3805-4d06-af4f-01dd49244bd6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 13:50:36 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002984958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  7 13:50:36.520: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun  7 13:50:36.520: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-4990  43aeb970-2951-4b28-8c52-27d2e8777651 33568444 2 2022-06-07 13:50:34 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d10663a5-3805-4d06-af4f-01dd49244bd6 0xc0029847a7 0xc0029847a8}] []  [{kube-controller-manager Update apps/v1 2022-06-07 13:50:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d10663a5-3805-4d06-af4f-01dd49244bd6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 13:50:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002984858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  7 13:50:36.522: INFO: Pod "test-recreate-deployment-85d47dcb4-9x5mk" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-9x5mk test-recreate-deployment-85d47dcb4- deployment-4990  cadbf247-8e63-44a9-8cda-8eee2279aa0a 33568455 0 2022-06-07 13:50:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 540e571c-83a0-417b-8d98-025e212f3ac6 0xc002984dd0 0xc002984dd1}] []  [{kube-controller-manager Update v1 2022-06-07 13:50:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"540e571c-83a0-417b-8d98-025e212f3ac6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 13:50:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qds68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qds68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:50:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:50:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:50:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:50:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.14,PodIP:,StartTime:2022-06-07 13:50:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:50:36.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4990" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":106,"skipped":1884,"failed":0}

------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:50:36.530: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-3284
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jun  7 13:50:36.582: INFO: Found 0 stateful pods, waiting for 3
Jun  7 13:50:46.595: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  7 13:50:46.595: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  7 13:50:46.595: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun  7 13:50:46.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-3284 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  7 13:50:46.823: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  7 13:50:46.823: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  7 13:50:46.823: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Jun  7 13:50:56.880: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun  7 13:51:06.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-3284 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  7 13:51:07.115: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  7 13:51:07.116: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  7 13:51:07.116: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Jun  7 13:51:27.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-3284 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  7 13:51:27.351: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  7 13:51:27.351: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  7 13:51:27.351: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  7 13:51:37.392: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun  7 13:51:47.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-3284 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  7 13:51:47.649: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  7 13:51:47.649: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  7 13:51:47.649: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun  7 13:51:57.680: INFO: Deleting all statefulset in ns statefulset-3284
Jun  7 13:51:57.685: INFO: Scaling statefulset ss2 to 0
Jun  7 13:52:07.704: INFO: Waiting for statefulset status.replicas updated to 0
Jun  7 13:52:07.707: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:52:07.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3284" for this suite.

• [SLOW TEST:91.206 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":107,"skipped":1884,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:52:07.743: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:52:07.788: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-fcfb1a0b-868e-4348-b9d0-21d6ae73784b" in namespace "security-context-test-181" to be "Succeeded or Failed"
Jun  7 13:52:07.793: INFO: Pod "busybox-readonly-false-fcfb1a0b-868e-4348-b9d0-21d6ae73784b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.251236ms
Jun  7 13:52:09.802: INFO: Pod "busybox-readonly-false-fcfb1a0b-868e-4348-b9d0-21d6ae73784b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013690383s
Jun  7 13:52:09.802: INFO: Pod "busybox-readonly-false-fcfb1a0b-868e-4348-b9d0-21d6ae73784b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:52:09.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-181" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":108,"skipped":1889,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:52:09.831: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun  7 13:52:19.913: INFO: The status of Pod kube-controller-manager-proact-prod01-cp003 is Running (Ready = true)
Jun  7 13:52:19.967: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:52:19.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-704" for this suite.

• [SLOW TEST:10.152 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":109,"skipped":1895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:52:19.984: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:52:20.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5543" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":110,"skipped":1917,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:52:20.114: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6781
Jun  7 13:52:20.148: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:52:22.160: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jun  7 13:52:22.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6781 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun  7 13:52:22.392: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jun  7 13:52:22.392: INFO: stdout: "iptables"
Jun  7 13:52:22.392: INFO: proxyMode: iptables
Jun  7 13:52:22.410: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun  7 13:52:22.416: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-6781
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6781
I0607 13:52:22.452286      23 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6781, replica count: 3
I0607 13:52:25.504380      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  7 13:52:25.514: INFO: Creating new exec pod
Jun  7 13:52:28.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6781 exec execpod-affinityntcp8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jun  7 13:52:28.740: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jun  7 13:52:28.740: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 13:52:28.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6781 exec execpod-affinityntcp8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.118.38 80'
Jun  7 13:52:28.917: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.118.38 80\nConnection to 10.105.118.38 80 port [tcp/http] succeeded!\n"
Jun  7 13:52:28.917: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 13:52:28.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6781 exec execpod-affinityntcp8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.55.210.14 31484'
Jun  7 13:52:29.114: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.55.210.14 31484\nConnection to 10.55.210.14 31484 port [tcp/*] succeeded!\n"
Jun  7 13:52:29.114: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 13:52:29.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6781 exec execpod-affinityntcp8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.55.210.16 31484'
Jun  7 13:52:29.299: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.55.210.16 31484\nConnection to 10.55.210.16 31484 port [tcp/*] succeeded!\n"
Jun  7 13:52:29.299: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 13:52:29.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6781 exec execpod-affinityntcp8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.55.210.13:31484/ ; done'
Jun  7 13:52:29.568: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n"
Jun  7 13:52:29.568: INFO: stdout: "\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt\naffinity-nodeport-timeout-nv7nt"
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Received response from host: affinity-nodeport-timeout-nv7nt
Jun  7 13:52:29.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6781 exec execpod-affinityntcp8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.55.210.13:31484/'
Jun  7 13:52:29.762: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n"
Jun  7 13:52:29.762: INFO: stdout: "affinity-nodeport-timeout-nv7nt"
Jun  7 13:52:49.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6781 exec execpod-affinityntcp8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.55.210.13:31484/'
Jun  7 13:52:49.964: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n"
Jun  7 13:52:49.964: INFO: stdout: "affinity-nodeport-timeout-nv7nt"
Jun  7 13:53:09.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6781 exec execpod-affinityntcp8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.55.210.13:31484/'
Jun  7 13:53:10.165: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n"
Jun  7 13:53:10.165: INFO: stdout: "affinity-nodeport-timeout-nv7nt"
Jun  7 13:53:30.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6781 exec execpod-affinityntcp8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.55.210.13:31484/'
Jun  7 13:53:30.395: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.55.210.13:31484/\n"
Jun  7 13:53:30.395: INFO: stdout: "affinity-nodeport-timeout-qt6sd"
Jun  7 13:53:30.395: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6781, will wait for the garbage collector to delete the pods
Jun  7 13:53:30.476: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.837908ms
Jun  7 13:53:30.577: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.572448ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:53:32.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6781" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:72.808 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":111,"skipped":1919,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:53:32.922: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-eaa7b40b-7aa8-42e2-816d-66fd382e658c
STEP: Creating configMap with name cm-test-opt-upd-c72e59a0-e4a7-4df8-9ff9-1a28e8f99951
STEP: Creating the pod
Jun  7 13:53:32.974: INFO: The status of Pod pod-projected-configmaps-3b170889-ed2e-439a-a634-102e2fd90ad1 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:53:34.984: INFO: The status of Pod pod-projected-configmaps-3b170889-ed2e-439a-a634-102e2fd90ad1 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-eaa7b40b-7aa8-42e2-816d-66fd382e658c
STEP: Updating configmap cm-test-opt-upd-c72e59a0-e4a7-4df8-9ff9-1a28e8f99951
STEP: Creating configMap with name cm-test-opt-create-d8ed19da-388c-46c6-b237-110cdab7cdaf
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:53:37.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7658" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":112,"skipped":1922,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:53:37.088: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jun  7 13:53:37.138: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:53:39.147: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jun  7 13:53:39.165: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jun  7 13:53:41.177: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun  7 13:53:41.182: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9934 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:53:41.182: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:53:41.298: INFO: Exec stderr: ""
Jun  7 13:53:41.298: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9934 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:53:41.298: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:53:41.410: INFO: Exec stderr: ""
Jun  7 13:53:41.410: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9934 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:53:41.410: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:53:41.537: INFO: Exec stderr: ""
Jun  7 13:53:41.537: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9934 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:53:41.537: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:53:41.653: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun  7 13:53:41.653: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9934 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:53:41.653: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:53:41.772: INFO: Exec stderr: ""
Jun  7 13:53:41.772: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9934 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:53:41.772: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:53:41.903: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun  7 13:53:41.904: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9934 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:53:41.904: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:53:42.033: INFO: Exec stderr: ""
Jun  7 13:53:42.033: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9934 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:53:42.033: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:53:42.144: INFO: Exec stderr: ""
Jun  7 13:53:42.144: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9934 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:53:42.144: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:53:42.246: INFO: Exec stderr: ""
Jun  7 13:53:42.246: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9934 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:53:42.246: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:53:42.337: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:53:42.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9934" for this suite.

• [SLOW TEST:5.260 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":1944,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:53:42.348: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:53:42.386: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:53:48.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4979" for this suite.

• [SLOW TEST:6.469 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":114,"skipped":1947,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:53:48.818: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 13:53:49.110: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 13:53:52.153: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 13:53:52.162: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:53:55.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6039" for this suite.
STEP: Destroying namespace "webhook-6039-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.577 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":115,"skipped":1959,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:53:55.400: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Jun  7 13:53:55.520: INFO: Pod name sample-pod: Found 0 pods out of 3
Jun  7 13:54:00.526: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jun  7 13:54:00.528: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:54:00.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7248" for this suite.

• [SLOW TEST:5.181 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":116,"skipped":1963,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:54:00.581: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jun  7 13:54:00.622: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jun  7 13:54:00.638: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:54:00.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7426" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":117,"skipped":1979,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:54:00.673: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun  7 13:54:00.700: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  7 13:54:00.710: INFO: Waiting for terminating namespaces to be deleted...
Jun  7 13:54:00.712: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk001 before test
Jun  7 13:54:00.730: INFO: argocd-application-controller-0 from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container application-controller ready: true, restart count 0
Jun  7 13:54:00.730: INFO: argocd-repo-server-5547b66bd9-dfnbc from argocd started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container repo-server ready: true, restart count 0
Jun  7 13:54:00.730: INFO: calico-node-mpbx2 from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container calico-node ready: true, restart count 0
Jun  7 13:54:00.730: INFO: mypostgres-7-0 from default started at 2022-05-24 07:06:43 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:54:00.730: INFO: 	Container mypostgres-7 ready: true, restart count 0
Jun  7 13:54:00.730: INFO: reviews-v1-545db77b95-z2hm8 from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:54:00.730: INFO: 	Container reviews ready: true, restart count 0
Jun  7 13:54:00.730: INFO: foobar-75685968d4-575v5 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.730: INFO: foobar-75685968d4-d24zc from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.730: INFO: foobar-75685968d4-dwnnl from foobar started at 2022-06-07 13:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.730: INFO: foobar-75685968d4-mlbsq from foobar started at 2022-06-07 13:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.730: INFO: foobar-75685968d4-s72dw from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.730: INFO: foobar-75685968d4-scvj7 from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.730: INFO: foobar-75685968d4-wtsqh from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.730: INFO: mypod from foobar2 started at 2022-04-28 07:54:48 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container myfrontend ready: true, restart count 0
Jun  7 13:54:00.730: INFO: nginx-deployment-66b6c48dd5-dbfcx from foobar2 started at 2022-04-28 08:35:34 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.730: INFO: nginx-deployment-66b6c48dd5-pfdqw from foobar2 started at 2022-04-28 07:46:04 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.730: INFO: nginx-deployment2-d5845d7c8-vfthc from foobar2 started at 2022-04-28 07:49:41 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container nginx2 ready: true, restart count 0
Jun  7 13:54:00.730: INFO: gitea-0 from gitea started at 2022-03-30 07:25:54 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container gitea ready: true, restart count 0
Jun  7 13:54:00.730: INFO: gitea-postgresql-0 from gitea started at 2022-03-30 07:25:56 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container gitea-postgresql ready: true, restart count 1
Jun  7 13:54:00.730: INFO: ingress-nginx-controller-848878cd85-9wqg9 from ingress-nginx started at 2022-06-03 07:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container controller ready: true, restart count 0
Jun  7 13:54:00.730: INFO: k8s-status-c44b9cb68-pcnd4 from k8s-status started at 2022-04-14 07:03:09 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container k8s-status ready: true, restart count 0
Jun  7 13:54:00.730: INFO: kube-proxy-4c7bj from kube-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  7 13:54:00.730: INFO: loki-fluent-bit-loki-gqzll from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 13:54:00.730: INFO: prometheus-alertmanager-6c845bbb9c-r6krx from monitoring started at 2022-05-23 17:37:20 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jun  7 13:54:00.730: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jun  7 13:54:00.730: INFO: prometheus-kube-state-metrics-6c44ff7fb6-9zx9s from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  7 13:54:00.730: INFO: prometheus-node-exporter-z6jpz from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 13:54:00.730: INFO: prometheus-pushgateway-86679dcf68-qk8x2 from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Jun  7 13:54:00.730: INFO: prometheus-server-869789c557-72jr6 from monitoring started at 2022-06-07 13:40:18 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container prometheus-server ready: true, restart count 0
Jun  7 13:54:00.730: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun  7 13:54:00.730: INFO: sonobuoy from sonobuoy started at 2022-06-07 13:32:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  7 13:54:00.730: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-525dj from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 13:54:00.730: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 13:54:00.730: INFO: trident-csi-fcff9fbb6-rs2fc from trident started at 2022-06-07 13:40:18 +0000 UTC (6 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  7 13:54:00.730: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  7 13:54:00.730: INFO: 	Container csi-resizer ready: true, restart count 0
Jun  7 13:54:00.730: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun  7 13:54:00.730: INFO: 	Container trident-autosupport ready: true, restart count 0
Jun  7 13:54:00.730: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 13:54:00.730: INFO: trident-csi-kqw7h from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 13:54:00.730: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 13:54:00.730: INFO: trident-operator-56d66bb95b-255bx from trident started at 2022-03-18 12:31:45 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.730: INFO: 	Container trident-operator ready: true, restart count 0
Jun  7 13:54:00.730: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk002 before test
Jun  7 13:54:00.744: INFO: calico-node-sv82j from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.744: INFO: 	Container calico-node ready: true, restart count 1
Jun  7 13:54:00.744: INFO: mypostgres-10-0 from default started at 2022-06-07 13:40:56 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.744: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:54:00.744: INFO: 	Container mypostgres-10 ready: true, restart count 0
Jun  7 13:54:00.744: INFO: kube-proxy-8sbg5 from kube-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.744: INFO: 	Container kube-proxy ready: true, restart count 1
Jun  7 13:54:00.744: INFO: loki-fluent-bit-loki-brtjw from loki started at 2022-06-07 13:40:44 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.744: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 13:54:00.744: INFO: prometheus-node-exporter-ngjbp from monitoring started at 2022-06-07 13:40:44 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.744: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 13:54:00.744: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-7w99f from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.744: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 13:54:00.744: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 13:54:00.744: INFO: trident-csi-b4p6l from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.744: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 13:54:00.744: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 13:54:00.744: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk003 before test
Jun  7 13:54:00.761: INFO: argocd-applicationset-controller-84bc7544cd-7zlhg from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container applicationset-controller ready: true, restart count 0
Jun  7 13:54:00.761: INFO: argocd-dex-server-6b5ffb8f5c-r5sf2 from argocd started at 2022-03-31 06:26:27 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container dex-server ready: true, restart count 0
Jun  7 13:54:00.761: INFO: calico-node-rmwmb from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container calico-node ready: true, restart count 0
Jun  7 13:54:00.761: INFO: calico-typha-54559758b4-6gmlw from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container calico-typha ready: true, restart count 0
Jun  7 13:54:00.761: INFO: cert-manager-6bbf595697-lr27t from cert-manager started at 2022-06-03 08:46:47 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container cert-manager ready: true, restart count 0
Jun  7 13:54:00.761: INFO: cert-manager-cainjector-6bc9d758b-vh4wl from cert-manager started at 2022-06-03 08:46:47 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container cert-manager ready: true, restart count 0
Jun  7 13:54:00.761: INFO: productpage-v1-6b746f74dc-xrnvw from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:54:00.761: INFO: 	Container productpage ready: true, restart count 0
Jun  7 13:54:00.761: INFO: ratings-v1-b6994bb9-vndct from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:54:00.761: INFO: 	Container ratings ready: true, restart count 0
Jun  7 13:54:00.761: INFO: reviews-v3-84779c7bbc-p4hjg from default started at 2022-04-22 12:33:02 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:54:00.761: INFO: 	Container reviews ready: true, restart count 0
Jun  7 13:54:00.761: INFO: dex-7b7c86db7-hcrrd from dex started at 2022-06-03 07:21:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container dex ready: true, restart count 0
Jun  7 13:54:00.761: INFO: dex-authenticator-dex-k8s-authenticator-5795dc9755-fgflk from dex started at 2022-03-22 15:07:54 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container dex-k8s-authenticator ready: true, restart count 0
Jun  7 13:54:00.761: INFO: foobar-75685968d4-7ktm8 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.761: INFO: foobar-75685968d4-cn6ww from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.761: INFO: foobar-75685968d4-fldfg from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.761: INFO: foobar-75685968d4-mjvcq from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.761: INFO: foobar-75685968d4-qpw2b from foobar started at 2022-06-07 13:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.761: INFO: foobar-75685968d4-wk7h8 from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.761: INFO: nginx-deployment-66b6c48dd5-kwzdn from foobar2 started at 2022-04-28 08:35:34 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.761: INFO: nginx-deployment2-d5845d7c8-bbm4g from foobar2 started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container nginx2 ready: true, restart count 0
Jun  7 13:54:00.761: INFO: simple from foobar2 started at 2022-04-27 08:47:49 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container simple ready: true, restart count 0
Jun  7 13:54:00.761: INFO: istio-egressgateway-66fdd867f4-drz9z from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:54:00.761: INFO: prometheus-699b7cc575-k5zl7 from istio-system started at 2022-06-07 13:40:18 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.761: INFO: 	Container prometheus-server ready: true, restart count 0
Jun  7 13:54:00.761: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun  7 13:54:00.761: INFO: k8s-status-c44b9cb68-dx6r8 from k8s-status started at 2022-04-14 07:03:09 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.762: INFO: 	Container k8s-status ready: true, restart count 0
Jun  7 13:54:00.762: INFO: kube-proxy-wx7st from kube-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.762: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  7 13:54:00.762: INFO: kubegres-controller-manager-755b4c48f6-zxmlm from kubegres-system started at 2022-06-07 13:40:18 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.762: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  7 13:54:00.762: INFO: 	Container manager ready: true, restart count 0
Jun  7 13:54:00.762: INFO: loki-0 from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.762: INFO: 	Container loki ready: true, restart count 0
Jun  7 13:54:00.762: INFO: loki-fluent-bit-loki-ss7rt from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.762: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 13:54:00.762: INFO: prometheus-node-exporter-rz8ws from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.762: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 13:54:00.762: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-jxr44 from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 13:54:00.762: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 13:54:00.762: INFO: trident-csi-7wfz2 from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.762: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 13:54:00.762: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 13:54:00.762: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk004 before test
Jun  7 13:54:00.780: INFO: argocd-notifications-controller-6dd95488b4-l4t7z from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container notifications-controller ready: true, restart count 0
Jun  7 13:54:00.780: INFO: argocd-redis-57bcc665bf-lpcsm from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container argocd-redis ready: true, restart count 0
Jun  7 13:54:00.780: INFO: argocd-server-7b98b7446b-n7nmr from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container server ready: true, restart count 0
Jun  7 13:54:00.780: INFO: calico-node-gmd98 from calico-system started at 2022-03-10 15:32:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container calico-node ready: true, restart count 0
Jun  7 13:54:00.780: INFO: cert-manager-webhook-586d45d5ff-pftz7 from cert-manager started at 2022-06-03 08:46:47 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container cert-manager ready: true, restart count 0
Jun  7 13:54:00.780: INFO: details-v1-79f774bdb9-pxpbj from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container details ready: true, restart count 0
Jun  7 13:54:00.780: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:54:00.780: INFO: mypostgres-6-0 from default started at 2022-05-16 17:57:42 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:54:00.780: INFO: 	Container mypostgres-6 ready: true, restart count 0
Jun  7 13:54:00.780: INFO: reviews-v2-7bf8c9648f-q86cj from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:54:00.780: INFO: 	Container reviews ready: true, restart count 0
Jun  7 13:54:00.780: INFO: oauth2-oauth2-proxy-85748949c4-rlj6t from dex started at 2022-06-03 07:21:23 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container oauth2-proxy ready: true, restart count 0
Jun  7 13:54:00.780: INFO: foobar-75685968d4-2wjms from foobar started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.780: INFO: foobar-75685968d4-8k5dq from foobar started at 2022-03-25 12:14:06 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.780: INFO: foobar-75685968d4-98jz8 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.780: INFO: foobar-75685968d4-qj6bn from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.780: INFO: foobar-75685968d4-skd4f from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.780: INFO: foobar-75685968d4-vtjhk from foobar started at 2022-06-07 13:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.780: INFO: foobar-75685968d4-zgz7t from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.780: INFO: nginx-deployment-66b6c48dd5-9zp8s from foobar2 started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.780: INFO: nginx-deployment-66b6c48dd5-psjh5 from foobar2 started at 2022-04-28 07:46:07 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.780: INFO: nginx-deployment2-d5845d7c8-xhm74 from foobar2 started at 2022-04-28 07:49:45 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container nginx2 ready: true, restart count 0
Jun  7 13:54:00.780: INFO: gitea-memcached-7b44bc5f74-6vssj from gitea started at 2022-03-30 07:25:52 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container memcached ready: true, restart count 0
Jun  7 13:54:00.780: INFO: ingress-nginx-controller-848878cd85-n5nsq from ingress-nginx started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container controller ready: true, restart count 0
Jun  7 13:54:00.780: INFO: grafana-6c5dc6df7c-t4q9p from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container grafana ready: true, restart count 0
Jun  7 13:54:00.780: INFO: istio-ingressgateway-77968dbd74-qrklf from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 13:54:00.780: INFO: istiod-699b647f8b-lgg5l from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container discovery ready: true, restart count 0
Jun  7 13:54:00.780: INFO: jaeger-9dd685668-5frgx from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container jaeger ready: true, restart count 0
Jun  7 13:54:00.780: INFO: kiali-699f98c497-rvfkc from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container kiali ready: true, restart count 0
Jun  7 13:54:00.780: INFO: foo-bf9cd6fb5-7nmh7 from jltest started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container nginx ready: true, restart count 0
Jun  7 13:54:00.780: INFO: k8s-status-c44b9cb68-f4rv9 from k8s-status started at 2022-04-14 07:03:09 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container k8s-status ready: true, restart count 0
Jun  7 13:54:00.780: INFO: kube-proxy-t5klm from kube-system started at 2022-03-10 15:32:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  7 13:54:00.780: INFO: grafana-5ff8c85cf8-gv9c2 from loki started at 2022-04-25 05:59:11 +0000 UTC (3 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container grafana ready: true, restart count 0
Jun  7 13:54:00.780: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun  7 13:54:00.780: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Jun  7 13:54:00.780: INFO: loki-fluent-bit-loki-mrrh2 from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 13:54:00.780: INFO: testpod from loki started at 2022-03-28 08:55:49 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container testpod ready: true, restart count 71
Jun  7 13:54:00.780: INFO: prometheus-node-exporter-5ph5n from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 13:54:00.780: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-pxgrv from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 13:54:00.780: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 13:54:00.780: INFO: trident-csi-cscfg from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 13:54:00.780: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 13:54:00.780: INFO: 	Container trident-main ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6d8a1115-0e6f-4dbc-a4ca-d689ccfd2931 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.55.210.14 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-6d8a1115-0e6f-4dbc-a4ca-d689ccfd2931 off the node proact-prod01-wk002
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6d8a1115-0e6f-4dbc-a4ca-d689ccfd2931
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:59:04.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6193" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.250 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":118,"skipped":1996,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:59:04.925: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Jun  7 13:59:04.965: INFO: Waiting up to 5m0s for pod "var-expansion-c8d8a389-9fd7-4d76-81ce-0fe16d335f7c" in namespace "var-expansion-6959" to be "Succeeded or Failed"
Jun  7 13:59:04.970: INFO: Pod "var-expansion-c8d8a389-9fd7-4d76-81ce-0fe16d335f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.557662ms
Jun  7 13:59:06.977: INFO: Pod "var-expansion-c8d8a389-9fd7-4d76-81ce-0fe16d335f7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011616929s
STEP: Saw pod success
Jun  7 13:59:06.977: INFO: Pod "var-expansion-c8d8a389-9fd7-4d76-81ce-0fe16d335f7c" satisfied condition "Succeeded or Failed"
Jun  7 13:59:06.981: INFO: Trying to get logs from node proact-prod01-wk002 pod var-expansion-c8d8a389-9fd7-4d76-81ce-0fe16d335f7c container dapi-container: <nil>
STEP: delete the pod
Jun  7 13:59:07.014: INFO: Waiting for pod var-expansion-c8d8a389-9fd7-4d76-81ce-0fe16d335f7c to disappear
Jun  7 13:59:07.018: INFO: Pod var-expansion-c8d8a389-9fd7-4d76-81ce-0fe16d335f7c no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:59:07.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6959" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":119,"skipped":2058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:59:07.032: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jun  7 13:59:07.097: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:59:09.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9125" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":120,"skipped":2087,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:59:09.127: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:59:22.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6593" for this suite.

• [SLOW TEST:13.144 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":121,"skipped":2170,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:59:22.272: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-012eecf8-10d0-4149-aa30-dff6a183cd5e
STEP: Creating a pod to test consume secrets
Jun  7 13:59:22.316: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-74e643a4-79f8-4e52-b7f8-d8b0059b580a" in namespace "projected-5201" to be "Succeeded or Failed"
Jun  7 13:59:22.319: INFO: Pod "pod-projected-secrets-74e643a4-79f8-4e52-b7f8-d8b0059b580a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.146544ms
Jun  7 13:59:24.326: INFO: Pod "pod-projected-secrets-74e643a4-79f8-4e52-b7f8-d8b0059b580a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010115524s
STEP: Saw pod success
Jun  7 13:59:24.326: INFO: Pod "pod-projected-secrets-74e643a4-79f8-4e52-b7f8-d8b0059b580a" satisfied condition "Succeeded or Failed"
Jun  7 13:59:24.331: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-secrets-74e643a4-79f8-4e52-b7f8-d8b0059b580a container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  7 13:59:24.352: INFO: Waiting for pod pod-projected-secrets-74e643a4-79f8-4e52-b7f8-d8b0059b580a to disappear
Jun  7 13:59:24.355: INFO: Pod pod-projected-secrets-74e643a4-79f8-4e52-b7f8-d8b0059b580a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:59:24.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5201" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":122,"skipped":2191,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:59:24.365: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
Jun  7 13:59:24.396: INFO: Creating simple deployment test-deployment-5mk86
Jun  7 13:59:24.408: INFO: deployment "test-deployment-5mk86" doesn't have the required revision set
STEP: Getting /status
Jun  7 13:59:26.435: INFO: Deployment test-deployment-5mk86 has Conditions: [{Available True 2022-06-07 13:59:26 +0000 UTC 2022-06-07 13:59:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-06-07 13:59:26 +0000 UTC 2022-06-07 13:59:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5mk86-794dd694d8" has successfully progressed.}]
STEP: updating Deployment Status
Jun  7 13:59:26.447: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790207166, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790207166, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790207166, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790207164, loc:(*time.Location)(0xa0aaf60)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-5mk86-794dd694d8\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jun  7 13:59:26.449: INFO: Observed &Deployment event: ADDED
Jun  7 13:59:26.449: INFO: Observed Deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-07 13:59:24 +0000 UTC 2022-06-07 13:59:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5mk86-794dd694d8"}
Jun  7 13:59:26.449: INFO: Observed &Deployment event: MODIFIED
Jun  7 13:59:26.449: INFO: Observed Deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-07 13:59:24 +0000 UTC 2022-06-07 13:59:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5mk86-794dd694d8"}
Jun  7 13:59:26.449: INFO: Observed Deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-07 13:59:24 +0000 UTC 2022-06-07 13:59:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  7 13:59:26.449: INFO: Observed &Deployment event: MODIFIED
Jun  7 13:59:26.449: INFO: Observed Deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-07 13:59:24 +0000 UTC 2022-06-07 13:59:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  7 13:59:26.449: INFO: Observed Deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-07 13:59:24 +0000 UTC 2022-06-07 13:59:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5mk86-794dd694d8" is progressing.}
Jun  7 13:59:26.449: INFO: Observed &Deployment event: MODIFIED
Jun  7 13:59:26.449: INFO: Observed Deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-07 13:59:26 +0000 UTC 2022-06-07 13:59:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  7 13:59:26.449: INFO: Observed Deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-07 13:59:26 +0000 UTC 2022-06-07 13:59:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5mk86-794dd694d8" has successfully progressed.}
Jun  7 13:59:26.450: INFO: Observed &Deployment event: MODIFIED
Jun  7 13:59:26.450: INFO: Observed Deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-07 13:59:26 +0000 UTC 2022-06-07 13:59:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  7 13:59:26.450: INFO: Observed Deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-07 13:59:26 +0000 UTC 2022-06-07 13:59:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5mk86-794dd694d8" has successfully progressed.}
Jun  7 13:59:26.450: INFO: Found Deployment test-deployment-5mk86 in namespace deployment-9716 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  7 13:59:26.450: INFO: Deployment test-deployment-5mk86 has an updated status
STEP: patching the Statefulset Status
Jun  7 13:59:26.450: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun  7 13:59:26.456: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jun  7 13:59:26.457: INFO: Observed &Deployment event: ADDED
Jun  7 13:59:26.457: INFO: Observed deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-07 13:59:24 +0000 UTC 2022-06-07 13:59:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5mk86-794dd694d8"}
Jun  7 13:59:26.457: INFO: Observed &Deployment event: MODIFIED
Jun  7 13:59:26.458: INFO: Observed deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-07 13:59:24 +0000 UTC 2022-06-07 13:59:24 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5mk86-794dd694d8"}
Jun  7 13:59:26.458: INFO: Observed deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-07 13:59:24 +0000 UTC 2022-06-07 13:59:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  7 13:59:26.458: INFO: Observed &Deployment event: MODIFIED
Jun  7 13:59:26.458: INFO: Observed deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-07 13:59:24 +0000 UTC 2022-06-07 13:59:24 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  7 13:59:26.458: INFO: Observed deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-07 13:59:24 +0000 UTC 2022-06-07 13:59:24 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5mk86-794dd694d8" is progressing.}
Jun  7 13:59:26.458: INFO: Observed &Deployment event: MODIFIED
Jun  7 13:59:26.458: INFO: Observed deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-07 13:59:26 +0000 UTC 2022-06-07 13:59:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  7 13:59:26.458: INFO: Observed deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-07 13:59:26 +0000 UTC 2022-06-07 13:59:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5mk86-794dd694d8" has successfully progressed.}
Jun  7 13:59:26.458: INFO: Observed &Deployment event: MODIFIED
Jun  7 13:59:26.458: INFO: Observed deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-07 13:59:26 +0000 UTC 2022-06-07 13:59:26 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  7 13:59:26.458: INFO: Observed deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-07 13:59:26 +0000 UTC 2022-06-07 13:59:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5mk86-794dd694d8" has successfully progressed.}
Jun  7 13:59:26.458: INFO: Observed deployment test-deployment-5mk86 in namespace deployment-9716 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  7 13:59:26.458: INFO: Observed &Deployment event: MODIFIED
Jun  7 13:59:26.458: INFO: Found deployment test-deployment-5mk86 in namespace deployment-9716 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jun  7 13:59:26.458: INFO: Deployment test-deployment-5mk86 has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  7 13:59:26.461: INFO: Deployment "test-deployment-5mk86":
&Deployment{ObjectMeta:{test-deployment-5mk86  deployment-9716  31ea128a-b40b-4283-b51a-5b5e9fe7065a 33572640 1 2022-06-07 13:59:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-06-07 13:59:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-06-07 13:59:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-06-07 13:59:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00383ec48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  7 13:59:26.465: INFO: New ReplicaSet "test-deployment-5mk86-794dd694d8" of Deployment "test-deployment-5mk86":
&ReplicaSet{ObjectMeta:{test-deployment-5mk86-794dd694d8  deployment-9716  f695669b-3b9c-4d18-bb2b-e250994a0640 33572635 1 2022-06-07 13:59:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-5mk86 31ea128a-b40b-4283-b51a-5b5e9fe7065a 0xc00a639ed7 0xc00a639ed8}] []  [{kube-controller-manager Update apps/v1 2022-06-07 13:59:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31ea128a-b40b-4283-b51a-5b5e9fe7065a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 13:59:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 794dd694d8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a639f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  7 13:59:26.470: INFO: Pod "test-deployment-5mk86-794dd694d8-vrnhm" is available:
&Pod{ObjectMeta:{test-deployment-5mk86-794dd694d8-vrnhm test-deployment-5mk86-794dd694d8- deployment-9716  862b3f21-0e4a-47e1-85b7-560695a874fd 33572634 0 2022-06-07 13:59:24 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:794dd694d8] map[cni.projectcalico.org/containerID:70b9956f7945d4288d67f546d389874fac81c74af31daec10c4e5b6a3aeec585 cni.projectcalico.org/podIP:192.168.39.225/32 cni.projectcalico.org/podIPs:192.168.39.225/32] [{apps/v1 ReplicaSet test-deployment-5mk86-794dd694d8 f695669b-3b9c-4d18-bb2b-e250994a0640 0xc00383f0e7 0xc00383f0e8}] []  [{kube-controller-manager Update v1 2022-06-07 13:59:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f695669b-3b9c-4d18-bb2b-e250994a0640\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-07 13:59:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-07 13:59:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.39.225\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbfz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbfz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:59:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:59:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:59:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 13:59:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.14,PodIP:192.168.39.225,StartTime:2022-06-07 13:59:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 13:59:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://ea34250abe5368f9e64438e4de70cd792cad3263345b581d27e14779b9da0608,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.39.225,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:59:26.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9716" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":123,"skipped":2205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:59:26.482: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jun  7 13:59:28.529: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6746 PodName:pod-sharedvolume-01e13c4f-0b12-490c-aa4e-114b16cde2fc ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 13:59:28.529: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 13:59:28.661: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 13:59:28.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6746" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":124,"skipped":2244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 13:59:28.678: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-7231
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7231
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7231
Jun  7 13:59:28.726: INFO: Found 0 stateful pods, waiting for 1
Jun  7 13:59:38.736: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun  7 13:59:38.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-7231 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  7 13:59:38.976: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  7 13:59:38.976: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  7 13:59:38.976: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  7 13:59:38.982: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  7 13:59:48.996: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  7 13:59:48.996: INFO: Waiting for statefulset status.replicas updated to 0
Jun  7 13:59:49.020: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999741s
Jun  7 13:59:50.026: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994942512s
Jun  7 13:59:51.039: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988253505s
Jun  7 13:59:52.050: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.97689898s
Jun  7 13:59:53.060: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.965205919s
Jun  7 13:59:54.067: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.9550255s
Jun  7 13:59:55.076: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.947434179s
Jun  7 13:59:56.082: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.939107114s
Jun  7 13:59:57.091: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.931620369s
Jun  7 13:59:58.096: INFO: Verifying statefulset ss doesn't scale past 1 for another 924.84253ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7231
Jun  7 13:59:59.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-7231 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  7 13:59:59.302: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  7 13:59:59.302: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  7 13:59:59.302: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  7 13:59:59.308: INFO: Found 1 stateful pods, waiting for 3
Jun  7 14:00:09.320: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  7 14:00:09.320: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  7 14:00:09.320: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun  7 14:00:09.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-7231 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  7 14:00:09.548: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  7 14:00:09.548: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  7 14:00:09.548: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  7 14:00:09.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-7231 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  7 14:00:09.788: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  7 14:00:09.788: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  7 14:00:09.788: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  7 14:00:09.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-7231 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  7 14:00:10.052: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  7 14:00:10.053: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  7 14:00:10.053: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  7 14:00:10.053: INFO: Waiting for statefulset status.replicas updated to 0
Jun  7 14:00:10.060: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jun  7 14:00:20.071: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  7 14:00:20.071: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  7 14:00:20.071: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  7 14:00:20.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999479s
Jun  7 14:00:21.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993698413s
Jun  7 14:00:22.104: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986047437s
Jun  7 14:00:23.112: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97651035s
Jun  7 14:00:24.120: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967830592s
Jun  7 14:00:25.127: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96049515s
Jun  7 14:00:26.140: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.952547181s
Jun  7 14:00:27.150: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.940045014s
Jun  7 14:00:28.162: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.930165552s
Jun  7 14:00:29.170: INFO: Verifying statefulset ss doesn't scale past 3 for another 917.648841ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7231
Jun  7 14:00:30.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-7231 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  7 14:00:30.361: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  7 14:00:30.361: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  7 14:00:30.361: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  7 14:00:30.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-7231 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  7 14:00:30.575: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  7 14:00:30.575: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  7 14:00:30.575: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  7 14:00:30.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-7231 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  7 14:00:30.742: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  7 14:00:30.742: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  7 14:00:30.742: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  7 14:00:30.742: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun  7 14:00:40.767: INFO: Deleting all statefulset in ns statefulset-7231
Jun  7 14:00:40.772: INFO: Scaling statefulset ss to 0
Jun  7 14:00:40.791: INFO: Waiting for statefulset status.replicas updated to 0
Jun  7 14:00:40.794: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:00:40.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7231" for this suite.

• [SLOW TEST:72.146 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":125,"skipped":2272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:00:40.825: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 14:00:41.180: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 14:00:44.227: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:00:44.236: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5810-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:00:47.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4595" for this suite.
STEP: Destroying namespace "webhook-4595-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.735 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":126,"skipped":2303,"failed":0}
S
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:00:47.560: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4127 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4127;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4127 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4127;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4127.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4127.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4127.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4127.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4127.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4127.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4127.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4127.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4127.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4127.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4127.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4127.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4127.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 223.26.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.26.223_udp@PTR;check="$$(dig +tcp +noall +answer +search 223.26.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.26.223_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4127 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4127;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4127 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4127;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4127.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4127.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4127.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4127.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4127.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4127.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4127.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4127.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4127.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4127.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4127.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4127.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4127.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 223.26.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.26.223_udp@PTR;check="$$(dig +tcp +noall +answer +search 223.26.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.26.223_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  7 14:00:55.658: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.662: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.668: INFO: Unable to read wheezy_udp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.671: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.674: INFO: Unable to read wheezy_udp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.676: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.684: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.688: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.709: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.712: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.715: INFO: Unable to read jessie_udp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.719: INFO: Unable to read jessie_tcp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.722: INFO: Unable to read jessie_udp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.725: INFO: Unable to read jessie_tcp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.728: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.730: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:00:55.749: INFO: Lookups using dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4127 wheezy_tcp@dns-test-service.dns-4127 wheezy_udp@dns-test-service.dns-4127.svc wheezy_tcp@dns-test-service.dns-4127.svc wheezy_udp@_http._tcp.dns-test-service.dns-4127.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4127.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4127 jessie_tcp@dns-test-service.dns-4127 jessie_udp@dns-test-service.dns-4127.svc jessie_tcp@dns-test-service.dns-4127.svc jessie_udp@_http._tcp.dns-test-service.dns-4127.svc jessie_tcp@_http._tcp.dns-test-service.dns-4127.svc]

Jun  7 14:01:00.757: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.764: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.772: INFO: Unable to read wheezy_udp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.777: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.782: INFO: Unable to read wheezy_udp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.786: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.792: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.797: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.838: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.843: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.848: INFO: Unable to read jessie_udp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.853: INFO: Unable to read jessie_tcp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.859: INFO: Unable to read jessie_udp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.864: INFO: Unable to read jessie_tcp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.868: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.876: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:00.906: INFO: Lookups using dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4127 wheezy_tcp@dns-test-service.dns-4127 wheezy_udp@dns-test-service.dns-4127.svc wheezy_tcp@dns-test-service.dns-4127.svc wheezy_udp@_http._tcp.dns-test-service.dns-4127.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4127.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4127 jessie_tcp@dns-test-service.dns-4127 jessie_udp@dns-test-service.dns-4127.svc jessie_tcp@dns-test-service.dns-4127.svc jessie_udp@_http._tcp.dns-test-service.dns-4127.svc jessie_tcp@_http._tcp.dns-test-service.dns-4127.svc]

Jun  7 14:01:05.759: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.762: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.768: INFO: Unable to read wheezy_udp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.771: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.775: INFO: Unable to read wheezy_udp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.780: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.783: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.787: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.812: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.816: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.820: INFO: Unable to read jessie_udp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.824: INFO: Unable to read jessie_tcp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.828: INFO: Unable to read jessie_udp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.832: INFO: Unable to read jessie_tcp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.836: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.840: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:05.862: INFO: Lookups using dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4127 wheezy_tcp@dns-test-service.dns-4127 wheezy_udp@dns-test-service.dns-4127.svc wheezy_tcp@dns-test-service.dns-4127.svc wheezy_udp@_http._tcp.dns-test-service.dns-4127.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4127.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4127 jessie_tcp@dns-test-service.dns-4127 jessie_udp@dns-test-service.dns-4127.svc jessie_tcp@dns-test-service.dns-4127.svc jessie_udp@_http._tcp.dns-test-service.dns-4127.svc jessie_tcp@_http._tcp.dns-test-service.dns-4127.svc]

Jun  7 14:01:10.761: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.767: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.772: INFO: Unable to read wheezy_udp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.780: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.786: INFO: Unable to read wheezy_udp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.791: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.796: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.800: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.839: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.844: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.849: INFO: Unable to read jessie_udp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.854: INFO: Unable to read jessie_tcp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.860: INFO: Unable to read jessie_udp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.863: INFO: Unable to read jessie_tcp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.867: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.871: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:10.893: INFO: Lookups using dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4127 wheezy_tcp@dns-test-service.dns-4127 wheezy_udp@dns-test-service.dns-4127.svc wheezy_tcp@dns-test-service.dns-4127.svc wheezy_udp@_http._tcp.dns-test-service.dns-4127.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4127.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4127 jessie_tcp@dns-test-service.dns-4127 jessie_udp@dns-test-service.dns-4127.svc jessie_tcp@dns-test-service.dns-4127.svc jessie_udp@_http._tcp.dns-test-service.dns-4127.svc jessie_tcp@_http._tcp.dns-test-service.dns-4127.svc]

Jun  7 14:01:15.761: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.768: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.773: INFO: Unable to read wheezy_udp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.779: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.785: INFO: Unable to read wheezy_udp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.791: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.795: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.801: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.836: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.840: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.844: INFO: Unable to read jessie_udp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.847: INFO: Unable to read jessie_tcp@dns-test-service.dns-4127 from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.851: INFO: Unable to read jessie_udp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.855: INFO: Unable to read jessie_tcp@dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.859: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.862: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4127.svc from pod dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5: the server could not find the requested resource (get pods dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5)
Jun  7 14:01:15.884: INFO: Lookups using dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4127 wheezy_tcp@dns-test-service.dns-4127 wheezy_udp@dns-test-service.dns-4127.svc wheezy_tcp@dns-test-service.dns-4127.svc wheezy_udp@_http._tcp.dns-test-service.dns-4127.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4127.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4127 jessie_tcp@dns-test-service.dns-4127 jessie_udp@dns-test-service.dns-4127.svc jessie_tcp@dns-test-service.dns-4127.svc jessie_udp@_http._tcp.dns-test-service.dns-4127.svc jessie_tcp@_http._tcp.dns-test-service.dns-4127.svc]

Jun  7 14:01:20.879: INFO: DNS probes using dns-4127/dns-test-3ae08408-9b29-41e4-a952-d547bd2c0da5 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:01:20.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4127" for this suite.

• [SLOW TEST:33.414 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":127,"skipped":2304,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:01:20.975: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-3a7b9d43-430c-441a-ad94-baf315a38d79
STEP: Creating a pod to test consume configMaps
Jun  7 14:01:21.020: INFO: Waiting up to 5m0s for pod "pod-configmaps-d8967f82-1b06-43f8-b7b3-cf36aa2e6f64" in namespace "configmap-688" to be "Succeeded or Failed"
Jun  7 14:01:21.029: INFO: Pod "pod-configmaps-d8967f82-1b06-43f8-b7b3-cf36aa2e6f64": Phase="Pending", Reason="", readiness=false. Elapsed: 8.980305ms
Jun  7 14:01:23.035: INFO: Pod "pod-configmaps-d8967f82-1b06-43f8-b7b3-cf36aa2e6f64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015325285s
STEP: Saw pod success
Jun  7 14:01:23.035: INFO: Pod "pod-configmaps-d8967f82-1b06-43f8-b7b3-cf36aa2e6f64" satisfied condition "Succeeded or Failed"
Jun  7 14:01:23.046: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-configmaps-d8967f82-1b06-43f8-b7b3-cf36aa2e6f64 container agnhost-container: <nil>
STEP: delete the pod
Jun  7 14:01:23.073: INFO: Waiting for pod pod-configmaps-d8967f82-1b06-43f8-b7b3-cf36aa2e6f64 to disappear
Jun  7 14:01:23.077: INFO: Pod pod-configmaps-d8967f82-1b06-43f8-b7b3-cf36aa2e6f64 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:01:23.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-688" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":128,"skipped":2309,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:01:23.090: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 14:01:23.440: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 14:01:26.477: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:01:26.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9022" for this suite.
STEP: Destroying namespace "webhook-9022-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":129,"skipped":2330,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:01:26.574: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Jun  7 14:01:26.619: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:01:28.625: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:01:29.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3352" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":130,"skipped":2335,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:01:29.678: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-6e0ae23c-e6bf-46d0-a35f-b5841ffd5cc6
STEP: Creating a pod to test consume secrets
Jun  7 14:01:29.742: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-16b026dd-acf3-41d2-9765-5b5f4ddedf9b" in namespace "projected-8192" to be "Succeeded or Failed"
Jun  7 14:01:29.747: INFO: Pod "pod-projected-secrets-16b026dd-acf3-41d2-9765-5b5f4ddedf9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.160587ms
Jun  7 14:01:31.750: INFO: Pod "pod-projected-secrets-16b026dd-acf3-41d2-9765-5b5f4ddedf9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007593067s
STEP: Saw pod success
Jun  7 14:01:31.750: INFO: Pod "pod-projected-secrets-16b026dd-acf3-41d2-9765-5b5f4ddedf9b" satisfied condition "Succeeded or Failed"
Jun  7 14:01:31.753: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-secrets-16b026dd-acf3-41d2-9765-5b5f4ddedf9b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  7 14:01:31.770: INFO: Waiting for pod pod-projected-secrets-16b026dd-acf3-41d2-9765-5b5f4ddedf9b to disappear
Jun  7 14:01:31.772: INFO: Pod pod-projected-secrets-16b026dd-acf3-41d2-9765-5b5f4ddedf9b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:01:31.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8192" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":131,"skipped":2337,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:01:31.802: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:01:31.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-883" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":132,"skipped":2387,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:01:31.853: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:01:31.881: INFO: Creating ReplicaSet my-hostname-basic-8997b193-4bee-45db-8fd7-4a53f0f91829
Jun  7 14:01:31.887: INFO: Pod name my-hostname-basic-8997b193-4bee-45db-8fd7-4a53f0f91829: Found 0 pods out of 1
Jun  7 14:01:36.893: INFO: Pod name my-hostname-basic-8997b193-4bee-45db-8fd7-4a53f0f91829: Found 1 pods out of 1
Jun  7 14:01:36.893: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8997b193-4bee-45db-8fd7-4a53f0f91829" is running
Jun  7 14:01:36.896: INFO: Pod "my-hostname-basic-8997b193-4bee-45db-8fd7-4a53f0f91829-w949b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-07 14:01:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-07 14:01:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-07 14:01:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-07 14:01:31 +0000 UTC Reason: Message:}])
Jun  7 14:01:36.896: INFO: Trying to dial the pod
Jun  7 14:01:41.917: INFO: Controller my-hostname-basic-8997b193-4bee-45db-8fd7-4a53f0f91829: Got expected result from replica 1 [my-hostname-basic-8997b193-4bee-45db-8fd7-4a53f0f91829-w949b]: "my-hostname-basic-8997b193-4bee-45db-8fd7-4a53f0f91829-w949b", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:01:41.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5705" for this suite.

• [SLOW TEST:10.080 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":133,"skipped":2415,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:01:41.935: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:01:41.972: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  7 14:01:47.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-5614 --namespace=crd-publish-openapi-5614 create -f -'
Jun  7 14:01:48.524: INFO: stderr: ""
Jun  7 14:01:48.524: INFO: stdout: "e2e-test-crd-publish-openapi-3828-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  7 14:01:48.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-5614 --namespace=crd-publish-openapi-5614 delete e2e-test-crd-publish-openapi-3828-crds test-cr'
Jun  7 14:01:48.595: INFO: stderr: ""
Jun  7 14:01:48.595: INFO: stdout: "e2e-test-crd-publish-openapi-3828-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun  7 14:01:48.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-5614 --namespace=crd-publish-openapi-5614 apply -f -'
Jun  7 14:01:49.374: INFO: stderr: ""
Jun  7 14:01:49.374: INFO: stdout: "e2e-test-crd-publish-openapi-3828-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  7 14:01:49.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-5614 --namespace=crd-publish-openapi-5614 delete e2e-test-crd-publish-openapi-3828-crds test-cr'
Jun  7 14:01:49.446: INFO: stderr: ""
Jun  7 14:01:49.446: INFO: stdout: "e2e-test-crd-publish-openapi-3828-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun  7 14:01:49.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-5614 explain e2e-test-crd-publish-openapi-3828-crds'
Jun  7 14:01:49.668: INFO: stderr: ""
Jun  7 14:01:49.668: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3828-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:01:54.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5614" for this suite.

• [SLOW TEST:12.818 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":134,"skipped":2425,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:01:54.753: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6775.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6775.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6775.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  7 14:01:56.840: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:01:56.845: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:01:56.850: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:01:56.857: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:01:56.871: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:01:56.875: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:01:56.880: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:01:56.885: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:01:56.893: INFO: Lookups using dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local]

Jun  7 14:02:01.901: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:01.907: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:01.912: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:01.917: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:01.932: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:01.936: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:01.941: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:01.946: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:01.955: INFO: Lookups using dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local]

Jun  7 14:02:06.902: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:06.908: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:06.915: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:06.921: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:06.937: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:06.941: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:06.946: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:06.949: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:06.956: INFO: Lookups using dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local]

Jun  7 14:02:11.901: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:11.906: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:11.910: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:11.913: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:11.922: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:11.927: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:11.930: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:11.933: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:11.938: INFO: Lookups using dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local]

Jun  7 14:02:16.902: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:16.909: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:16.914: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:16.919: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:16.934: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:16.938: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:16.942: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:16.947: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:16.962: INFO: Lookups using dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local]

Jun  7 14:02:21.903: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:21.910: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:21.916: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:21.921: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:21.935: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:21.940: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:21.944: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:21.948: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local from pod dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac: the server could not find the requested resource (get pods dns-test-243606bf-c6fe-4064-aa01-cdd72563beac)
Jun  7 14:02:21.956: INFO: Lookups using dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6775.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6775.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6775.svc.cluster.local jessie_udp@dns-test-service-2.dns-6775.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6775.svc.cluster.local]

Jun  7 14:02:26.952: INFO: DNS probes using dns-6775/dns-test-243606bf-c6fe-4064-aa01-cdd72563beac succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:27.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6775" for this suite.

• [SLOW TEST:32.258 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":135,"skipped":2428,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:27.011: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:27.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5285" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":136,"skipped":2438,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:27.050: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  7 14:02:29.096: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:29.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7475" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:29.124: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 14:02:29.167: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d954bc2-5c32-4965-a355-e9572f921dd0" in namespace "downward-api-3685" to be "Succeeded or Failed"
Jun  7 14:02:29.170: INFO: Pod "downwardapi-volume-4d954bc2-5c32-4965-a355-e9572f921dd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.319902ms
Jun  7 14:02:31.178: INFO: Pod "downwardapi-volume-4d954bc2-5c32-4965-a355-e9572f921dd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010224613s
STEP: Saw pod success
Jun  7 14:02:31.178: INFO: Pod "downwardapi-volume-4d954bc2-5c32-4965-a355-e9572f921dd0" satisfied condition "Succeeded or Failed"
Jun  7 14:02:31.183: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-4d954bc2-5c32-4965-a355-e9572f921dd0 container client-container: <nil>
STEP: delete the pod
Jun  7 14:02:31.205: INFO: Waiting for pod downwardapi-volume-4d954bc2-5c32-4965-a355-e9572f921dd0 to disappear
Jun  7 14:02:31.209: INFO: Pod downwardapi-volume-4d954bc2-5c32-4965-a355-e9572f921dd0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:31.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3685" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":138,"skipped":2477,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:31.219: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:02:31.268: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-19aaf5ec-9b95-41c8-9cc6-bdf59163864d" in namespace "security-context-test-3352" to be "Succeeded or Failed"
Jun  7 14:02:31.275: INFO: Pod "busybox-privileged-false-19aaf5ec-9b95-41c8-9cc6-bdf59163864d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.432691ms
Jun  7 14:02:33.283: INFO: Pod "busybox-privileged-false-19aaf5ec-9b95-41c8-9cc6-bdf59163864d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014627239s
Jun  7 14:02:33.283: INFO: Pod "busybox-privileged-false-19aaf5ec-9b95-41c8-9cc6-bdf59163864d" satisfied condition "Succeeded or Failed"
Jun  7 14:02:33.295: INFO: Got logs for pod "busybox-privileged-false-19aaf5ec-9b95-41c8-9cc6-bdf59163864d": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:33.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3352" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":139,"skipped":2482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:33.308: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:02:33.341: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:34.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4959" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":140,"skipped":2530,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:34.397: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun  7 14:02:34.437: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  7 14:02:34.446: INFO: Waiting for terminating namespaces to be deleted...
Jun  7 14:02:34.449: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk001 before test
Jun  7 14:02:34.473: INFO: argocd-application-controller-0 from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.473: INFO: 	Container application-controller ready: true, restart count 0
Jun  7 14:02:34.473: INFO: argocd-repo-server-5547b66bd9-dfnbc from argocd started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.474: INFO: 	Container repo-server ready: true, restart count 0
Jun  7 14:02:34.474: INFO: calico-node-mpbx2 from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.474: INFO: 	Container calico-node ready: true, restart count 0
Jun  7 14:02:34.474: INFO: mypostgres-7-0 from default started at 2022-05-24 07:06:43 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.474: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:02:34.474: INFO: 	Container mypostgres-7 ready: true, restart count 0
Jun  7 14:02:34.474: INFO: reviews-v1-545db77b95-z2hm8 from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.474: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:02:34.474: INFO: 	Container reviews ready: true, restart count 0
Jun  7 14:02:34.474: INFO: foobar-75685968d4-575v5 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.474: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.474: INFO: foobar-75685968d4-d24zc from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.474: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.475: INFO: foobar-75685968d4-dwnnl from foobar started at 2022-06-07 13:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.475: INFO: foobar-75685968d4-mlbsq from foobar started at 2022-06-07 13:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.475: INFO: foobar-75685968d4-s72dw from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.475: INFO: foobar-75685968d4-scvj7 from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.475: INFO: foobar-75685968d4-wtsqh from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.475: INFO: mypod from foobar2 started at 2022-04-28 07:54:48 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container myfrontend ready: true, restart count 0
Jun  7 14:02:34.475: INFO: nginx-deployment-66b6c48dd5-dbfcx from foobar2 started at 2022-04-28 08:35:34 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.475: INFO: nginx-deployment-66b6c48dd5-pfdqw from foobar2 started at 2022-04-28 07:46:04 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.475: INFO: nginx-deployment2-d5845d7c8-vfthc from foobar2 started at 2022-04-28 07:49:41 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container nginx2 ready: true, restart count 0
Jun  7 14:02:34.475: INFO: gitea-0 from gitea started at 2022-03-30 07:25:54 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container gitea ready: true, restart count 0
Jun  7 14:02:34.475: INFO: gitea-postgresql-0 from gitea started at 2022-03-30 07:25:56 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container gitea-postgresql ready: true, restart count 1
Jun  7 14:02:34.475: INFO: ingress-nginx-controller-848878cd85-9wqg9 from ingress-nginx started at 2022-06-03 07:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container controller ready: true, restart count 0
Jun  7 14:02:34.475: INFO: k8s-status-c44b9cb68-pcnd4 from k8s-status started at 2022-04-14 07:03:09 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container k8s-status ready: true, restart count 0
Jun  7 14:02:34.475: INFO: kube-proxy-4c7bj from kube-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  7 14:02:34.475: INFO: loki-fluent-bit-loki-gqzll from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 14:02:34.475: INFO: prometheus-alertmanager-6c845bbb9c-r6krx from monitoring started at 2022-05-23 17:37:20 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jun  7 14:02:34.475: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jun  7 14:02:34.475: INFO: prometheus-kube-state-metrics-6c44ff7fb6-9zx9s from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  7 14:02:34.475: INFO: prometheus-node-exporter-z6jpz from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 14:02:34.475: INFO: prometheus-pushgateway-86679dcf68-qk8x2 from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Jun  7 14:02:34.475: INFO: prometheus-server-869789c557-72jr6 from monitoring started at 2022-06-07 13:40:18 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.475: INFO: 	Container prometheus-server ready: true, restart count 0
Jun  7 14:02:34.475: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun  7 14:02:34.475: INFO: sonobuoy from sonobuoy started at 2022-06-07 13:32:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.476: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  7 14:02:34.476: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-525dj from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.476: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 14:02:34.476: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 14:02:34.476: INFO: trident-csi-fcff9fbb6-rs2fc from trident started at 2022-06-07 13:40:18 +0000 UTC (6 container statuses recorded)
Jun  7 14:02:34.476: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  7 14:02:34.476: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  7 14:02:34.476: INFO: 	Container csi-resizer ready: true, restart count 0
Jun  7 14:02:34.476: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun  7 14:02:34.476: INFO: 	Container trident-autosupport ready: true, restart count 0
Jun  7 14:02:34.476: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 14:02:34.476: INFO: trident-csi-kqw7h from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.476: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 14:02:34.476: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 14:02:34.476: INFO: trident-operator-56d66bb95b-255bx from trident started at 2022-03-18 12:31:45 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.476: INFO: 	Container trident-operator ready: true, restart count 0
Jun  7 14:02:34.476: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk002 before test
Jun  7 14:02:34.496: INFO: calico-node-sv82j from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.496: INFO: 	Container calico-node ready: true, restart count 1
Jun  7 14:02:34.496: INFO: mypostgres-10-0 from default started at 2022-06-07 13:40:56 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.496: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:02:34.496: INFO: 	Container mypostgres-10 ready: true, restart count 0
Jun  7 14:02:34.496: INFO: kube-proxy-8sbg5 from kube-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.496: INFO: 	Container kube-proxy ready: true, restart count 1
Jun  7 14:02:34.496: INFO: loki-fluent-bit-loki-brtjw from loki started at 2022-06-07 13:40:44 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.496: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 14:02:34.496: INFO: prometheus-node-exporter-ngjbp from monitoring started at 2022-06-07 13:40:44 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.496: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 14:02:34.496: INFO: busybox-privileged-false-19aaf5ec-9b95-41c8-9cc6-bdf59163864d from security-context-test-3352 started at 2022-06-07 14:02:31 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.496: INFO: 	Container busybox-privileged-false-19aaf5ec-9b95-41c8-9cc6-bdf59163864d ready: false, restart count 0
Jun  7 14:02:34.496: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-7w99f from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.496: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 14:02:34.496: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 14:02:34.496: INFO: trident-csi-b4p6l from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.496: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 14:02:34.496: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 14:02:34.496: INFO: webhook-to-be-mutated from webhook-9022 started at 2022-06-07 14:01:26 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.496: INFO: 	Container example ready: false, restart count 0
Jun  7 14:02:34.496: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk003 before test
Jun  7 14:02:34.519: INFO: argocd-applicationset-controller-84bc7544cd-7zlhg from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.519: INFO: 	Container applicationset-controller ready: true, restart count 0
Jun  7 14:02:34.519: INFO: argocd-dex-server-6b5ffb8f5c-r5sf2 from argocd started at 2022-03-31 06:26:27 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.519: INFO: 	Container dex-server ready: true, restart count 0
Jun  7 14:02:34.520: INFO: calico-node-rmwmb from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.520: INFO: 	Container calico-node ready: true, restart count 0
Jun  7 14:02:34.520: INFO: calico-typha-54559758b4-6gmlw from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.520: INFO: 	Container calico-typha ready: true, restart count 0
Jun  7 14:02:34.520: INFO: cert-manager-6bbf595697-lr27t from cert-manager started at 2022-06-03 08:46:47 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.520: INFO: 	Container cert-manager ready: true, restart count 0
Jun  7 14:02:34.520: INFO: cert-manager-cainjector-6bc9d758b-vh4wl from cert-manager started at 2022-06-03 08:46:47 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.520: INFO: 	Container cert-manager ready: true, restart count 0
Jun  7 14:02:34.520: INFO: productpage-v1-6b746f74dc-xrnvw from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.520: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:02:34.520: INFO: 	Container productpage ready: true, restart count 0
Jun  7 14:02:34.520: INFO: ratings-v1-b6994bb9-vndct from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.520: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:02:34.520: INFO: 	Container ratings ready: true, restart count 0
Jun  7 14:02:34.521: INFO: reviews-v3-84779c7bbc-p4hjg from default started at 2022-04-22 12:33:02 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.521: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:02:34.521: INFO: 	Container reviews ready: true, restart count 0
Jun  7 14:02:34.521: INFO: dex-7b7c86db7-hcrrd from dex started at 2022-06-03 07:21:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.521: INFO: 	Container dex ready: true, restart count 0
Jun  7 14:02:34.521: INFO: dex-authenticator-dex-k8s-authenticator-5795dc9755-fgflk from dex started at 2022-03-22 15:07:54 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.521: INFO: 	Container dex-k8s-authenticator ready: true, restart count 0
Jun  7 14:02:34.521: INFO: foobar-75685968d4-7ktm8 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.521: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.521: INFO: foobar-75685968d4-cn6ww from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.521: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.521: INFO: foobar-75685968d4-fldfg from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.521: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.521: INFO: foobar-75685968d4-mjvcq from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.521: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.521: INFO: foobar-75685968d4-qpw2b from foobar started at 2022-06-07 13:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.521: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.521: INFO: foobar-75685968d4-wk7h8 from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.521: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.521: INFO: nginx-deployment-66b6c48dd5-kwzdn from foobar2 started at 2022-04-28 08:35:34 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.521: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.522: INFO: nginx-deployment2-d5845d7c8-bbm4g from foobar2 started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.522: INFO: 	Container nginx2 ready: true, restart count 0
Jun  7 14:02:34.522: INFO: simple from foobar2 started at 2022-04-27 08:47:49 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.522: INFO: 	Container simple ready: true, restart count 0
Jun  7 14:02:34.522: INFO: istio-egressgateway-66fdd867f4-drz9z from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.522: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:02:34.522: INFO: prometheus-699b7cc575-k5zl7 from istio-system started at 2022-06-07 13:40:18 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.522: INFO: 	Container prometheus-server ready: true, restart count 0
Jun  7 14:02:34.522: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun  7 14:02:34.522: INFO: k8s-status-c44b9cb68-dx6r8 from k8s-status started at 2022-04-14 07:03:09 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.522: INFO: 	Container k8s-status ready: true, restart count 0
Jun  7 14:02:34.522: INFO: kube-proxy-wx7st from kube-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.522: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  7 14:02:34.522: INFO: kubegres-controller-manager-755b4c48f6-zxmlm from kubegres-system started at 2022-06-07 13:40:18 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  7 14:02:34.522: INFO: 	Container manager ready: true, restart count 0
Jun  7 14:02:34.522: INFO: loki-0 from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.522: INFO: 	Container loki ready: true, restart count 0
Jun  7 14:02:34.522: INFO: loki-fluent-bit-loki-ss7rt from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.522: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 14:02:34.522: INFO: prometheus-node-exporter-rz8ws from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.523: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 14:02:34.523: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-jxr44 from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 14:02:34.523: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 14:02:34.523: INFO: trident-csi-7wfz2 from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.523: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 14:02:34.523: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 14:02:34.523: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk004 before test
Jun  7 14:02:34.543: INFO: argocd-notifications-controller-6dd95488b4-l4t7z from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container notifications-controller ready: true, restart count 0
Jun  7 14:02:34.543: INFO: argocd-redis-57bcc665bf-lpcsm from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container argocd-redis ready: true, restart count 0
Jun  7 14:02:34.543: INFO: argocd-server-7b98b7446b-n7nmr from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container server ready: true, restart count 0
Jun  7 14:02:34.543: INFO: calico-node-gmd98 from calico-system started at 2022-03-10 15:32:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container calico-node ready: true, restart count 0
Jun  7 14:02:34.543: INFO: cert-manager-webhook-586d45d5ff-pftz7 from cert-manager started at 2022-06-03 08:46:47 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container cert-manager ready: true, restart count 0
Jun  7 14:02:34.543: INFO: details-v1-79f774bdb9-pxpbj from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container details ready: true, restart count 0
Jun  7 14:02:34.543: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:02:34.543: INFO: mypostgres-6-0 from default started at 2022-05-16 17:57:42 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:02:34.543: INFO: 	Container mypostgres-6 ready: true, restart count 0
Jun  7 14:02:34.543: INFO: reviews-v2-7bf8c9648f-q86cj from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:02:34.543: INFO: 	Container reviews ready: true, restart count 0
Jun  7 14:02:34.543: INFO: oauth2-oauth2-proxy-85748949c4-rlj6t from dex started at 2022-06-03 07:21:23 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container oauth2-proxy ready: true, restart count 0
Jun  7 14:02:34.543: INFO: foobar-75685968d4-2wjms from foobar started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.543: INFO: foobar-75685968d4-8k5dq from foobar started at 2022-03-25 12:14:06 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.543: INFO: foobar-75685968d4-98jz8 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.543: INFO: foobar-75685968d4-qj6bn from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.543: INFO: foobar-75685968d4-skd4f from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.543: INFO: foobar-75685968d4-vtjhk from foobar started at 2022-06-07 13:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.543: INFO: foobar-75685968d4-zgz7t from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.543: INFO: nginx-deployment-66b6c48dd5-9zp8s from foobar2 started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.543: INFO: nginx-deployment-66b6c48dd5-psjh5 from foobar2 started at 2022-04-28 07:46:07 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.543: INFO: nginx-deployment2-d5845d7c8-xhm74 from foobar2 started at 2022-04-28 07:49:45 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container nginx2 ready: true, restart count 0
Jun  7 14:02:34.543: INFO: gitea-memcached-7b44bc5f74-6vssj from gitea started at 2022-03-30 07:25:52 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container memcached ready: true, restart count 0
Jun  7 14:02:34.543: INFO: ingress-nginx-controller-848878cd85-n5nsq from ingress-nginx started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container controller ready: true, restart count 0
Jun  7 14:02:34.543: INFO: grafana-6c5dc6df7c-t4q9p from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container grafana ready: true, restart count 0
Jun  7 14:02:34.543: INFO: istio-ingressgateway-77968dbd74-qrklf from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:02:34.543: INFO: istiod-699b647f8b-lgg5l from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container discovery ready: true, restart count 0
Jun  7 14:02:34.543: INFO: jaeger-9dd685668-5frgx from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container jaeger ready: true, restart count 0
Jun  7 14:02:34.543: INFO: kiali-699f98c497-rvfkc from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container kiali ready: true, restart count 0
Jun  7 14:02:34.543: INFO: foo-bf9cd6fb5-7nmh7 from jltest started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:02:34.543: INFO: k8s-status-c44b9cb68-f4rv9 from k8s-status started at 2022-04-14 07:03:09 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container k8s-status ready: true, restart count 0
Jun  7 14:02:34.543: INFO: kube-proxy-t5klm from kube-system started at 2022-03-10 15:32:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  7 14:02:34.543: INFO: grafana-5ff8c85cf8-gv9c2 from loki started at 2022-04-25 05:59:11 +0000 UTC (3 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container grafana ready: true, restart count 0
Jun  7 14:02:34.543: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun  7 14:02:34.543: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Jun  7 14:02:34.543: INFO: loki-fluent-bit-loki-mrrh2 from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 14:02:34.543: INFO: testpod from loki started at 2022-03-28 08:55:49 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container testpod ready: true, restart count 71
Jun  7 14:02:34.543: INFO: prometheus-node-exporter-5ph5n from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 14:02:34.543: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-pxgrv from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 14:02:34.543: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 14:02:34.543: INFO: trident-csi-cscfg from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 14:02:34.543: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 14:02:34.543: INFO: 	Container trident-main ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-494d220b-92de-400e-8c09-05e24644a898 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-494d220b-92de-400e-8c09-05e24644a898 off the node proact-prod01-wk002
STEP: verifying the node doesn't have the label kubernetes.io/e2e-494d220b-92de-400e-8c09-05e24644a898
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:38.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2866" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":141,"skipped":2543,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:38.635: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-da7122d5-7107-40c4-b526-df9178979079
STEP: Creating a pod to test consume configMaps
Jun  7 14:02:38.671: INFO: Waiting up to 5m0s for pod "pod-configmaps-b1d42f3b-b677-45a2-8ba5-5a904d2b4ecd" in namespace "configmap-8117" to be "Succeeded or Failed"
Jun  7 14:02:38.673: INFO: Pod "pod-configmaps-b1d42f3b-b677-45a2-8ba5-5a904d2b4ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032044ms
Jun  7 14:02:40.685: INFO: Pod "pod-configmaps-b1d42f3b-b677-45a2-8ba5-5a904d2b4ecd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014142223s
STEP: Saw pod success
Jun  7 14:02:40.685: INFO: Pod "pod-configmaps-b1d42f3b-b677-45a2-8ba5-5a904d2b4ecd" satisfied condition "Succeeded or Failed"
Jun  7 14:02:40.691: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-configmaps-b1d42f3b-b677-45a2-8ba5-5a904d2b4ecd container agnhost-container: <nil>
STEP: delete the pod
Jun  7 14:02:40.710: INFO: Waiting for pod pod-configmaps-b1d42f3b-b677-45a2-8ba5-5a904d2b4ecd to disappear
Jun  7 14:02:40.716: INFO: Pod pod-configmaps-b1d42f3b-b677-45a2-8ba5-5a904d2b4ecd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:40.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8117" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":142,"skipped":2651,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:40.733: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun  7 14:02:40.782: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6579  fba0f508-c217-403d-96f7-e1bd19e1b25a 33574693 0 2022-06-07 14:02:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-06-07 14:02:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 14:02:40.783: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6579  fba0f508-c217-403d-96f7-e1bd19e1b25a 33574694 0 2022-06-07 14:02:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-06-07 14:02:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:40.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6579" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":143,"skipped":2658,"failed":0}
SSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:40.791: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:40.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2675" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":144,"skipped":2665,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:40.832: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-78a2cd4b-43da-4249-90a0-363dcc83ede5
STEP: Creating a pod to test consume configMaps
Jun  7 14:02:40.876: INFO: Waiting up to 5m0s for pod "pod-configmaps-ea3839f0-8bc8-4133-b55b-d9ceabf7ebd7" in namespace "configmap-9850" to be "Succeeded or Failed"
Jun  7 14:02:40.880: INFO: Pod "pod-configmaps-ea3839f0-8bc8-4133-b55b-d9ceabf7ebd7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8609ms
Jun  7 14:02:42.889: INFO: Pod "pod-configmaps-ea3839f0-8bc8-4133-b55b-d9ceabf7ebd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013769152s
STEP: Saw pod success
Jun  7 14:02:42.890: INFO: Pod "pod-configmaps-ea3839f0-8bc8-4133-b55b-d9ceabf7ebd7" satisfied condition "Succeeded or Failed"
Jun  7 14:02:42.895: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-configmaps-ea3839f0-8bc8-4133-b55b-d9ceabf7ebd7 container agnhost-container: <nil>
STEP: delete the pod
Jun  7 14:02:42.923: INFO: Waiting for pod pod-configmaps-ea3839f0-8bc8-4133-b55b-d9ceabf7ebd7 to disappear
Jun  7 14:02:42.927: INFO: Pod pod-configmaps-ea3839f0-8bc8-4133-b55b-d9ceabf7ebd7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:42.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9850" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":145,"skipped":2671,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:42.941: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2154.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2154.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2154.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2154.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2154.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2154.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  7 14:02:47.062: INFO: DNS probes using dns-2154/dns-test-58e5423f-be9f-4850-bca6-0b5fd546c45d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:47.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2154" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":146,"skipped":2671,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:47.106: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-2bcf8e6c-a2b2-45ca-8420-89a75c50bcde
STEP: Creating a pod to test consume configMaps
Jun  7 14:02:47.148: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-15b835d0-4d11-4e83-b9f4-2a54fa84574c" in namespace "projected-7922" to be "Succeeded or Failed"
Jun  7 14:02:47.150: INFO: Pod "pod-projected-configmaps-15b835d0-4d11-4e83-b9f4-2a54fa84574c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.913153ms
Jun  7 14:02:49.160: INFO: Pod "pod-projected-configmaps-15b835d0-4d11-4e83-b9f4-2a54fa84574c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012086744s
STEP: Saw pod success
Jun  7 14:02:49.160: INFO: Pod "pod-projected-configmaps-15b835d0-4d11-4e83-b9f4-2a54fa84574c" satisfied condition "Succeeded or Failed"
Jun  7 14:02:49.163: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-configmaps-15b835d0-4d11-4e83-b9f4-2a54fa84574c container agnhost-container: <nil>
STEP: delete the pod
Jun  7 14:02:49.195: INFO: Waiting for pod pod-projected-configmaps-15b835d0-4d11-4e83-b9f4-2a54fa84574c to disappear
Jun  7 14:02:49.197: INFO: Pod pod-projected-configmaps-15b835d0-4d11-4e83-b9f4-2a54fa84574c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:02:49.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7922" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":147,"skipped":2674,"failed":0}
SSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:02:49.207: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:04:01.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5470" for this suite.

• [SLOW TEST:72.084 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":148,"skipped":2677,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:04:01.292: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  7 14:04:01.356: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun  7 14:04:01.361: INFO: starting watch
STEP: patching
STEP: updating
Jun  7 14:04:01.374: INFO: waiting for watch events with expected annotations
Jun  7 14:04:01.374: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:04:01.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8155" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":149,"skipped":2693,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:04:01.411: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:04:01.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9595" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":150,"skipped":2695,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:04:01.479: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-b833ea55-2288-4294-bbe5-b42789889134
STEP: Creating a pod to test consume configMaps
Jun  7 14:04:01.519: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6ac9877f-55b3-4921-8f69-f4920b9fc5d3" in namespace "projected-4606" to be "Succeeded or Failed"
Jun  7 14:04:01.521: INFO: Pod "pod-projected-configmaps-6ac9877f-55b3-4921-8f69-f4920b9fc5d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.44655ms
Jun  7 14:04:03.531: INFO: Pod "pod-projected-configmaps-6ac9877f-55b3-4921-8f69-f4920b9fc5d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011765348s
STEP: Saw pod success
Jun  7 14:04:03.531: INFO: Pod "pod-projected-configmaps-6ac9877f-55b3-4921-8f69-f4920b9fc5d3" satisfied condition "Succeeded or Failed"
Jun  7 14:04:03.535: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-configmaps-6ac9877f-55b3-4921-8f69-f4920b9fc5d3 container agnhost-container: <nil>
STEP: delete the pod
Jun  7 14:04:03.555: INFO: Waiting for pod pod-projected-configmaps-6ac9877f-55b3-4921-8f69-f4920b9fc5d3 to disappear
Jun  7 14:04:03.558: INFO: Pod pod-projected-configmaps-6ac9877f-55b3-4921-8f69-f4920b9fc5d3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:04:03.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4606" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":151,"skipped":2695,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:04:03.572: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Jun  7 14:06:04.160: INFO: Successfully updated pod "var-expansion-8b258ec6-10e6-4d1b-87d9-748d2c351cdb"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jun  7 14:06:06.179: INFO: Deleting pod "var-expansion-8b258ec6-10e6-4d1b-87d9-748d2c351cdb" in namespace "var-expansion-9422"
Jun  7 14:06:06.193: INFO: Wait up to 5m0s for pod "var-expansion-8b258ec6-10e6-4d1b-87d9-748d2c351cdb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:06:38.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9422" for this suite.

• [SLOW TEST:154.661 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":152,"skipped":2709,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:06:38.234: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:06:38.282: INFO: Creating pod...
Jun  7 14:06:38.294: INFO: Pod Quantity: 1 Status: Pending
Jun  7 14:06:39.304: INFO: Pod Quantity: 1 Status: Pending
Jun  7 14:06:40.305: INFO: Pod Status: Running
Jun  7 14:06:40.305: INFO: Creating service...
Jun  7 14:06:40.327: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/pods/agnhost/proxy/some/path/with/DELETE
Jun  7 14:06:40.333: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun  7 14:06:40.333: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/pods/agnhost/proxy/some/path/with/GET
Jun  7 14:06:40.340: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun  7 14:06:40.340: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/pods/agnhost/proxy/some/path/with/HEAD
Jun  7 14:06:40.345: INFO: http.Client request:HEAD | StatusCode:200
Jun  7 14:06:40.345: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/pods/agnhost/proxy/some/path/with/OPTIONS
Jun  7 14:06:40.356: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun  7 14:06:40.356: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/pods/agnhost/proxy/some/path/with/PATCH
Jun  7 14:06:40.359: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun  7 14:06:40.359: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/pods/agnhost/proxy/some/path/with/POST
Jun  7 14:06:40.362: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun  7 14:06:40.362: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/pods/agnhost/proxy/some/path/with/PUT
Jun  7 14:06:40.371: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun  7 14:06:40.371: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/services/test-service/proxy/some/path/with/DELETE
Jun  7 14:06:40.376: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun  7 14:06:40.376: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/services/test-service/proxy/some/path/with/GET
Jun  7 14:06:40.380: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun  7 14:06:40.380: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/services/test-service/proxy/some/path/with/HEAD
Jun  7 14:06:40.384: INFO: http.Client request:HEAD | StatusCode:200
Jun  7 14:06:40.384: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/services/test-service/proxy/some/path/with/OPTIONS
Jun  7 14:06:40.387: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun  7 14:06:40.387: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/services/test-service/proxy/some/path/with/PATCH
Jun  7 14:06:40.389: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun  7 14:06:40.389: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/services/test-service/proxy/some/path/with/POST
Jun  7 14:06:40.392: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun  7 14:06:40.392: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8752/services/test-service/proxy/some/path/with/PUT
Jun  7 14:06:40.395: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:06:40.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8752" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":153,"skipped":2735,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:06:40.407: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Jun  7 14:06:40.452: INFO: The status of Pod pod-hostip-431d489c-da1b-4465-a8e3-bd7f890228cd is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:06:42.461: INFO: The status of Pod pod-hostip-431d489c-da1b-4465-a8e3-bd7f890228cd is Running (Ready = true)
Jun  7 14:06:42.470: INFO: Pod pod-hostip-431d489c-da1b-4465-a8e3-bd7f890228cd has hostIP: 10.55.210.14
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:06:42.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1730" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":154,"skipped":2745,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:06:42.486: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun  7 14:06:42.530: INFO: Waiting up to 5m0s for pod "pod-25f1d848-b2ad-40bd-9db5-a232e61e60e2" in namespace "emptydir-494" to be "Succeeded or Failed"
Jun  7 14:06:42.532: INFO: Pod "pod-25f1d848-b2ad-40bd-9db5-a232e61e60e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.818182ms
Jun  7 14:06:44.539: INFO: Pod "pod-25f1d848-b2ad-40bd-9db5-a232e61e60e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008486324s
STEP: Saw pod success
Jun  7 14:06:44.539: INFO: Pod "pod-25f1d848-b2ad-40bd-9db5-a232e61e60e2" satisfied condition "Succeeded or Failed"
Jun  7 14:06:44.543: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-25f1d848-b2ad-40bd-9db5-a232e61e60e2 container test-container: <nil>
STEP: delete the pod
Jun  7 14:06:44.570: INFO: Waiting for pod pod-25f1d848-b2ad-40bd-9db5-a232e61e60e2 to disappear
Jun  7 14:06:44.572: INFO: Pod pod-25f1d848-b2ad-40bd-9db5-a232e61e60e2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:06:44.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-494" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":155,"skipped":2761,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:06:44.584: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Jun  7 14:06:44.623: INFO: Waiting up to 5m0s for pod "test-pod-55c7a4c7-5103-4402-8ebe-b60f1c4f1159" in namespace "svcaccounts-2495" to be "Succeeded or Failed"
Jun  7 14:06:44.625: INFO: Pod "test-pod-55c7a4c7-5103-4402-8ebe-b60f1c4f1159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033509ms
Jun  7 14:06:46.633: INFO: Pod "test-pod-55c7a4c7-5103-4402-8ebe-b60f1c4f1159": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010105103s
STEP: Saw pod success
Jun  7 14:06:46.633: INFO: Pod "test-pod-55c7a4c7-5103-4402-8ebe-b60f1c4f1159" satisfied condition "Succeeded or Failed"
Jun  7 14:06:46.637: INFO: Trying to get logs from node proact-prod01-wk002 pod test-pod-55c7a4c7-5103-4402-8ebe-b60f1c4f1159 container agnhost-container: <nil>
STEP: delete the pod
Jun  7 14:06:46.660: INFO: Waiting for pod test-pod-55c7a4c7-5103-4402-8ebe-b60f1c4f1159 to disappear
Jun  7 14:06:46.663: INFO: Pod test-pod-55c7a4c7-5103-4402-8ebe-b60f1c4f1159 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:06:46.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2495" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":156,"skipped":2786,"failed":0}
S
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:06:46.673: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:06:46.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3783" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":157,"skipped":2787,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:06:46.750: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jun  7 14:06:46.792: INFO: The status of Pod labelsupdate2ddfdf5a-5de5-4830-a594-6d0a45194202 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:06:48.806: INFO: The status of Pod labelsupdate2ddfdf5a-5de5-4830-a594-6d0a45194202 is Running (Ready = true)
Jun  7 14:06:49.337: INFO: Successfully updated pod "labelsupdate2ddfdf5a-5de5-4830-a594-6d0a45194202"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:06:53.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-94" for this suite.

• [SLOW TEST:6.627 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":158,"skipped":2799,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:06:53.380: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Jun  7 14:06:53.472: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-8740 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:06:53.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8740" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":159,"skipped":2808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:06:53.540: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 14:06:53.912: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 14:06:56.957: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:07.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5953" for this suite.
STEP: Destroying namespace "webhook-5953-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.647 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":160,"skipped":2843,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:07.187: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Jun  7 14:07:07.229: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-8634 proxy --unix-socket=/tmp/kubectl-proxy-unix324231922/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:07.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8634" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":161,"skipped":2870,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:07.280: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 14:07:07.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2cec31e-113c-4b69-9463-729ece58c0b8" in namespace "downward-api-1887" to be "Succeeded or Failed"
Jun  7 14:07:07.317: INFO: Pod "downwardapi-volume-c2cec31e-113c-4b69-9463-729ece58c0b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.41546ms
Jun  7 14:07:09.326: INFO: Pod "downwardapi-volume-c2cec31e-113c-4b69-9463-729ece58c0b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011410009s
STEP: Saw pod success
Jun  7 14:07:09.326: INFO: Pod "downwardapi-volume-c2cec31e-113c-4b69-9463-729ece58c0b8" satisfied condition "Succeeded or Failed"
Jun  7 14:07:09.331: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-c2cec31e-113c-4b69-9463-729ece58c0b8 container client-container: <nil>
STEP: delete the pod
Jun  7 14:07:09.353: INFO: Waiting for pod downwardapi-volume-c2cec31e-113c-4b69-9463-729ece58c0b8 to disappear
Jun  7 14:07:09.357: INFO: Pod downwardapi-volume-c2cec31e-113c-4b69-9463-729ece58c0b8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:09.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1887" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":162,"skipped":2870,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:09.369: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jun  7 14:07:09.424: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:07:11.440: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun  7 14:07:12.469: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:13.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-131" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":163,"skipped":2872,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:13.515: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun  7 14:07:19.648: INFO: The status of Pod kube-controller-manager-proact-prod01-cp003 is Running (Ready = true)
Jun  7 14:07:19.731: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:19.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4830" for this suite.

• [SLOW TEST:6.227 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":164,"skipped":2900,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:19.742: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  7 14:07:19.799: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun  7 14:07:19.803: INFO: starting watch
STEP: patching
STEP: updating
Jun  7 14:07:19.819: INFO: waiting for watch events with expected annotations
Jun  7 14:07:19.819: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:19.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-469" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":165,"skipped":2909,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:19.859: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:07:19.889: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun  7 14:07:25.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 --namespace=crd-publish-openapi-2484 create -f -'
Jun  7 14:07:26.559: INFO: stderr: ""
Jun  7 14:07:26.559: INFO: stdout: "e2e-test-crd-publish-openapi-3332-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  7 14:07:26.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 --namespace=crd-publish-openapi-2484 delete e2e-test-crd-publish-openapi-3332-crds test-foo'
Jun  7 14:07:26.617: INFO: stderr: ""
Jun  7 14:07:26.617: INFO: stdout: "e2e-test-crd-publish-openapi-3332-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun  7 14:07:26.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 --namespace=crd-publish-openapi-2484 apply -f -'
Jun  7 14:07:27.353: INFO: stderr: ""
Jun  7 14:07:27.353: INFO: stdout: "e2e-test-crd-publish-openapi-3332-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  7 14:07:27.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 --namespace=crd-publish-openapi-2484 delete e2e-test-crd-publish-openapi-3332-crds test-foo'
Jun  7 14:07:27.416: INFO: stderr: ""
Jun  7 14:07:27.416: INFO: stdout: "e2e-test-crd-publish-openapi-3332-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun  7 14:07:27.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 --namespace=crd-publish-openapi-2484 create -f -'
Jun  7 14:07:27.573: INFO: rc: 1
Jun  7 14:07:27.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 --namespace=crd-publish-openapi-2484 apply -f -'
Jun  7 14:07:27.742: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun  7 14:07:27.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 --namespace=crd-publish-openapi-2484 create -f -'
Jun  7 14:07:27.906: INFO: rc: 1
Jun  7 14:07:27.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 --namespace=crd-publish-openapi-2484 apply -f -'
Jun  7 14:07:28.741: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun  7 14:07:28.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 explain e2e-test-crd-publish-openapi-3332-crds'
Jun  7 14:07:28.928: INFO: stderr: ""
Jun  7 14:07:28.928: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3332-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun  7 14:07:28.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 explain e2e-test-crd-publish-openapi-3332-crds.metadata'
Jun  7 14:07:29.100: INFO: stderr: ""
Jun  7 14:07:29.100: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3332-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun  7 14:07:29.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 explain e2e-test-crd-publish-openapi-3332-crds.spec'
Jun  7 14:07:29.299: INFO: stderr: ""
Jun  7 14:07:29.299: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3332-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun  7 14:07:29.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 explain e2e-test-crd-publish-openapi-3332-crds.spec.bars'
Jun  7 14:07:29.480: INFO: stderr: ""
Jun  7 14:07:29.480: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3332-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun  7 14:07:29.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-2484 explain e2e-test-crd-publish-openapi-3332-crds.spec.bars2'
Jun  7 14:07:29.648: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:34.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2484" for this suite.

• [SLOW TEST:14.955 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":166,"skipped":2910,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:34.815: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 14:07:34.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0be1a87f-7d95-49f2-8140-b1d0adb0e757" in namespace "projected-1592" to be "Succeeded or Failed"
Jun  7 14:07:34.867: INFO: Pod "downwardapi-volume-0be1a87f-7d95-49f2-8140-b1d0adb0e757": Phase="Pending", Reason="", readiness=false. Elapsed: 2.998139ms
Jun  7 14:07:36.875: INFO: Pod "downwardapi-volume-0be1a87f-7d95-49f2-8140-b1d0adb0e757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01083707s
STEP: Saw pod success
Jun  7 14:07:36.875: INFO: Pod "downwardapi-volume-0be1a87f-7d95-49f2-8140-b1d0adb0e757" satisfied condition "Succeeded or Failed"
Jun  7 14:07:36.880: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-0be1a87f-7d95-49f2-8140-b1d0adb0e757 container client-container: <nil>
STEP: delete the pod
Jun  7 14:07:36.908: INFO: Waiting for pod downwardapi-volume-0be1a87f-7d95-49f2-8140-b1d0adb0e757 to disappear
Jun  7 14:07:36.911: INFO: Pod downwardapi-volume-0be1a87f-7d95-49f2-8140-b1d0adb0e757 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:36.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1592" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":167,"skipped":2952,"failed":0}

------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:36.922: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:07:36.957: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun  7 14:07:36.966: INFO: The status of Pod pod-logs-websocket-0b699b55-7228-4c18-9d9b-b563b66dec19 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:07:38.980: INFO: The status of Pod pod-logs-websocket-0b699b55-7228-4c18-9d9b-b563b66dec19 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:39.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4722" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":168,"skipped":2952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:39.022: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-6919
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6919
STEP: Deleting pre-stop pod
Jun  7 14:07:48.128: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:48.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6919" for this suite.

• [SLOW TEST:9.146 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":169,"skipped":2985,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:48.170: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jun  7 14:07:48.220: INFO: Waiting up to 5m0s for pod "downward-api-614e46ac-b5f4-42a3-baae-112329c3e9bf" in namespace "downward-api-369" to be "Succeeded or Failed"
Jun  7 14:07:48.224: INFO: Pod "downward-api-614e46ac-b5f4-42a3-baae-112329c3e9bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.814841ms
Jun  7 14:07:50.232: INFO: Pod "downward-api-614e46ac-b5f4-42a3-baae-112329c3e9bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01129083s
STEP: Saw pod success
Jun  7 14:07:50.232: INFO: Pod "downward-api-614e46ac-b5f4-42a3-baae-112329c3e9bf" satisfied condition "Succeeded or Failed"
Jun  7 14:07:50.235: INFO: Trying to get logs from node proact-prod01-wk002 pod downward-api-614e46ac-b5f4-42a3-baae-112329c3e9bf container dapi-container: <nil>
STEP: delete the pod
Jun  7 14:07:50.255: INFO: Waiting for pod downward-api-614e46ac-b5f4-42a3-baae-112329c3e9bf to disappear
Jun  7 14:07:50.258: INFO: Pod downward-api-614e46ac-b5f4-42a3-baae-112329c3e9bf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:50.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-369" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":170,"skipped":2991,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:50.281: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
STEP: creating the pod
Jun  7 14:07:50.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9434 create -f -'
Jun  7 14:07:51.195: INFO: stderr: ""
Jun  7 14:07:51.195: INFO: stdout: "pod/pause created\n"
Jun  7 14:07:51.195: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun  7 14:07:51.195: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9434" to be "running and ready"
Jun  7 14:07:51.200: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.809932ms
Jun  7 14:07:53.212: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016734187s
Jun  7 14:07:53.212: INFO: Pod "pause" satisfied condition "running and ready"
Jun  7 14:07:53.212: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Jun  7 14:07:53.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9434 label pods pause testing-label=testing-label-value'
Jun  7 14:07:53.302: INFO: stderr: ""
Jun  7 14:07:53.302: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun  7 14:07:53.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9434 get pod pause -L testing-label'
Jun  7 14:07:53.370: INFO: stderr: ""
Jun  7 14:07:53.370: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun  7 14:07:53.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9434 label pods pause testing-label-'
Jun  7 14:07:53.452: INFO: stderr: ""
Jun  7 14:07:53.452: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun  7 14:07:53.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9434 get pod pause -L testing-label'
Jun  7 14:07:53.512: INFO: stderr: ""
Jun  7 14:07:53.512: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
STEP: using delete to clean up resources
Jun  7 14:07:53.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9434 delete --grace-period=0 --force -f -'
Jun  7 14:07:53.596: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  7 14:07:53.596: INFO: stdout: "pod \"pause\" force deleted\n"
Jun  7 14:07:53.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9434 get rc,svc -l name=pause --no-headers'
Jun  7 14:07:53.677: INFO: stderr: "No resources found in kubectl-9434 namespace.\n"
Jun  7 14:07:53.677: INFO: stdout: ""
Jun  7 14:07:53.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9434 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  7 14:07:53.737: INFO: stderr: ""
Jun  7 14:07:53.737: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:53.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9434" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":171,"skipped":2994,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:53.755: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jun  7 14:07:53.801: INFO: Waiting up to 5m0s for pod "downward-api-e4ba67ca-90ff-4378-b63a-e4841d07de4d" in namespace "downward-api-9313" to be "Succeeded or Failed"
Jun  7 14:07:53.804: INFO: Pod "downward-api-e4ba67ca-90ff-4378-b63a-e4841d07de4d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.343025ms
Jun  7 14:07:55.809: INFO: Pod "downward-api-e4ba67ca-90ff-4378-b63a-e4841d07de4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008006106s
STEP: Saw pod success
Jun  7 14:07:55.809: INFO: Pod "downward-api-e4ba67ca-90ff-4378-b63a-e4841d07de4d" satisfied condition "Succeeded or Failed"
Jun  7 14:07:55.812: INFO: Trying to get logs from node proact-prod01-wk002 pod downward-api-e4ba67ca-90ff-4378-b63a-e4841d07de4d container dapi-container: <nil>
STEP: delete the pod
Jun  7 14:07:55.829: INFO: Waiting for pod downward-api-e4ba67ca-90ff-4378-b63a-e4841d07de4d to disappear
Jun  7 14:07:55.831: INFO: Pod downward-api-e4ba67ca-90ff-4378-b63a-e4841d07de4d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:55.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9313" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":172,"skipped":2997,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:55.841: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-3414/configmap-test-564c534f-0a6c-4440-b52e-5762ef6cecd8
STEP: Creating a pod to test consume configMaps
Jun  7 14:07:55.876: INFO: Waiting up to 5m0s for pod "pod-configmaps-305fd120-e603-49a7-a1bd-5b17827c53ae" in namespace "configmap-3414" to be "Succeeded or Failed"
Jun  7 14:07:55.878: INFO: Pod "pod-configmaps-305fd120-e603-49a7-a1bd-5b17827c53ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.218387ms
Jun  7 14:07:57.887: INFO: Pod "pod-configmaps-305fd120-e603-49a7-a1bd-5b17827c53ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011526417s
STEP: Saw pod success
Jun  7 14:07:57.887: INFO: Pod "pod-configmaps-305fd120-e603-49a7-a1bd-5b17827c53ae" satisfied condition "Succeeded or Failed"
Jun  7 14:07:57.891: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-configmaps-305fd120-e603-49a7-a1bd-5b17827c53ae container env-test: <nil>
STEP: delete the pod
Jun  7 14:07:57.914: INFO: Waiting for pod pod-configmaps-305fd120-e603-49a7-a1bd-5b17827c53ae to disappear
Jun  7 14:07:57.917: INFO: Pod pod-configmaps-305fd120-e603-49a7-a1bd-5b17827c53ae no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:07:57.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3414" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":173,"skipped":3025,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:07:57.927: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jun  7 14:07:57.977: INFO: Waiting up to 5m0s for pod "downward-api-01c2548f-8c3f-4617-b73e-f18987a66c48" in namespace "downward-api-8182" to be "Succeeded or Failed"
Jun  7 14:07:57.980: INFO: Pod "downward-api-01c2548f-8c3f-4617-b73e-f18987a66c48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.693498ms
Jun  7 14:07:59.994: INFO: Pod "downward-api-01c2548f-8c3f-4617-b73e-f18987a66c48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016687511s
STEP: Saw pod success
Jun  7 14:07:59.994: INFO: Pod "downward-api-01c2548f-8c3f-4617-b73e-f18987a66c48" satisfied condition "Succeeded or Failed"
Jun  7 14:07:59.998: INFO: Trying to get logs from node proact-prod01-wk002 pod downward-api-01c2548f-8c3f-4617-b73e-f18987a66c48 container dapi-container: <nil>
STEP: delete the pod
Jun  7 14:08:00.023: INFO: Waiting for pod downward-api-01c2548f-8c3f-4617-b73e-f18987a66c48 to disappear
Jun  7 14:08:00.026: INFO: Pod downward-api-01c2548f-8c3f-4617-b73e-f18987a66c48 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:08:00.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8182" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":174,"skipped":3070,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:08:00.040: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun  7 14:08:00.353: INFO: Pod name wrapped-volume-race-5de047b3-aa51-416c-a17e-8f1e64d4a9d1: Found 3 pods out of 5
Jun  7 14:08:05.372: INFO: Pod name wrapped-volume-race-5de047b3-aa51-416c-a17e-8f1e64d4a9d1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5de047b3-aa51-416c-a17e-8f1e64d4a9d1 in namespace emptydir-wrapper-1977, will wait for the garbage collector to delete the pods
Jun  7 14:08:15.462: INFO: Deleting ReplicationController wrapped-volume-race-5de047b3-aa51-416c-a17e-8f1e64d4a9d1 took: 10.628139ms
Jun  7 14:08:15.564: INFO: Terminating ReplicationController wrapped-volume-race-5de047b3-aa51-416c-a17e-8f1e64d4a9d1 pods took: 101.597131ms
STEP: Creating RC which spawns configmap-volume pods
Jun  7 14:08:19.305: INFO: Pod name wrapped-volume-race-41834fe5-0395-4895-aa54-91ebfda2730c: Found 0 pods out of 5
Jun  7 14:08:24.326: INFO: Pod name wrapped-volume-race-41834fe5-0395-4895-aa54-91ebfda2730c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-41834fe5-0395-4895-aa54-91ebfda2730c in namespace emptydir-wrapper-1977, will wait for the garbage collector to delete the pods
Jun  7 14:08:36.451: INFO: Deleting ReplicationController wrapped-volume-race-41834fe5-0395-4895-aa54-91ebfda2730c took: 12.415061ms
Jun  7 14:08:36.552: INFO: Terminating ReplicationController wrapped-volume-race-41834fe5-0395-4895-aa54-91ebfda2730c pods took: 100.18316ms
STEP: Creating RC which spawns configmap-volume pods
Jun  7 14:08:39.689: INFO: Pod name wrapped-volume-race-2c28101d-d8d4-450c-822a-08363825877d: Found 0 pods out of 5
Jun  7 14:08:44.708: INFO: Pod name wrapped-volume-race-2c28101d-d8d4-450c-822a-08363825877d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2c28101d-d8d4-450c-822a-08363825877d in namespace emptydir-wrapper-1977, will wait for the garbage collector to delete the pods
Jun  7 14:08:54.823: INFO: Deleting ReplicationController wrapped-volume-race-2c28101d-d8d4-450c-822a-08363825877d took: 17.985802ms
Jun  7 14:08:54.924: INFO: Terminating ReplicationController wrapped-volume-race-2c28101d-d8d4-450c-822a-08363825877d pods took: 100.471209ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:08:57.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1977" for this suite.

• [SLOW TEST:57.788 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":175,"skipped":3074,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:08:57.829: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jun  7 14:08:57.866: INFO: Waiting up to 5m0s for pod "security-context-97559c68-632b-4d5a-a902-d287226e69c8" in namespace "security-context-6961" to be "Succeeded or Failed"
Jun  7 14:08:57.868: INFO: Pod "security-context-97559c68-632b-4d5a-a902-d287226e69c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.808363ms
Jun  7 14:08:59.879: INFO: Pod "security-context-97559c68-632b-4d5a-a902-d287226e69c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013139664s
STEP: Saw pod success
Jun  7 14:08:59.879: INFO: Pod "security-context-97559c68-632b-4d5a-a902-d287226e69c8" satisfied condition "Succeeded or Failed"
Jun  7 14:08:59.884: INFO: Trying to get logs from node proact-prod01-wk002 pod security-context-97559c68-632b-4d5a-a902-d287226e69c8 container test-container: <nil>
STEP: delete the pod
Jun  7 14:08:59.905: INFO: Waiting for pod security-context-97559c68-632b-4d5a-a902-d287226e69c8 to disappear
Jun  7 14:08:59.908: INFO: Pod security-context-97559c68-632b-4d5a-a902-d287226e69c8 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:08:59.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6961" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":176,"skipped":3087,"failed":0}
SS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:08:59.924: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-4710/secret-test-10a679b6-b7c2-489d-aa42-6ab284d69bc3
STEP: Creating a pod to test consume secrets
Jun  7 14:08:59.969: INFO: Waiting up to 5m0s for pod "pod-configmaps-5f4a91cf-6fb6-4b67-83cd-11d1924cf944" in namespace "secrets-4710" to be "Succeeded or Failed"
Jun  7 14:08:59.972: INFO: Pod "pod-configmaps-5f4a91cf-6fb6-4b67-83cd-11d1924cf944": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597089ms
Jun  7 14:09:01.984: INFO: Pod "pod-configmaps-5f4a91cf-6fb6-4b67-83cd-11d1924cf944": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014282919s
STEP: Saw pod success
Jun  7 14:09:01.984: INFO: Pod "pod-configmaps-5f4a91cf-6fb6-4b67-83cd-11d1924cf944" satisfied condition "Succeeded or Failed"
Jun  7 14:09:01.989: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-configmaps-5f4a91cf-6fb6-4b67-83cd-11d1924cf944 container env-test: <nil>
STEP: delete the pod
Jun  7 14:09:02.013: INFO: Waiting for pod pod-configmaps-5f4a91cf-6fb6-4b67-83cd-11d1924cf944 to disappear
Jun  7 14:09:02.020: INFO: Pod pod-configmaps-5f4a91cf-6fb6-4b67-83cd-11d1924cf944 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:09:02.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4710" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":3089,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:09:02.034: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  7 14:09:02.764: INFO: starting watch
STEP: patching
STEP: updating
Jun  7 14:09:02.782: INFO: waiting for watch events with expected annotations
Jun  7 14:09:02.782: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:09:02.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9923" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":178,"skipped":3127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:09:02.844: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6489
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6489
STEP: creating replication controller externalsvc in namespace services-6489
I0607 14:09:02.930048      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6489, replica count: 2
I0607 14:09:05.981625      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun  7 14:09:06.030: INFO: Creating new exec pod
Jun  7 14:09:08.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6489 exec execpodbxqdw -- /bin/sh -x -c nslookup nodeport-service.services-6489.svc.cluster.local'
Jun  7 14:09:08.255: INFO: stderr: "+ nslookup nodeport-service.services-6489.svc.cluster.local\n"
Jun  7 14:09:08.255: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-6489.svc.cluster.local\tcanonical name = externalsvc.services-6489.svc.cluster.local.\nName:\texternalsvc.services-6489.svc.cluster.local\nAddress: 10.102.67.218\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6489, will wait for the garbage collector to delete the pods
Jun  7 14:09:08.322: INFO: Deleting ReplicationController externalsvc took: 12.444517ms
Jun  7 14:09:08.424: INFO: Terminating ReplicationController externalsvc pods took: 101.093801ms
Jun  7 14:09:10.448: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:09:10.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6489" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.623 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":179,"skipped":3154,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:09:10.467: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:09:10.492: INFO: Creating simple deployment test-new-deployment
Jun  7 14:09:10.501: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  7 14:09:12.555: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-7177  067610a0-fa0a-4b61-ac4f-2a0725c22764 33579342 3 2022-06-07 14:09:10 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-06-07 14:09:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 14:09:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a60cb78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-07 14:09:11 +0000 UTC,LastTransitionTime:2022-06-07 14:09:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2022-06-07 14:09:11 +0000 UTC,LastTransitionTime:2022-06-07 14:09:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  7 14:09:12.560: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-7177  fdb540b5-8375-45e7-bf01-e3f7e44cd002 33579341 2 2022-06-07 14:09:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 067610a0-fa0a-4b61-ac4f-2a0725c22764 0xc003bb5a67 0xc003bb5a68}] []  [{kube-controller-manager Update apps/v1 2022-06-07 14:09:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"067610a0-fa0a-4b61-ac4f-2a0725c22764\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 14:09:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bb5af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  7 14:09:12.566: INFO: Pod "test-new-deployment-847dcfb7fb-btsks" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-btsks test-new-deployment-847dcfb7fb- deployment-7177  e2d969c1-a36f-4b82-af60-7acb977621a3 33579336 0 2022-06-07 14:09:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:7412d086513eb5a31b95d91d16e42c0b0a0d2af5b14c5ede56031c840615c490 cni.projectcalico.org/podIP:192.168.39.218/32 cni.projectcalico.org/podIPs:192.168.39.218/32] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb fdb540b5-8375-45e7-bf01-e3f7e44cd002 0xc00a60cf67 0xc00a60cf68}] []  [{kube-controller-manager Update v1 2022-06-07 14:09:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdb540b5-8375-45e7-bf01-e3f7e44cd002\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-07 14:09:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-07 14:09:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.39.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zcbt5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zcbt5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:09:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:09:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:09:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:09:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.14,PodIP:192.168.39.218,StartTime:2022-06-07 14:09:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 14:09:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://d43717875bf27a685ddbc4c354048f2bd57ced22e1fe2ab8a2bc1bc519bf4322,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.39.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:09:12.566: INFO: Pod "test-new-deployment-847dcfb7fb-vqdps" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-vqdps test-new-deployment-847dcfb7fb- deployment-7177  a0798b73-fd24-4183-9e32-cbda6e49eb9a 33579347 0 2022-06-07 14:09:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb fdb540b5-8375-45e7-bf01-e3f7e44cd002 0xc00a60d177 0xc00a60d178}] []  [{kube-controller-manager Update v1 2022-06-07 14:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdb540b5-8375-45e7-bf01-e3f7e44cd002\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ffxsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ffxsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:09:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:09:12.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7177" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":180,"skipped":3161,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:09:12.577: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Jun  7 14:09:12.617: INFO: Waiting up to 5m0s for pod "client-containers-613a1c9b-7086-45a4-9cc4-af01bdf00bae" in namespace "containers-5438" to be "Succeeded or Failed"
Jun  7 14:09:12.624: INFO: Pod "client-containers-613a1c9b-7086-45a4-9cc4-af01bdf00bae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.992387ms
Jun  7 14:09:14.633: INFO: Pod "client-containers-613a1c9b-7086-45a4-9cc4-af01bdf00bae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016437518s
STEP: Saw pod success
Jun  7 14:09:14.633: INFO: Pod "client-containers-613a1c9b-7086-45a4-9cc4-af01bdf00bae" satisfied condition "Succeeded or Failed"
Jun  7 14:09:14.638: INFO: Trying to get logs from node proact-prod01-wk002 pod client-containers-613a1c9b-7086-45a4-9cc4-af01bdf00bae container agnhost-container: <nil>
STEP: delete the pod
Jun  7 14:09:14.664: INFO: Waiting for pod client-containers-613a1c9b-7086-45a4-9cc4-af01bdf00bae to disappear
Jun  7 14:09:14.668: INFO: Pod client-containers-613a1c9b-7086-45a4-9cc4-af01bdf00bae no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:09:14.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5438" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":181,"skipped":3170,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:09:14.679: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jun  7 14:09:16.746: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:09:18.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5416" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":182,"skipped":3191,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:09:18.818: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun  7 14:09:19.179: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 14:09:22.221: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:09:22.230: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:09:25.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2560" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.693 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":183,"skipped":3201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:09:25.511: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Jun  7 14:09:25.577: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Jun  7 14:09:27.604: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Jun  7 14:09:29.629: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:09:31.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-6872" for this suite.

• [SLOW TEST:6.163 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":184,"skipped":3242,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:09:31.674: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun  7 14:09:31.718: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  7 14:10:31.852: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:10:31.857: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jun  7 14:10:33.952: INFO: found a healthy node: proact-prod01-wk002
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:10:46.062: INFO: pods created so far: [1 1 1]
Jun  7 14:10:46.062: INFO: length of pods created so far: 3
Jun  7 14:10:48.085: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:10:55.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-325" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:10:55.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9110" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:83.552 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":185,"skipped":3252,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:10:55.226: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 14:10:55.266: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54d2c327-2499-4e38-a9c2-f1434d0b3a4b" in namespace "downward-api-5900" to be "Succeeded or Failed"
Jun  7 14:10:55.269: INFO: Pod "downwardapi-volume-54d2c327-2499-4e38-a9c2-f1434d0b3a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.423227ms
Jun  7 14:10:57.275: INFO: Pod "downwardapi-volume-54d2c327-2499-4e38-a9c2-f1434d0b3a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009216363s
Jun  7 14:10:59.289: INFO: Pod "downwardapi-volume-54d2c327-2499-4e38-a9c2-f1434d0b3a4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02253608s
STEP: Saw pod success
Jun  7 14:10:59.290: INFO: Pod "downwardapi-volume-54d2c327-2499-4e38-a9c2-f1434d0b3a4b" satisfied condition "Succeeded or Failed"
Jun  7 14:10:59.296: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-54d2c327-2499-4e38-a9c2-f1434d0b3a4b container client-container: <nil>
STEP: delete the pod
Jun  7 14:10:59.325: INFO: Waiting for pod downwardapi-volume-54d2c327-2499-4e38-a9c2-f1434d0b3a4b to disappear
Jun  7 14:10:59.328: INFO: Pod downwardapi-volume-54d2c327-2499-4e38-a9c2-f1434d0b3a4b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:10:59.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5900" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":186,"skipped":3285,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:10:59.341: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  7 14:11:01.410: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:11:01.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2943" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":346,"completed":187,"skipped":3297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:11:01.433: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-b9c12d40-b98a-477e-8947-39f770a0ca4e
STEP: Creating a pod to test consume configMaps
Jun  7 14:11:01.488: INFO: Waiting up to 5m0s for pod "pod-configmaps-7af67119-f066-4fe5-9114-c5519fcaaa55" in namespace "configmap-4425" to be "Succeeded or Failed"
Jun  7 14:11:01.492: INFO: Pod "pod-configmaps-7af67119-f066-4fe5-9114-c5519fcaaa55": Phase="Pending", Reason="", readiness=false. Elapsed: 3.372016ms
Jun  7 14:11:03.502: INFO: Pod "pod-configmaps-7af67119-f066-4fe5-9114-c5519fcaaa55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013661912s
STEP: Saw pod success
Jun  7 14:11:03.503: INFO: Pod "pod-configmaps-7af67119-f066-4fe5-9114-c5519fcaaa55" satisfied condition "Succeeded or Failed"
Jun  7 14:11:03.508: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-configmaps-7af67119-f066-4fe5-9114-c5519fcaaa55 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  7 14:11:03.529: INFO: Waiting for pod pod-configmaps-7af67119-f066-4fe5-9114-c5519fcaaa55 to disappear
Jun  7 14:11:03.532: INFO: Pod pod-configmaps-7af67119-f066-4fe5-9114-c5519fcaaa55 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:11:03.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4425" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":188,"skipped":3358,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:11:03.546: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jun  7 14:11:03.583: INFO: namespace kubectl-2972
Jun  7 14:11:03.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-2972 create -f -'
Jun  7 14:11:04.765: INFO: stderr: ""
Jun  7 14:11:04.765: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun  7 14:11:05.779: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  7 14:11:05.779: INFO: Found 0 / 1
Jun  7 14:11:06.773: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  7 14:11:06.773: INFO: Found 1 / 1
Jun  7 14:11:06.773: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  7 14:11:06.775: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  7 14:11:06.775: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  7 14:11:06.775: INFO: wait on agnhost-primary startup in kubectl-2972 
Jun  7 14:11:06.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-2972 logs agnhost-primary-57mrx agnhost-primary'
Jun  7 14:11:06.863: INFO: stderr: ""
Jun  7 14:11:06.863: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun  7 14:11:06.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-2972 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun  7 14:11:06.950: INFO: stderr: ""
Jun  7 14:11:06.950: INFO: stdout: "service/rm2 exposed\n"
Jun  7 14:11:06.954: INFO: Service rm2 in namespace kubectl-2972 found.
STEP: exposing service
Jun  7 14:11:08.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-2972 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun  7 14:11:09.064: INFO: stderr: ""
Jun  7 14:11:09.064: INFO: stdout: "service/rm3 exposed\n"
Jun  7 14:11:09.068: INFO: Service rm3 in namespace kubectl-2972 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:11:11.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2972" for this suite.

• [SLOW TEST:7.554 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1233
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":189,"skipped":3374,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:11:11.100: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 14:11:11.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f0ca16b-1829-4baf-9ec3-71c6ae2c6b27" in namespace "projected-3877" to be "Succeeded or Failed"
Jun  7 14:11:11.151: INFO: Pod "downwardapi-volume-5f0ca16b-1829-4baf-9ec3-71c6ae2c6b27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.968119ms
Jun  7 14:11:13.159: INFO: Pod "downwardapi-volume-5f0ca16b-1829-4baf-9ec3-71c6ae2c6b27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011032267s
STEP: Saw pod success
Jun  7 14:11:13.159: INFO: Pod "downwardapi-volume-5f0ca16b-1829-4baf-9ec3-71c6ae2c6b27" satisfied condition "Succeeded or Failed"
Jun  7 14:11:13.163: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-5f0ca16b-1829-4baf-9ec3-71c6ae2c6b27 container client-container: <nil>
STEP: delete the pod
Jun  7 14:11:13.185: INFO: Waiting for pod downwardapi-volume-5f0ca16b-1829-4baf-9ec3-71c6ae2c6b27 to disappear
Jun  7 14:11:13.188: INFO: Pod downwardapi-volume-5f0ca16b-1829-4baf-9ec3-71c6ae2c6b27 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:11:13.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3877" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":190,"skipped":3390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:11:13.203: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-qlh8
STEP: Creating a pod to test atomic-volume-subpath
Jun  7 14:11:13.252: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qlh8" in namespace "subpath-9548" to be "Succeeded or Failed"
Jun  7 14:11:13.254: INFO: Pod "pod-subpath-test-secret-qlh8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.229764ms
Jun  7 14:11:15.265: INFO: Pod "pod-subpath-test-secret-qlh8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013646773s
Jun  7 14:11:17.276: INFO: Pod "pod-subpath-test-secret-qlh8": Phase="Running", Reason="", readiness=true. Elapsed: 4.024459805s
Jun  7 14:11:19.290: INFO: Pod "pod-subpath-test-secret-qlh8": Phase="Running", Reason="", readiness=true. Elapsed: 6.038577065s
Jun  7 14:11:21.302: INFO: Pod "pod-subpath-test-secret-qlh8": Phase="Running", Reason="", readiness=true. Elapsed: 8.050732894s
Jun  7 14:11:23.311: INFO: Pod "pod-subpath-test-secret-qlh8": Phase="Running", Reason="", readiness=true. Elapsed: 10.059848036s
Jun  7 14:11:25.327: INFO: Pod "pod-subpath-test-secret-qlh8": Phase="Running", Reason="", readiness=true. Elapsed: 12.075406042s
Jun  7 14:11:27.334: INFO: Pod "pod-subpath-test-secret-qlh8": Phase="Running", Reason="", readiness=true. Elapsed: 14.082804108s
Jun  7 14:11:29.350: INFO: Pod "pod-subpath-test-secret-qlh8": Phase="Running", Reason="", readiness=true. Elapsed: 16.098565417s
Jun  7 14:11:31.364: INFO: Pod "pod-subpath-test-secret-qlh8": Phase="Running", Reason="", readiness=true. Elapsed: 18.11213975s
Jun  7 14:11:33.374: INFO: Pod "pod-subpath-test-secret-qlh8": Phase="Running", Reason="", readiness=true. Elapsed: 20.122818839s
Jun  7 14:11:35.385: INFO: Pod "pod-subpath-test-secret-qlh8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.13385501s
STEP: Saw pod success
Jun  7 14:11:35.386: INFO: Pod "pod-subpath-test-secret-qlh8" satisfied condition "Succeeded or Failed"
Jun  7 14:11:35.391: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-subpath-test-secret-qlh8 container test-container-subpath-secret-qlh8: <nil>
STEP: delete the pod
Jun  7 14:11:35.415: INFO: Waiting for pod pod-subpath-test-secret-qlh8 to disappear
Jun  7 14:11:35.418: INFO: Pod pod-subpath-test-secret-qlh8 no longer exists
STEP: Deleting pod pod-subpath-test-secret-qlh8
Jun  7 14:11:35.418: INFO: Deleting pod "pod-subpath-test-secret-qlh8" in namespace "subpath-9548"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:11:35.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9548" for this suite.

• [SLOW TEST:22.230 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":346,"completed":191,"skipped":3458,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:11:35.434: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun  7 14:11:35.486: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:11:39.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6856" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":192,"skipped":3500,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:11:39.725: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun  7 14:11:39.762: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5922  c20f4c35-0d3a-443a-9c5d-956ea0ed7ee8 33580825 0 2022-06-07 14:11:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-07 14:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 14:11:39.762: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5922  c20f4c35-0d3a-443a-9c5d-956ea0ed7ee8 33580825 0 2022-06-07 14:11:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-07 14:11:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun  7 14:11:49.784: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5922  c20f4c35-0d3a-443a-9c5d-956ea0ed7ee8 33580903 0 2022-06-07 14:11:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-07 14:11:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 14:11:49.784: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5922  c20f4c35-0d3a-443a-9c5d-956ea0ed7ee8 33580903 0 2022-06-07 14:11:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-07 14:11:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun  7 14:11:59.806: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5922  c20f4c35-0d3a-443a-9c5d-956ea0ed7ee8 33580954 0 2022-06-07 14:11:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-07 14:11:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 14:11:59.807: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5922  c20f4c35-0d3a-443a-9c5d-956ea0ed7ee8 33580954 0 2022-06-07 14:11:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-07 14:11:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun  7 14:12:09.832: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5922  c20f4c35-0d3a-443a-9c5d-956ea0ed7ee8 33581004 0 2022-06-07 14:11:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-07 14:11:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 14:12:09.832: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5922  c20f4c35-0d3a-443a-9c5d-956ea0ed7ee8 33581004 0 2022-06-07 14:11:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-07 14:11:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun  7 14:12:19.861: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5922  5d3da65d-eed3-490a-bd29-2795416cdeb1 33581055 0 2022-06-07 14:12:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-07 14:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 14:12:19.861: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5922  5d3da65d-eed3-490a-bd29-2795416cdeb1 33581055 0 2022-06-07 14:12:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-07 14:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun  7 14:12:29.876: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5922  5d3da65d-eed3-490a-bd29-2795416cdeb1 33581105 0 2022-06-07 14:12:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-07 14:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 14:12:29.876: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5922  5d3da65d-eed3-490a-bd29-2795416cdeb1 33581105 0 2022-06-07 14:12:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-07 14:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:12:39.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5922" for this suite.

• [SLOW TEST:60.187 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":193,"skipped":3506,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:12:39.913: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-0c60f184-c1a8-44e5-b7a4-b0d0242f02cb
STEP: Creating a pod to test consume configMaps
Jun  7 14:12:39.974: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b54431cf-51d4-4d46-a645-f3d49404d484" in namespace "projected-9699" to be "Succeeded or Failed"
Jun  7 14:12:39.976: INFO: Pod "pod-projected-configmaps-b54431cf-51d4-4d46-a645-f3d49404d484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.611583ms
Jun  7 14:12:41.990: INFO: Pod "pod-projected-configmaps-b54431cf-51d4-4d46-a645-f3d49404d484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015943065s
STEP: Saw pod success
Jun  7 14:12:41.990: INFO: Pod "pod-projected-configmaps-b54431cf-51d4-4d46-a645-f3d49404d484" satisfied condition "Succeeded or Failed"
Jun  7 14:12:41.994: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-configmaps-b54431cf-51d4-4d46-a645-f3d49404d484 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  7 14:12:42.027: INFO: Waiting for pod pod-projected-configmaps-b54431cf-51d4-4d46-a645-f3d49404d484 to disappear
Jun  7 14:12:42.030: INFO: Pod pod-projected-configmaps-b54431cf-51d4-4d46-a645-f3d49404d484 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:12:42.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9699" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":194,"skipped":3508,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:12:42.039: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jun  7 14:12:44.098: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2712 PodName:var-expansion-872785a3-7041-4c2a-8b0b-56e35109b601 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:12:44.098: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: test for file in mounted path
Jun  7 14:12:44.227: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2712 PodName:var-expansion-872785a3-7041-4c2a-8b0b-56e35109b601 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:12:44.227: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: updating the annotation value
Jun  7 14:12:44.867: INFO: Successfully updated pod "var-expansion-872785a3-7041-4c2a-8b0b-56e35109b601"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jun  7 14:12:44.872: INFO: Deleting pod "var-expansion-872785a3-7041-4c2a-8b0b-56e35109b601" in namespace "var-expansion-2712"
Jun  7 14:12:44.879: INFO: Wait up to 5m0s for pod "var-expansion-872785a3-7041-4c2a-8b0b-56e35109b601" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:13:18.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2712" for this suite.

• [SLOW TEST:36.880 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":195,"skipped":3512,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:13:18.919: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-90115b15-1415-440c-99d8-c6f21e3761d5
STEP: Creating a pod to test consume secrets
Jun  7 14:13:18.985: INFO: Waiting up to 5m0s for pod "pod-secrets-944739a1-e8a5-419d-876d-57b26f2dc8df" in namespace "secrets-4127" to be "Succeeded or Failed"
Jun  7 14:13:18.988: INFO: Pod "pod-secrets-944739a1-e8a5-419d-876d-57b26f2dc8df": Phase="Pending", Reason="", readiness=false. Elapsed: 3.158776ms
Jun  7 14:13:20.997: INFO: Pod "pod-secrets-944739a1-e8a5-419d-876d-57b26f2dc8df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0118887s
Jun  7 14:13:23.010: INFO: Pod "pod-secrets-944739a1-e8a5-419d-876d-57b26f2dc8df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02552056s
STEP: Saw pod success
Jun  7 14:13:23.010: INFO: Pod "pod-secrets-944739a1-e8a5-419d-876d-57b26f2dc8df" satisfied condition "Succeeded or Failed"
Jun  7 14:13:23.015: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-secrets-944739a1-e8a5-419d-876d-57b26f2dc8df container secret-volume-test: <nil>
STEP: delete the pod
Jun  7 14:13:23.044: INFO: Waiting for pod pod-secrets-944739a1-e8a5-419d-876d-57b26f2dc8df to disappear
Jun  7 14:13:23.048: INFO: Pod pod-secrets-944739a1-e8a5-419d-876d-57b26f2dc8df no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:13:23.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4127" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":196,"skipped":3514,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:13:23.061: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 14:13:23.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-046447d5-078c-4fa9-a3b1-7d3e01e98873" in namespace "downward-api-6227" to be "Succeeded or Failed"
Jun  7 14:13:23.114: INFO: Pod "downwardapi-volume-046447d5-078c-4fa9-a3b1-7d3e01e98873": Phase="Pending", Reason="", readiness=false. Elapsed: 3.060641ms
Jun  7 14:13:25.126: INFO: Pod "downwardapi-volume-046447d5-078c-4fa9-a3b1-7d3e01e98873": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015401815s
STEP: Saw pod success
Jun  7 14:13:25.126: INFO: Pod "downwardapi-volume-046447d5-078c-4fa9-a3b1-7d3e01e98873" satisfied condition "Succeeded or Failed"
Jun  7 14:13:25.132: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-046447d5-078c-4fa9-a3b1-7d3e01e98873 container client-container: <nil>
STEP: delete the pod
Jun  7 14:13:25.160: INFO: Waiting for pod downwardapi-volume-046447d5-078c-4fa9-a3b1-7d3e01e98873 to disappear
Jun  7 14:13:25.174: INFO: Pod downwardapi-volume-046447d5-078c-4fa9-a3b1-7d3e01e98873 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:13:25.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6227" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":197,"skipped":3515,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:13:25.223: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jun  7 14:13:25.285: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  7 14:13:30.303: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:13:30.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4594" for this suite.

• [SLOW TEST:5.122 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":198,"skipped":3552,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:13:30.344: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:13:37.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8488" for this suite.

• [SLOW TEST:7.087 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":199,"skipped":3571,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:13:37.432: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun  7 14:13:37.472: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 14:13:43.053: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:14:02.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4712" for this suite.

• [SLOW TEST:24.624 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":200,"skipped":3577,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:14:02.057: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jun  7 14:14:22.278: INFO: EndpointSlice for Service endpointslice-958/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:14:32.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-958" for this suite.

• [SLOW TEST:30.269 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":201,"skipped":3595,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:14:32.326: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 14:14:32.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51f30900-2612-4b16-aad2-c2be87fd3369" in namespace "projected-4843" to be "Succeeded or Failed"
Jun  7 14:14:32.392: INFO: Pod "downwardapi-volume-51f30900-2612-4b16-aad2-c2be87fd3369": Phase="Pending", Reason="", readiness=false. Elapsed: 3.873158ms
Jun  7 14:14:34.402: INFO: Pod "downwardapi-volume-51f30900-2612-4b16-aad2-c2be87fd3369": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01417508s
STEP: Saw pod success
Jun  7 14:14:34.402: INFO: Pod "downwardapi-volume-51f30900-2612-4b16-aad2-c2be87fd3369" satisfied condition "Succeeded or Failed"
Jun  7 14:14:34.407: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-51f30900-2612-4b16-aad2-c2be87fd3369 container client-container: <nil>
STEP: delete the pod
Jun  7 14:14:34.431: INFO: Waiting for pod downwardapi-volume-51f30900-2612-4b16-aad2-c2be87fd3369 to disappear
Jun  7 14:14:34.435: INFO: Pod downwardapi-volume-51f30900-2612-4b16-aad2-c2be87fd3369 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:14:34.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4843" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":202,"skipped":3597,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:14:34.449: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-78367f4e-1b0d-4099-98e1-d427b1c6538e
STEP: Creating a pod to test consume configMaps
Jun  7 14:14:34.517: INFO: Waiting up to 5m0s for pod "pod-configmaps-f98b11ac-0acb-4317-a21f-12379604e0f8" in namespace "configmap-6300" to be "Succeeded or Failed"
Jun  7 14:14:34.520: INFO: Pod "pod-configmaps-f98b11ac-0acb-4317-a21f-12379604e0f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.268477ms
Jun  7 14:14:36.531: INFO: Pod "pod-configmaps-f98b11ac-0acb-4317-a21f-12379604e0f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014555037s
STEP: Saw pod success
Jun  7 14:14:36.531: INFO: Pod "pod-configmaps-f98b11ac-0acb-4317-a21f-12379604e0f8" satisfied condition "Succeeded or Failed"
Jun  7 14:14:36.536: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-configmaps-f98b11ac-0acb-4317-a21f-12379604e0f8 container agnhost-container: <nil>
STEP: delete the pod
Jun  7 14:14:36.569: INFO: Waiting for pod pod-configmaps-f98b11ac-0acb-4317-a21f-12379604e0f8 to disappear
Jun  7 14:14:36.572: INFO: Pod pod-configmaps-f98b11ac-0acb-4317-a21f-12379604e0f8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:14:36.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6300" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":203,"skipped":3615,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:14:36.584: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-9151
STEP: creating replication controller nodeport-test in namespace services-9151
I0607 14:14:36.652436      23 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-9151, replica count: 2
Jun  7 14:14:39.704: INFO: Creating new exec pod
I0607 14:14:39.704522      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  7 14:14:42.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-9151 exec execpods5fcm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun  7 14:14:42.961: INFO: stderr: "+ + nc -v -t -w 2 nodeport-test 80\necho hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun  7 14:14:42.961: INFO: stdout: ""
Jun  7 14:14:43.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-9151 exec execpods5fcm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun  7 14:14:44.171: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun  7 14:14:44.171: INFO: stdout: ""
Jun  7 14:14:44.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-9151 exec execpods5fcm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun  7 14:14:45.154: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun  7 14:14:45.154: INFO: stdout: "nodeport-test-wknbn"
Jun  7 14:14:45.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-9151 exec execpods5fcm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.117.220 80'
Jun  7 14:14:45.369: INFO: stderr: "+ + echo hostNamenc\n -v -t -w 2 10.102.117.220 80\nConnection to 10.102.117.220 80 port [tcp/http] succeeded!\n"
Jun  7 14:14:45.369: INFO: stdout: "nodeport-test-wknbn"
Jun  7 14:14:45.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-9151 exec execpods5fcm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.55.210.13 31628'
Jun  7 14:14:45.556: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.55.210.13 31628\nConnection to 10.55.210.13 31628 port [tcp/*] succeeded!\n"
Jun  7 14:14:45.556: INFO: stdout: "nodeport-test-5lgz9"
Jun  7 14:14:45.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-9151 exec execpods5fcm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.55.210.16 31628'
Jun  7 14:14:45.746: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.55.210.16 31628\nConnection to 10.55.210.16 31628 port [tcp/*] succeeded!\n"
Jun  7 14:14:45.746: INFO: stdout: "nodeport-test-5lgz9"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:14:45.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9151" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.181 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":204,"skipped":3626,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:14:45.765: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  7 14:14:45.848: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:45.848: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:45.848: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:45.852: INFO: Number of nodes with available pods: 0
Jun  7 14:14:45.852: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:14:46.865: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:46.865: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:46.865: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:46.871: INFO: Number of nodes with available pods: 1
Jun  7 14:14:46.871: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:14:47.862: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:47.862: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:47.862: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:47.867: INFO: Number of nodes with available pods: 4
Jun  7 14:14:47.868: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun  7 14:14:47.896: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:47.896: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:47.896: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:47.900: INFO: Number of nodes with available pods: 3
Jun  7 14:14:47.900: INFO: Node proact-prod01-wk002 is running more than one daemon pod
Jun  7 14:14:48.915: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:48.916: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:48.916: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:48.922: INFO: Number of nodes with available pods: 3
Jun  7 14:14:48.923: INFO: Node proact-prod01-wk002 is running more than one daemon pod
Jun  7 14:14:49.910: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:49.910: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:49.910: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:49.916: INFO: Number of nodes with available pods: 3
Jun  7 14:14:49.916: INFO: Node proact-prod01-wk002 is running more than one daemon pod
Jun  7 14:14:50.920: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:50.920: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:50.920: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:50.924: INFO: Number of nodes with available pods: 3
Jun  7 14:14:50.924: INFO: Node proact-prod01-wk002 is running more than one daemon pod
Jun  7 14:14:51.912: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:51.912: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:51.912: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:14:51.916: INFO: Number of nodes with available pods: 4
Jun  7 14:14:51.916: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4272, will wait for the garbage collector to delete the pods
Jun  7 14:14:51.986: INFO: Deleting DaemonSet.extensions daemon-set took: 12.0713ms
Jun  7 14:14:52.088: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.336483ms
Jun  7 14:14:54.694: INFO: Number of nodes with available pods: 0
Jun  7 14:14:54.694: INFO: Number of running nodes: 0, number of available pods: 0
Jun  7 14:14:54.698: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33582508"},"items":null}

Jun  7 14:14:54.702: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33582508"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:14:54.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4272" for this suite.

• [SLOW TEST:8.974 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":205,"skipped":3641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:14:54.743: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:14:54.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4424" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":206,"skipped":3680,"failed":0}
SSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:14:54.842: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:20:00.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2155" for this suite.

• [SLOW TEST:306.111 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":207,"skipped":3684,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:20:00.954: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:20:03.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2717" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":208,"skipped":3691,"failed":0}
SSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:20:03.653: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jun  7 14:20:03.691: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  7 14:21:03.807: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:21:03.812: INFO: Starting informer...
STEP: Starting pod...
Jun  7 14:21:04.033: INFO: Pod is running on proact-prod01-wk002. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun  7 14:21:04.073: INFO: Pod wasn't evicted. Proceeding
Jun  7 14:21:04.073: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun  7 14:22:19.097: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:22:19.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7954" for this suite.

• [SLOW TEST:135.467 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":209,"skipped":3694,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:22:19.121: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-97
STEP: creating service affinity-clusterip-transition in namespace services-97
STEP: creating replication controller affinity-clusterip-transition in namespace services-97
I0607 14:22:19.195018      23 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-97, replica count: 3
I0607 14:22:22.249104      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  7 14:22:22.264: INFO: Creating new exec pod
Jun  7 14:22:25.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-97 exec execpod-affinitykssn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jun  7 14:22:25.542: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun  7 14:22:25.542: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 14:22:25.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-97 exec execpod-affinitykssn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.236.225 80'
Jun  7 14:22:25.752: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.236.225 80\nConnection to 10.101.236.225 80 port [tcp/http] succeeded!\n"
Jun  7 14:22:25.752: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 14:22:25.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-97 exec execpod-affinitykssn8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.236.225:80/ ; done'
Jun  7 14:22:26.054: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n"
Jun  7 14:22:26.054: INFO: stdout: "\naffinity-clusterip-transition-qhmsp\naffinity-clusterip-transition-hn9rp\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-qhmsp\naffinity-clusterip-transition-qhmsp\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-hn9rp\naffinity-clusterip-transition-hn9rp\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-hn9rp\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-qhmsp\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-hn9rp\naffinity-clusterip-transition-qhmsp\naffinity-clusterip-transition-qhmsp"
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-qhmsp
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-hn9rp
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-qhmsp
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-qhmsp
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-hn9rp
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-hn9rp
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-hn9rp
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-qhmsp
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-hn9rp
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-qhmsp
Jun  7 14:22:26.054: INFO: Received response from host: affinity-clusterip-transition-qhmsp
Jun  7 14:22:26.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-97 exec execpod-affinitykssn8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.236.225:80/ ; done'
Jun  7 14:22:26.299: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.236.225:80/\n"
Jun  7 14:22:26.299: INFO: stdout: "\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw\naffinity-clusterip-transition-xfbtw"
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Received response from host: affinity-clusterip-transition-xfbtw
Jun  7 14:22:26.299: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-97, will wait for the garbage collector to delete the pods
Jun  7 14:22:26.385: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.744942ms
Jun  7 14:22:26.486: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.338037ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:22:28.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-97" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.606 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":210,"skipped":3716,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:22:28.727: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:22:36.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4173" for this suite.

• [SLOW TEST:8.070 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":211,"skipped":3727,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:22:36.798: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jun  7 14:22:36.849: INFO: Waiting up to 5m0s for pod "security-context-81251212-15a5-4b33-b1d0-e1b8e85d8fce" in namespace "security-context-7767" to be "Succeeded or Failed"
Jun  7 14:22:36.852: INFO: Pod "security-context-81251212-15a5-4b33-b1d0-e1b8e85d8fce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.252518ms
Jun  7 14:22:38.858: INFO: Pod "security-context-81251212-15a5-4b33-b1d0-e1b8e85d8fce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008273065s
STEP: Saw pod success
Jun  7 14:22:38.858: INFO: Pod "security-context-81251212-15a5-4b33-b1d0-e1b8e85d8fce" satisfied condition "Succeeded or Failed"
Jun  7 14:22:38.862: INFO: Trying to get logs from node proact-prod01-wk002 pod security-context-81251212-15a5-4b33-b1d0-e1b8e85d8fce container test-container: <nil>
STEP: delete the pod
Jun  7 14:22:38.896: INFO: Waiting for pod security-context-81251212-15a5-4b33-b1d0-e1b8e85d8fce to disappear
Jun  7 14:22:38.899: INFO: Pod security-context-81251212-15a5-4b33-b1d0-e1b8e85d8fce no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:22:38.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7767" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":212,"skipped":3741,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:22:38.913: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-8409
STEP: creating service affinity-nodeport-transition in namespace services-8409
STEP: creating replication controller affinity-nodeport-transition in namespace services-8409
I0607 14:22:38.986952      23 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-8409, replica count: 3
I0607 14:22:42.038113      23 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  7 14:22:42.053: INFO: Creating new exec pod
Jun  7 14:22:45.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-8409 exec execpod-affinity2s5tr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jun  7 14:22:45.314: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun  7 14:22:45.314: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 14:22:45.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-8409 exec execpod-affinity2s5tr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.210.134 80'
Jun  7 14:22:45.489: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.210.134 80\nConnection to 10.107.210.134 80 port [tcp/http] succeeded!\n"
Jun  7 14:22:45.489: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 14:22:45.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-8409 exec execpod-affinity2s5tr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.55.210.15 31806'
Jun  7 14:22:45.687: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.55.210.15 31806\nConnection to 10.55.210.15 31806 port [tcp/*] succeeded!\n"
Jun  7 14:22:45.687: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 14:22:45.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-8409 exec execpod-affinity2s5tr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.55.210.14 31806'
Jun  7 14:22:45.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.55.210.14 31806\nConnection to 10.55.210.14 31806 port [tcp/*] succeeded!\n"
Jun  7 14:22:45.871: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 14:22:45.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-8409 exec execpod-affinity2s5tr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.55.210.13:31806/ ; done'
Jun  7 14:22:46.165: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n"
Jun  7 14:22:46.165: INFO: stdout: "\naffinity-nodeport-transition-kbp2c\naffinity-nodeport-transition-kbp2c\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-2s9p9\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-2s9p9\naffinity-nodeport-transition-2s9p9\naffinity-nodeport-transition-2s9p9\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-2s9p9\naffinity-nodeport-transition-kbp2c\naffinity-nodeport-transition-2s9p9\naffinity-nodeport-transition-9lzfs"
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-kbp2c
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-kbp2c
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-2s9p9
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-2s9p9
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-2s9p9
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-2s9p9
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-2s9p9
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-kbp2c
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-2s9p9
Jun  7 14:22:46.165: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-8409 exec execpod-affinity2s5tr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.55.210.13:31806/ ; done'
Jun  7 14:22:46.425: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:31806/\n"
Jun  7 14:22:46.425: INFO: stdout: "\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs\naffinity-nodeport-transition-9lzfs"
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Received response from host: affinity-nodeport-transition-9lzfs
Jun  7 14:22:46.425: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8409, will wait for the garbage collector to delete the pods
Jun  7 14:22:46.509: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.117931ms
Jun  7 14:22:46.609: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.444758ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:22:48.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8409" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.046 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":213,"skipped":3773,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:22:48.959: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun  7 14:22:49.003: INFO: Waiting up to 5m0s for pod "pod-f1d1985a-c855-4159-9a75-19b940fb8b49" in namespace "emptydir-9339" to be "Succeeded or Failed"
Jun  7 14:22:49.007: INFO: Pod "pod-f1d1985a-c855-4159-9a75-19b940fb8b49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.143148ms
Jun  7 14:22:51.018: INFO: Pod "pod-f1d1985a-c855-4159-9a75-19b940fb8b49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014269334s
STEP: Saw pod success
Jun  7 14:22:51.018: INFO: Pod "pod-f1d1985a-c855-4159-9a75-19b940fb8b49" satisfied condition "Succeeded or Failed"
Jun  7 14:22:51.021: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-f1d1985a-c855-4159-9a75-19b940fb8b49 container test-container: <nil>
STEP: delete the pod
Jun  7 14:22:51.050: INFO: Waiting for pod pod-f1d1985a-c855-4159-9a75-19b940fb8b49 to disappear
Jun  7 14:22:51.052: INFO: Pod pod-f1d1985a-c855-4159-9a75-19b940fb8b49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:22:51.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9339" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":3777,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:22:51.063: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun  7 14:22:51.094: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  7 14:22:51.101: INFO: Waiting for terminating namespaces to be deleted...
Jun  7 14:22:51.103: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk001 before test
Jun  7 14:22:51.118: INFO: argocd-application-controller-0 from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container application-controller ready: true, restart count 0
Jun  7 14:22:51.118: INFO: argocd-repo-server-5547b66bd9-dfnbc from argocd started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container repo-server ready: true, restart count 0
Jun  7 14:22:51.118: INFO: calico-node-mpbx2 from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container calico-node ready: true, restart count 0
Jun  7 14:22:51.118: INFO: mypostgres-7-0 from default started at 2022-05-24 07:06:43 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:22:51.118: INFO: 	Container mypostgres-7 ready: true, restart count 0
Jun  7 14:22:51.118: INFO: reviews-v1-545db77b95-z2hm8 from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:22:51.118: INFO: 	Container reviews ready: true, restart count 0
Jun  7 14:22:51.118: INFO: foobar-75685968d4-575v5 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.118: INFO: foobar-75685968d4-d24zc from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.118: INFO: foobar-75685968d4-dwnnl from foobar started at 2022-06-07 13:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.118: INFO: foobar-75685968d4-mlbsq from foobar started at 2022-06-07 13:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.118: INFO: foobar-75685968d4-s72dw from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.118: INFO: foobar-75685968d4-scvj7 from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.118: INFO: foobar-75685968d4-wtsqh from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.118: INFO: mypod from foobar2 started at 2022-04-28 07:54:48 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container myfrontend ready: true, restart count 0
Jun  7 14:22:51.118: INFO: nginx-deployment-66b6c48dd5-dbfcx from foobar2 started at 2022-04-28 08:35:34 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.118: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.118: INFO: nginx-deployment-66b6c48dd5-pfdqw from foobar2 started at 2022-04-28 07:46:04 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.119: INFO: nginx-deployment2-d5845d7c8-vfthc from foobar2 started at 2022-04-28 07:49:41 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container nginx2 ready: true, restart count 0
Jun  7 14:22:51.119: INFO: gitea-0 from gitea started at 2022-03-30 07:25:54 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container gitea ready: true, restart count 0
Jun  7 14:22:51.119: INFO: gitea-postgresql-0 from gitea started at 2022-03-30 07:25:56 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container gitea-postgresql ready: true, restart count 1
Jun  7 14:22:51.119: INFO: ingress-nginx-controller-848878cd85-9wqg9 from ingress-nginx started at 2022-06-03 07:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container controller ready: true, restart count 0
Jun  7 14:22:51.119: INFO: k8s-status-c44b9cb68-pcnd4 from k8s-status started at 2022-04-14 07:03:09 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container k8s-status ready: true, restart count 0
Jun  7 14:22:51.119: INFO: kube-proxy-4c7bj from kube-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  7 14:22:51.119: INFO: loki-fluent-bit-loki-gqzll from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 14:22:51.119: INFO: prometheus-alertmanager-6c845bbb9c-r6krx from monitoring started at 2022-05-23 17:37:20 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Jun  7 14:22:51.119: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Jun  7 14:22:51.119: INFO: prometheus-kube-state-metrics-6c44ff7fb6-9zx9s from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  7 14:22:51.119: INFO: prometheus-node-exporter-z6jpz from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 14:22:51.119: INFO: prometheus-pushgateway-86679dcf68-qk8x2 from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Jun  7 14:22:51.119: INFO: prometheus-server-869789c557-72jr6 from monitoring started at 2022-06-07 13:40:18 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container prometheus-server ready: true, restart count 0
Jun  7 14:22:51.119: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun  7 14:22:51.119: INFO: sonobuoy from sonobuoy started at 2022-06-07 13:32:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  7 14:22:51.119: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-525dj from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 14:22:51.119: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 14:22:51.119: INFO: trident-csi-fcff9fbb6-rs2fc from trident started at 2022-06-07 13:40:18 +0000 UTC (6 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  7 14:22:51.119: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  7 14:22:51.119: INFO: 	Container csi-resizer ready: true, restart count 0
Jun  7 14:22:51.119: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jun  7 14:22:51.119: INFO: 	Container trident-autosupport ready: true, restart count 0
Jun  7 14:22:51.119: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 14:22:51.119: INFO: trident-csi-kqw7h from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 14:22:51.119: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 14:22:51.119: INFO: trident-operator-56d66bb95b-255bx from trident started at 2022-03-18 12:31:45 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.119: INFO: 	Container trident-operator ready: true, restart count 0
Jun  7 14:22:51.119: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk002 before test
Jun  7 14:22:51.131: INFO: calico-node-sv82j from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.131: INFO: 	Container calico-node ready: true, restart count 1
Jun  7 14:22:51.131: INFO: kube-proxy-8sbg5 from kube-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.131: INFO: 	Container kube-proxy ready: true, restart count 1
Jun  7 14:22:51.131: INFO: loki-fluent-bit-loki-wgmgx from loki started at 2022-06-07 14:21:09 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.131: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 14:22:51.131: INFO: prometheus-node-exporter-gw2s9 from monitoring started at 2022-06-07 14:21:05 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.131: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 14:22:51.131: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-7w99f from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.131: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 14:22:51.131: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 14:22:51.131: INFO: trident-csi-b4p6l from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.131: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 14:22:51.131: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 14:22:51.131: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk003 before test
Jun  7 14:22:51.151: INFO: argocd-applicationset-controller-84bc7544cd-7zlhg from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.151: INFO: 	Container applicationset-controller ready: true, restart count 0
Jun  7 14:22:51.152: INFO: argocd-dex-server-6b5ffb8f5c-r5sf2 from argocd started at 2022-03-31 06:26:27 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container dex-server ready: true, restart count 0
Jun  7 14:22:51.152: INFO: calico-node-rmwmb from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container calico-node ready: true, restart count 0
Jun  7 14:22:51.152: INFO: calico-typha-54559758b4-6gmlw from calico-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container calico-typha ready: true, restart count 0
Jun  7 14:22:51.152: INFO: cert-manager-6bbf595697-lr27t from cert-manager started at 2022-06-03 08:46:47 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container cert-manager ready: true, restart count 0
Jun  7 14:22:51.152: INFO: cert-manager-cainjector-6bc9d758b-vh4wl from cert-manager started at 2022-06-03 08:46:47 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container cert-manager ready: true, restart count 0
Jun  7 14:22:51.152: INFO: mypostgres-11-0 from default started at 2022-06-07 14:21:14 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:22:51.152: INFO: 	Container mypostgres-11 ready: true, restart count 0
Jun  7 14:22:51.152: INFO: productpage-v1-6b746f74dc-xrnvw from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:22:51.152: INFO: 	Container productpage ready: true, restart count 0
Jun  7 14:22:51.152: INFO: ratings-v1-b6994bb9-vndct from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:22:51.152: INFO: 	Container ratings ready: true, restart count 0
Jun  7 14:22:51.152: INFO: reviews-v3-84779c7bbc-p4hjg from default started at 2022-04-22 12:33:02 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:22:51.152: INFO: 	Container reviews ready: true, restart count 0
Jun  7 14:22:51.152: INFO: dex-7b7c86db7-hcrrd from dex started at 2022-06-03 07:21:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container dex ready: true, restart count 0
Jun  7 14:22:51.152: INFO: dex-authenticator-dex-k8s-authenticator-5795dc9755-fgflk from dex started at 2022-03-22 15:07:54 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container dex-k8s-authenticator ready: true, restart count 0
Jun  7 14:22:51.152: INFO: foobar-75685968d4-7ktm8 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.152: INFO: foobar-75685968d4-cn6ww from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.152: INFO: foobar-75685968d4-fldfg from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.152: INFO: foobar-75685968d4-mjvcq from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.152: INFO: foobar-75685968d4-qpw2b from foobar started at 2022-06-07 13:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.152: INFO: foobar-75685968d4-wk7h8 from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.152: INFO: nginx-deployment-66b6c48dd5-kwzdn from foobar2 started at 2022-04-28 08:35:34 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.152: INFO: nginx-deployment2-d5845d7c8-bbm4g from foobar2 started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container nginx2 ready: true, restart count 0
Jun  7 14:22:51.152: INFO: simple from foobar2 started at 2022-04-27 08:47:49 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container simple ready: true, restart count 0
Jun  7 14:22:51.152: INFO: istio-egressgateway-66fdd867f4-drz9z from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:22:51.152: INFO: prometheus-699b7cc575-k5zl7 from istio-system started at 2022-06-07 13:40:18 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container prometheus-server ready: true, restart count 0
Jun  7 14:22:51.152: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Jun  7 14:22:51.152: INFO: k8s-status-c44b9cb68-dx6r8 from k8s-status started at 2022-04-14 07:03:09 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container k8s-status ready: true, restart count 0
Jun  7 14:22:51.152: INFO: kube-proxy-wx7st from kube-system started at 2022-03-10 14:40:37 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  7 14:22:51.152: INFO: kubegres-controller-manager-755b4c48f6-zxmlm from kubegres-system started at 2022-06-07 13:40:18 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  7 14:22:51.152: INFO: 	Container manager ready: true, restart count 0
Jun  7 14:22:51.152: INFO: loki-0 from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container loki ready: true, restart count 0
Jun  7 14:22:51.152: INFO: loki-fluent-bit-loki-ss7rt from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 14:22:51.152: INFO: prometheus-node-exporter-rz8ws from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 14:22:51.152: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-jxr44 from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 14:22:51.152: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 14:22:51.152: INFO: trident-csi-7wfz2 from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.152: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 14:22:51.152: INFO: 	Container trident-main ready: true, restart count 0
Jun  7 14:22:51.152: INFO: 
Logging pods the apiserver thinks is on node proact-prod01-wk004 before test
Jun  7 14:22:51.177: INFO: argocd-notifications-controller-6dd95488b4-l4t7z from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container notifications-controller ready: true, restart count 0
Jun  7 14:22:51.177: INFO: argocd-redis-57bcc665bf-lpcsm from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container argocd-redis ready: true, restart count 0
Jun  7 14:22:51.177: INFO: argocd-server-7b98b7446b-n7nmr from argocd started at 2022-03-31 06:26:28 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container server ready: true, restart count 0
Jun  7 14:22:51.177: INFO: calico-node-gmd98 from calico-system started at 2022-03-10 15:32:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container calico-node ready: true, restart count 0
Jun  7 14:22:51.177: INFO: cert-manager-webhook-586d45d5ff-pftz7 from cert-manager started at 2022-06-03 08:46:47 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container cert-manager ready: true, restart count 0
Jun  7 14:22:51.177: INFO: details-v1-79f774bdb9-pxpbj from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container details ready: true, restart count 0
Jun  7 14:22:51.177: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:22:51.177: INFO: mypostgres-6-0 from default started at 2022-05-16 17:57:42 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:22:51.177: INFO: 	Container mypostgres-6 ready: true, restart count 0
Jun  7 14:22:51.177: INFO: reviews-v2-7bf8c9648f-q86cj from default started at 2022-03-16 14:53:32 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:22:51.177: INFO: 	Container reviews ready: true, restart count 0
Jun  7 14:22:51.177: INFO: oauth2-oauth2-proxy-85748949c4-rlj6t from dex started at 2022-06-03 07:21:23 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container oauth2-proxy ready: true, restart count 0
Jun  7 14:22:51.177: INFO: foobar-75685968d4-2wjms from foobar started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.177: INFO: foobar-75685968d4-8k5dq from foobar started at 2022-03-25 12:14:06 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.177: INFO: foobar-75685968d4-98jz8 from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.177: INFO: foobar-75685968d4-qj6bn from foobar started at 2022-03-25 12:14:48 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.177: INFO: foobar-75685968d4-skd4f from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.177: INFO: foobar-75685968d4-vtjhk from foobar started at 2022-06-07 13:40:19 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.177: INFO: foobar-75685968d4-zgz7t from foobar started at 2022-03-25 12:16:24 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.177: INFO: nginx-deployment-66b6c48dd5-9zp8s from foobar2 started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.177: INFO: nginx-deployment-66b6c48dd5-psjh5 from foobar2 started at 2022-04-28 07:46:07 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.177: INFO: nginx-deployment2-d5845d7c8-xhm74 from foobar2 started at 2022-04-28 07:49:45 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container nginx2 ready: true, restart count 0
Jun  7 14:22:51.177: INFO: gitea-memcached-7b44bc5f74-6vssj from gitea started at 2022-03-30 07:25:52 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container memcached ready: true, restart count 0
Jun  7 14:22:51.177: INFO: ingress-nginx-controller-848878cd85-n5nsq from ingress-nginx started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container controller ready: true, restart count 0
Jun  7 14:22:51.177: INFO: grafana-6c5dc6df7c-t4q9p from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container grafana ready: true, restart count 0
Jun  7 14:22:51.177: INFO: istio-ingressgateway-77968dbd74-qrklf from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container istio-proxy ready: true, restart count 0
Jun  7 14:22:51.177: INFO: istiod-699b647f8b-lgg5l from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container discovery ready: true, restart count 0
Jun  7 14:22:51.177: INFO: jaeger-9dd685668-5frgx from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.177: INFO: 	Container jaeger ready: true, restart count 0
Jun  7 14:22:51.177: INFO: kiali-699f98c497-rvfkc from istio-system started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.178: INFO: 	Container kiali ready: true, restart count 0
Jun  7 14:22:51.178: INFO: foo-bf9cd6fb5-7nmh7 from jltest started at 2022-06-07 13:40:18 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.178: INFO: 	Container nginx ready: true, restart count 0
Jun  7 14:22:51.178: INFO: k8s-status-c44b9cb68-f4rv9 from k8s-status started at 2022-04-14 07:03:09 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.178: INFO: 	Container k8s-status ready: true, restart count 0
Jun  7 14:22:51.178: INFO: kube-proxy-t5klm from kube-system started at 2022-03-10 15:32:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.178: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  7 14:22:51.178: INFO: grafana-5ff8c85cf8-gv9c2 from loki started at 2022-04-25 05:59:11 +0000 UTC (3 container statuses recorded)
Jun  7 14:22:51.178: INFO: 	Container grafana ready: true, restart count 0
Jun  7 14:22:51.178: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jun  7 14:22:51.178: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Jun  7 14:22:51.178: INFO: loki-fluent-bit-loki-mrrh2 from loki started at 2022-04-12 08:56:07 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.178: INFO: 	Container fluent-bit-loki ready: true, restart count 0
Jun  7 14:22:51.178: INFO: testpod from loki started at 2022-03-28 08:55:49 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.178: INFO: 	Container testpod ready: true, restart count 71
Jun  7 14:22:51.178: INFO: prometheus-node-exporter-5ph5n from monitoring started at 2022-03-18 13:12:58 +0000 UTC (1 container statuses recorded)
Jun  7 14:22:51.178: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun  7 14:22:51.178: INFO: sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-pxgrv from sonobuoy started at 2022-06-07 13:32:51 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.178: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  7 14:22:51.178: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  7 14:22:51.178: INFO: trident-csi-cscfg from trident started at 2022-03-18 12:31:59 +0000 UTC (2 container statuses recorded)
Jun  7 14:22:51.178: INFO: 	Container driver-registrar ready: true, restart count 0
Jun  7 14:22:51.178: INFO: 	Container trident-main ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16f65ca3b1fca847], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 4 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:22:52.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3959" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":215,"skipped":3794,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:22:52.268: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jun  7 14:22:52.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-6395 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Jun  7 14:22:52.384: INFO: stderr: ""
Jun  7 14:22:52.384: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1528
Jun  7 14:22:52.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-6395 delete pods e2e-test-httpd-pod'
Jun  7 14:22:54.875: INFO: stderr: ""
Jun  7 14:22:54.875: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:22:54.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6395" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":216,"skipped":3814,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:22:54.893: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:22:54.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6429" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":217,"skipped":3818,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:22:54.988: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:23:23.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4501" for this suite.

• [SLOW TEST:28.138 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":218,"skipped":3854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:23:23.130: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jun  7 14:23:23.196: INFO: The status of Pod annotationupdate005053e2-4cad-49ed-9ef4-0184048dac1e is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:23:25.205: INFO: The status of Pod annotationupdate005053e2-4cad-49ed-9ef4-0184048dac1e is Running (Ready = true)
Jun  7 14:23:25.738: INFO: Successfully updated pod "annotationupdate005053e2-4cad-49ed-9ef4-0184048dac1e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:23:29.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4431" for this suite.

• [SLOW TEST:6.655 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":219,"skipped":3878,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:23:29.785: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Jun  7 14:23:29.834: INFO: Major version: 1
STEP: Confirm minor version
Jun  7 14:23:29.834: INFO: cleanMinorVersion: 22
Jun  7 14:23:29.834: INFO: Minor version: 22
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:23:29.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-9032" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":220,"skipped":3901,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:23:29.851: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:23:29.903: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c146dcf9-b300-453d-a49d-bb4c4081d7e5" in namespace "security-context-test-5176" to be "Succeeded or Failed"
Jun  7 14:23:29.906: INFO: Pod "alpine-nnp-false-c146dcf9-b300-453d-a49d-bb4c4081d7e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.785125ms
Jun  7 14:23:31.914: INFO: Pod "alpine-nnp-false-c146dcf9-b300-453d-a49d-bb4c4081d7e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010972969s
Jun  7 14:23:33.923: INFO: Pod "alpine-nnp-false-c146dcf9-b300-453d-a49d-bb4c4081d7e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019643121s
Jun  7 14:23:33.923: INFO: Pod "alpine-nnp-false-c146dcf9-b300-453d-a49d-bb4c4081d7e5" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:23:33.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5176" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":221,"skipped":3906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:23:33.955: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:23:33.999: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-6433
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:23:40.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-1453" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:23:40.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6433" for this suite.

• [SLOW TEST:6.193 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":222,"skipped":3961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:23:40.150: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-93485255-0ae8-4881-83c8-629d0b20e8fd
STEP: Creating a pod to test consume secrets
Jun  7 14:23:40.247: INFO: Waiting up to 5m0s for pod "pod-secrets-5076047d-c5a2-4512-8b54-7cdfb100796f" in namespace "secrets-2757" to be "Succeeded or Failed"
Jun  7 14:23:40.252: INFO: Pod "pod-secrets-5076047d-c5a2-4512-8b54-7cdfb100796f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.976922ms
Jun  7 14:23:42.258: INFO: Pod "pod-secrets-5076047d-c5a2-4512-8b54-7cdfb100796f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011396924s
STEP: Saw pod success
Jun  7 14:23:42.258: INFO: Pod "pod-secrets-5076047d-c5a2-4512-8b54-7cdfb100796f" satisfied condition "Succeeded or Failed"
Jun  7 14:23:42.262: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-secrets-5076047d-c5a2-4512-8b54-7cdfb100796f container secret-volume-test: <nil>
STEP: delete the pod
Jun  7 14:23:42.292: INFO: Waiting for pod pod-secrets-5076047d-c5a2-4512-8b54-7cdfb100796f to disappear
Jun  7 14:23:42.295: INFO: Pod pod-secrets-5076047d-c5a2-4512-8b54-7cdfb100796f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:23:42.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2757" for this suite.
STEP: Destroying namespace "secret-namespace-6011" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":223,"skipped":3986,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:23:42.320: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun  7 14:23:42.368: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  7 14:24:42.501: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Jun  7 14:24:42.537: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun  7 14:24:42.544: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun  7 14:24:42.568: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun  7 14:24:42.576: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun  7 14:24:42.595: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun  7 14:24:42.600: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jun  7 14:24:42.617: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jun  7 14:24:42.620: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:24:56.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6591" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:74.512 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":224,"skipped":4024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:24:56.835: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Jun  7 14:24:56.879: INFO: created test-podtemplate-1
Jun  7 14:24:56.884: INFO: created test-podtemplate-2
Jun  7 14:24:56.889: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jun  7 14:24:56.893: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jun  7 14:24:56.912: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:24:56.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4565" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":225,"skipped":4065,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:24:56.932: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Jun  7 14:24:56.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-238 create -f -'
Jun  7 14:24:57.824: INFO: stderr: ""
Jun  7 14:24:57.824: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jun  7 14:24:57.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-238 diff -f -'
Jun  7 14:24:58.044: INFO: rc: 1
Jun  7 14:24:58.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-238 delete -f -'
Jun  7 14:24:58.114: INFO: stderr: ""
Jun  7 14:24:58.114: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:24:58.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-238" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":226,"skipped":4072,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:24:58.133: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-c0a7615c-f747-4314-9843-c74a6fb51a43 in namespace container-probe-2547
Jun  7 14:25:00.194: INFO: Started pod busybox-c0a7615c-f747-4314-9843-c74a6fb51a43 in namespace container-probe-2547
STEP: checking the pod's current state and verifying that restartCount is present
Jun  7 14:25:00.198: INFO: Initial restart count of pod busybox-c0a7615c-f747-4314-9843-c74a6fb51a43 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:29:01.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2547" for this suite.

• [SLOW TEST:243.234 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":4082,"failed":0}
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:29:01.369: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jun  7 14:29:01.415: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:29:04.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1454" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":228,"skipped":4090,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:29:04.516: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-8b7397af-66ca-472c-906d-6716663aeba8 in namespace container-probe-1102
Jun  7 14:29:06.584: INFO: Started pod test-webserver-8b7397af-66ca-472c-906d-6716663aeba8 in namespace container-probe-1102
STEP: checking the pod's current state and verifying that restartCount is present
Jun  7 14:29:06.589: INFO: Initial restart count of pod test-webserver-8b7397af-66ca-472c-906d-6716663aeba8 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:33:07.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1102" for this suite.

• [SLOW TEST:243.349 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":229,"skipped":4102,"failed":0}
SSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:33:07.865: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:33:07.910: INFO: Waiting up to 5m0s for pod "busybox-user-65534-4dfa6714-f5c9-4c43-b23b-29724260a676" in namespace "security-context-test-628" to be "Succeeded or Failed"
Jun  7 14:33:07.913: INFO: Pod "busybox-user-65534-4dfa6714-f5c9-4c43-b23b-29724260a676": Phase="Pending", Reason="", readiness=false. Elapsed: 3.648263ms
Jun  7 14:33:09.919: INFO: Pod "busybox-user-65534-4dfa6714-f5c9-4c43-b23b-29724260a676": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009014845s
Jun  7 14:33:09.919: INFO: Pod "busybox-user-65534-4dfa6714-f5c9-4c43-b23b-29724260a676" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:33:09.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-628" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":230,"skipped":4105,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:33:09.936: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun  7 14:33:09.985: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  7 14:34:10.112: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 4/5 of node resources.
Jun  7 14:34:10.147: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun  7 14:34:10.157: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun  7 14:34:10.170: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun  7 14:34:10.180: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun  7 14:34:10.196: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun  7 14:34:10.201: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jun  7 14:34:10.216: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jun  7 14:34:10.223: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:34:16.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6803" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:66.485 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":231,"skipped":4120,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:34:16.421: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:34:16.472: INFO: The status of Pod pod-secrets-694d4c6a-bb19-4433-9f44-e76ad3b80f30 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:34:18.483: INFO: The status of Pod pod-secrets-694d4c6a-bb19-4433-9f44-e76ad3b80f30 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:34:18.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-839" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":232,"skipped":4126,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:34:18.542: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:34:18.608: INFO: Create a RollingUpdate DaemonSet
Jun  7 14:34:18.614: INFO: Check that daemon pods launch on every node of the cluster
Jun  7 14:34:18.620: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:18.620: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:18.620: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:18.624: INFO: Number of nodes with available pods: 0
Jun  7 14:34:18.624: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:34:19.636: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:19.636: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:19.636: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:19.645: INFO: Number of nodes with available pods: 1
Jun  7 14:34:19.645: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:34:20.636: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:20.637: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:20.637: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:20.642: INFO: Number of nodes with available pods: 4
Jun  7 14:34:20.644: INFO: Number of running nodes: 4, number of available pods: 4
Jun  7 14:34:20.644: INFO: Update the DaemonSet to trigger a rollout
Jun  7 14:34:20.658: INFO: Updating DaemonSet daemon-set
Jun  7 14:34:23.684: INFO: Roll back the DaemonSet before rollout is complete
Jun  7 14:34:23.696: INFO: Updating DaemonSet daemon-set
Jun  7 14:34:23.697: INFO: Make sure DaemonSet rollback is complete
Jun  7 14:34:23.701: INFO: Wrong image for pod: daemon-set-ddxrx. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Jun  7 14:34:23.701: INFO: Pod daemon-set-ddxrx is not available
Jun  7 14:34:23.710: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:23.712: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:23.712: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:24.741: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:24.741: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:24.741: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:25.738: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:25.738: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:25.738: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:26.727: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:26.727: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:26.727: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:27.733: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:27.733: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:27.733: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:28.719: INFO: Pod daemon-set-hkxj6 is not available
Jun  7 14:34:28.725: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:28.725: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:34:28.725: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7063, will wait for the garbage collector to delete the pods
Jun  7 14:34:28.794: INFO: Deleting DaemonSet.extensions daemon-set took: 7.092234ms
Jun  7 14:34:28.896: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.581661ms
Jun  7 14:34:31.003: INFO: Number of nodes with available pods: 0
Jun  7 14:34:31.003: INFO: Number of running nodes: 0, number of available pods: 0
Jun  7 14:34:31.008: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33590446"},"items":null}

Jun  7 14:34:31.013: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33590446"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:34:31.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7063" for this suite.

• [SLOW TEST:12.520 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":233,"skipped":4136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:34:31.063: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-56bbe879-a085-4f1c-ad6a-636362090926
STEP: Creating a pod to test consume configMaps
Jun  7 14:34:31.130: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e47233b2-e2e7-44f1-a6b3-9da7275495f6" in namespace "projected-2144" to be "Succeeded or Failed"
Jun  7 14:34:31.134: INFO: Pod "pod-projected-configmaps-e47233b2-e2e7-44f1-a6b3-9da7275495f6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.780552ms
Jun  7 14:34:33.144: INFO: Pod "pod-projected-configmaps-e47233b2-e2e7-44f1-a6b3-9da7275495f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013681987s
STEP: Saw pod success
Jun  7 14:34:33.144: INFO: Pod "pod-projected-configmaps-e47233b2-e2e7-44f1-a6b3-9da7275495f6" satisfied condition "Succeeded or Failed"
Jun  7 14:34:33.149: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-configmaps-e47233b2-e2e7-44f1-a6b3-9da7275495f6 container agnhost-container: <nil>
STEP: delete the pod
Jun  7 14:34:33.178: INFO: Waiting for pod pod-projected-configmaps-e47233b2-e2e7-44f1-a6b3-9da7275495f6 to disappear
Jun  7 14:34:33.182: INFO: Pod pod-projected-configmaps-e47233b2-e2e7-44f1-a6b3-9da7275495f6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:34:33.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2144" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":234,"skipped":4177,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:34:33.193: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Jun  7 14:34:33.229: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun  7 14:34:33.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-5194 create -f -'
Jun  7 14:34:33.448: INFO: stderr: ""
Jun  7 14:34:33.448: INFO: stdout: "service/agnhost-replica created\n"
Jun  7 14:34:33.448: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun  7 14:34:33.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-5194 create -f -'
Jun  7 14:34:34.192: INFO: stderr: ""
Jun  7 14:34:34.192: INFO: stdout: "service/agnhost-primary created\n"
Jun  7 14:34:34.192: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun  7 14:34:34.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-5194 create -f -'
Jun  7 14:34:34.399: INFO: stderr: ""
Jun  7 14:34:34.399: INFO: stdout: "service/frontend created\n"
Jun  7 14:34:34.399: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun  7 14:34:34.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-5194 create -f -'
Jun  7 14:34:35.174: INFO: stderr: ""
Jun  7 14:34:35.174: INFO: stdout: "deployment.apps/frontend created\n"
Jun  7 14:34:35.174: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  7 14:34:35.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-5194 create -f -'
Jun  7 14:34:35.386: INFO: stderr: ""
Jun  7 14:34:35.386: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun  7 14:34:35.386: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  7 14:34:35.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-5194 create -f -'
Jun  7 14:34:35.605: INFO: stderr: ""
Jun  7 14:34:35.605: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jun  7 14:34:35.605: INFO: Waiting for all frontend pods to be Running.
Jun  7 14:34:40.656: INFO: Waiting for frontend to serve content.
Jun  7 14:34:45.674: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Jun  7 14:34:50.696: INFO: Trying to add a new entry to the guestbook.
Jun  7 14:34:50.715: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun  7 14:34:50.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-5194 delete --grace-period=0 --force -f -'
Jun  7 14:34:50.924: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  7 14:34:50.924: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jun  7 14:34:50.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-5194 delete --grace-period=0 --force -f -'
Jun  7 14:34:51.019: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  7 14:34:51.019: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun  7 14:34:51.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-5194 delete --grace-period=0 --force -f -'
Jun  7 14:34:51.100: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  7 14:34:51.100: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun  7 14:34:51.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-5194 delete --grace-period=0 --force -f -'
Jun  7 14:34:51.162: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  7 14:34:51.162: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun  7 14:34:51.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-5194 delete --grace-period=0 --force -f -'
Jun  7 14:34:51.215: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  7 14:34:51.215: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun  7 14:34:51.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-5194 delete --grace-period=0 --force -f -'
Jun  7 14:34:51.268: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  7 14:34:51.268: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:34:51.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5194" for this suite.

• [SLOW TEST:18.092 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":235,"skipped":4181,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:34:51.285: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6069
Jun  7 14:34:51.331: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:34:53.339: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jun  7 14:34:53.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6069 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun  7 14:34:53.708: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jun  7 14:34:53.708: INFO: stdout: "iptables"
Jun  7 14:34:53.708: INFO: proxyMode: iptables
Jun  7 14:34:53.729: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun  7 14:34:53.735: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6069
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6069
I0607 14:34:53.773129      23 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6069, replica count: 3
I0607 14:34:56.824324      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  7 14:34:56.837: INFO: Creating new exec pod
Jun  7 14:34:59.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6069 exec execpod-affinity6v8fq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jun  7 14:35:00.081: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jun  7 14:35:00.081: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 14:35:00.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6069 exec execpod-affinity6v8fq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.170.181 80'
Jun  7 14:35:00.297: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.170.181 80\nConnection to 10.99.170.181 80 port [tcp/http] succeeded!\n"
Jun  7 14:35:00.297: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 14:35:00.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6069 exec execpod-affinity6v8fq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.170.181:80/ ; done'
Jun  7 14:35:00.591: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n"
Jun  7 14:35:00.591: INFO: stdout: "\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk\naffinity-clusterip-timeout-kr6lk"
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Received response from host: affinity-clusterip-timeout-kr6lk
Jun  7 14:35:00.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6069 exec execpod-affinity6v8fq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.99.170.181:80/'
Jun  7 14:35:00.792: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n"
Jun  7 14:35:00.792: INFO: stdout: "affinity-clusterip-timeout-kr6lk"
Jun  7 14:35:20.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-6069 exec execpod-affinity6v8fq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.99.170.181:80/'
Jun  7 14:35:21.006: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.99.170.181:80/\n"
Jun  7 14:35:21.006: INFO: stdout: "affinity-clusterip-timeout-z8klw"
Jun  7 14:35:21.006: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6069, will wait for the garbage collector to delete the pods
Jun  7 14:35:21.102: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 12.449157ms
Jun  7 14:35:21.204: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.001123ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:35:23.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6069" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:31.955 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":236,"skipped":4191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:35:23.241: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-c72701d1-9a8d-4cf1-907a-10aaa1f512b5 in namespace container-probe-6449
Jun  7 14:35:25.294: INFO: Started pod liveness-c72701d1-9a8d-4cf1-907a-10aaa1f512b5 in namespace container-probe-6449
STEP: checking the pod's current state and verifying that restartCount is present
Jun  7 14:35:25.298: INFO: Initial restart count of pod liveness-c72701d1-9a8d-4cf1-907a-10aaa1f512b5 is 0
Jun  7 14:35:45.408: INFO: Restart count of pod container-probe-6449/liveness-c72701d1-9a8d-4cf1-907a-10aaa1f512b5 is now 1 (20.110098877s elapsed)
Jun  7 14:36:05.513: INFO: Restart count of pod container-probe-6449/liveness-c72701d1-9a8d-4cf1-907a-10aaa1f512b5 is now 2 (40.215187361s elapsed)
Jun  7 14:36:25.631: INFO: Restart count of pod container-probe-6449/liveness-c72701d1-9a8d-4cf1-907a-10aaa1f512b5 is now 3 (1m0.332540228s elapsed)
Jun  7 14:36:45.735: INFO: Restart count of pod container-probe-6449/liveness-c72701d1-9a8d-4cf1-907a-10aaa1f512b5 is now 4 (1m20.436737351s elapsed)
Jun  7 14:37:56.045: INFO: Restart count of pod container-probe-6449/liveness-c72701d1-9a8d-4cf1-907a-10aaa1f512b5 is now 5 (2m30.747369243s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:37:56.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6449" for this suite.

• [SLOW TEST:152.839 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":237,"skipped":4229,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:37:56.082: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-42791e4f-fb52-4a55-842e-8356845eb415
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:37:56.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1316" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":238,"skipped":4276,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:37:56.130: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun  7 14:37:56.166: INFO: Waiting up to 5m0s for pod "pod-e7ca4f9b-4f90-4f4a-a8ea-c185e9c40585" in namespace "emptydir-6654" to be "Succeeded or Failed"
Jun  7 14:37:56.169: INFO: Pod "pod-e7ca4f9b-4f90-4f4a-a8ea-c185e9c40585": Phase="Pending", Reason="", readiness=false. Elapsed: 2.574456ms
Jun  7 14:37:58.177: INFO: Pod "pod-e7ca4f9b-4f90-4f4a-a8ea-c185e9c40585": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010742825s
STEP: Saw pod success
Jun  7 14:37:58.177: INFO: Pod "pod-e7ca4f9b-4f90-4f4a-a8ea-c185e9c40585" satisfied condition "Succeeded or Failed"
Jun  7 14:37:58.188: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-e7ca4f9b-4f90-4f4a-a8ea-c185e9c40585 container test-container: <nil>
STEP: delete the pod
Jun  7 14:37:58.242: INFO: Waiting for pod pod-e7ca4f9b-4f90-4f4a-a8ea-c185e9c40585 to disappear
Jun  7 14:37:58.261: INFO: Pod pod-e7ca4f9b-4f90-4f4a-a8ea-c185e9c40585 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:37:58.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6654" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":239,"skipped":4332,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:37:58.284: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun  7 14:37:58.330: INFO: Waiting up to 5m0s for pod "pod-8e2b954d-dd7f-4944-8cc4-b8fd9977e788" in namespace "emptydir-3801" to be "Succeeded or Failed"
Jun  7 14:37:58.333: INFO: Pod "pod-8e2b954d-dd7f-4944-8cc4-b8fd9977e788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.980761ms
Jun  7 14:38:00.340: INFO: Pod "pod-8e2b954d-dd7f-4944-8cc4-b8fd9977e788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009546695s
STEP: Saw pod success
Jun  7 14:38:00.340: INFO: Pod "pod-8e2b954d-dd7f-4944-8cc4-b8fd9977e788" satisfied condition "Succeeded or Failed"
Jun  7 14:38:00.345: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-8e2b954d-dd7f-4944-8cc4-b8fd9977e788 container test-container: <nil>
STEP: delete the pod
Jun  7 14:38:00.368: INFO: Waiting for pod pod-8e2b954d-dd7f-4944-8cc4-b8fd9977e788 to disappear
Jun  7 14:38:00.371: INFO: Pod pod-8e2b954d-dd7f-4944-8cc4-b8fd9977e788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:38:00.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3801" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":240,"skipped":4347,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:38:00.387: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jun  7 14:38:00.440: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jun  7 14:38:00.449: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun  7 14:38:00.449: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jun  7 14:38:00.459: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun  7 14:38:00.459: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jun  7 14:38:00.469: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun  7 14:38:00.469: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jun  7 14:38:07.518: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:38:07.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6358" for this suite.

• [SLOW TEST:7.165 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":241,"skipped":4353,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:38:07.552: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 14:38:07.590: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aaf62db5-5c00-463b-8c3b-76c51fb18980" in namespace "projected-1448" to be "Succeeded or Failed"
Jun  7 14:38:07.595: INFO: Pod "downwardapi-volume-aaf62db5-5c00-463b-8c3b-76c51fb18980": Phase="Pending", Reason="", readiness=false. Elapsed: 4.996134ms
Jun  7 14:38:09.605: INFO: Pod "downwardapi-volume-aaf62db5-5c00-463b-8c3b-76c51fb18980": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014680636s
STEP: Saw pod success
Jun  7 14:38:09.605: INFO: Pod "downwardapi-volume-aaf62db5-5c00-463b-8c3b-76c51fb18980" satisfied condition "Succeeded or Failed"
Jun  7 14:38:09.611: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-aaf62db5-5c00-463b-8c3b-76c51fb18980 container client-container: <nil>
STEP: delete the pod
Jun  7 14:38:09.637: INFO: Waiting for pod downwardapi-volume-aaf62db5-5c00-463b-8c3b-76c51fb18980 to disappear
Jun  7 14:38:09.640: INFO: Pod downwardapi-volume-aaf62db5-5c00-463b-8c3b-76c51fb18980 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:38:09.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1448" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4386,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:38:09.661: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-49aa4a77-c8ed-4ab2-a8ea-23267c1112f8
STEP: Creating a pod to test consume configMaps
Jun  7 14:38:09.723: INFO: Waiting up to 5m0s for pod "pod-configmaps-4792b00b-6973-4320-92e1-9c0c5109c4dd" in namespace "configmap-6068" to be "Succeeded or Failed"
Jun  7 14:38:09.726: INFO: Pod "pod-configmaps-4792b00b-6973-4320-92e1-9c0c5109c4dd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.148452ms
Jun  7 14:38:11.738: INFO: Pod "pod-configmaps-4792b00b-6973-4320-92e1-9c0c5109c4dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015370193s
STEP: Saw pod success
Jun  7 14:38:11.738: INFO: Pod "pod-configmaps-4792b00b-6973-4320-92e1-9c0c5109c4dd" satisfied condition "Succeeded or Failed"
Jun  7 14:38:11.745: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-configmaps-4792b00b-6973-4320-92e1-9c0c5109c4dd container agnhost-container: <nil>
STEP: delete the pod
Jun  7 14:38:11.773: INFO: Waiting for pod pod-configmaps-4792b00b-6973-4320-92e1-9c0c5109c4dd to disappear
Jun  7 14:38:11.777: INFO: Pod pod-configmaps-4792b00b-6973-4320-92e1-9c0c5109c4dd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:38:11.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6068" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":243,"skipped":4397,"failed":0}
SS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:38:11.794: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  7 14:38:11.855: INFO: starting watch
STEP: patching
STEP: updating
Jun  7 14:38:11.864: INFO: waiting for watch events with expected annotations
Jun  7 14:38:11.864: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:38:11.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-8371" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":244,"skipped":4399,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:38:11.905: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:38:11.973: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8e376703-a28f-4272-9f16-5d5b608403b4", Controller:(*bool)(0xc005fcb46a), BlockOwnerDeletion:(*bool)(0xc005fcb46b)}}
Jun  7 14:38:11.980: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"dbe86fc4-236d-4feb-ba98-e7e83e40a4af", Controller:(*bool)(0xc005fcb6d2), BlockOwnerDeletion:(*bool)(0xc005fcb6d3)}}
Jun  7 14:38:11.985: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7243aa82-0a15-422e-b7b1-626320c34dce", Controller:(*bool)(0xc005fcb942), BlockOwnerDeletion:(*bool)(0xc005fcb943)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:38:16.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5883" for this suite.

• [SLOW TEST:5.110 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":245,"skipped":4410,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:38:17.015: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jun  7 14:38:17.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3654 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun  7 14:38:17.149: INFO: stderr: ""
Jun  7 14:38:17.150: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun  7 14:38:22.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3654 get pod e2e-test-httpd-pod -o json'
Jun  7 14:38:22.259: INFO: stderr: ""
Jun  7 14:38:22.259: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"03134661a720b57decdee11cfdbc4c7031d4a35aa0c31d520d449e303405ac13\",\n            \"cni.projectcalico.org/podIP\": \"192.168.39.224/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.39.224/32\"\n        },\n        \"creationTimestamp\": \"2022-06-07T14:38:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3654\",\n        \"resourceVersion\": \"33592445\",\n        \"uid\": \"d9bb0afb-7845-460f-b541-6e17a4bcb31f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-trcbk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"proact-prod01-wk002\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-trcbk\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-07T14:38:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-07T14:38:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-07T14:38:17Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-07T14:38:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://6f1820a0c99c893742b73e2a7da6f79329eaab357936408142c59707adc35802\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-06-07T14:38:17Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.55.210.14\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.39.224\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.39.224\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-06-07T14:38:17Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun  7 14:38:22.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3654 replace -f -'
Jun  7 14:38:22.454: INFO: stderr: ""
Jun  7 14:38:22.454: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Jun  7 14:38:22.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3654 delete pods e2e-test-httpd-pod'
Jun  7 14:38:24.074: INFO: stderr: ""
Jun  7 14:38:24.074: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:38:24.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3654" for this suite.

• [SLOW TEST:7.077 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1555
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":246,"skipped":4412,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:38:24.093: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-6310efb5-264b-4895-b231-b362a4d84313
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:38:24.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5132" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":247,"skipped":4466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:38:24.145: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-23852a4f-04ab-4d6a-a76b-70883ef5a863
STEP: Creating a pod to test consume secrets
Jun  7 14:38:24.193: INFO: Waiting up to 5m0s for pod "pod-secrets-6a13df92-d6c6-4d4c-be31-4b26759f01a1" in namespace "secrets-4562" to be "Succeeded or Failed"
Jun  7 14:38:24.196: INFO: Pod "pod-secrets-6a13df92-d6c6-4d4c-be31-4b26759f01a1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.522066ms
Jun  7 14:38:26.209: INFO: Pod "pod-secrets-6a13df92-d6c6-4d4c-be31-4b26759f01a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016104985s
STEP: Saw pod success
Jun  7 14:38:26.209: INFO: Pod "pod-secrets-6a13df92-d6c6-4d4c-be31-4b26759f01a1" satisfied condition "Succeeded or Failed"
Jun  7 14:38:26.213: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-secrets-6a13df92-d6c6-4d4c-be31-4b26759f01a1 container secret-env-test: <nil>
STEP: delete the pod
Jun  7 14:38:26.239: INFO: Waiting for pod pod-secrets-6a13df92-d6c6-4d4c-be31-4b26759f01a1 to disappear
Jun  7 14:38:26.242: INFO: Pod pod-secrets-6a13df92-d6c6-4d4c-be31-4b26759f01a1 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:38:26.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4562" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":248,"skipped":4500,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:38:26.256: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-643ef722-58bb-4e76-bd7f-6bd3d9d0080d
STEP: Creating a pod to test consume configMaps
Jun  7 14:38:26.300: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-20bb765a-f392-4cdf-8d98-96e0edcdbacc" in namespace "projected-5039" to be "Succeeded or Failed"
Jun  7 14:38:26.303: INFO: Pod "pod-projected-configmaps-20bb765a-f392-4cdf-8d98-96e0edcdbacc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.969301ms
Jun  7 14:38:28.310: INFO: Pod "pod-projected-configmaps-20bb765a-f392-4cdf-8d98-96e0edcdbacc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00957824s
STEP: Saw pod success
Jun  7 14:38:28.310: INFO: Pod "pod-projected-configmaps-20bb765a-f392-4cdf-8d98-96e0edcdbacc" satisfied condition "Succeeded or Failed"
Jun  7 14:38:28.314: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-configmaps-20bb765a-f392-4cdf-8d98-96e0edcdbacc container agnhost-container: <nil>
STEP: delete the pod
Jun  7 14:38:28.337: INFO: Waiting for pod pod-projected-configmaps-20bb765a-f392-4cdf-8d98-96e0edcdbacc to disappear
Jun  7 14:38:28.340: INFO: Pod pod-projected-configmaps-20bb765a-f392-4cdf-8d98-96e0edcdbacc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:38:28.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5039" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":249,"skipped":4521,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:38:28.353: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-3767
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-3767
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3767
Jun  7 14:38:28.400: INFO: Found 0 stateful pods, waiting for 1
Jun  7 14:38:38.410: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun  7 14:38:38.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-3767 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  7 14:38:38.641: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  7 14:38:38.641: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  7 14:38:38.641: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  7 14:38:38.649: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  7 14:38:48.660: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  7 14:38:48.660: INFO: Waiting for statefulset status.replicas updated to 0
Jun  7 14:38:48.681: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jun  7 14:38:48.681: INFO: ss-0  proact-prod01-wk002  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:28 +0000 UTC  }]
Jun  7 14:38:48.681: INFO: 
Jun  7 14:38:48.681: INFO: StatefulSet ss has not reached scale 3, at 1
Jun  7 14:38:49.688: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996640326s
Jun  7 14:38:50.698: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989514481s
Jun  7 14:38:51.709: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978842921s
Jun  7 14:38:52.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969139555s
Jun  7 14:38:53.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.959957616s
Jun  7 14:38:54.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.952580071s
Jun  7 14:38:55.746: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.941351172s
Jun  7 14:38:56.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.930917981s
Jun  7 14:38:57.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 921.62282ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3767
Jun  7 14:38:58.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-3767 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  7 14:38:58.953: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  7 14:38:58.953: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  7 14:38:58.953: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  7 14:38:58.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-3767 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  7 14:38:59.148: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  7 14:38:59.148: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  7 14:38:59.148: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  7 14:38:59.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-3767 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  7 14:38:59.366: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  7 14:38:59.366: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  7 14:38:59.366: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  7 14:38:59.373: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jun  7 14:39:09.380: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  7 14:39:09.380: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  7 14:39:09.381: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun  7 14:39:09.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-3767 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  7 14:39:09.611: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  7 14:39:09.611: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  7 14:39:09.611: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  7 14:39:09.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-3767 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  7 14:39:09.802: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  7 14:39:09.802: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  7 14:39:09.802: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  7 14:39:09.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=statefulset-3767 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  7 14:39:09.996: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  7 14:39:09.996: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  7 14:39:09.996: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  7 14:39:09.996: INFO: Waiting for statefulset status.replicas updated to 0
Jun  7 14:39:10.002: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun  7 14:39:20.018: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  7 14:39:20.018: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  7 14:39:20.018: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  7 14:39:20.036: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jun  7 14:39:20.036: INFO: ss-0  proact-prod01-wk002  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:39:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:39:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:28 +0000 UTC  }]
Jun  7 14:39:20.036: INFO: ss-1  proact-prod01-wk001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:39:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:39:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:48 +0000 UTC  }]
Jun  7 14:39:20.036: INFO: ss-2  proact-prod01-wk004  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:39:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:39:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:48 +0000 UTC  }]
Jun  7 14:39:20.036: INFO: 
Jun  7 14:39:20.036: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  7 14:39:21.048: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Jun  7 14:39:21.048: INFO: ss-0  proact-prod01-wk002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:39:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:39:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:28 +0000 UTC  }]
Jun  7 14:39:21.048: INFO: ss-1  proact-prod01-wk001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:39:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:39:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:48 +0000 UTC  }]
Jun  7 14:39:21.048: INFO: ss-2  proact-prod01-wk004  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:39:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:39:10 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:38:48 +0000 UTC  }]
Jun  7 14:39:21.048: INFO: 
Jun  7 14:39:21.048: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  7 14:39:22.056: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.98192813s
Jun  7 14:39:23.063: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.974612201s
Jun  7 14:39:24.070: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.967607327s
Jun  7 14:39:25.076: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.961022923s
Jun  7 14:39:26.085: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.954618802s
Jun  7 14:39:27.097: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.945340841s
Jun  7 14:39:28.105: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.933922772s
Jun  7 14:39:29.110: INFO: Verifying statefulset ss doesn't scale past 0 for another 926.216683ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3767
Jun  7 14:39:30.120: INFO: Scaling statefulset ss to 0
Jun  7 14:39:30.139: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun  7 14:39:30.143: INFO: Deleting all statefulset in ns statefulset-3767
Jun  7 14:39:30.148: INFO: Scaling statefulset ss to 0
Jun  7 14:39:30.163: INFO: Waiting for statefulset status.replicas updated to 0
Jun  7 14:39:30.166: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:39:30.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3767" for this suite.

• [SLOW TEST:61.841 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":250,"skipped":4524,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:39:30.194: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun  7 14:39:30.238: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1322  47bc916c-78a9-4507-9d62-f80b760b7ea4 33593148 0 2022-06-07 14:39:30 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-06-07 14:39:30 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-874nz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-874nz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:30.242: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:39:32.254: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jun  7 14:39:32.254: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1322 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:39:32.254: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Verifying customized DNS server is configured on pod...
Jun  7 14:39:32.424: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1322 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:39:32.424: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 14:39:32.554: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:39:32.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1322" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":251,"skipped":4533,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:39:32.591: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jun  7 14:39:32.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-1395 create -f -'
Jun  7 14:39:32.852: INFO: stderr: ""
Jun  7 14:39:32.852: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  7 14:39:32.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-1395 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  7 14:39:32.921: INFO: stderr: ""
Jun  7 14:39:32.921: INFO: stdout: "update-demo-nautilus-5k68n update-demo-nautilus-7n4rm "
Jun  7 14:39:32.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-1395 get pods update-demo-nautilus-5k68n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  7 14:39:32.986: INFO: stderr: ""
Jun  7 14:39:32.986: INFO: stdout: ""
Jun  7 14:39:32.986: INFO: update-demo-nautilus-5k68n is created but not running
Jun  7 14:39:37.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-1395 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  7 14:39:38.057: INFO: stderr: ""
Jun  7 14:39:38.057: INFO: stdout: "update-demo-nautilus-5k68n update-demo-nautilus-7n4rm "
Jun  7 14:39:38.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-1395 get pods update-demo-nautilus-5k68n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  7 14:39:38.112: INFO: stderr: ""
Jun  7 14:39:38.112: INFO: stdout: "true"
Jun  7 14:39:38.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-1395 get pods update-demo-nautilus-5k68n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  7 14:39:38.169: INFO: stderr: ""
Jun  7 14:39:38.169: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun  7 14:39:38.169: INFO: validating pod update-demo-nautilus-5k68n
Jun  7 14:39:38.176: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  7 14:39:38.176: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  7 14:39:38.176: INFO: update-demo-nautilus-5k68n is verified up and running
Jun  7 14:39:38.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-1395 get pods update-demo-nautilus-7n4rm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  7 14:39:38.236: INFO: stderr: ""
Jun  7 14:39:38.236: INFO: stdout: "true"
Jun  7 14:39:38.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-1395 get pods update-demo-nautilus-7n4rm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  7 14:39:38.298: INFO: stderr: ""
Jun  7 14:39:38.298: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jun  7 14:39:38.298: INFO: validating pod update-demo-nautilus-7n4rm
Jun  7 14:39:38.308: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  7 14:39:38.308: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  7 14:39:38.308: INFO: update-demo-nautilus-7n4rm is verified up and running
STEP: using delete to clean up resources
Jun  7 14:39:38.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-1395 delete --grace-period=0 --force -f -'
Jun  7 14:39:38.382: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  7 14:39:38.382: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  7 14:39:38.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-1395 get rc,svc -l name=update-demo --no-headers'
Jun  7 14:39:38.444: INFO: stderr: "No resources found in kubectl-1395 namespace.\n"
Jun  7 14:39:38.445: INFO: stdout: ""
Jun  7 14:39:38.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-1395 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  7 14:39:38.503: INFO: stderr: ""
Jun  7 14:39:38.503: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:39:38.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1395" for this suite.

• [SLOW TEST:5.930 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":252,"skipped":4570,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:39:38.522: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 14:39:38.966: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 14:39:42.014: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun  7 14:39:42.050: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:39:42.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2947" for this suite.
STEP: Destroying namespace "webhook-2947-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":253,"skipped":4578,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:39:42.143: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun  7 14:39:42.714: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 14:39:45.755: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:39:45.766: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:39:48.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5963" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.859 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":254,"skipped":4578,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:39:49.006: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:39:49.044: INFO: Creating deployment "webserver-deployment"
Jun  7 14:39:49.048: INFO: Waiting for observed generation 1
Jun  7 14:39:51.061: INFO: Waiting for all required pods to come up
Jun  7 14:39:51.070: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun  7 14:39:51.070: INFO: Waiting for deployment "webserver-deployment" to complete
Jun  7 14:39:51.079: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun  7 14:39:51.089: INFO: Updating deployment webserver-deployment
Jun  7 14:39:51.090: INFO: Waiting for observed generation 2
Jun  7 14:39:53.101: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun  7 14:39:53.107: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun  7 14:39:53.112: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  7 14:39:53.126: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun  7 14:39:53.126: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun  7 14:39:53.130: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  7 14:39:53.139: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun  7 14:39:53.139: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun  7 14:39:53.151: INFO: Updating deployment webserver-deployment
Jun  7 14:39:53.151: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun  7 14:39:53.160: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun  7 14:39:53.165: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  7 14:39:53.178: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6044  dfdb92a1-fa0a-443b-a66a-0f54506dd1d3 33593772 3 2022-06-07 14:39:49 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00368c3f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-07 14:39:50 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2022-06-07 14:39:51 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun  7 14:39:53.183: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-6044  9532580d-e167-4c4c-8269-06147b9f96fc 33593775 3 2022-06-07 14:39:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment dfdb92a1-fa0a-443b-a66a-0f54506dd1d3 0xc00368c807 0xc00368c808}] []  [{kube-controller-manager Update apps/v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfdb92a1-fa0a-443b-a66a-0f54506dd1d3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00368c8a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  7 14:39:53.183: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun  7 14:39:53.183: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-6044  ba83e3c4-2d15-4879-bd92-12a78259e62d 33593773 3 2022-06-07 14:39:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment dfdb92a1-fa0a-443b-a66a-0f54506dd1d3 0xc00368c907 0xc00368c908}] []  [{kube-controller-manager Update apps/v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfdb92a1-fa0a-443b-a66a-0f54506dd1d3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 14:39:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00368c998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun  7 14:39:53.208: INFO: Pod "webserver-deployment-795d758f88-9jkdk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9jkdk webserver-deployment-795d758f88- deployment-6044  e69c42d3-f586-4ef5-81b0-8203d742666d 33593784 0 2022-06-07 14:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9532580d-e167-4c4c-8269-06147b9f96fc 0xc00368ce27 0xc00368ce28}] []  [{kube-controller-manager Update v1 2022-06-07 14:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9532580d-e167-4c4c-8269-06147b9f96fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7nbq2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7nbq2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.209: INFO: Pod "webserver-deployment-795d758f88-b57sv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-b57sv webserver-deployment-795d758f88- deployment-6044  07eb83e2-b744-48de-8901-0199178d3ee3 33593746 0 2022-06-07 14:39:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:4480548433fa20c2bd66a56b6983a14fa08a6fc72b23a2ed94ae89aae243bc09 cni.projectcalico.org/podIP:192.168.46.123/32 cni.projectcalico.org/podIPs:192.168.46.123/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9532580d-e167-4c4c-8269-06147b9f96fc 0xc00368cf90 0xc00368cf91}] []  [{calico Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9532580d-e167-4c4c-8269-06147b9f96fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62ntw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62ntw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.15,PodIP:,StartTime:2022-06-07 14:39:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.210: INFO: Pod "webserver-deployment-795d758f88-gv9h4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gv9h4 webserver-deployment-795d758f88- deployment-6044  8bd0b0ba-cbf6-4f60-a5f7-6af96cc2fef7 33593745 0 2022-06-07 14:39:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:9c2ac83674fb2e88317785ac442f10ae27d08cbef748d5dc4eb8371d7520c925 cni.projectcalico.org/podIP:192.168.56.198/32 cni.projectcalico.org/podIPs:192.168.56.198/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9532580d-e167-4c4c-8269-06147b9f96fc 0xc00368d187 0xc00368d188}] []  [{calico Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9532580d-e167-4c4c-8269-06147b9f96fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p47sw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p47sw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.13,PodIP:,StartTime:2022-06-07 14:39:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.212: INFO: Pod "webserver-deployment-795d758f88-hkvdt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hkvdt webserver-deployment-795d758f88- deployment-6044  5c7fbc93-5289-4f6a-b145-535740b73e89 33593755 0 2022-06-07 14:39:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:59c1f5c1818527ac5def641c67e8345a607a5b727fca7dd1631850680024d546 cni.projectcalico.org/podIP:192.168.39.255/32 cni.projectcalico.org/podIPs:192.168.39.255/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9532580d-e167-4c4c-8269-06147b9f96fc 0xc00368d377 0xc00368d378}] []  [{kube-controller-manager Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9532580d-e167-4c4c-8269-06147b9f96fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-07 14:39:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jkfhb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jkfhb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.14,PodIP:,StartTime:2022-06-07 14:39:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.213: INFO: Pod "webserver-deployment-795d758f88-lrqg6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lrqg6 webserver-deployment-795d758f88- deployment-6044  16ef8e5a-9bd3-43f5-abaf-4fafd169a5c5 33593744 0 2022-06-07 14:39:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:9cf098ed64f856ca46c961afef860826229105bfd21fda5b4a4260c746f99f6e cni.projectcalico.org/podIP:192.168.7.8/32 cni.projectcalico.org/podIPs:192.168.7.8/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9532580d-e167-4c4c-8269-06147b9f96fc 0xc00368d597 0xc00368d598}] []  [{calico Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9532580d-e167-4c4c-8269-06147b9f96fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cz4sk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cz4sk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.16,PodIP:,StartTime:2022-06-07 14:39:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.213: INFO: Pod "webserver-deployment-795d758f88-ngrsz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ngrsz webserver-deployment-795d758f88- deployment-6044  07b40115-3569-44e1-a3fa-5ca3a6621cd6 33593786 0 2022-06-07 14:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9532580d-e167-4c4c-8269-06147b9f96fc 0xc00368d787 0xc00368d788}] []  [{kube-controller-manager Update v1 2022-06-07 14:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9532580d-e167-4c4c-8269-06147b9f96fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lf8qh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lf8qh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.213: INFO: Pod "webserver-deployment-795d758f88-nhhzb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nhhzb webserver-deployment-795d758f88- deployment-6044  bcc6f153-f9fb-42db-980e-cfb7d8ac667e 33593783 0 2022-06-07 14:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9532580d-e167-4c4c-8269-06147b9f96fc 0xc00368d8e0 0xc00368d8e1}] []  [{kube-controller-manager Update v1 2022-06-07 14:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9532580d-e167-4c4c-8269-06147b9f96fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l7q2b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l7q2b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.214: INFO: Pod "webserver-deployment-795d758f88-tvnnj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tvnnj webserver-deployment-795d758f88- deployment-6044  80267eff-6df3-4d53-89bd-a586b8197f46 33593743 0 2022-06-07 14:39:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/containerID:e76e002698797a86841305383e74c79faae6cab4067695dcfdc4676f40bcd3d6 cni.projectcalico.org/podIP:192.168.39.203/32 cni.projectcalico.org/podIPs:192.168.39.203/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9532580d-e167-4c4c-8269-06147b9f96fc 0xc00368da47 0xc00368da48}] []  [{calico Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9532580d-e167-4c4c-8269-06147b9f96fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 14:39:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bwgrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bwgrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.14,PodIP:,StartTime:2022-06-07 14:39:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.215: INFO: Pod "webserver-deployment-847dcfb7fb-2g9s2" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2g9s2 webserver-deployment-847dcfb7fb- deployment-6044  3e486e83-9192-418d-9dcf-2f966b0ab379 33593679 0 2022-06-07 14:39:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:2182f0047ef0b3e52d0bd75fcdfdc86fcc6f9931fa1d2c029415d6ed1beb4528 cni.projectcalico.org/podIP:192.168.46.117/32 cni.projectcalico.org/podIPs:192.168.46.117/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb ba83e3c4-2d15-4879-bd92-12a78259e62d 0xc00368dc37 0xc00368dc38}] []  [{calico Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba83e3c4-2d15-4879-bd92-12a78259e62d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 14:39:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.46.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mc99,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mc99,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.15,PodIP:192.168.46.117,StartTime:2022-06-07 14:39:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 14:39:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://d36285dd14e109d1417f1089aa6ecfab0a127f422ba3221e81bc9050e67c420f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.46.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.215: INFO: Pod "webserver-deployment-847dcfb7fb-5hdd8" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-5hdd8 webserver-deployment-847dcfb7fb- deployment-6044  8e3a8485-43cf-4512-83dd-66bed59a8fc5 33593664 0 2022-06-07 14:39:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:bdee7d0023ab251d8b9eb6bdc8acfff9be4b087c85c1d200fa6740be54f479b9 cni.projectcalico.org/podIP:192.168.39.196/32 cni.projectcalico.org/podIPs:192.168.39.196/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb ba83e3c4-2d15-4879-bd92-12a78259e62d 0xc00368de27 0xc00368de28}] []  [{calico Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba83e3c4-2d15-4879-bd92-12a78259e62d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 14:39:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.39.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvr2r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvr2r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.14,PodIP:192.168.39.196,StartTime:2022-06-07 14:39:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 14:39:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://b2ddaf208589e7331593e786cb31754ff6cfda4c2506a231a3cc56e632575739,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.39.196,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.215: INFO: Pod "webserver-deployment-847dcfb7fb-djrwl" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-djrwl webserver-deployment-847dcfb7fb- deployment-6044  ee58c493-1dd6-4936-9eae-5f931f26ab7f 33593781 0 2022-06-07 14:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb ba83e3c4-2d15-4879-bd92-12a78259e62d 0xc008e4a017 0xc008e4a018}] []  [{kube-controller-manager Update v1 2022-06-07 14:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba83e3c4-2d15-4879-bd92-12a78259e62d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2kfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2kfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.215: INFO: Pod "webserver-deployment-847dcfb7fb-lq6st" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-lq6st webserver-deployment-847dcfb7fb- deployment-6044  3fa77540-6d65-43e4-88a4-5a8497b1028e 33593651 0 2022-06-07 14:39:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:68e63b01fb4abeb57201bf4e8fdbfb0397fe54c8b6fae5e52991022ba0934f07 cni.projectcalico.org/podIP:192.168.56.215/32 cni.projectcalico.org/podIPs:192.168.56.215/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb ba83e3c4-2d15-4879-bd92-12a78259e62d 0xc008e4a180 0xc008e4a181}] []  [{calico Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba83e3c4-2d15-4879-bd92-12a78259e62d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 14:39:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.56.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jp9ds,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jp9ds,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.13,PodIP:192.168.56.215,StartTime:2022-06-07 14:39:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 14:39:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://8679c2993db09eb30dc13706e36138836cb120ab0874dd6e8b45eced8ae20d03,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.56.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.215: INFO: Pod "webserver-deployment-847dcfb7fb-m5dg2" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-m5dg2 webserver-deployment-847dcfb7fb- deployment-6044  1cddea3f-267b-4ab2-9115-409af73dd37f 33593654 0 2022-06-07 14:39:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:9a24b996098198e4e5f60a8dc67594dad0f777de02fad53e0d8266c73362693a cni.projectcalico.org/podIP:192.168.56.212/32 cni.projectcalico.org/podIPs:192.168.56.212/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb ba83e3c4-2d15-4879-bd92-12a78259e62d 0xc008e4a377 0xc008e4a378}] []  [{calico Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba83e3c4-2d15-4879-bd92-12a78259e62d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 14:39:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.56.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jh5v7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jh5v7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.13,PodIP:192.168.56.212,StartTime:2022-06-07 14:39:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 14:39:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://067b97ad7ae704c1fa0fbbd83548a6e324b4ffe29760e71a2b3c4d6848b63b91,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.56.212,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.216: INFO: Pod "webserver-deployment-847dcfb7fb-qtrxb" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-qtrxb webserver-deployment-847dcfb7fb- deployment-6044  9d5cb84b-c904-4c4f-b778-73df5eb6e676 33593787 0 2022-06-07 14:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb ba83e3c4-2d15-4879-bd92-12a78259e62d 0xc008e4a567 0xc008e4a568}] []  [{kube-controller-manager Update v1 2022-06-07 14:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba83e3c4-2d15-4879-bd92-12a78259e62d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x4pkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x4pkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.216: INFO: Pod "webserver-deployment-847dcfb7fb-rqxmn" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-rqxmn webserver-deployment-847dcfb7fb- deployment-6044  d129372b-8304-4914-8216-2e4bb65c51cf 33593673 0 2022-06-07 14:39:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:3f0764f1bf6c22d603198ce179010c0650008935e4e4ac1fb393abd03f88e464 cni.projectcalico.org/podIP:192.168.7.38/32 cni.projectcalico.org/podIPs:192.168.7.38/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb ba83e3c4-2d15-4879-bd92-12a78259e62d 0xc008e4a6f7 0xc008e4a6f8}] []  [{calico Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba83e3c4-2d15-4879-bd92-12a78259e62d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 14:39:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6dsmb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6dsmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.16,PodIP:192.168.7.38,StartTime:2022-06-07 14:39:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 14:39:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://fffda58a4091a851907078df0ddf07dc65afd5c93d2646d1ccdc32c21c6f4676,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.216: INFO: Pod "webserver-deployment-847dcfb7fb-sp478" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-sp478 webserver-deployment-847dcfb7fb- deployment-6044  e3df2a0e-e463-4770-a193-53f5f05d0bf6 33593670 0 2022-06-07 14:39:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:1bf9090d2bead2d3e3a9b8e0e941f0277ef15d78f1c18cfebe7c64b584d093e8 cni.projectcalico.org/podIP:192.168.7.40/32 cni.projectcalico.org/podIPs:192.168.7.40/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb ba83e3c4-2d15-4879-bd92-12a78259e62d 0xc008e4a907 0xc008e4a908}] []  [{calico Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba83e3c4-2d15-4879-bd92-12a78259e62d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 14:39:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9ksp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9ksp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.16,PodIP:192.168.7.40,StartTime:2022-06-07 14:39:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 14:39:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://13a214487ca0d7a65b3224dfa974da1549725595ef1152ca0a3d5e36f826687b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.220: INFO: Pod "webserver-deployment-847dcfb7fb-vjnfz" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-vjnfz webserver-deployment-847dcfb7fb- deployment-6044  1086717f-8b16-4f92-9bd4-b6a541c763a9 33593777 0 2022-06-07 14:39:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb ba83e3c4-2d15-4879-bd92-12a78259e62d 0xc008e4aaf7 0xc008e4aaf8}] []  [{kube-controller-manager Update v1 2022-06-07 14:39:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba83e3c4-2d15-4879-bd92-12a78259e62d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-brfbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brfbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.220: INFO: Pod "webserver-deployment-847dcfb7fb-wggjn" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wggjn webserver-deployment-847dcfb7fb- deployment-6044  d4be49d4-a337-4656-b400-1e71d93a275f 33593668 0 2022-06-07 14:39:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:244dc02e80665549b2bc2475591b2f4b685d527b782d139a14be9618096f99d3 cni.projectcalico.org/podIP:192.168.7.60/32 cni.projectcalico.org/podIPs:192.168.7.60/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb ba83e3c4-2d15-4879-bd92-12a78259e62d 0xc008e4ac77 0xc008e4ac78}] []  [{kube-controller-manager Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba83e3c4-2d15-4879-bd92-12a78259e62d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-07 14:39:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-07 14:39:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9cjnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9cjnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.16,PodIP:192.168.7.60,StartTime:2022-06-07 14:39:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 14:39:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://e73387785ee7873ef28e81c166ccab020b0e6e60535f96127c970042383be3e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  7 14:39:53.221: INFO: Pod "webserver-deployment-847dcfb7fb-x65wx" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-x65wx webserver-deployment-847dcfb7fb- deployment-6044  ac12f7b2-a78e-424e-9dcf-75c41c071052 33593677 0 2022-06-07 14:39:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/containerID:03c7d227b175273d5b968272629ff1e8e429d9ae12900882fc13a47fb1e2f986 cni.projectcalico.org/podIP:192.168.46.120/32 cni.projectcalico.org/podIPs:192.168.46.120/32] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb ba83e3c4-2d15-4879-bd92-12a78259e62d 0xc008e4ae87 0xc008e4ae88}] []  [{calico Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 14:39:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba83e3c4-2d15-4879-bd92-12a78259e62d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 14:39:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.46.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kvkt6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kvkt6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk003,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:39:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.15,PodIP:192.168.46.120,StartTime:2022-06-07 14:39:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 14:39:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://6cedc351808ec4bfce714bd3b5d3242ac1c3835fc0ec34d1e8dcc5f3f4497e4c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.46.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:39:53.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6044" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":255,"skipped":4595,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:39:53.247: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 14:39:53.916: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  7 14:39:55.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209593, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209593, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209594, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209593, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 14:39:57.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209593, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209593, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209594, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209593, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 14:39:59.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209593, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209593, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209594, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209593, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 14:40:02.968: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:40:03.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3199" for this suite.
STEP: Destroying namespace "webhook-3199-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.868 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":256,"skipped":4607,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:40:03.115: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-10eb7340-bb4f-459a-bafb-93e05b46ec92
STEP: Creating a pod to test consume secrets
Jun  7 14:40:03.173: INFO: Waiting up to 5m0s for pod "pod-secrets-8b7a3743-2706-4b65-abe5-bb32f6f6a11f" in namespace "secrets-7867" to be "Succeeded or Failed"
Jun  7 14:40:03.175: INFO: Pod "pod-secrets-8b7a3743-2706-4b65-abe5-bb32f6f6a11f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.284204ms
Jun  7 14:40:05.183: INFO: Pod "pod-secrets-8b7a3743-2706-4b65-abe5-bb32f6f6a11f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010501642s
Jun  7 14:40:07.194: INFO: Pod "pod-secrets-8b7a3743-2706-4b65-abe5-bb32f6f6a11f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02114137s
STEP: Saw pod success
Jun  7 14:40:07.194: INFO: Pod "pod-secrets-8b7a3743-2706-4b65-abe5-bb32f6f6a11f" satisfied condition "Succeeded or Failed"
Jun  7 14:40:07.199: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-secrets-8b7a3743-2706-4b65-abe5-bb32f6f6a11f container secret-volume-test: <nil>
STEP: delete the pod
Jun  7 14:40:07.239: INFO: Waiting for pod pod-secrets-8b7a3743-2706-4b65-abe5-bb32f6f6a11f to disappear
Jun  7 14:40:07.244: INFO: Pod pod-secrets-8b7a3743-2706-4b65-abe5-bb32f6f6a11f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:40:07.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7867" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":257,"skipped":4611,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:40:07.258: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:40:07.305: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun  7 14:40:12.309: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  7 14:40:12.309: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun  7 14:40:14.318: INFO: Creating deployment "test-rollover-deployment"
Jun  7 14:40:14.332: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun  7 14:40:16.351: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun  7 14:40:16.360: INFO: Ensure that both replica sets have 1 created replica
Jun  7 14:40:16.373: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun  7 14:40:16.382: INFO: Updating deployment test-rollover-deployment
Jun  7 14:40:16.382: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun  7 14:40:18.392: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun  7 14:40:18.401: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun  7 14:40:18.409: INFO: all replica sets need to contain the pod-template-hash label
Jun  7 14:40:18.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209617, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 14:40:20.422: INFO: all replica sets need to contain the pod-template-hash label
Jun  7 14:40:20.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209617, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 14:40:22.421: INFO: all replica sets need to contain the pod-template-hash label
Jun  7 14:40:22.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209617, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 14:40:24.424: INFO: all replica sets need to contain the pod-template-hash label
Jun  7 14:40:24.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209617, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 14:40:26.426: INFO: all replica sets need to contain the pod-template-hash label
Jun  7 14:40:26.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209617, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790209614, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 14:40:28.426: INFO: 
Jun  7 14:40:28.426: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  7 14:40:28.439: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1100  76bf589d-fc39-462d-b566-edb0f5b023e6 33594461 2 2022-06-07 14:40:14 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-06-07 14:40:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 14:40:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008dd5fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-07 14:40:14 +0000 UTC,LastTransitionTime:2022-06-07 14:40:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2022-06-07 14:40:27 +0000 UTC,LastTransitionTime:2022-06-07 14:40:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  7 14:40:28.445: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-1100  c4988e1b-9a6a-4a05-a685-5529f4aba170 33594451 2 2022-06-07 14:40:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 76bf589d-fc39-462d-b566-edb0f5b023e6 0xc0032ade60 0xc0032ade61}] []  [{kube-controller-manager Update apps/v1 2022-06-07 14:40:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76bf589d-fc39-462d-b566-edb0f5b023e6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 14:40:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008b1c038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  7 14:40:28.445: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun  7 14:40:28.445: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1100  b66a1ff2-58ce-43f6-af47-693ea531a2b7 33594460 2 2022-06-07 14:40:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 76bf589d-fc39-462d-b566-edb0f5b023e6 0xc0032adc17 0xc0032adc18}] []  [{e2e.test Update apps/v1 2022-06-07 14:40:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 14:40:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76bf589d-fc39-462d-b566-edb0f5b023e6\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-06-07 14:40:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0032adcd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  7 14:40:28.445: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-1100  ad56c605-670a-462f-9643-2a3145583096 33594378 2 2022-06-07 14:40:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 76bf589d-fc39-462d-b566-edb0f5b023e6 0xc0032add47 0xc0032add48}] []  [{kube-controller-manager Update apps/v1 2022-06-07 14:40:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76bf589d-fc39-462d-b566-edb0f5b023e6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 14:40:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032addf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  7 14:40:28.450: INFO: Pod "test-rollover-deployment-98c5f4599-22hmc" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-22hmc test-rollover-deployment-98c5f4599- deployment-1100  2b8442a1-6f9c-4ac9-aede-818e4113bf5c 33594397 0 2022-06-07 14:40:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/containerID:2e5576a5f93c83690341aaf54c7dc2a924361a7df140fd3914b5a089d1c43304 cni.projectcalico.org/podIP:192.168.39.234/32 cni.projectcalico.org/podIPs:192.168.39.234/32] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 c4988e1b-9a6a-4a05-a685-5529f4aba170 0xc008b1c560 0xc008b1c561}] []  [{kube-controller-manager Update v1 2022-06-07 14:40:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4988e1b-9a6a-4a05-a685-5529f4aba170\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-07 14:40:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-07 14:40:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.39.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-knntv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-knntv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:40:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:40:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:40:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:40:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.14,PodIP:192.168.39.234,StartTime:2022-06-07 14:40:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 14:40:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://2c2285f62f8a0a0ce10b75c3565a2df107f3d5da7fe642efbba5b64b72d6d39c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.39.234,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:40:28.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1100" for this suite.

• [SLOW TEST:21.207 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":258,"skipped":4622,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:40:28.465: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-2773
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:40:28.522: INFO: Found 0 stateful pods, waiting for 1
Jun  7 14:40:38.531: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jun  7 14:40:38.563: INFO: Found 1 stateful pods, waiting for 2
Jun  7 14:40:48.573: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  7 14:40:48.573: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun  7 14:40:48.607: INFO: Deleting all statefulset in ns statefulset-2773
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:40:48.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2773" for this suite.

• [SLOW TEST:20.176 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":259,"skipped":4623,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:40:48.642: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun  7 14:40:48.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1872  410047f8-f654-481d-b366-8a11c916ca76 33594697 0 2022-06-07 14:40:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-07 14:40:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 14:40:48.700: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1872  410047f8-f654-481d-b366-8a11c916ca76 33594698 0 2022-06-07 14:40:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-07 14:40:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun  7 14:40:48.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1872  410047f8-f654-481d-b366-8a11c916ca76 33594699 0 2022-06-07 14:40:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-07 14:40:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  7 14:40:48.711: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1872  410047f8-f654-481d-b366-8a11c916ca76 33594700 0 2022-06-07 14:40:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-07 14:40:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:40:48.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1872" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":260,"skipped":4628,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:40:48.721: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5749
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  7 14:40:48.753: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  7 14:40:48.783: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:40:50.794: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:40:52.791: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:40:54.791: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:40:56.801: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:40:58.795: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:41:00.791: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:41:02.794: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:41:04.796: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:41:06.796: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:41:08.791: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:41:10.794: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun  7 14:41:10.804: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun  7 14:41:10.815: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jun  7 14:41:10.824: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jun  7 14:41:12.873: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jun  7 14:41:12.873: INFO: Going to poll 192.168.56.241 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jun  7 14:41:12.876: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.56.241:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5749 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:41:12.876: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 14:41:13.026: INFO: Found all 1 expected endpoints: [netserver-0]
Jun  7 14:41:13.027: INFO: Going to poll 192.168.39.252 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jun  7 14:41:13.033: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.39.252:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5749 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:41:13.033: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 14:41:13.169: INFO: Found all 1 expected endpoints: [netserver-1]
Jun  7 14:41:13.169: INFO: Going to poll 192.168.46.116 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jun  7 14:41:13.175: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.46.116:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5749 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:41:13.175: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 14:41:13.286: INFO: Found all 1 expected endpoints: [netserver-2]
Jun  7 14:41:13.286: INFO: Going to poll 192.168.7.59 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Jun  7 14:41:13.291: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.7.59:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5749 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:41:13.291: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 14:41:13.410: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:41:13.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5749" for this suite.

• [SLOW TEST:24.710 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":261,"skipped":4643,"failed":0}
SSSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:41:13.431: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun  7 14:41:15.509: INFO: &Pod{ObjectMeta:{send-events-90321f18-f632-4817-83b1-b237f412513e  events-7650  7fc38b0b-36dc-47b3-af85-78264572c394 33594972 0 2022-06-07 14:41:13 +0000 UTC <nil> <nil> map[name:foo time:477370667] map[cni.projectcalico.org/containerID:e507991ecc149e59bd09b247c8104c563d3625e87b13b132ff48d026e86c8edd cni.projectcalico.org/podIP:192.168.39.250/32 cni.projectcalico.org/podIPs:192.168.39.250/32] [] []  [{e2e.test Update v1 2022-06-07 14:41:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-07 14:41:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-07 14:41:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.39.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qvctt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qvctt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:41:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:41:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:41:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 14:41:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.14,PodIP:192.168.39.250,StartTime:2022-06-07 14:41:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 14:41:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:cri-o://a6a47acae9c2bf1b6442f58025a997a670a1edc22f1abe79c8a7396808de465a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.39.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun  7 14:41:17.517: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun  7 14:41:19.525: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:41:19.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7650" for this suite.

• [SLOW TEST:6.124 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":346,"completed":262,"skipped":4648,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:41:19.556: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-mj7p
STEP: Creating a pod to test atomic-volume-subpath
Jun  7 14:41:19.608: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mj7p" in namespace "subpath-1806" to be "Succeeded or Failed"
Jun  7 14:41:19.611: INFO: Pod "pod-subpath-test-downwardapi-mj7p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.306424ms
Jun  7 14:41:21.620: INFO: Pod "pod-subpath-test-downwardapi-mj7p": Phase="Running", Reason="", readiness=true. Elapsed: 2.012223168s
Jun  7 14:41:23.628: INFO: Pod "pod-subpath-test-downwardapi-mj7p": Phase="Running", Reason="", readiness=true. Elapsed: 4.020198097s
Jun  7 14:41:25.637: INFO: Pod "pod-subpath-test-downwardapi-mj7p": Phase="Running", Reason="", readiness=true. Elapsed: 6.028371975s
Jun  7 14:41:27.644: INFO: Pod "pod-subpath-test-downwardapi-mj7p": Phase="Running", Reason="", readiness=true. Elapsed: 8.035452097s
Jun  7 14:41:29.653: INFO: Pod "pod-subpath-test-downwardapi-mj7p": Phase="Running", Reason="", readiness=true. Elapsed: 10.044866003s
Jun  7 14:41:31.659: INFO: Pod "pod-subpath-test-downwardapi-mj7p": Phase="Running", Reason="", readiness=true. Elapsed: 12.051056067s
Jun  7 14:41:33.666: INFO: Pod "pod-subpath-test-downwardapi-mj7p": Phase="Running", Reason="", readiness=true. Elapsed: 14.057782676s
Jun  7 14:41:35.673: INFO: Pod "pod-subpath-test-downwardapi-mj7p": Phase="Running", Reason="", readiness=true. Elapsed: 16.064894264s
Jun  7 14:41:37.679: INFO: Pod "pod-subpath-test-downwardapi-mj7p": Phase="Running", Reason="", readiness=true. Elapsed: 18.070931979s
Jun  7 14:41:39.686: INFO: Pod "pod-subpath-test-downwardapi-mj7p": Phase="Running", Reason="", readiness=true. Elapsed: 20.077462612s
Jun  7 14:41:41.691: INFO: Pod "pod-subpath-test-downwardapi-mj7p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.083129878s
STEP: Saw pod success
Jun  7 14:41:41.691: INFO: Pod "pod-subpath-test-downwardapi-mj7p" satisfied condition "Succeeded or Failed"
Jun  7 14:41:41.695: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-subpath-test-downwardapi-mj7p container test-container-subpath-downwardapi-mj7p: <nil>
STEP: delete the pod
Jun  7 14:41:41.716: INFO: Waiting for pod pod-subpath-test-downwardapi-mj7p to disappear
Jun  7 14:41:41.719: INFO: Pod pod-subpath-test-downwardapi-mj7p no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mj7p
Jun  7 14:41:41.719: INFO: Deleting pod "pod-subpath-test-downwardapi-mj7p" in namespace "subpath-1806"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:41:41.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1806" for this suite.

• [SLOW TEST:22.177 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":346,"completed":263,"skipped":4654,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:41:41.734: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:42:41.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7812" for this suite.

• [SLOW TEST:60.072 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":4679,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:42:41.806: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1799
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-1799
Jun  7 14:42:41.862: INFO: Found 0 stateful pods, waiting for 1
Jun  7 14:42:51.874: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jun  7 14:42:51.900: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jun  7 14:42:51.912: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jun  7 14:42:51.916: INFO: Observed &StatefulSet event: ADDED
Jun  7 14:42:51.916: INFO: Found Statefulset ss in namespace statefulset-1799 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  7 14:42:51.917: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jun  7 14:42:51.918: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun  7 14:42:51.927: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jun  7 14:42:51.930: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun  7 14:42:51.930: INFO: Deleting all statefulset in ns statefulset-1799
Jun  7 14:42:51.934: INFO: Scaling statefulset ss to 0
Jun  7 14:43:01.958: INFO: Waiting for statefulset status.replicas updated to 0
Jun  7 14:43:01.963: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:43:01.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1799" for this suite.

• [SLOW TEST:20.192 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":265,"skipped":4693,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:43:01.998: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-hjp4
STEP: Creating a pod to test atomic-volume-subpath
Jun  7 14:43:02.054: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hjp4" in namespace "subpath-9105" to be "Succeeded or Failed"
Jun  7 14:43:02.056: INFO: Pod "pod-subpath-test-configmap-hjp4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.456996ms
Jun  7 14:43:04.065: INFO: Pod "pod-subpath-test-configmap-hjp4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011794097s
Jun  7 14:43:06.076: INFO: Pod "pod-subpath-test-configmap-hjp4": Phase="Running", Reason="", readiness=true. Elapsed: 4.022252513s
Jun  7 14:43:08.084: INFO: Pod "pod-subpath-test-configmap-hjp4": Phase="Running", Reason="", readiness=true. Elapsed: 6.030792767s
Jun  7 14:43:10.093: INFO: Pod "pod-subpath-test-configmap-hjp4": Phase="Running", Reason="", readiness=true. Elapsed: 8.039546732s
Jun  7 14:43:12.103: INFO: Pod "pod-subpath-test-configmap-hjp4": Phase="Running", Reason="", readiness=true. Elapsed: 10.049763144s
Jun  7 14:43:14.115: INFO: Pod "pod-subpath-test-configmap-hjp4": Phase="Running", Reason="", readiness=true. Elapsed: 12.061222523s
Jun  7 14:43:16.127: INFO: Pod "pod-subpath-test-configmap-hjp4": Phase="Running", Reason="", readiness=true. Elapsed: 14.073347393s
Jun  7 14:43:18.137: INFO: Pod "pod-subpath-test-configmap-hjp4": Phase="Running", Reason="", readiness=true. Elapsed: 16.083122244s
Jun  7 14:43:20.152: INFO: Pod "pod-subpath-test-configmap-hjp4": Phase="Running", Reason="", readiness=true. Elapsed: 18.098777458s
Jun  7 14:43:22.164: INFO: Pod "pod-subpath-test-configmap-hjp4": Phase="Running", Reason="", readiness=true. Elapsed: 20.110811037s
Jun  7 14:43:24.174: INFO: Pod "pod-subpath-test-configmap-hjp4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.120767068s
STEP: Saw pod success
Jun  7 14:43:24.174: INFO: Pod "pod-subpath-test-configmap-hjp4" satisfied condition "Succeeded or Failed"
Jun  7 14:43:24.179: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-subpath-test-configmap-hjp4 container test-container-subpath-configmap-hjp4: <nil>
STEP: delete the pod
Jun  7 14:43:24.206: INFO: Waiting for pod pod-subpath-test-configmap-hjp4 to disappear
Jun  7 14:43:24.210: INFO: Pod pod-subpath-test-configmap-hjp4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hjp4
Jun  7 14:43:24.210: INFO: Deleting pod "pod-subpath-test-configmap-hjp4" in namespace "subpath-9105"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:43:24.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9105" for this suite.

• [SLOW TEST:22.227 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":346,"completed":266,"skipped":4714,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:43:24.226: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 14:43:24.286: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7b32bef-850a-4455-84ea-f0fe49a77c8a" in namespace "projected-5613" to be "Succeeded or Failed"
Jun  7 14:43:24.289: INFO: Pod "downwardapi-volume-d7b32bef-850a-4455-84ea-f0fe49a77c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.354964ms
Jun  7 14:43:26.296: INFO: Pod "downwardapi-volume-d7b32bef-850a-4455-84ea-f0fe49a77c8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010421656s
STEP: Saw pod success
Jun  7 14:43:26.296: INFO: Pod "downwardapi-volume-d7b32bef-850a-4455-84ea-f0fe49a77c8a" satisfied condition "Succeeded or Failed"
Jun  7 14:43:26.300: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-d7b32bef-850a-4455-84ea-f0fe49a77c8a container client-container: <nil>
STEP: delete the pod
Jun  7 14:43:26.330: INFO: Waiting for pod downwardapi-volume-d7b32bef-850a-4455-84ea-f0fe49a77c8a to disappear
Jun  7 14:43:26.334: INFO: Pod downwardapi-volume-d7b32bef-850a-4455-84ea-f0fe49a77c8a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:43:26.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5613" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":267,"skipped":4718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:43:26.355: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jun  7 14:43:28.443: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:43:34.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1594" for this suite.

• [SLOW TEST:8.273 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":268,"skipped":4759,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:43:34.629: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jun  7 14:43:34.673: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:44:01.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5555" for this suite.

• [SLOW TEST:26.604 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":269,"skipped":4767,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:44:01.233: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:44:01.272: INFO: Got root ca configmap in namespace "svcaccounts-8465"
Jun  7 14:44:01.278: INFO: Deleted root ca configmap in namespace "svcaccounts-8465"
STEP: waiting for a new root ca configmap created
Jun  7 14:44:01.784: INFO: Recreated root ca configmap in namespace "svcaccounts-8465"
Jun  7 14:44:01.792: INFO: Updated root ca configmap in namespace "svcaccounts-8465"
STEP: waiting for the root ca configmap reconciled
Jun  7 14:44:02.300: INFO: Reconciled root ca configmap in namespace "svcaccounts-8465"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:44:02.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8465" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":270,"skipped":4770,"failed":0}
SSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:44:02.319: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jun  7 14:44:02.374: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:44:02.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7314" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":271,"skipped":4774,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:44:02.406: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun  7 14:44:02.429: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun  7 14:44:19.994: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 14:44:25.091: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:44:43.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9878" for this suite.

• [SLOW TEST:41.375 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":272,"skipped":4776,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:44:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jun  7 14:44:43.837: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:44:45.846: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jun  7 14:44:45.865: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:44:47.871: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun  7 14:44:47.901: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  7 14:44:47.906: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  7 14:44:49.908: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  7 14:44:49.920: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:44:49.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3837" for this suite.

• [SLOW TEST:6.152 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":273,"skipped":4795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:44:49.937: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6525.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6525.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  7 14:44:52.031: INFO: DNS probes using dns-test-49e1d1ce-9d6f-4ff8-bf9b-762cc46ea7ce succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6525.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6525.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  7 14:44:54.100: INFO: File wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local from pod  dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  7 14:44:54.106: INFO: File jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local from pod  dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  7 14:44:54.106: INFO: Lookups using dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e failed for: [wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local]

Jun  7 14:44:59.112: INFO: File wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local from pod  dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e contains '' instead of 'bar.example.com.'
Jun  7 14:44:59.117: INFO: File jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local from pod  dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  7 14:44:59.117: INFO: Lookups using dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e failed for: [wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local]

Jun  7 14:45:04.114: INFO: File wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local from pod  dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  7 14:45:04.120: INFO: File jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local from pod  dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  7 14:45:04.120: INFO: Lookups using dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e failed for: [wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local]

Jun  7 14:45:09.116: INFO: File wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local from pod  dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  7 14:45:09.122: INFO: File jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local from pod  dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  7 14:45:09.122: INFO: Lookups using dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e failed for: [wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local]

Jun  7 14:45:14.116: INFO: File wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local from pod  dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  7 14:45:14.121: INFO: File jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local from pod  dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  7 14:45:14.121: INFO: Lookups using dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e failed for: [wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local]

Jun  7 14:45:19.115: INFO: File wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local from pod  dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  7 14:45:19.120: INFO: File jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local from pod  dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  7 14:45:19.120: INFO: Lookups using dns-6525/dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e failed for: [wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local]

Jun  7 14:45:24.122: INFO: DNS probes using dns-test-0720e1a5-cc53-4e3b-ba0c-2612f22b234e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6525.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6525.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6525.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6525.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  7 14:45:26.207: INFO: DNS probes using dns-test-058be5e6-6dd1-408d-b6f6-3057ae559a3b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:45:26.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6525" for this suite.

• [SLOW TEST:36.323 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":274,"skipped":4823,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:45:26.261: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 14:45:26.649: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 14:45:29.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:45:29.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1109" for this suite.
STEP: Destroying namespace "webhook-1109-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":275,"skipped":4904,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:45:29.795: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-4761
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  7 14:45:29.834: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  7 14:45:29.865: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:45:31.872: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:45:33.872: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:45:35.873: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:45:37.871: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:45:39.872: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:45:41.871: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:45:43.871: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:45:45.870: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:45:47.871: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  7 14:45:49.884: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun  7 14:45:49.891: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun  7 14:45:51.906: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun  7 14:45:51.913: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jun  7 14:45:51.923: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
Jun  7 14:45:53.955: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Jun  7 14:45:53.955: INFO: Breadth first check of 192.168.56.242 on host 10.55.210.13...
Jun  7 14:45:53.959: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.39.199:9080/dial?request=hostname&protocol=udp&host=192.168.56.242&port=8081&tries=1'] Namespace:pod-network-test-4761 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:45:53.959: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 14:45:54.091: INFO: Waiting for responses: map[]
Jun  7 14:45:54.091: INFO: reached 192.168.56.242 after 0/1 tries
Jun  7 14:45:54.091: INFO: Breadth first check of 192.168.39.193 on host 10.55.210.14...
Jun  7 14:45:54.099: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.39.199:9080/dial?request=hostname&protocol=udp&host=192.168.39.193&port=8081&tries=1'] Namespace:pod-network-test-4761 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:45:54.099: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 14:45:54.215: INFO: Waiting for responses: map[]
Jun  7 14:45:54.216: INFO: reached 192.168.39.193 after 0/1 tries
Jun  7 14:45:54.216: INFO: Breadth first check of 192.168.46.109 on host 10.55.210.15...
Jun  7 14:45:54.222: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.39.199:9080/dial?request=hostname&protocol=udp&host=192.168.46.109&port=8081&tries=1'] Namespace:pod-network-test-4761 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:45:54.222: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 14:45:54.332: INFO: Waiting for responses: map[]
Jun  7 14:45:54.332: INFO: reached 192.168.46.109 after 0/1 tries
Jun  7 14:45:54.332: INFO: Breadth first check of 192.168.7.63 on host 10.55.210.16...
Jun  7 14:45:54.338: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.39.199:9080/dial?request=hostname&protocol=udp&host=192.168.7.63&port=8081&tries=1'] Namespace:pod-network-test-4761 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:45:54.338: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 14:45:54.461: INFO: Waiting for responses: map[]
Jun  7 14:45:54.462: INFO: reached 192.168.7.63 after 0/1 tries
Jun  7 14:45:54.462: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:45:54.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4761" for this suite.

• [SLOW TEST:24.688 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":4905,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:45:54.483: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:46:07.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1432" for this suite.
STEP: Destroying namespace "nsdeletetest-2852" for this suite.
Jun  7 14:46:07.679: INFO: Namespace nsdeletetest-2852 was already deleted
STEP: Destroying namespace "nsdeletetest-9575" for this suite.

• [SLOW TEST:13.200 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":277,"skipped":4907,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:46:07.683: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun  7 14:46:07.734: INFO: Pod name pod-release: Found 0 pods out of 1
Jun  7 14:46:12.742: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:46:13.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3527" for this suite.

• [SLOW TEST:6.109 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":278,"skipped":4911,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:46:13.793: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 14:46:14.351: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 14:46:17.395: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:46:17.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8748" for this suite.
STEP: Destroying namespace "webhook-8748-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":279,"skipped":4917,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:46:17.525: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:46:17.570: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun  7 14:46:17.596: INFO: The status of Pod pod-exec-websocket-77e8e540-fc7c-47d2-a886-6ac1c80ac6f8 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:46:19.603: INFO: The status of Pod pod-exec-websocket-77e8e540-fc7c-47d2-a886-6ac1c80ac6f8 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:46:19.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3261" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":280,"skipped":4925,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:46:19.749: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:46:19.806: INFO: created pod
Jun  7 14:46:19.806: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8581" to be "Succeeded or Failed"
Jun  7 14:46:19.811: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.814972ms
Jun  7 14:46:21.823: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017011204s
STEP: Saw pod success
Jun  7 14:46:21.823: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jun  7 14:46:51.824: INFO: polling logs
Jun  7 14:46:51.848: INFO: Pod logs: 
2022/06/07 14:46:20 OK: Got token
2022/06/07 14:46:20 validating with in-cluster discovery
2022/06/07 14:46:20 OK: got issuer https://kubernetes.default.svc.cluster.local
2022/06/07 14:46:20 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8581:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1654613779, NotBefore:1654613179, IssuedAt:1654613179, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8581", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"756bc507-ee3b-4cf6-bb2d-69c97b4c0183"}}}
2022/06/07 14:46:20 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2022/06/07 14:46:20 OK: Validated signature on JWT
2022/06/07 14:46:20 OK: Got valid claims from token!
2022/06/07 14:46:20 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8581:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1654613779, NotBefore:1654613179, IssuedAt:1654613179, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8581", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"756bc507-ee3b-4cf6-bb2d-69c97b4c0183"}}}

Jun  7 14:46:51.848: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:46:51.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8581" for this suite.

• [SLOW TEST:32.132 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":281,"skipped":4927,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:46:51.882: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun  7 14:46:51.942: INFO: The status of Pod pod-update-73796564-b61c-4832-a07d-5d5a5534c3f0 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:46:53.949: INFO: The status of Pod pod-update-73796564-b61c-4832-a07d-5d5a5534c3f0 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun  7 14:46:54.483: INFO: Successfully updated pod "pod-update-73796564-b61c-4832-a07d-5d5a5534c3f0"
STEP: verifying the updated pod is in kubernetes
Jun  7 14:46:54.492: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:46:54.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1345" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":282,"skipped":4930,"failed":0}
S
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:46:54.507: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jun  7 14:46:54.574: INFO: observed Pod pod-test in namespace pods-3481 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jun  7 14:46:54.576: INFO: observed Pod pod-test in namespace pods-3481 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:54 +0000 UTC  }]
Jun  7 14:46:54.593: INFO: observed Pod pod-test in namespace pods-3481 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:54 +0000 UTC  }]
Jun  7 14:46:55.194: INFO: observed Pod pod-test in namespace pods-3481 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:54 +0000 UTC  }]
Jun  7 14:46:55.577: INFO: Found Pod pod-test in namespace pods-3481 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-07 14:46:54 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jun  7 14:46:55.593: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jun  7 14:46:55.614: INFO: observed event type ADDED
Jun  7 14:46:55.614: INFO: observed event type MODIFIED
Jun  7 14:46:55.614: INFO: observed event type MODIFIED
Jun  7 14:46:55.614: INFO: observed event type MODIFIED
Jun  7 14:46:55.614: INFO: observed event type MODIFIED
Jun  7 14:46:55.614: INFO: observed event type MODIFIED
Jun  7 14:46:55.614: INFO: observed event type MODIFIED
Jun  7 14:46:55.614: INFO: observed event type MODIFIED
Jun  7 14:46:57.599: INFO: observed event type MODIFIED
Jun  7 14:46:57.853: INFO: observed event type MODIFIED
Jun  7 14:46:58.601: INFO: observed event type MODIFIED
Jun  7 14:46:58.611: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:46:58.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3481" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":283,"skipped":4931,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:46:58.629: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1918, will wait for the garbage collector to delete the pods
Jun  7 14:47:00.755: INFO: Deleting Job.batch foo took: 14.150367ms
Jun  7 14:47:00.857: INFO: Terminating Job.batch foo pods took: 102.215228ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:47:32.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1918" for this suite.

• [SLOW TEST:34.251 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":284,"skipped":4933,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:47:32.880: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:47:50.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5941" for this suite.

• [SLOW TEST:17.174 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":285,"skipped":4933,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:47:50.057: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:47:50.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-941" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":286,"skipped":4953,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:47:50.167: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  7 14:47:50.229: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:47:50.229: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:47:50.229: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:47:50.232: INFO: Number of nodes with available pods: 0
Jun  7 14:47:50.232: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:47:51.245: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:47:51.245: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:47:51.245: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:47:51.251: INFO: Number of nodes with available pods: 1
Jun  7 14:47:51.251: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:47:52.242: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:47:52.242: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:47:52.242: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:47:52.248: INFO: Number of nodes with available pods: 4
Jun  7 14:47:52.248: INFO: Number of running nodes: 4, number of available pods: 4
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
Jun  7 14:47:52.281: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33598490"},"items":null}

Jun  7 14:47:52.288: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33598493"},"items":[{"metadata":{"name":"daemon-set-q82qr","generateName":"daemon-set-","namespace":"daemonsets-6122","uid":"8f21395c-148d-45fb-a55e-ceb9abcddfd6","resourceVersion":"33598492","creationTimestamp":"2022-06-07T14:47:50Z","deletionTimestamp":"2022-06-07T14:48:22Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"dc9105488ab95891c3f61461333618d2202da288db4848cc7bfaf4b147f95e81","cni.projectcalico.org/podIP":"192.168.46.119/32","cni.projectcalico.org/podIPs":"192.168.46.119/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6fc9478b-e445-42fb-8a0c-ab3ed8d41ab7","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-06-07T14:47:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-06-07T14:47:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fc9478b-e445-42fb-8a0c-ab3ed8d41ab7\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-06-07T14:47:52Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.46.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hxqz2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hxqz2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"proact-prod01-wk003","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["proact-prod01-wk003"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:50Z"}],"hostIP":"10.55.210.15","podIP":"192.168.46.119","podIPs":[{"ip":"192.168.46.119"}],"startTime":"2022-06-07T14:47:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-06-07T14:47:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"cri-o://b7cc5ec3a10163e5e595f5f5ab7710974ec4a2d840b7917aa797ba2ca5fde6b0","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qznb5","generateName":"daemon-set-","namespace":"daemonsets-6122","uid":"19b3e1b0-f71d-439c-a56b-07fb3ece1d2b","resourceVersion":"33598493","creationTimestamp":"2022-06-07T14:47:50Z","deletionTimestamp":"2022-06-07T14:48:22Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"2d5d8e284db47c708a28762dad667da3e6f37846ed61b735b1ae9a30fa35b15f","cni.projectcalico.org/podIP":"192.168.39.244/32","cni.projectcalico.org/podIPs":"192.168.39.244/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6fc9478b-e445-42fb-8a0c-ab3ed8d41ab7","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-06-07T14:47:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-06-07T14:47:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fc9478b-e445-42fb-8a0c-ab3ed8d41ab7\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-06-07T14:47:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.39.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dhfhr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dhfhr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"proact-prod01-wk002","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["proact-prod01-wk002"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:50Z"}],"hostIP":"10.55.210.14","podIP":"192.168.39.244","podIPs":[{"ip":"192.168.39.244"}],"startTime":"2022-06-07T14:47:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-06-07T14:47:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"cri-o://0d5f69887a459691eeab7ceec32c5d3bbaad9a1949ba87eb193f7d62bff9f3fd","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rnzhs","generateName":"daemon-set-","namespace":"daemonsets-6122","uid":"07c1bf44-fee4-499e-8acd-988ce8d78c62","resourceVersion":"33598489","creationTimestamp":"2022-06-07T14:47:50Z","deletionTimestamp":"2022-06-07T14:48:22Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7acaafd6e537f34b0fd05db4dc70777f3cd16266c631e9f93308f55d9a705904","cni.projectcalico.org/podIP":"192.168.7.10/32","cni.projectcalico.org/podIPs":"192.168.7.10/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6fc9478b-e445-42fb-8a0c-ab3ed8d41ab7","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-06-07T14:47:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-06-07T14:47:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fc9478b-e445-42fb-8a0c-ab3ed8d41ab7\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-06-07T14:47:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2zslq","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2zslq","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"proact-prod01-wk004","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["proact-prod01-wk004"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:50Z"}],"hostIP":"10.55.210.16","podIP":"192.168.7.10","podIPs":[{"ip":"192.168.7.10"}],"startTime":"2022-06-07T14:47:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-06-07T14:47:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"cri-o://c3814f4bbfd1b6c42689ccae3e63c10cc32c262a1eae3a322c04781ea1ceebe1","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-zw4hk","generateName":"daemon-set-","namespace":"daemonsets-6122","uid":"34ee3370-94d9-4c01-b901-fc55aead574b","resourceVersion":"33598491","creationTimestamp":"2022-06-07T14:47:50Z","deletionTimestamp":"2022-06-07T14:48:22Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"577749b6b","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"e099e0096f9517c313e5d74b1212d9aabd52a0cb61d42121c4828c81666eb066","cni.projectcalico.org/podIP":"192.168.56.245/32","cni.projectcalico.org/podIPs":"192.168.56.245/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6fc9478b-e445-42fb-8a0c-ab3ed8d41ab7","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-06-07T14:47:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-06-07T14:47:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fc9478b-e445-42fb-8a0c-ab3ed8d41ab7\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-06-07T14:47:51Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.56.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bfqgw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bfqgw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"proact-prod01-wk001","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["proact-prod01-wk001"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-07T14:47:50Z"}],"hostIP":"10.55.210.13","podIP":"192.168.56.245","podIPs":[{"ip":"192.168.56.245"}],"startTime":"2022-06-07T14:47:50Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-06-07T14:47:51Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50","containerID":"cri-o://ff4ba96a37bfd566a253aa0da5cf8441eaabf74eacbbcad0852b4921a4509a2a","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:47:52.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6122" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":287,"skipped":4958,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:47:52.312: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:47:52.349: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Creating first CR 
Jun  7 14:47:54.947: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-07T14:47:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-07T14:47:54Z]] name:name1 resourceVersion:33598545 uid:2802e0ec-117a-499b-b6e6-b675cf468f2a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun  7 14:48:04.959: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-07T14:48:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-07T14:48:04Z]] name:name2 resourceVersion:33598644 uid:758f3306-34dc-4912-96cd-a76fea94c410] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun  7 14:48:14.972: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-07T14:47:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-07T14:48:14Z]] name:name1 resourceVersion:33598694 uid:2802e0ec-117a-499b-b6e6-b675cf468f2a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun  7 14:48:24.984: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-07T14:48:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-07T14:48:24Z]] name:name2 resourceVersion:33598745 uid:758f3306-34dc-4912-96cd-a76fea94c410] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun  7 14:48:34.997: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-07T14:47:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-07T14:48:14Z]] name:name1 resourceVersion:33598796 uid:2802e0ec-117a-499b-b6e6-b675cf468f2a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun  7 14:48:45.018: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-07T14:48:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-07T14:48:24Z]] name:name2 resourceVersion:33598845 uid:758f3306-34dc-4912-96cd-a76fea94c410] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:48:55.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8100" for this suite.

• [SLOW TEST:63.250 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":288,"skipped":4959,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:48:55.564: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:48:55.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2604" for this suite.
STEP: Destroying namespace "nspatchtest-045e2a1d-e521-4b27-8c58-958c49b93c5e-5828" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":289,"skipped":4973,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:48:55.692: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2722.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2722.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2722.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2722.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2722.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2722.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2722.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2722.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2722.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2722.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2722.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2722.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 155.218.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.218.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.218.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.218.155_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2722.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2722.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2722.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2722.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2722.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2722.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2722.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2722.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2722.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2722.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2722.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2722.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2722.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 155.218.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.218.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.218.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.218.155_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  7 14:48:57.866: INFO: Unable to read wheezy_udp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:48:57.871: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:48:57.877: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:48:57.881: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:48:57.912: INFO: Unable to read jessie_udp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:48:57.917: INFO: Unable to read jessie_tcp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:48:57.922: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:48:57.927: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:48:57.956: INFO: Lookups using dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96 failed for: [wheezy_udp@dns-test-service.dns-2722.svc.cluster.local wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2722.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2722.svc.cluster.local jessie_udp@dns-test-service.dns-2722.svc.cluster.local jessie_tcp@dns-test-service.dns-2722.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2722.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2722.svc.cluster.local]

Jun  7 14:49:02.964: INFO: Unable to read wheezy_udp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:02.970: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:03.016: INFO: Unable to read jessie_udp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:03.021: INFO: Unable to read jessie_tcp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:03.062: INFO: Lookups using dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96 failed for: [wheezy_udp@dns-test-service.dns-2722.svc.cluster.local wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local jessie_udp@dns-test-service.dns-2722.svc.cluster.local jessie_tcp@dns-test-service.dns-2722.svc.cluster.local]

Jun  7 14:49:07.965: INFO: Unable to read wheezy_udp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:07.970: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:08.016: INFO: Unable to read jessie_udp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:08.020: INFO: Unable to read jessie_tcp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:08.061: INFO: Lookups using dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96 failed for: [wheezy_udp@dns-test-service.dns-2722.svc.cluster.local wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local jessie_udp@dns-test-service.dns-2722.svc.cluster.local jessie_tcp@dns-test-service.dns-2722.svc.cluster.local]

Jun  7 14:49:12.964: INFO: Unable to read wheezy_udp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:12.970: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:13.016: INFO: Unable to read jessie_udp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:13.021: INFO: Unable to read jessie_tcp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:13.059: INFO: Lookups using dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96 failed for: [wheezy_udp@dns-test-service.dns-2722.svc.cluster.local wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local jessie_udp@dns-test-service.dns-2722.svc.cluster.local jessie_tcp@dns-test-service.dns-2722.svc.cluster.local]

Jun  7 14:49:17.964: INFO: Unable to read wheezy_udp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:17.970: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:18.020: INFO: Unable to read jessie_udp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:18.025: INFO: Unable to read jessie_tcp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:18.061: INFO: Lookups using dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96 failed for: [wheezy_udp@dns-test-service.dns-2722.svc.cluster.local wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local jessie_udp@dns-test-service.dns-2722.svc.cluster.local jessie_tcp@dns-test-service.dns-2722.svc.cluster.local]

Jun  7 14:49:22.964: INFO: Unable to read wheezy_udp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:22.970: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:23.016: INFO: Unable to read jessie_udp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:23.020: INFO: Unable to read jessie_tcp@dns-test-service.dns-2722.svc.cluster.local from pod dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96: the server could not find the requested resource (get pods dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96)
Jun  7 14:49:23.062: INFO: Lookups using dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96 failed for: [wheezy_udp@dns-test-service.dns-2722.svc.cluster.local wheezy_tcp@dns-test-service.dns-2722.svc.cluster.local jessie_udp@dns-test-service.dns-2722.svc.cluster.local jessie_tcp@dns-test-service.dns-2722.svc.cluster.local]

Jun  7 14:49:28.069: INFO: DNS probes using dns-2722/dns-test-5852d87e-79ca-4e24-88b3-27fb76b4be96 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:49:28.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2722" for this suite.

• [SLOW TEST:32.443 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":290,"skipped":4974,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:49:28.137: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-afff17d0-0589-474c-b6b8-88d60bf2e621
STEP: Creating a pod to test consume secrets
Jun  7 14:49:28.192: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f38ae7a2-1deb-4764-a156-3e639a6e76a5" in namespace "projected-3516" to be "Succeeded or Failed"
Jun  7 14:49:28.197: INFO: Pod "pod-projected-secrets-f38ae7a2-1deb-4764-a156-3e639a6e76a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.273354ms
Jun  7 14:49:30.205: INFO: Pod "pod-projected-secrets-f38ae7a2-1deb-4764-a156-3e639a6e76a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012705322s
STEP: Saw pod success
Jun  7 14:49:30.205: INFO: Pod "pod-projected-secrets-f38ae7a2-1deb-4764-a156-3e639a6e76a5" satisfied condition "Succeeded or Failed"
Jun  7 14:49:30.208: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-secrets-f38ae7a2-1deb-4764-a156-3e639a6e76a5 container secret-volume-test: <nil>
STEP: delete the pod
Jun  7 14:49:30.229: INFO: Waiting for pod pod-projected-secrets-f38ae7a2-1deb-4764-a156-3e639a6e76a5 to disappear
Jun  7 14:49:30.232: INFO: Pod pod-projected-secrets-f38ae7a2-1deb-4764-a156-3e639a6e76a5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:49:30.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3516" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":4979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:49:30.244: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun  7 14:49:30.289: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  7 14:50:30.403: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:50:30.408: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:50:30.477: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jun  7 14:50:30.481: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:50:30.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6957" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:50:30.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6340" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.351 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":292,"skipped":5018,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:50:30.599: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:50:30.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1161" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":293,"skipped":5040,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:50:30.661: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-5265
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5265
STEP: Waiting until pod test-pod will start running in namespace statefulset-5265
STEP: Creating statefulset with conflicting port in namespace statefulset-5265
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5265
Jun  7 14:50:32.757: INFO: Observed stateful pod in namespace: statefulset-5265, name: ss-0, uid: df4c35c0-1f08-4daa-baeb-057e377161b7, status phase: Pending. Waiting for statefulset controller to delete.
Jun  7 14:50:32.774: INFO: Observed stateful pod in namespace: statefulset-5265, name: ss-0, uid: df4c35c0-1f08-4daa-baeb-057e377161b7, status phase: Failed. Waiting for statefulset controller to delete.
Jun  7 14:50:32.782: INFO: Observed stateful pod in namespace: statefulset-5265, name: ss-0, uid: df4c35c0-1f08-4daa-baeb-057e377161b7, status phase: Failed. Waiting for statefulset controller to delete.
Jun  7 14:50:32.787: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5265
STEP: Removing pod with conflicting port in namespace statefulset-5265
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5265 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun  7 14:50:36.824: INFO: Deleting all statefulset in ns statefulset-5265
Jun  7 14:50:36.829: INFO: Scaling statefulset ss to 0
Jun  7 14:50:46.857: INFO: Waiting for statefulset status.replicas updated to 0
Jun  7 14:50:46.862: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:50:46.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5265" for this suite.

• [SLOW TEST:16.239 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":294,"skipped":5046,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:50:46.900: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Jun  7 14:50:46.945: INFO: Waiting up to 5m0s for pod "var-expansion-7782b345-2a30-411a-a241-0891d3e2d373" in namespace "var-expansion-9107" to be "Succeeded or Failed"
Jun  7 14:50:46.949: INFO: Pod "var-expansion-7782b345-2a30-411a-a241-0891d3e2d373": Phase="Pending", Reason="", readiness=false. Elapsed: 3.832968ms
Jun  7 14:50:48.955: INFO: Pod "var-expansion-7782b345-2a30-411a-a241-0891d3e2d373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009210393s
STEP: Saw pod success
Jun  7 14:50:48.955: INFO: Pod "var-expansion-7782b345-2a30-411a-a241-0891d3e2d373" satisfied condition "Succeeded or Failed"
Jun  7 14:50:48.957: INFO: Trying to get logs from node proact-prod01-wk002 pod var-expansion-7782b345-2a30-411a-a241-0891d3e2d373 container dapi-container: <nil>
STEP: delete the pod
Jun  7 14:50:48.977: INFO: Waiting for pod var-expansion-7782b345-2a30-411a-a241-0891d3e2d373 to disappear
Jun  7 14:50:48.980: INFO: Pod var-expansion-7782b345-2a30-411a-a241-0891d3e2d373 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:50:48.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9107" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":295,"skipped":5059,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:50:48.991: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun  7 14:50:49.026: INFO: Waiting up to 5m0s for pod "pod-5790032b-32bc-4570-b960-6fc7dcf44293" in namespace "emptydir-5002" to be "Succeeded or Failed"
Jun  7 14:50:49.034: INFO: Pod "pod-5790032b-32bc-4570-b960-6fc7dcf44293": Phase="Pending", Reason="", readiness=false. Elapsed: 7.730194ms
Jun  7 14:50:51.039: INFO: Pod "pod-5790032b-32bc-4570-b960-6fc7dcf44293": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012660905s
STEP: Saw pod success
Jun  7 14:50:51.039: INFO: Pod "pod-5790032b-32bc-4570-b960-6fc7dcf44293" satisfied condition "Succeeded or Failed"
Jun  7 14:50:51.043: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-5790032b-32bc-4570-b960-6fc7dcf44293 container test-container: <nil>
STEP: delete the pod
Jun  7 14:50:51.063: INFO: Waiting for pod pod-5790032b-32bc-4570-b960-6fc7dcf44293 to disappear
Jun  7 14:50:51.065: INFO: Pod pod-5790032b-32bc-4570-b960-6fc7dcf44293 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:50:51.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5002" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5073,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:50:51.081: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:50:51.406: INFO: Checking APIGroup: apiregistration.k8s.io
Jun  7 14:50:51.408: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun  7 14:50:51.408: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jun  7 14:50:51.408: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun  7 14:50:51.408: INFO: Checking APIGroup: apps
Jun  7 14:50:51.410: INFO: PreferredVersion.GroupVersion: apps/v1
Jun  7 14:50:51.410: INFO: Versions found [{apps/v1 v1}]
Jun  7 14:50:51.410: INFO: apps/v1 matches apps/v1
Jun  7 14:50:51.410: INFO: Checking APIGroup: events.k8s.io
Jun  7 14:50:51.411: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun  7 14:50:51.411: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jun  7 14:50:51.411: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun  7 14:50:51.411: INFO: Checking APIGroup: authentication.k8s.io
Jun  7 14:50:51.412: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun  7 14:50:51.412: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jun  7 14:50:51.412: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun  7 14:50:51.412: INFO: Checking APIGroup: authorization.k8s.io
Jun  7 14:50:51.414: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun  7 14:50:51.414: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jun  7 14:50:51.414: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun  7 14:50:51.414: INFO: Checking APIGroup: autoscaling
Jun  7 14:50:51.416: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jun  7 14:50:51.416: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jun  7 14:50:51.416: INFO: autoscaling/v1 matches autoscaling/v1
Jun  7 14:50:51.416: INFO: Checking APIGroup: batch
Jun  7 14:50:51.417: INFO: PreferredVersion.GroupVersion: batch/v1
Jun  7 14:50:51.417: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jun  7 14:50:51.417: INFO: batch/v1 matches batch/v1
Jun  7 14:50:51.417: INFO: Checking APIGroup: certificates.k8s.io
Jun  7 14:50:51.419: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun  7 14:50:51.419: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jun  7 14:50:51.419: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun  7 14:50:51.419: INFO: Checking APIGroup: networking.k8s.io
Jun  7 14:50:51.420: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun  7 14:50:51.420: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jun  7 14:50:51.420: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun  7 14:50:51.420: INFO: Checking APIGroup: policy
Jun  7 14:50:51.421: INFO: PreferredVersion.GroupVersion: policy/v1
Jun  7 14:50:51.421: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jun  7 14:50:51.421: INFO: policy/v1 matches policy/v1
Jun  7 14:50:51.421: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun  7 14:50:51.423: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun  7 14:50:51.423: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jun  7 14:50:51.423: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun  7 14:50:51.423: INFO: Checking APIGroup: storage.k8s.io
Jun  7 14:50:51.424: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun  7 14:50:51.424: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun  7 14:50:51.424: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun  7 14:50:51.424: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun  7 14:50:51.426: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun  7 14:50:51.426: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jun  7 14:50:51.426: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun  7 14:50:51.426: INFO: Checking APIGroup: apiextensions.k8s.io
Jun  7 14:50:51.427: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun  7 14:50:51.427: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jun  7 14:50:51.427: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun  7 14:50:51.427: INFO: Checking APIGroup: scheduling.k8s.io
Jun  7 14:50:51.428: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun  7 14:50:51.428: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jun  7 14:50:51.428: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun  7 14:50:51.428: INFO: Checking APIGroup: coordination.k8s.io
Jun  7 14:50:51.429: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun  7 14:50:51.429: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jun  7 14:50:51.429: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun  7 14:50:51.429: INFO: Checking APIGroup: node.k8s.io
Jun  7 14:50:51.430: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun  7 14:50:51.430: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jun  7 14:50:51.430: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun  7 14:50:51.430: INFO: Checking APIGroup: discovery.k8s.io
Jun  7 14:50:51.431: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jun  7 14:50:51.431: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jun  7 14:50:51.431: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jun  7 14:50:51.431: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun  7 14:50:51.432: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Jun  7 14:50:51.432: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jun  7 14:50:51.432: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Jun  7 14:50:51.432: INFO: Checking APIGroup: acme.cert-manager.io
Jun  7 14:50:51.433: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Jun  7 14:50:51.433: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Jun  7 14:50:51.433: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Jun  7 14:50:51.433: INFO: Checking APIGroup: cert-manager.io
Jun  7 14:50:51.434: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Jun  7 14:50:51.434: INFO: Versions found [{cert-manager.io/v1 v1}]
Jun  7 14:50:51.434: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Jun  7 14:50:51.434: INFO: Checking APIGroup: crd.projectcalico.org
Jun  7 14:50:51.435: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jun  7 14:50:51.435: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jun  7 14:50:51.435: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jun  7 14:50:51.435: INFO: Checking APIGroup: dex.coreos.com
Jun  7 14:50:51.436: INFO: PreferredVersion.GroupVersion: dex.coreos.com/v1
Jun  7 14:50:51.436: INFO: Versions found [{dex.coreos.com/v1 v1}]
Jun  7 14:50:51.436: INFO: dex.coreos.com/v1 matches dex.coreos.com/v1
Jun  7 14:50:51.436: INFO: Checking APIGroup: kubegres.reactive-tech.io
Jun  7 14:50:51.437: INFO: PreferredVersion.GroupVersion: kubegres.reactive-tech.io/v1
Jun  7 14:50:51.437: INFO: Versions found [{kubegres.reactive-tech.io/v1 v1}]
Jun  7 14:50:51.437: INFO: kubegres.reactive-tech.io/v1 matches kubegres.reactive-tech.io/v1
Jun  7 14:50:51.437: INFO: Checking APIGroup: operator.tigera.io
Jun  7 14:50:51.438: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Jun  7 14:50:51.438: INFO: Versions found [{operator.tigera.io/v1 v1}]
Jun  7 14:50:51.438: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Jun  7 14:50:51.438: INFO: Checking APIGroup: trident.netapp.io
Jun  7 14:50:51.439: INFO: PreferredVersion.GroupVersion: trident.netapp.io/v1
Jun  7 14:50:51.440: INFO: Versions found [{trident.netapp.io/v1 v1}]
Jun  7 14:50:51.440: INFO: trident.netapp.io/v1 matches trident.netapp.io/v1
Jun  7 14:50:51.440: INFO: Checking APIGroup: argoproj.io
Jun  7 14:50:51.440: INFO: PreferredVersion.GroupVersion: argoproj.io/v1alpha1
Jun  7 14:50:51.440: INFO: Versions found [{argoproj.io/v1alpha1 v1alpha1}]
Jun  7 14:50:51.440: INFO: argoproj.io/v1alpha1 matches argoproj.io/v1alpha1
Jun  7 14:50:51.440: INFO: Checking APIGroup: extensions.istio.io
Jun  7 14:50:51.441: INFO: PreferredVersion.GroupVersion: extensions.istio.io/v1alpha1
Jun  7 14:50:51.441: INFO: Versions found [{extensions.istio.io/v1alpha1 v1alpha1}]
Jun  7 14:50:51.441: INFO: extensions.istio.io/v1alpha1 matches extensions.istio.io/v1alpha1
Jun  7 14:50:51.441: INFO: Checking APIGroup: install.istio.io
Jun  7 14:50:51.442: INFO: PreferredVersion.GroupVersion: install.istio.io/v1alpha1
Jun  7 14:50:51.442: INFO: Versions found [{install.istio.io/v1alpha1 v1alpha1}]
Jun  7 14:50:51.442: INFO: install.istio.io/v1alpha1 matches install.istio.io/v1alpha1
Jun  7 14:50:51.442: INFO: Checking APIGroup: telemetry.istio.io
Jun  7 14:50:51.443: INFO: PreferredVersion.GroupVersion: telemetry.istio.io/v1alpha1
Jun  7 14:50:51.443: INFO: Versions found [{telemetry.istio.io/v1alpha1 v1alpha1}]
Jun  7 14:50:51.444: INFO: telemetry.istio.io/v1alpha1 matches telemetry.istio.io/v1alpha1
Jun  7 14:50:51.444: INFO: Checking APIGroup: networking.istio.io
Jun  7 14:50:51.445: INFO: PreferredVersion.GroupVersion: networking.istio.io/v1beta1
Jun  7 14:50:51.445: INFO: Versions found [{networking.istio.io/v1beta1 v1beta1} {networking.istio.io/v1alpha3 v1alpha3}]
Jun  7 14:50:51.445: INFO: networking.istio.io/v1beta1 matches networking.istio.io/v1beta1
Jun  7 14:50:51.445: INFO: Checking APIGroup: security.istio.io
Jun  7 14:50:51.446: INFO: PreferredVersion.GroupVersion: security.istio.io/v1beta1
Jun  7 14:50:51.446: INFO: Versions found [{security.istio.io/v1beta1 v1beta1}]
Jun  7 14:50:51.446: INFO: security.istio.io/v1beta1 matches security.istio.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:50:51.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3573" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":297,"skipped":5115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:50:51.460: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 14:50:51.503: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f40f447-dc39-4553-8e53-9a9344b4208a" in namespace "downward-api-823" to be "Succeeded or Failed"
Jun  7 14:50:51.506: INFO: Pod "downwardapi-volume-1f40f447-dc39-4553-8e53-9a9344b4208a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.625483ms
Jun  7 14:50:53.512: INFO: Pod "downwardapi-volume-1f40f447-dc39-4553-8e53-9a9344b4208a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008368615s
STEP: Saw pod success
Jun  7 14:50:53.512: INFO: Pod "downwardapi-volume-1f40f447-dc39-4553-8e53-9a9344b4208a" satisfied condition "Succeeded or Failed"
Jun  7 14:50:53.516: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-1f40f447-dc39-4553-8e53-9a9344b4208a container client-container: <nil>
STEP: delete the pod
Jun  7 14:50:53.540: INFO: Waiting for pod downwardapi-volume-1f40f447-dc39-4553-8e53-9a9344b4208a to disappear
Jun  7 14:50:53.543: INFO: Pod downwardapi-volume-1f40f447-dc39-4553-8e53-9a9344b4208a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:50:53.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-823" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":298,"skipped":5146,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:50:53.566: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jun  7 14:50:53.608: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Jun  7 14:50:54.080: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun  7 14:50:56.154: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 14:50:58.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 14:51:00.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 14:51:02.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63790210254, loc:(*time.Location)(0xa0aaf60)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  7 14:51:04.314: INFO: Waited 137.067442ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jun  7 14:51:04.545: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:51:05.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3534" for this suite.

• [SLOW TEST:11.879 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":299,"skipped":5169,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:51:05.445: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:51:05.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9549 create -f -'
Jun  7 14:51:06.684: INFO: stderr: ""
Jun  7 14:51:06.684: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun  7 14:51:06.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9549 create -f -'
Jun  7 14:51:07.864: INFO: stderr: ""
Jun  7 14:51:07.864: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun  7 14:51:08.875: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  7 14:51:08.875: INFO: Found 1 / 1
Jun  7 14:51:08.875: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  7 14:51:08.879: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  7 14:51:08.880: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  7 14:51:08.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9549 describe pod agnhost-primary-6hxtf'
Jun  7 14:51:08.971: INFO: stderr: ""
Jun  7 14:51:08.971: INFO: stdout: "Name:         agnhost-primary-6hxtf\nNamespace:    kubectl-9549\nPriority:     0\nNode:         proact-prod01-wk002/10.55.210.14\nStart Time:   Tue, 07 Jun 2022 14:51:06 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 428d11fab600c91f2959b3cec9f880978c4a819914912b1becb09eabf1382b09\n              cni.projectcalico.org/podIP: 192.168.39.232/32\n              cni.projectcalico.org/podIPs: 192.168.39.232/32\nStatus:       Running\nIP:           192.168.39.232\nIPs:\n  IP:           192.168.39.232\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://99c789d5e68ed74da8a41c0f56afd4d058e51417f0cfaa9164784c548dd8ed47\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 07 Jun 2022 14:51:07 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fz2fz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-fz2fz:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9549/agnhost-primary-6hxtf to proact-prod01-wk002\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jun  7 14:51:08.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9549 describe rc agnhost-primary'
Jun  7 14:51:09.066: INFO: stderr: ""
Jun  7 14:51:09.066: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9549\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-6hxtf\n"
Jun  7 14:51:09.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9549 describe service agnhost-primary'
Jun  7 14:51:09.149: INFO: stderr: ""
Jun  7 14:51:09.149: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9549\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.104.175.230\nIPs:               10.104.175.230\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.39.232:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun  7 14:51:09.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9549 describe node proact-prod01-cp001'
Jun  7 14:51:09.293: INFO: stderr: ""
Jun  7 14:51:09.293: INFO: stdout: "Name:               proact-prod01-cp001\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=proact-prod01-cp001\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi.trident.netapp.io\":\"proact-prod01-cp001\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.55.210.10/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.163.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 10 Mar 2022 14:39:26 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  proact-prod01-cp001\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 07 Jun 2022 14:51:06 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 10 Mar 2022 14:40:49 +0000   Thu, 10 Mar 2022 14:40:49 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 07 Jun 2022 14:48:42 +0000   Mon, 16 May 2022 17:57:46 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 07 Jun 2022 14:48:42 +0000   Mon, 16 May 2022 17:57:46 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 07 Jun 2022 14:48:42 +0000   Mon, 16 May 2022 17:57:46 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 07 Jun 2022 14:48:42 +0000   Mon, 16 May 2022 17:57:46 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.55.210.10\n  Hostname:    proact-prod01-cp001\nCapacity:\n  cpu:                4\n  ephemeral-storage:  101141520Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             61735724Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  93212024678\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             61633324Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c4da51a9b4d8414d85d590e4c2e133f2\n  System UUID:                b2cd6a23-6f0b-48f7-88ca-d81afe548520\n  Boot ID:                    cfe222e6-ef4e-44ed-9f42-e04738cd13d9\n  Kernel Version:             5.10.0-12-amd64\n  OS Image:                   Debian GNU/Linux 11 (bullseye)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.22.2\n  Kubelet Version:            v1.22.5\n  Kube-Proxy Version:         v1.22.5\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system               calico-kube-controllers-868b656ff4-m825j                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         89d\n  calico-system               calico-node-fcrlh                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         89d\n  calico-system               calico-typha-54559758b4-nrb6p                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         89d\n  kube-system                 etcd-proact-prod01-cp001                                   100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         89d\n  kube-system                 kube-apiserver-proact-prod01-cp001                         250m (6%)     0 (0%)      0 (0%)           0 (0%)         89d\n  kube-system                 kube-controller-manager-proact-prod01-cp001                200m (5%)     0 (0%)      0 (0%)           0 (0%)         89d\n  kube-system                 kube-proxy-9vl85                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         89d\n  kube-system                 kube-scheduler-proact-prod01-cp001                         100m (2%)     0 (0%)      0 (0%)           0 (0%)         89d\n  loki                        loki-fluent-bit-loki-tjrg5                                 100m (2%)     0 (0%)      100Mi (0%)       100Mi (0%)     56d\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0d25c3a56cbb4d81-tg66q    0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  tigera-operator             tigera-operator-698876cbb5-mclv9                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         89d\n  trident                     trident-csi-hml7r                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         81d\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                750m (18%)  0 (0%)\n  memory             200Mi (0%)  100Mi (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Jun  7 14:51:09.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-9549 describe namespace kubectl-9549'
Jun  7 14:51:09.380: INFO: stderr: ""
Jun  7 14:51:09.380: INFO: stdout: "Name:         kubectl-9549\nLabels:       e2e-framework=kubectl\n              e2e-run=a63240b1-8262-47d7-ba40-543c78f25b7a\n              kubernetes.io/metadata.name=kubectl-9549\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:51:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9549" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":300,"skipped":5171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:51:09.398: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:51:09.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-6704 version'
Jun  7 14:51:09.491: INFO: stderr: ""
Jun  7 14:51:09.492: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.7\", GitCommit:\"b56e432f2191419647a6a13b9f5867801850f969\", GitTreeState:\"clean\", BuildDate:\"2022-02-16T11:50:27Z\", GoVersion:\"go1.16.14\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.7\", GitCommit:\"b56e432f2191419647a6a13b9f5867801850f969\", GitTreeState:\"clean\", BuildDate:\"2022-02-16T11:43:55Z\", GoVersion:\"go1.16.14\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:51:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6704" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":301,"skipped":5204,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:51:09.507: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jun  7 14:51:09.560: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:51:11.568: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jun  7 14:51:11.589: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:51:13.597: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jun  7 14:51:13.613: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  7 14:51:13.619: INFO: Pod pod-with-prestop-http-hook still exists
Jun  7 14:51:15.620: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  7 14:51:15.629: INFO: Pod pod-with-prestop-http-hook still exists
Jun  7 14:51:17.620: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  7 14:51:17.628: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:51:17.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6278" for this suite.

• [SLOW TEST:8.150 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":302,"skipped":5224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:51:17.658: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:51:19.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-873" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":303,"skipped":5249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:51:19.782: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-902
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jun  7 14:51:19.834: INFO: Found 0 stateful pods, waiting for 3
Jun  7 14:51:29.842: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  7 14:51:29.842: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  7 14:51:29.842: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Jun  7 14:51:29.884: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun  7 14:51:39.929: INFO: Updating stateful set ss2
Jun  7 14:51:39.936: INFO: Waiting for Pod statefulset-902/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Jun  7 14:51:49.995: INFO: Found 1 stateful pods, waiting for 3
Jun  7 14:52:00.014: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  7 14:52:00.014: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  7 14:52:00.014: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun  7 14:52:00.048: INFO: Updating stateful set ss2
Jun  7 14:52:00.064: INFO: Waiting for Pod statefulset-902/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jun  7 14:52:10.110: INFO: Updating stateful set ss2
Jun  7 14:52:10.122: INFO: Waiting for StatefulSet statefulset-902/ss2 to complete update
Jun  7 14:52:10.122: INFO: Waiting for Pod statefulset-902/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun  7 14:52:20.140: INFO: Deleting all statefulset in ns statefulset-902
Jun  7 14:52:20.144: INFO: Scaling statefulset ss2 to 0
Jun  7 14:52:30.176: INFO: Waiting for statefulset status.replicas updated to 0
Jun  7 14:52:30.181: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:52:30.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-902" for this suite.

• [SLOW TEST:70.440 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":304,"skipped":5279,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:52:30.222: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-pxkj
STEP: Creating a pod to test atomic-volume-subpath
Jun  7 14:52:30.280: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pxkj" in namespace "subpath-131" to be "Succeeded or Failed"
Jun  7 14:52:30.284: INFO: Pod "pod-subpath-test-configmap-pxkj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.51711ms
Jun  7 14:52:32.298: INFO: Pod "pod-subpath-test-configmap-pxkj": Phase="Running", Reason="", readiness=true. Elapsed: 2.017364517s
Jun  7 14:52:34.311: INFO: Pod "pod-subpath-test-configmap-pxkj": Phase="Running", Reason="", readiness=true. Elapsed: 4.030461636s
Jun  7 14:52:36.317: INFO: Pod "pod-subpath-test-configmap-pxkj": Phase="Running", Reason="", readiness=true. Elapsed: 6.036995351s
Jun  7 14:52:38.324: INFO: Pod "pod-subpath-test-configmap-pxkj": Phase="Running", Reason="", readiness=true. Elapsed: 8.044104459s
Jun  7 14:52:40.332: INFO: Pod "pod-subpath-test-configmap-pxkj": Phase="Running", Reason="", readiness=true. Elapsed: 10.052081278s
Jun  7 14:52:42.346: INFO: Pod "pod-subpath-test-configmap-pxkj": Phase="Running", Reason="", readiness=true. Elapsed: 12.065804799s
Jun  7 14:52:44.355: INFO: Pod "pod-subpath-test-configmap-pxkj": Phase="Running", Reason="", readiness=true. Elapsed: 14.07473968s
Jun  7 14:52:46.363: INFO: Pod "pod-subpath-test-configmap-pxkj": Phase="Running", Reason="", readiness=true. Elapsed: 16.082640764s
Jun  7 14:52:48.370: INFO: Pod "pod-subpath-test-configmap-pxkj": Phase="Running", Reason="", readiness=true. Elapsed: 18.089851617s
Jun  7 14:52:50.383: INFO: Pod "pod-subpath-test-configmap-pxkj": Phase="Running", Reason="", readiness=true. Elapsed: 20.102616094s
Jun  7 14:52:52.394: INFO: Pod "pod-subpath-test-configmap-pxkj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.113568374s
STEP: Saw pod success
Jun  7 14:52:52.394: INFO: Pod "pod-subpath-test-configmap-pxkj" satisfied condition "Succeeded or Failed"
Jun  7 14:52:52.399: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-subpath-test-configmap-pxkj container test-container-subpath-configmap-pxkj: <nil>
STEP: delete the pod
Jun  7 14:52:52.438: INFO: Waiting for pod pod-subpath-test-configmap-pxkj to disappear
Jun  7 14:52:52.442: INFO: Pod pod-subpath-test-configmap-pxkj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pxkj
Jun  7 14:52:52.442: INFO: Deleting pod "pod-subpath-test-configmap-pxkj" in namespace "subpath-131"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:52:52.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-131" for this suite.

• [SLOW TEST:22.240 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":346,"completed":305,"skipped":5287,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:52:52.463: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jun  7 14:52:52.516: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:52:54.528: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jun  7 14:52:54.547: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:52:56.555: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jun  7 14:52:56.571: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  7 14:52:56.576: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  7 14:52:58.577: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  7 14:52:58.588: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  7 14:53:00.576: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  7 14:53:00.583: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:53:00.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1930" for this suite.

• [SLOW TEST:8.151 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":306,"skipped":5300,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:53:00.615: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2720
STEP: creating service affinity-nodeport in namespace services-2720
STEP: creating replication controller affinity-nodeport in namespace services-2720
I0607 14:53:00.698575      23 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2720, replica count: 3
I0607 14:53:03.756327      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  7 14:53:03.775: INFO: Creating new exec pod
Jun  7 14:53:06.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-2720 exec execpod-affinityd6xx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jun  7 14:53:07.016: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun  7 14:53:07.016: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 14:53:07.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-2720 exec execpod-affinityd6xx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.221.13 80'
Jun  7 14:53:07.195: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.221.13 80\nConnection to 10.98.221.13 80 port [tcp/http] succeeded!\n"
Jun  7 14:53:07.195: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 14:53:07.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-2720 exec execpod-affinityd6xx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.55.210.15 32435'
Jun  7 14:53:07.383: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.55.210.15 32435\nConnection to 10.55.210.15 32435 port [tcp/*] succeeded!\n"
Jun  7 14:53:07.383: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 14:53:07.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-2720 exec execpod-affinityd6xx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.55.210.14 32435'
Jun  7 14:53:07.581: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.55.210.14 32435\nConnection to 10.55.210.14 32435 port [tcp/*] succeeded!\n"
Jun  7 14:53:07.581: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 14:53:07.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-2720 exec execpod-affinityd6xx5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.55.210.13:32435/ ; done'
Jun  7 14:53:07.878: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.55.210.13:32435/\n"
Jun  7 14:53:07.878: INFO: stdout: "\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq\naffinity-nodeport-b5tdq"
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Received response from host: affinity-nodeport-b5tdq
Jun  7 14:53:07.878: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2720, will wait for the garbage collector to delete the pods
Jun  7 14:53:07.960: INFO: Deleting ReplicationController affinity-nodeport took: 10.426645ms
Jun  7 14:53:08.062: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.199101ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:53:10.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2720" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.889 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":307,"skipped":5350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:53:10.504: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:53:16.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1567" for this suite.
STEP: Destroying namespace "nsdeletetest-2649" for this suite.
Jun  7 14:53:16.656: INFO: Namespace nsdeletetest-2649 was already deleted
STEP: Destroying namespace "nsdeletetest-8737" for this suite.

• [SLOW TEST:6.157 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":308,"skipped":5378,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:53:16.661: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Jun  7 14:53:16.698: INFO: created test-event-1
Jun  7 14:53:16.701: INFO: created test-event-2
Jun  7 14:53:16.704: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jun  7 14:53:16.706: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jun  7 14:53:16.728: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:53:16.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4773" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":309,"skipped":5457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:53:16.741: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:53:16.799: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun  7 14:53:16.807: INFO: Number of nodes with available pods: 0
Jun  7 14:53:16.807: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun  7 14:53:16.835: INFO: Number of nodes with available pods: 0
Jun  7 14:53:16.835: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:53:17.842: INFO: Number of nodes with available pods: 0
Jun  7 14:53:17.842: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:53:18.845: INFO: Number of nodes with available pods: 1
Jun  7 14:53:18.845: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun  7 14:53:18.878: INFO: Number of nodes with available pods: 1
Jun  7 14:53:18.879: INFO: Number of running nodes: 0, number of available pods: 1
Jun  7 14:53:19.887: INFO: Number of nodes with available pods: 0
Jun  7 14:53:19.887: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun  7 14:53:19.902: INFO: Number of nodes with available pods: 0
Jun  7 14:53:19.903: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:53:20.910: INFO: Number of nodes with available pods: 0
Jun  7 14:53:20.911: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:53:21.906: INFO: Number of nodes with available pods: 0
Jun  7 14:53:21.906: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:53:22.909: INFO: Number of nodes with available pods: 1
Jun  7 14:53:22.909: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6050, will wait for the garbage collector to delete the pods
Jun  7 14:53:22.987: INFO: Deleting DaemonSet.extensions daemon-set took: 12.334376ms
Jun  7 14:53:23.088: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.157031ms
Jun  7 14:53:25.492: INFO: Number of nodes with available pods: 0
Jun  7 14:53:25.492: INFO: Number of running nodes: 0, number of available pods: 0
Jun  7 14:53:25.496: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33601715"},"items":null}

Jun  7 14:53:25.499: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33601715"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:53:25.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6050" for this suite.

• [SLOW TEST:8.797 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":310,"skipped":5511,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:53:25.539: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 14:53:25.577: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c68c2dde-f263-4369-aeb1-e7f80f54d583" in namespace "projected-3261" to be "Succeeded or Failed"
Jun  7 14:53:25.580: INFO: Pod "downwardapi-volume-c68c2dde-f263-4369-aeb1-e7f80f54d583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.514438ms
Jun  7 14:53:27.587: INFO: Pod "downwardapi-volume-c68c2dde-f263-4369-aeb1-e7f80f54d583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009495657s
STEP: Saw pod success
Jun  7 14:53:27.587: INFO: Pod "downwardapi-volume-c68c2dde-f263-4369-aeb1-e7f80f54d583" satisfied condition "Succeeded or Failed"
Jun  7 14:53:27.592: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-c68c2dde-f263-4369-aeb1-e7f80f54d583 container client-container: <nil>
STEP: delete the pod
Jun  7 14:53:27.617: INFO: Waiting for pod downwardapi-volume-c68c2dde-f263-4369-aeb1-e7f80f54d583 to disappear
Jun  7 14:53:27.623: INFO: Pod downwardapi-volume-c68c2dde-f263-4369-aeb1-e7f80f54d583 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:53:27.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3261" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":311,"skipped":5545,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:53:27.633: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-6ca14154-26e9-440f-882c-0df6d2b3251f
STEP: Creating a pod to test consume secrets
Jun  7 14:53:27.672: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1e999d43-bd4a-4406-84ff-b0a2a02d6c4f" in namespace "projected-6392" to be "Succeeded or Failed"
Jun  7 14:53:27.676: INFO: Pod "pod-projected-secrets-1e999d43-bd4a-4406-84ff-b0a2a02d6c4f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.860062ms
Jun  7 14:53:29.682: INFO: Pod "pod-projected-secrets-1e999d43-bd4a-4406-84ff-b0a2a02d6c4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009696637s
STEP: Saw pod success
Jun  7 14:53:29.682: INFO: Pod "pod-projected-secrets-1e999d43-bd4a-4406-84ff-b0a2a02d6c4f" satisfied condition "Succeeded or Failed"
Jun  7 14:53:29.686: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-secrets-1e999d43-bd4a-4406-84ff-b0a2a02d6c4f container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  7 14:53:29.714: INFO: Waiting for pod pod-projected-secrets-1e999d43-bd4a-4406-84ff-b0a2a02d6c4f to disappear
Jun  7 14:53:29.717: INFO: Pod pod-projected-secrets-1e999d43-bd4a-4406-84ff-b0a2a02d6c4f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:53:29.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6392" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":312,"skipped":5569,"failed":0}

------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:53:29.730: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:55:01.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2748" for this suite.

• [SLOW TEST:92.099 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":313,"skipped":5569,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:55:01.831: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:55:01.864: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  7 14:55:06.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-5141 --namespace=crd-publish-openapi-5141 create -f -'
Jun  7 14:55:07.851: INFO: stderr: ""
Jun  7 14:55:07.851: INFO: stdout: "e2e-test-crd-publish-openapi-1535-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  7 14:55:07.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-5141 --namespace=crd-publish-openapi-5141 delete e2e-test-crd-publish-openapi-1535-crds test-cr'
Jun  7 14:55:07.932: INFO: stderr: ""
Jun  7 14:55:07.932: INFO: stdout: "e2e-test-crd-publish-openapi-1535-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun  7 14:55:07.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-5141 --namespace=crd-publish-openapi-5141 apply -f -'
Jun  7 14:55:08.793: INFO: stderr: ""
Jun  7 14:55:08.793: INFO: stdout: "e2e-test-crd-publish-openapi-1535-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  7 14:55:08.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-5141 --namespace=crd-publish-openapi-5141 delete e2e-test-crd-publish-openapi-1535-crds test-cr'
Jun  7 14:55:08.874: INFO: stderr: ""
Jun  7 14:55:08.874: INFO: stdout: "e2e-test-crd-publish-openapi-1535-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun  7 14:55:08.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=crd-publish-openapi-5141 explain e2e-test-crd-publish-openapi-1535-crds'
Jun  7 14:55:09.656: INFO: stderr: ""
Jun  7 14:55:09.656: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1535-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:55:14.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5141" for this suite.

• [SLOW TEST:12.416 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":314,"skipped":5652,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:55:14.248: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:55:14.292: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:55:14.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-793" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":315,"skipped":5656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:55:14.905: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-27ee6010-66f1-4312-9af6-a97f2b0e760f
STEP: Creating secret with name s-test-opt-upd-77a40813-06b7-4246-bc85-a171dc95137f
STEP: Creating the pod
Jun  7 14:55:14.959: INFO: The status of Pod pod-projected-secrets-12230111-0e21-4cc2-93c8-73df557e3046 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:55:16.965: INFO: The status of Pod pod-projected-secrets-12230111-0e21-4cc2-93c8-73df557e3046 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:55:18.972: INFO: The status of Pod pod-projected-secrets-12230111-0e21-4cc2-93c8-73df557e3046 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-27ee6010-66f1-4312-9af6-a97f2b0e760f
STEP: Updating secret s-test-opt-upd-77a40813-06b7-4246-bc85-a171dc95137f
STEP: Creating secret with name s-test-opt-create-9739b523-1680-44b7-aca3-500ce9a2f3ff
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:56:47.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8197" for this suite.

• [SLOW TEST:92.706 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":316,"skipped":5683,"failed":0}
SSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:56:47.612: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jun  7 14:56:47.661: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:56:49.673: INFO: The status of Pod pod1 is Running (Ready = false)
Jun  7 14:56:51.675: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.55.210.16 on the node which pod1 resides and expect scheduled
Jun  7 14:56:51.700: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:56:53.714: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.55.210.16 but use UDP protocol on the node which pod2 resides
Jun  7 14:56:53.735: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:56:55.748: INFO: The status of Pod pod3 is Running (Ready = false)
Jun  7 14:56:57.742: INFO: The status of Pod pod3 is Running (Ready = true)
Jun  7 14:56:57.760: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun  7 14:56:59.773: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jun  7 14:56:59.777: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.55.210.16 http://127.0.0.1:54323/hostname] Namespace:hostport-6099 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:56:59.777: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.55.210.16, port: 54323
Jun  7 14:56:59.923: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.55.210.16:54323/hostname] Namespace:hostport-6099 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:56:59.923: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.55.210.16, port: 54323 UDP
Jun  7 14:57:00.050: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.55.210.16 54323] Namespace:hostport-6099 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  7 14:57:00.050: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:57:05.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-6099" for this suite.

• [SLOW TEST:17.587 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":317,"skipped":5689,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:57:05.199: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1396
STEP: creating an pod
Jun  7 14:57:05.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3233 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun  7 14:57:05.315: INFO: stderr: ""
Jun  7 14:57:05.315: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Jun  7 14:57:05.315: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun  7 14:57:05.315: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3233" to be "running and ready, or succeeded"
Jun  7 14:57:05.318: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.482198ms
Jun  7 14:57:07.326: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.011115998s
Jun  7 14:57:07.326: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun  7 14:57:07.326: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun  7 14:57:07.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3233 logs logs-generator logs-generator'
Jun  7 14:57:07.412: INFO: stderr: ""
Jun  7 14:57:07.412: INFO: stdout: "I0607 14:57:06.159920       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/9xb 433\nI0607 14:57:06.360343       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/xpt 558\nI0607 14:57:06.561005       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/7sk5 419\nI0607 14:57:06.761296       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/5l5h 563\nI0607 14:57:06.960294       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/pvxt 243\nI0607 14:57:07.160881       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/t72 597\nI0607 14:57:07.360927       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/7bbm 509\n"
STEP: limiting log lines
Jun  7 14:57:07.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3233 logs logs-generator logs-generator --tail=1'
Jun  7 14:57:07.491: INFO: stderr: ""
Jun  7 14:57:07.491: INFO: stdout: "I0607 14:57:07.360927       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/7bbm 509\n"
Jun  7 14:57:07.491: INFO: got output "I0607 14:57:07.360927       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/7bbm 509\n"
STEP: limiting log bytes
Jun  7 14:57:07.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3233 logs logs-generator logs-generator --limit-bytes=1'
Jun  7 14:57:07.572: INFO: stderr: ""
Jun  7 14:57:07.572: INFO: stdout: "I"
Jun  7 14:57:07.572: INFO: got output "I"
STEP: exposing timestamps
Jun  7 14:57:07.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3233 logs logs-generator logs-generator --tail=1 --timestamps'
Jun  7 14:57:07.641: INFO: stderr: ""
Jun  7 14:57:07.641: INFO: stdout: "2022-06-07T16:57:07.560785904+02:00 I0607 14:57:07.560683       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/777 202\n"
Jun  7 14:57:07.641: INFO: got output "2022-06-07T16:57:07.560785904+02:00 I0607 14:57:07.560683       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/777 202\n"
STEP: restricting to a time range
Jun  7 14:57:10.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3233 logs logs-generator logs-generator --since=1s'
Jun  7 14:57:10.239: INFO: stderr: ""
Jun  7 14:57:10.239: INFO: stdout: "I0607 14:57:09.361082       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/v6zd 589\nI0607 14:57:09.560651       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/cp2 290\nI0607 14:57:09.760974       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/6zfl 329\nI0607 14:57:09.960443       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/ptm 346\nI0607 14:57:10.160932       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/8kvm 470\n"
Jun  7 14:57:10.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3233 logs logs-generator logs-generator --since=24h'
Jun  7 14:57:10.312: INFO: stderr: ""
Jun  7 14:57:10.313: INFO: stdout: "I0607 14:57:06.159920       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/9xb 433\nI0607 14:57:06.360343       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/xpt 558\nI0607 14:57:06.561005       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/7sk5 419\nI0607 14:57:06.761296       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/5l5h 563\nI0607 14:57:06.960294       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/pvxt 243\nI0607 14:57:07.160881       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/t72 597\nI0607 14:57:07.360927       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/7bbm 509\nI0607 14:57:07.560683       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/777 202\nI0607 14:57:07.760142       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/6ls 388\nI0607 14:57:07.960764       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/6fg7 389\nI0607 14:57:08.160201       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/bn9 436\nI0607 14:57:08.360626       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/vp7 332\nI0607 14:57:08.561044       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/57n 511\nI0607 14:57:08.760537       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/qjpc 342\nI0607 14:57:08.960996       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/pclz 558\nI0607 14:57:09.160530       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/xtd 577\nI0607 14:57:09.361082       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/v6zd 589\nI0607 14:57:09.560651       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/cp2 290\nI0607 14:57:09.760974       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/6zfl 329\nI0607 14:57:09.960443       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/ptm 346\nI0607 14:57:10.160932       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/8kvm 470\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1401
Jun  7 14:57:10.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=kubectl-3233 delete pod logs-generator'
Jun  7 14:57:11.659: INFO: stderr: ""
Jun  7 14:57:11.659: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:57:11.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3233" for this suite.

• [SLOW TEST:6.473 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1393
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":318,"skipped":5701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:57:11.672: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 14:57:11.931: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 14:57:14.973: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:57:15.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1007" for this suite.
STEP: Destroying namespace "webhook-1007-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":319,"skipped":5731,"failed":0}

------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:57:15.122: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8881.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8881.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8881.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8881.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8881.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8881.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  7 14:57:17.215: INFO: DNS probes using dns-8881/dns-test-10bb79a6-9fc2-4f93-9ce4-e6540a94b7b4 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:57:17.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8881" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":320,"skipped":5731,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:57:17.238: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-45a5a160-7fde-457e-947a-c71cdb0592b4
STEP: Creating a pod to test consume secrets
Jun  7 14:57:17.277: INFO: Waiting up to 5m0s for pod "pod-secrets-3cfdd768-3a32-404d-ba71-3bd886f077ff" in namespace "secrets-351" to be "Succeeded or Failed"
Jun  7 14:57:17.280: INFO: Pod "pod-secrets-3cfdd768-3a32-404d-ba71-3bd886f077ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.684041ms
Jun  7 14:57:19.289: INFO: Pod "pod-secrets-3cfdd768-3a32-404d-ba71-3bd886f077ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011708634s
STEP: Saw pod success
Jun  7 14:57:19.289: INFO: Pod "pod-secrets-3cfdd768-3a32-404d-ba71-3bd886f077ff" satisfied condition "Succeeded or Failed"
Jun  7 14:57:19.293: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-secrets-3cfdd768-3a32-404d-ba71-3bd886f077ff container secret-volume-test: <nil>
STEP: delete the pod
Jun  7 14:57:19.326: INFO: Waiting for pod pod-secrets-3cfdd768-3a32-404d-ba71-3bd886f077ff to disappear
Jun  7 14:57:19.329: INFO: Pod pod-secrets-3cfdd768-3a32-404d-ba71-3bd886f077ff no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:57:19.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-351" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":321,"skipped":5732,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:57:19.339: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:142
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 14:57:19.390: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun  7 14:57:19.403: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:19.404: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:19.404: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:19.407: INFO: Number of nodes with available pods: 0
Jun  7 14:57:19.407: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:57:20.413: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:20.414: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:20.414: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:20.417: INFO: Number of nodes with available pods: 1
Jun  7 14:57:20.417: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:57:21.422: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:21.422: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:21.422: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:21.428: INFO: Number of nodes with available pods: 4
Jun  7 14:57:21.428: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun  7 14:57:21.476: INFO: Wrong image for pod: daemon-set-7ldt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:21.476: INFO: Wrong image for pod: daemon-set-mpwz5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:21.476: INFO: Wrong image for pod: daemon-set-mxsbw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:21.476: INFO: Wrong image for pod: daemon-set-p2mjb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:21.483: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:21.483: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:21.483: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:22.495: INFO: Wrong image for pod: daemon-set-7ldt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:22.495: INFO: Wrong image for pod: daemon-set-mpwz5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:22.495: INFO: Wrong image for pod: daemon-set-mxsbw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:22.500: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:22.500: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:22.500: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:23.492: INFO: Wrong image for pod: daemon-set-7ldt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:23.492: INFO: Wrong image for pod: daemon-set-mpwz5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:23.492: INFO: Wrong image for pod: daemon-set-mxsbw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:23.501: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:23.501: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:23.501: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:24.494: INFO: Wrong image for pod: daemon-set-7ldt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:24.494: INFO: Pod daemon-set-d6c84 is not available
Jun  7 14:57:24.494: INFO: Wrong image for pod: daemon-set-mpwz5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:24.494: INFO: Wrong image for pod: daemon-set-mxsbw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:24.500: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:24.500: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:24.500: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:25.489: INFO: Pod daemon-set-d4gcn is not available
Jun  7 14:57:25.490: INFO: Wrong image for pod: daemon-set-mpwz5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:25.490: INFO: Wrong image for pod: daemon-set-mxsbw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:25.495: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:25.495: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:25.496: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:26.491: INFO: Wrong image for pod: daemon-set-mxsbw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:26.496: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:26.496: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:26.496: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:27.495: INFO: Wrong image for pod: daemon-set-mxsbw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jun  7 14:57:27.496: INFO: Pod daemon-set-rrxh8 is not available
Jun  7 14:57:27.504: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:27.504: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:27.504: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:28.502: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:28.502: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:28.502: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:29.491: INFO: Pod daemon-set-lcjq4 is not available
Jun  7 14:57:29.496: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:29.496: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:29.496: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jun  7 14:57:29.500: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:29.501: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:29.501: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:29.504: INFO: Number of nodes with available pods: 3
Jun  7 14:57:29.504: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:57:30.521: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:30.521: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:30.521: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:30.526: INFO: Number of nodes with available pods: 3
Jun  7 14:57:30.526: INFO: Node proact-prod01-wk001 is running more than one daemon pod
Jun  7 14:57:31.519: INFO: DaemonSet pods can't tolerate node proact-prod01-cp001 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:31.520: INFO: DaemonSet pods can't tolerate node proact-prod01-cp002 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:31.520: INFO: DaemonSet pods can't tolerate node proact-prod01-cp003 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun  7 14:57:31.524: INFO: Number of nodes with available pods: 4
Jun  7 14:57:31.524: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:108
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2025, will wait for the garbage collector to delete the pods
Jun  7 14:57:31.613: INFO: Deleting DaemonSet.extensions daemon-set took: 15.13992ms
Jun  7 14:57:31.714: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.693103ms
Jun  7 14:57:33.826: INFO: Number of nodes with available pods: 0
Jun  7 14:57:33.826: INFO: Number of running nodes: 0, number of available pods: 0
Jun  7 14:57:33.830: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33603740"},"items":null}

Jun  7 14:57:33.834: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33603740"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 14:57:33.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2025" for this suite.

• [SLOW TEST:14.530 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":322,"skipped":5733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 14:57:33.870: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:02:33.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1101" for this suite.

• [SLOW TEST:300.095 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":323,"skipped":5759,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:02:33.966: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:92
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:107
STEP: Creating service test in namespace statefulset-1497
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-1497
Jun  7 15:02:34.016: INFO: Found 0 stateful pods, waiting for 1
Jun  7 15:02:44.026: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:118
Jun  7 15:02:44.060: INFO: Deleting all statefulset in ns statefulset-1497
Jun  7 15:02:44.062: INFO: Scaling statefulset ss to 0
Jun  7 15:02:54.099: INFO: Waiting for statefulset status.replicas updated to 0
Jun  7 15:02:54.106: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:02:54.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1497" for this suite.

• [SLOW TEST:20.175 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:97
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":324,"skipped":5764,"failed":0}
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:02:54.141: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:02:54.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5346" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":325,"skipped":5764,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:02:54.202: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 15:02:54.684: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 15:02:57.729: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:03:09.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7593" for this suite.
STEP: Destroying namespace "webhook-7593-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.795 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":326,"skipped":5768,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:03:09.999: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Jun  7 15:03:10.043: INFO: Waiting up to 5m0s for pod "client-containers-93a9589c-4ab5-4f8e-87e7-807a4d2953d0" in namespace "containers-5772" to be "Succeeded or Failed"
Jun  7 15:03:10.045: INFO: Pod "client-containers-93a9589c-4ab5-4f8e-87e7-807a4d2953d0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.980172ms
Jun  7 15:03:12.058: INFO: Pod "client-containers-93a9589c-4ab5-4f8e-87e7-807a4d2953d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014782855s
STEP: Saw pod success
Jun  7 15:03:12.058: INFO: Pod "client-containers-93a9589c-4ab5-4f8e-87e7-807a4d2953d0" satisfied condition "Succeeded or Failed"
Jun  7 15:03:12.060: INFO: Trying to get logs from node proact-prod01-wk002 pod client-containers-93a9589c-4ab5-4f8e-87e7-807a4d2953d0 container agnhost-container: <nil>
STEP: delete the pod
Jun  7 15:03:12.081: INFO: Waiting for pod client-containers-93a9589c-4ab5-4f8e-87e7-807a4d2953d0 to disappear
Jun  7 15:03:12.084: INFO: Pod client-containers-93a9589c-4ab5-4f8e-87e7-807a4d2953d0 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:03:12.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5772" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":5776,"failed":0}
SS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:03:12.095: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:03:14.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4306" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":328,"skipped":5778,"failed":0}
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:03:14.181: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:03:16.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4442" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":329,"skipped":5783,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:03:16.276: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun  7 15:03:16.319: INFO: The status of Pod pod-update-activedeadlineseconds-711ccfa3-5f77-4fb8-95b7-c1a79d853eb2 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 15:03:18.332: INFO: The status of Pod pod-update-activedeadlineseconds-711ccfa3-5f77-4fb8-95b7-c1a79d853eb2 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun  7 15:03:18.871: INFO: Successfully updated pod "pod-update-activedeadlineseconds-711ccfa3-5f77-4fb8-95b7-c1a79d853eb2"
Jun  7 15:03:18.871: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-711ccfa3-5f77-4fb8-95b7-c1a79d853eb2" in namespace "pods-8743" to be "terminated due to deadline exceeded"
Jun  7 15:03:18.877: INFO: Pod "pod-update-activedeadlineseconds-711ccfa3-5f77-4fb8-95b7-c1a79d853eb2": Phase="Running", Reason="", readiness=true. Elapsed: 5.051816ms
Jun  7 15:03:20.888: INFO: Pod "pod-update-activedeadlineseconds-711ccfa3-5f77-4fb8-95b7-c1a79d853eb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016378303s
Jun  7 15:03:22.897: INFO: Pod "pod-update-activedeadlineseconds-711ccfa3-5f77-4fb8-95b7-c1a79d853eb2": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 4.025440648s
Jun  7 15:03:22.897: INFO: Pod "pod-update-activedeadlineseconds-711ccfa3-5f77-4fb8-95b7-c1a79d853eb2" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:03:22.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8743" for this suite.

• [SLOW TEST:6.640 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":330,"skipped":5789,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:03:22.916: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:188
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Jun  7 15:03:22.971: INFO: created test-pod-1
Jun  7 15:03:22.976: INFO: created test-pod-2
Jun  7 15:03:22.982: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Jun  7 15:03:23.009: INFO: Pod quantity 3 is different from expected quantity 0
Jun  7 15:03:24.018: INFO: Pod quantity 3 is different from expected quantity 0
Jun  7 15:03:25.017: INFO: Pod quantity 3 is different from expected quantity 0
Jun  7 15:03:26.017: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:03:27.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2407" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":331,"skipped":5796,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:03:27.030: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-75652c93-b41a-4da7-9e1f-3ab2896930df
STEP: Creating a pod to test consume configMaps
Jun  7 15:03:27.092: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-27605d2e-a9c3-4377-acef-0c6ef8965938" in namespace "projected-8814" to be "Succeeded or Failed"
Jun  7 15:03:27.095: INFO: Pod "pod-projected-configmaps-27605d2e-a9c3-4377-acef-0c6ef8965938": Phase="Pending", Reason="", readiness=false. Elapsed: 2.741737ms
Jun  7 15:03:29.104: INFO: Pod "pod-projected-configmaps-27605d2e-a9c3-4377-acef-0c6ef8965938": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011867195s
Jun  7 15:03:31.117: INFO: Pod "pod-projected-configmaps-27605d2e-a9c3-4377-acef-0c6ef8965938": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024491373s
STEP: Saw pod success
Jun  7 15:03:31.117: INFO: Pod "pod-projected-configmaps-27605d2e-a9c3-4377-acef-0c6ef8965938" satisfied condition "Succeeded or Failed"
Jun  7 15:03:31.122: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-configmaps-27605d2e-a9c3-4377-acef-0c6ef8965938 container agnhost-container: <nil>
STEP: delete the pod
Jun  7 15:03:31.148: INFO: Waiting for pod pod-projected-configmaps-27605d2e-a9c3-4377-acef-0c6ef8965938 to disappear
Jun  7 15:03:31.151: INFO: Pod pod-projected-configmaps-27605d2e-a9c3-4377-acef-0c6ef8965938 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:03:31.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8814" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":332,"skipped":5798,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:03:31.162: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 15:03:31.203: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8531
I0607 15:03:31.216406      23 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8531, replica count: 1
I0607 15:03:32.267587      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0607 15:03:33.267980      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  7 15:03:33.389: INFO: Created: latency-svc-wbstg
Jun  7 15:03:33.403: INFO: Got endpoints: latency-svc-wbstg [35.002695ms]
Jun  7 15:03:33.424: INFO: Created: latency-svc-njzw8
Jun  7 15:03:33.429: INFO: Got endpoints: latency-svc-njzw8 [25.776141ms]
Jun  7 15:03:33.434: INFO: Created: latency-svc-27gvg
Jun  7 15:03:33.440: INFO: Got endpoints: latency-svc-27gvg [36.665373ms]
Jun  7 15:03:33.441: INFO: Created: latency-svc-xzns6
Jun  7 15:03:33.449: INFO: Got endpoints: latency-svc-xzns6 [45.79731ms]
Jun  7 15:03:33.456: INFO: Created: latency-svc-d656p
Jun  7 15:03:33.462: INFO: Got endpoints: latency-svc-d656p [58.121763ms]
Jun  7 15:03:33.464: INFO: Created: latency-svc-sfvv9
Jun  7 15:03:33.470: INFO: Got endpoints: latency-svc-sfvv9 [66.682907ms]
Jun  7 15:03:33.473: INFO: Created: latency-svc-kl5n7
Jun  7 15:03:33.478: INFO: Got endpoints: latency-svc-kl5n7 [74.848845ms]
Jun  7 15:03:33.481: INFO: Created: latency-svc-z4fjl
Jun  7 15:03:33.485: INFO: Got endpoints: latency-svc-z4fjl [82.247147ms]
Jun  7 15:03:33.488: INFO: Created: latency-svc-98tg6
Jun  7 15:03:33.492: INFO: Got endpoints: latency-svc-98tg6 [88.480203ms]
Jun  7 15:03:33.495: INFO: Created: latency-svc-tgdtl
Jun  7 15:03:33.500: INFO: Got endpoints: latency-svc-tgdtl [95.90338ms]
Jun  7 15:03:33.503: INFO: Created: latency-svc-llcjx
Jun  7 15:03:33.509: INFO: Got endpoints: latency-svc-llcjx [105.644091ms]
Jun  7 15:03:33.511: INFO: Created: latency-svc-wsbt2
Jun  7 15:03:33.516: INFO: Got endpoints: latency-svc-wsbt2 [112.753892ms]
Jun  7 15:03:33.521: INFO: Created: latency-svc-8dkg4
Jun  7 15:03:33.527: INFO: Got endpoints: latency-svc-8dkg4 [123.117491ms]
Jun  7 15:03:33.530: INFO: Created: latency-svc-gvgv9
Jun  7 15:03:33.541: INFO: Created: latency-svc-9zscs
Jun  7 15:03:33.541: INFO: Got endpoints: latency-svc-gvgv9 [137.732981ms]
Jun  7 15:03:33.546: INFO: Got endpoints: latency-svc-9zscs [141.953268ms]
Jun  7 15:03:33.553: INFO: Created: latency-svc-qssq9
Jun  7 15:03:33.558: INFO: Got endpoints: latency-svc-qssq9 [154.016176ms]
Jun  7 15:03:33.561: INFO: Created: latency-svc-4z7zp
Jun  7 15:03:33.570: INFO: Got endpoints: latency-svc-4z7zp [140.449643ms]
Jun  7 15:03:33.570: INFO: Created: latency-svc-zhcb8
Jun  7 15:03:33.576: INFO: Got endpoints: latency-svc-zhcb8 [135.986055ms]
Jun  7 15:03:33.584: INFO: Created: latency-svc-zkczh
Jun  7 15:03:33.591: INFO: Got endpoints: latency-svc-zkczh [141.656814ms]
Jun  7 15:03:33.593: INFO: Created: latency-svc-5cz9c
Jun  7 15:03:33.599: INFO: Got endpoints: latency-svc-5cz9c [137.138883ms]
Jun  7 15:03:33.601: INFO: Created: latency-svc-5847z
Jun  7 15:03:33.603: INFO: Got endpoints: latency-svc-5847z [133.474085ms]
Jun  7 15:03:33.613: INFO: Created: latency-svc-lwhwx
Jun  7 15:03:33.617: INFO: Got endpoints: latency-svc-lwhwx [139.360068ms]
Jun  7 15:03:33.625: INFO: Created: latency-svc-969st
Jun  7 15:03:33.627: INFO: Got endpoints: latency-svc-969st [141.731128ms]
Jun  7 15:03:33.633: INFO: Created: latency-svc-9cm64
Jun  7 15:03:33.636: INFO: Got endpoints: latency-svc-9cm64 [144.228534ms]
Jun  7 15:03:33.645: INFO: Created: latency-svc-zf7g5
Jun  7 15:03:33.660: INFO: Got endpoints: latency-svc-zf7g5 [160.153979ms]
Jun  7 15:03:33.660: INFO: Created: latency-svc-5zb2c
Jun  7 15:03:33.664: INFO: Created: latency-svc-qjjvt
Jun  7 15:03:33.667: INFO: Got endpoints: latency-svc-5zb2c [157.373893ms]
Jun  7 15:03:33.669: INFO: Got endpoints: latency-svc-qjjvt [152.879787ms]
Jun  7 15:03:33.671: INFO: Created: latency-svc-bfzc8
Jun  7 15:03:33.676: INFO: Got endpoints: latency-svc-bfzc8 [149.566257ms]
Jun  7 15:03:33.679: INFO: Created: latency-svc-q666g
Jun  7 15:03:33.697: INFO: Got endpoints: latency-svc-q666g [155.921146ms]
Jun  7 15:03:33.705: INFO: Created: latency-svc-pcxkw
Jun  7 15:03:33.709: INFO: Got endpoints: latency-svc-pcxkw [163.466795ms]
Jun  7 15:03:33.713: INFO: Created: latency-svc-57cxc
Jun  7 15:03:33.720: INFO: Got endpoints: latency-svc-57cxc [162.284799ms]
Jun  7 15:03:33.724: INFO: Created: latency-svc-xdshd
Jun  7 15:03:33.726: INFO: Got endpoints: latency-svc-xdshd [156.553059ms]
Jun  7 15:03:33.729: INFO: Created: latency-svc-mz9vq
Jun  7 15:03:33.732: INFO: Got endpoints: latency-svc-mz9vq [156.379485ms]
Jun  7 15:03:33.737: INFO: Created: latency-svc-hfftv
Jun  7 15:03:33.741: INFO: Got endpoints: latency-svc-hfftv [150.402831ms]
Jun  7 15:03:33.746: INFO: Created: latency-svc-fjhfx
Jun  7 15:03:33.748: INFO: Got endpoints: latency-svc-fjhfx [149.556795ms]
Jun  7 15:03:33.753: INFO: Created: latency-svc-q7vm5
Jun  7 15:03:33.755: INFO: Got endpoints: latency-svc-q7vm5 [151.179109ms]
Jun  7 15:03:33.762: INFO: Created: latency-svc-hkvvq
Jun  7 15:03:33.767: INFO: Got endpoints: latency-svc-hkvvq [149.977076ms]
Jun  7 15:03:33.769: INFO: Created: latency-svc-jmntn
Jun  7 15:03:33.777: INFO: Created: latency-svc-hnzrr
Jun  7 15:03:33.782: INFO: Created: latency-svc-g27g8
Jun  7 15:03:33.789: INFO: Created: latency-svc-wlscx
Jun  7 15:03:33.801: INFO: Got endpoints: latency-svc-jmntn [172.184837ms]
Jun  7 15:03:33.801: INFO: Created: latency-svc-27zkz
Jun  7 15:03:33.810: INFO: Created: latency-svc-kqv6l
Jun  7 15:03:33.820: INFO: Created: latency-svc-rkw7r
Jun  7 15:03:33.828: INFO: Created: latency-svc-r6td8
Jun  7 15:03:33.836: INFO: Created: latency-svc-qjtd2
Jun  7 15:03:33.843: INFO: Created: latency-svc-sdcch
Jun  7 15:03:33.845: INFO: Got endpoints: latency-svc-hnzrr [208.273856ms]
Jun  7 15:03:33.851: INFO: Created: latency-svc-n9h7b
Jun  7 15:03:33.858: INFO: Created: latency-svc-glwk4
Jun  7 15:03:33.864: INFO: Created: latency-svc-gj8bz
Jun  7 15:03:33.872: INFO: Created: latency-svc-rmjqg
Jun  7 15:03:33.879: INFO: Created: latency-svc-mls6k
Jun  7 15:03:33.885: INFO: Created: latency-svc-jv6b2
Jun  7 15:03:33.892: INFO: Created: latency-svc-8djt8
Jun  7 15:03:33.896: INFO: Got endpoints: latency-svc-g27g8 [236.393718ms]
Jun  7 15:03:33.914: INFO: Created: latency-svc-mxrpf
Jun  7 15:03:33.950: INFO: Got endpoints: latency-svc-wlscx [282.994567ms]
Jun  7 15:03:33.967: INFO: Created: latency-svc-l8m9z
Jun  7 15:03:33.999: INFO: Got endpoints: latency-svc-27zkz [329.646389ms]
Jun  7 15:03:34.019: INFO: Created: latency-svc-42644
Jun  7 15:03:34.050: INFO: Got endpoints: latency-svc-kqv6l [371.205753ms]
Jun  7 15:03:34.074: INFO: Created: latency-svc-zgx7v
Jun  7 15:03:34.100: INFO: Got endpoints: latency-svc-rkw7r [402.125453ms]
Jun  7 15:03:34.121: INFO: Created: latency-svc-4jxx6
Jun  7 15:03:34.147: INFO: Got endpoints: latency-svc-r6td8 [437.583191ms]
Jun  7 15:03:34.160: INFO: Created: latency-svc-7pndt
Jun  7 15:03:34.198: INFO: Got endpoints: latency-svc-qjtd2 [478.472941ms]
Jun  7 15:03:34.216: INFO: Created: latency-svc-mq4pq
Jun  7 15:03:34.250: INFO: Got endpoints: latency-svc-sdcch [523.341421ms]
Jun  7 15:03:34.266: INFO: Created: latency-svc-llk2x
Jun  7 15:03:34.298: INFO: Got endpoints: latency-svc-n9h7b [565.419014ms]
Jun  7 15:03:34.312: INFO: Created: latency-svc-gtfzg
Jun  7 15:03:34.348: INFO: Got endpoints: latency-svc-glwk4 [606.176144ms]
Jun  7 15:03:34.364: INFO: Created: latency-svc-5t4zn
Jun  7 15:03:34.396: INFO: Got endpoints: latency-svc-gj8bz [647.946202ms]
Jun  7 15:03:34.408: INFO: Created: latency-svc-95bkq
Jun  7 15:03:34.448: INFO: Got endpoints: latency-svc-rmjqg [693.627126ms]
Jun  7 15:03:34.462: INFO: Created: latency-svc-ttlnc
Jun  7 15:03:34.497: INFO: Got endpoints: latency-svc-mls6k [729.950467ms]
Jun  7 15:03:34.512: INFO: Created: latency-svc-xw4p8
Jun  7 15:03:34.548: INFO: Got endpoints: latency-svc-jv6b2 [746.876649ms]
Jun  7 15:03:34.572: INFO: Created: latency-svc-lx62h
Jun  7 15:03:34.596: INFO: Got endpoints: latency-svc-8djt8 [751.413902ms]
Jun  7 15:03:34.614: INFO: Created: latency-svc-pjgfh
Jun  7 15:03:34.655: INFO: Got endpoints: latency-svc-mxrpf [759.015838ms]
Jun  7 15:03:34.672: INFO: Created: latency-svc-dpx5n
Jun  7 15:03:34.695: INFO: Got endpoints: latency-svc-l8m9z [745.534739ms]
Jun  7 15:03:34.712: INFO: Created: latency-svc-rhfp5
Jun  7 15:03:34.747: INFO: Got endpoints: latency-svc-42644 [748.191608ms]
Jun  7 15:03:34.764: INFO: Created: latency-svc-8jqfj
Jun  7 15:03:34.797: INFO: Got endpoints: latency-svc-zgx7v [746.455521ms]
Jun  7 15:03:34.814: INFO: Created: latency-svc-4rhkq
Jun  7 15:03:34.850: INFO: Got endpoints: latency-svc-4jxx6 [748.849951ms]
Jun  7 15:03:34.870: INFO: Created: latency-svc-whvhj
Jun  7 15:03:34.900: INFO: Got endpoints: latency-svc-7pndt [753.127627ms]
Jun  7 15:03:34.916: INFO: Created: latency-svc-l7trz
Jun  7 15:03:34.948: INFO: Got endpoints: latency-svc-mq4pq [749.72378ms]
Jun  7 15:03:34.966: INFO: Created: latency-svc-7g2kb
Jun  7 15:03:35.000: INFO: Got endpoints: latency-svc-llk2x [750.220088ms]
Jun  7 15:03:35.019: INFO: Created: latency-svc-6pg6s
Jun  7 15:03:35.051: INFO: Got endpoints: latency-svc-gtfzg [752.670411ms]
Jun  7 15:03:35.067: INFO: Created: latency-svc-zk9x5
Jun  7 15:03:35.097: INFO: Got endpoints: latency-svc-5t4zn [749.466231ms]
Jun  7 15:03:35.115: INFO: Created: latency-svc-t5hmh
Jun  7 15:03:35.152: INFO: Got endpoints: latency-svc-95bkq [755.506826ms]
Jun  7 15:03:35.169: INFO: Created: latency-svc-cgcn9
Jun  7 15:03:35.198: INFO: Got endpoints: latency-svc-ttlnc [749.434591ms]
Jun  7 15:03:35.216: INFO: Created: latency-svc-c2n5l
Jun  7 15:03:35.247: INFO: Got endpoints: latency-svc-xw4p8 [750.065065ms]
Jun  7 15:03:35.265: INFO: Created: latency-svc-8m72b
Jun  7 15:03:35.300: INFO: Got endpoints: latency-svc-lx62h [752.724802ms]
Jun  7 15:03:35.332: INFO: Created: latency-svc-9r7qq
Jun  7 15:03:35.363: INFO: Got endpoints: latency-svc-pjgfh [766.450077ms]
Jun  7 15:03:35.401: INFO: Created: latency-svc-xgg6x
Jun  7 15:03:35.402: INFO: Got endpoints: latency-svc-dpx5n [746.9111ms]
Jun  7 15:03:35.420: INFO: Created: latency-svc-99hd2
Jun  7 15:03:35.448: INFO: Got endpoints: latency-svc-rhfp5 [752.184802ms]
Jun  7 15:03:35.462: INFO: Created: latency-svc-77n9t
Jun  7 15:03:35.502: INFO: Got endpoints: latency-svc-8jqfj [753.896519ms]
Jun  7 15:03:35.513: INFO: Created: latency-svc-slblq
Jun  7 15:03:35.550: INFO: Got endpoints: latency-svc-4rhkq [753.365625ms]
Jun  7 15:03:35.563: INFO: Created: latency-svc-q9snm
Jun  7 15:03:35.599: INFO: Got endpoints: latency-svc-whvhj [749.340299ms]
Jun  7 15:03:35.617: INFO: Created: latency-svc-dxjmr
Jun  7 15:03:35.648: INFO: Got endpoints: latency-svc-l7trz [747.750759ms]
Jun  7 15:03:35.663: INFO: Created: latency-svc-8gcbh
Jun  7 15:03:35.698: INFO: Got endpoints: latency-svc-7g2kb [749.845758ms]
Jun  7 15:03:35.719: INFO: Created: latency-svc-tpm9w
Jun  7 15:03:35.752: INFO: Got endpoints: latency-svc-6pg6s [751.842575ms]
Jun  7 15:03:35.769: INFO: Created: latency-svc-bb65r
Jun  7 15:03:35.798: INFO: Got endpoints: latency-svc-zk9x5 [747.090396ms]
Jun  7 15:03:35.813: INFO: Created: latency-svc-cn5zl
Jun  7 15:03:35.849: INFO: Got endpoints: latency-svc-t5hmh [751.604633ms]
Jun  7 15:03:35.868: INFO: Created: latency-svc-7tchc
Jun  7 15:03:35.901: INFO: Got endpoints: latency-svc-cgcn9 [748.731694ms]
Jun  7 15:03:35.919: INFO: Created: latency-svc-fkmwr
Jun  7 15:03:35.950: INFO: Got endpoints: latency-svc-c2n5l [751.750536ms]
Jun  7 15:03:35.968: INFO: Created: latency-svc-gkwsq
Jun  7 15:03:35.999: INFO: Got endpoints: latency-svc-8m72b [752.017644ms]
Jun  7 15:03:36.015: INFO: Created: latency-svc-bq6nj
Jun  7 15:03:36.051: INFO: Got endpoints: latency-svc-9r7qq [750.129074ms]
Jun  7 15:03:36.068: INFO: Created: latency-svc-b6lb5
Jun  7 15:03:36.099: INFO: Got endpoints: latency-svc-xgg6x [736.582313ms]
Jun  7 15:03:36.118: INFO: Created: latency-svc-zhk27
Jun  7 15:03:36.150: INFO: Got endpoints: latency-svc-99hd2 [746.842332ms]
Jun  7 15:03:36.164: INFO: Created: latency-svc-8kpd2
Jun  7 15:03:36.197: INFO: Got endpoints: latency-svc-77n9t [748.736718ms]
Jun  7 15:03:36.210: INFO: Created: latency-svc-4q9pn
Jun  7 15:03:36.246: INFO: Got endpoints: latency-svc-slblq [744.115721ms]
Jun  7 15:03:36.261: INFO: Created: latency-svc-xwkkq
Jun  7 15:03:36.296: INFO: Got endpoints: latency-svc-q9snm [745.982051ms]
Jun  7 15:03:36.309: INFO: Created: latency-svc-lh72h
Jun  7 15:03:36.344: INFO: Got endpoints: latency-svc-dxjmr [744.803035ms]
Jun  7 15:03:36.359: INFO: Created: latency-svc-sfrns
Jun  7 15:03:36.394: INFO: Got endpoints: latency-svc-8gcbh [746.474375ms]
Jun  7 15:03:36.411: INFO: Created: latency-svc-jf9rw
Jun  7 15:03:36.448: INFO: Got endpoints: latency-svc-tpm9w [749.832479ms]
Jun  7 15:03:36.463: INFO: Created: latency-svc-qz6dm
Jun  7 15:03:36.496: INFO: Got endpoints: latency-svc-bb65r [743.720945ms]
Jun  7 15:03:36.507: INFO: Created: latency-svc-9gpst
Jun  7 15:03:36.546: INFO: Got endpoints: latency-svc-cn5zl [747.877774ms]
Jun  7 15:03:36.556: INFO: Created: latency-svc-bgtcg
Jun  7 15:03:36.595: INFO: Got endpoints: latency-svc-7tchc [746.349309ms]
Jun  7 15:03:36.607: INFO: Created: latency-svc-fzxbs
Jun  7 15:03:36.648: INFO: Got endpoints: latency-svc-fkmwr [747.043855ms]
Jun  7 15:03:36.661: INFO: Created: latency-svc-85x59
Jun  7 15:03:36.698: INFO: Got endpoints: latency-svc-gkwsq [748.449565ms]
Jun  7 15:03:36.709: INFO: Created: latency-svc-lcrdw
Jun  7 15:03:36.750: INFO: Got endpoints: latency-svc-bq6nj [750.759472ms]
Jun  7 15:03:36.766: INFO: Created: latency-svc-vp9pd
Jun  7 15:03:36.797: INFO: Got endpoints: latency-svc-b6lb5 [746.264732ms]
Jun  7 15:03:36.813: INFO: Created: latency-svc-56gjr
Jun  7 15:03:36.850: INFO: Got endpoints: latency-svc-zhk27 [750.929431ms]
Jun  7 15:03:36.866: INFO: Created: latency-svc-mgt8g
Jun  7 15:03:36.898: INFO: Got endpoints: latency-svc-8kpd2 [748.787024ms]
Jun  7 15:03:36.924: INFO: Created: latency-svc-5s245
Jun  7 15:03:36.948: INFO: Got endpoints: latency-svc-4q9pn [751.524985ms]
Jun  7 15:03:36.965: INFO: Created: latency-svc-rdlln
Jun  7 15:03:37.001: INFO: Got endpoints: latency-svc-xwkkq [755.111081ms]
Jun  7 15:03:37.018: INFO: Created: latency-svc-lsw89
Jun  7 15:03:37.049: INFO: Got endpoints: latency-svc-lh72h [752.223232ms]
Jun  7 15:03:37.068: INFO: Created: latency-svc-xwkfm
Jun  7 15:03:37.101: INFO: Got endpoints: latency-svc-sfrns [756.98145ms]
Jun  7 15:03:37.114: INFO: Created: latency-svc-7wxmn
Jun  7 15:03:37.146: INFO: Got endpoints: latency-svc-jf9rw [751.549275ms]
Jun  7 15:03:37.161: INFO: Created: latency-svc-dbt9j
Jun  7 15:03:37.200: INFO: Got endpoints: latency-svc-qz6dm [751.488114ms]
Jun  7 15:03:37.216: INFO: Created: latency-svc-77rrn
Jun  7 15:03:37.248: INFO: Got endpoints: latency-svc-9gpst [752.11771ms]
Jun  7 15:03:37.263: INFO: Created: latency-svc-gbb9p
Jun  7 15:03:37.300: INFO: Got endpoints: latency-svc-bgtcg [753.664523ms]
Jun  7 15:03:37.320: INFO: Created: latency-svc-kgpf9
Jun  7 15:03:37.346: INFO: Got endpoints: latency-svc-fzxbs [751.001851ms]
Jun  7 15:03:37.360: INFO: Created: latency-svc-kdds8
Jun  7 15:03:37.399: INFO: Got endpoints: latency-svc-85x59 [750.319911ms]
Jun  7 15:03:37.412: INFO: Created: latency-svc-4tkrw
Jun  7 15:03:37.449: INFO: Got endpoints: latency-svc-lcrdw [750.494885ms]
Jun  7 15:03:37.464: INFO: Created: latency-svc-f24z4
Jun  7 15:03:37.500: INFO: Got endpoints: latency-svc-vp9pd [749.507712ms]
Jun  7 15:03:37.514: INFO: Created: latency-svc-lwlfr
Jun  7 15:03:37.550: INFO: Got endpoints: latency-svc-56gjr [749.810324ms]
Jun  7 15:03:37.564: INFO: Created: latency-svc-n82fv
Jun  7 15:03:37.599: INFO: Got endpoints: latency-svc-mgt8g [748.383891ms]
Jun  7 15:03:37.614: INFO: Created: latency-svc-49hvc
Jun  7 15:03:37.645: INFO: Got endpoints: latency-svc-5s245 [746.21103ms]
Jun  7 15:03:37.659: INFO: Created: latency-svc-5xz28
Jun  7 15:03:37.699: INFO: Got endpoints: latency-svc-rdlln [750.870661ms]
Jun  7 15:03:37.715: INFO: Created: latency-svc-7qr49
Jun  7 15:03:37.748: INFO: Got endpoints: latency-svc-lsw89 [746.935747ms]
Jun  7 15:03:37.764: INFO: Created: latency-svc-htgd8
Jun  7 15:03:37.797: INFO: Got endpoints: latency-svc-xwkfm [748.720772ms]
Jun  7 15:03:37.814: INFO: Created: latency-svc-w6hnb
Jun  7 15:03:37.850: INFO: Got endpoints: latency-svc-7wxmn [748.309688ms]
Jun  7 15:03:37.864: INFO: Created: latency-svc-2tnlj
Jun  7 15:03:37.901: INFO: Got endpoints: latency-svc-dbt9j [755.431775ms]
Jun  7 15:03:37.918: INFO: Created: latency-svc-6q9td
Jun  7 15:03:37.953: INFO: Got endpoints: latency-svc-77rrn [753.797927ms]
Jun  7 15:03:37.970: INFO: Created: latency-svc-hhv6r
Jun  7 15:03:38.001: INFO: Got endpoints: latency-svc-gbb9p [753.599144ms]
Jun  7 15:03:38.017: INFO: Created: latency-svc-wvm8h
Jun  7 15:03:38.051: INFO: Got endpoints: latency-svc-kgpf9 [751.764262ms]
Jun  7 15:03:38.067: INFO: Created: latency-svc-qlw7x
Jun  7 15:03:38.098: INFO: Got endpoints: latency-svc-kdds8 [752.041412ms]
Jun  7 15:03:38.115: INFO: Created: latency-svc-6s776
Jun  7 15:03:38.148: INFO: Got endpoints: latency-svc-4tkrw [749.6646ms]
Jun  7 15:03:38.169: INFO: Created: latency-svc-s486j
Jun  7 15:03:38.200: INFO: Got endpoints: latency-svc-f24z4 [750.846947ms]
Jun  7 15:03:38.218: INFO: Created: latency-svc-66m29
Jun  7 15:03:38.250: INFO: Got endpoints: latency-svc-lwlfr [750.608023ms]
Jun  7 15:03:38.274: INFO: Created: latency-svc-hj778
Jun  7 15:03:38.300: INFO: Got endpoints: latency-svc-n82fv [750.471375ms]
Jun  7 15:03:38.317: INFO: Created: latency-svc-kfjlp
Jun  7 15:03:38.347: INFO: Got endpoints: latency-svc-49hvc [748.184011ms]
Jun  7 15:03:38.372: INFO: Created: latency-svc-6k9w4
Jun  7 15:03:38.399: INFO: Got endpoints: latency-svc-5xz28 [754.495979ms]
Jun  7 15:03:38.413: INFO: Created: latency-svc-v4gd4
Jun  7 15:03:38.448: INFO: Got endpoints: latency-svc-7qr49 [748.432452ms]
Jun  7 15:03:38.466: INFO: Created: latency-svc-tww72
Jun  7 15:03:38.499: INFO: Got endpoints: latency-svc-htgd8 [749.813071ms]
Jun  7 15:03:38.511: INFO: Created: latency-svc-stpgw
Jun  7 15:03:38.548: INFO: Got endpoints: latency-svc-w6hnb [750.663563ms]
Jun  7 15:03:38.566: INFO: Created: latency-svc-zz2pt
Jun  7 15:03:38.600: INFO: Got endpoints: latency-svc-2tnlj [750.19276ms]
Jun  7 15:03:38.619: INFO: Created: latency-svc-rfxl8
Jun  7 15:03:38.645: INFO: Got endpoints: latency-svc-6q9td [743.5995ms]
Jun  7 15:03:38.656: INFO: Created: latency-svc-fkb5g
Jun  7 15:03:38.699: INFO: Got endpoints: latency-svc-hhv6r [745.02765ms]
Jun  7 15:03:38.713: INFO: Created: latency-svc-77cwv
Jun  7 15:03:38.750: INFO: Got endpoints: latency-svc-wvm8h [748.09109ms]
Jun  7 15:03:38.768: INFO: Created: latency-svc-8zbks
Jun  7 15:03:38.799: INFO: Got endpoints: latency-svc-qlw7x [747.936647ms]
Jun  7 15:03:38.817: INFO: Created: latency-svc-2pfms
Jun  7 15:03:38.848: INFO: Got endpoints: latency-svc-6s776 [749.028602ms]
Jun  7 15:03:38.859: INFO: Created: latency-svc-tx7fj
Jun  7 15:03:38.900: INFO: Got endpoints: latency-svc-s486j [751.797407ms]
Jun  7 15:03:38.923: INFO: Created: latency-svc-5v9sc
Jun  7 15:03:38.952: INFO: Got endpoints: latency-svc-66m29 [752.483884ms]
Jun  7 15:03:38.973: INFO: Created: latency-svc-nrjfg
Jun  7 15:03:39.000: INFO: Got endpoints: latency-svc-hj778 [749.907207ms]
Jun  7 15:03:39.022: INFO: Created: latency-svc-zsx6m
Jun  7 15:03:39.048: INFO: Got endpoints: latency-svc-kfjlp [747.379698ms]
Jun  7 15:03:39.065: INFO: Created: latency-svc-8785z
Jun  7 15:03:39.098: INFO: Got endpoints: latency-svc-6k9w4 [751.10981ms]
Jun  7 15:03:39.117: INFO: Created: latency-svc-glx7f
Jun  7 15:03:39.150: INFO: Got endpoints: latency-svc-v4gd4 [750.740928ms]
Jun  7 15:03:39.167: INFO: Created: latency-svc-66mjp
Jun  7 15:03:39.198: INFO: Got endpoints: latency-svc-tww72 [750.43005ms]
Jun  7 15:03:39.218: INFO: Created: latency-svc-h56bj
Jun  7 15:03:39.251: INFO: Got endpoints: latency-svc-stpgw [752.725888ms]
Jun  7 15:03:39.267: INFO: Created: latency-svc-sg8bd
Jun  7 15:03:39.298: INFO: Got endpoints: latency-svc-zz2pt [749.924235ms]
Jun  7 15:03:39.318: INFO: Created: latency-svc-hvv4r
Jun  7 15:03:39.348: INFO: Got endpoints: latency-svc-rfxl8 [747.940956ms]
Jun  7 15:03:39.365: INFO: Created: latency-svc-hxfst
Jun  7 15:03:39.398: INFO: Got endpoints: latency-svc-fkb5g [753.272525ms]
Jun  7 15:03:39.413: INFO: Created: latency-svc-nrpjb
Jun  7 15:03:39.451: INFO: Got endpoints: latency-svc-77cwv [752.370978ms]
Jun  7 15:03:39.470: INFO: Created: latency-svc-4nrdk
Jun  7 15:03:39.499: INFO: Got endpoints: latency-svc-8zbks [749.510212ms]
Jun  7 15:03:39.513: INFO: Created: latency-svc-vnh8m
Jun  7 15:03:39.550: INFO: Got endpoints: latency-svc-2pfms [750.291415ms]
Jun  7 15:03:39.567: INFO: Created: latency-svc-67tjd
Jun  7 15:03:39.599: INFO: Got endpoints: latency-svc-tx7fj [751.069475ms]
Jun  7 15:03:39.617: INFO: Created: latency-svc-2lhdb
Jun  7 15:03:39.647: INFO: Got endpoints: latency-svc-5v9sc [747.043733ms]
Jun  7 15:03:39.668: INFO: Created: latency-svc-7d24p
Jun  7 15:03:39.712: INFO: Got endpoints: latency-svc-nrjfg [759.893973ms]
Jun  7 15:03:39.725: INFO: Created: latency-svc-vjjvr
Jun  7 15:03:39.746: INFO: Got endpoints: latency-svc-zsx6m [745.827313ms]
Jun  7 15:03:39.761: INFO: Created: latency-svc-jvhbn
Jun  7 15:03:39.799: INFO: Got endpoints: latency-svc-8785z [750.728919ms]
Jun  7 15:03:39.814: INFO: Created: latency-svc-9fj7r
Jun  7 15:03:39.850: INFO: Got endpoints: latency-svc-glx7f [751.718971ms]
Jun  7 15:03:39.869: INFO: Created: latency-svc-bfsbs
Jun  7 15:03:39.898: INFO: Got endpoints: latency-svc-66mjp [748.091034ms]
Jun  7 15:03:39.920: INFO: Created: latency-svc-lmq45
Jun  7 15:03:39.948: INFO: Got endpoints: latency-svc-h56bj [749.612584ms]
Jun  7 15:03:39.970: INFO: Created: latency-svc-z9z7d
Jun  7 15:03:39.997: INFO: Got endpoints: latency-svc-sg8bd [745.852741ms]
Jun  7 15:03:40.021: INFO: Created: latency-svc-xppnx
Jun  7 15:03:40.051: INFO: Got endpoints: latency-svc-hvv4r [750.812448ms]
Jun  7 15:03:40.068: INFO: Created: latency-svc-c5zsm
Jun  7 15:03:40.099: INFO: Got endpoints: latency-svc-hxfst [751.443643ms]
Jun  7 15:03:40.117: INFO: Created: latency-svc-lbkjn
Jun  7 15:03:40.151: INFO: Got endpoints: latency-svc-nrpjb [752.459024ms]
Jun  7 15:03:40.168: INFO: Created: latency-svc-hzj2w
Jun  7 15:03:40.199: INFO: Got endpoints: latency-svc-4nrdk [748.130406ms]
Jun  7 15:03:40.215: INFO: Created: latency-svc-fqmkl
Jun  7 15:03:40.247: INFO: Got endpoints: latency-svc-vnh8m [747.726258ms]
Jun  7 15:03:40.264: INFO: Created: latency-svc-7sbhf
Jun  7 15:03:40.298: INFO: Got endpoints: latency-svc-67tjd [747.840988ms]
Jun  7 15:03:40.316: INFO: Created: latency-svc-cb9mv
Jun  7 15:03:40.349: INFO: Got endpoints: latency-svc-2lhdb [750.606058ms]
Jun  7 15:03:40.367: INFO: Created: latency-svc-xfp59
Jun  7 15:03:40.402: INFO: Got endpoints: latency-svc-7d24p [754.26088ms]
Jun  7 15:03:40.418: INFO: Created: latency-svc-78bnc
Jun  7 15:03:40.449: INFO: Got endpoints: latency-svc-vjjvr [736.280322ms]
Jun  7 15:03:40.460: INFO: Created: latency-svc-qh8wj
Jun  7 15:03:40.499: INFO: Got endpoints: latency-svc-jvhbn [752.349975ms]
Jun  7 15:03:40.517: INFO: Created: latency-svc-xh8kf
Jun  7 15:03:40.548: INFO: Got endpoints: latency-svc-9fj7r [749.276418ms]
Jun  7 15:03:40.564: INFO: Created: latency-svc-k87mj
Jun  7 15:03:40.597: INFO: Got endpoints: latency-svc-bfsbs [746.463829ms]
Jun  7 15:03:40.617: INFO: Created: latency-svc-s8dzv
Jun  7 15:03:40.648: INFO: Got endpoints: latency-svc-lmq45 [749.521662ms]
Jun  7 15:03:40.666: INFO: Created: latency-svc-kf5fj
Jun  7 15:03:40.700: INFO: Got endpoints: latency-svc-z9z7d [751.698876ms]
Jun  7 15:03:40.719: INFO: Created: latency-svc-b4krh
Jun  7 15:03:40.746: INFO: Got endpoints: latency-svc-xppnx [748.448411ms]
Jun  7 15:03:40.763: INFO: Created: latency-svc-x95hf
Jun  7 15:03:40.799: INFO: Got endpoints: latency-svc-c5zsm [748.071574ms]
Jun  7 15:03:40.816: INFO: Created: latency-svc-h747f
Jun  7 15:03:40.848: INFO: Got endpoints: latency-svc-lbkjn [748.803424ms]
Jun  7 15:03:40.867: INFO: Created: latency-svc-nrb76
Jun  7 15:03:40.900: INFO: Got endpoints: latency-svc-hzj2w [749.195253ms]
Jun  7 15:03:40.919: INFO: Created: latency-svc-68q7l
Jun  7 15:03:40.948: INFO: Got endpoints: latency-svc-fqmkl [748.775853ms]
Jun  7 15:03:40.965: INFO: Created: latency-svc-9v62k
Jun  7 15:03:40.998: INFO: Got endpoints: latency-svc-7sbhf [750.655025ms]
Jun  7 15:03:41.017: INFO: Created: latency-svc-dhgrd
Jun  7 15:03:41.053: INFO: Got endpoints: latency-svc-cb9mv [754.783416ms]
Jun  7 15:03:41.069: INFO: Created: latency-svc-pq72g
Jun  7 15:03:41.098: INFO: Got endpoints: latency-svc-xfp59 [748.069542ms]
Jun  7 15:03:41.117: INFO: Created: latency-svc-r5njc
Jun  7 15:03:41.151: INFO: Got endpoints: latency-svc-78bnc [749.187418ms]
Jun  7 15:03:41.168: INFO: Created: latency-svc-v92rb
Jun  7 15:03:41.197: INFO: Got endpoints: latency-svc-qh8wj [748.594938ms]
Jun  7 15:03:41.213: INFO: Created: latency-svc-dkhxg
Jun  7 15:03:41.250: INFO: Got endpoints: latency-svc-xh8kf [751.405774ms]
Jun  7 15:03:41.301: INFO: Got endpoints: latency-svc-k87mj [753.176727ms]
Jun  7 15:03:41.351: INFO: Got endpoints: latency-svc-s8dzv [754.490837ms]
Jun  7 15:03:41.398: INFO: Got endpoints: latency-svc-kf5fj [750.300482ms]
Jun  7 15:03:41.447: INFO: Got endpoints: latency-svc-b4krh [747.458327ms]
Jun  7 15:03:41.496: INFO: Got endpoints: latency-svc-x95hf [750.394558ms]
Jun  7 15:03:41.551: INFO: Got endpoints: latency-svc-h747f [751.348352ms]
Jun  7 15:03:41.599: INFO: Got endpoints: latency-svc-nrb76 [750.609407ms]
Jun  7 15:03:41.651: INFO: Got endpoints: latency-svc-68q7l [750.500459ms]
Jun  7 15:03:41.699: INFO: Got endpoints: latency-svc-9v62k [750.817472ms]
Jun  7 15:03:41.749: INFO: Got endpoints: latency-svc-dhgrd [748.972856ms]
Jun  7 15:03:41.800: INFO: Got endpoints: latency-svc-pq72g [747.206022ms]
Jun  7 15:03:41.851: INFO: Got endpoints: latency-svc-r5njc [753.187504ms]
Jun  7 15:03:41.900: INFO: Got endpoints: latency-svc-v92rb [749.380848ms]
Jun  7 15:03:41.948: INFO: Got endpoints: latency-svc-dkhxg [750.779178ms]
Jun  7 15:03:41.948: INFO: Latencies: [25.776141ms 36.665373ms 45.79731ms 58.121763ms 66.682907ms 74.848845ms 82.247147ms 88.480203ms 95.90338ms 105.644091ms 112.753892ms 123.117491ms 133.474085ms 135.986055ms 137.138883ms 137.732981ms 139.360068ms 140.449643ms 141.656814ms 141.731128ms 141.953268ms 144.228534ms 149.556795ms 149.566257ms 149.977076ms 150.402831ms 151.179109ms 152.879787ms 154.016176ms 155.921146ms 156.379485ms 156.553059ms 157.373893ms 160.153979ms 162.284799ms 163.466795ms 172.184837ms 208.273856ms 236.393718ms 282.994567ms 329.646389ms 371.205753ms 402.125453ms 437.583191ms 478.472941ms 523.341421ms 565.419014ms 606.176144ms 647.946202ms 693.627126ms 729.950467ms 736.280322ms 736.582313ms 743.5995ms 743.720945ms 744.115721ms 744.803035ms 745.02765ms 745.534739ms 745.827313ms 745.852741ms 745.982051ms 746.21103ms 746.264732ms 746.349309ms 746.455521ms 746.463829ms 746.474375ms 746.842332ms 746.876649ms 746.9111ms 746.935747ms 747.043733ms 747.043855ms 747.090396ms 747.206022ms 747.379698ms 747.458327ms 747.726258ms 747.750759ms 747.840988ms 747.877774ms 747.936647ms 747.940956ms 748.069542ms 748.071574ms 748.091034ms 748.09109ms 748.130406ms 748.184011ms 748.191608ms 748.309688ms 748.383891ms 748.432452ms 748.448411ms 748.449565ms 748.594938ms 748.720772ms 748.731694ms 748.736718ms 748.775853ms 748.787024ms 748.803424ms 748.849951ms 748.972856ms 749.028602ms 749.187418ms 749.195253ms 749.276418ms 749.340299ms 749.380848ms 749.434591ms 749.466231ms 749.507712ms 749.510212ms 749.521662ms 749.612584ms 749.6646ms 749.72378ms 749.810324ms 749.813071ms 749.832479ms 749.845758ms 749.907207ms 749.924235ms 750.065065ms 750.129074ms 750.19276ms 750.220088ms 750.291415ms 750.300482ms 750.319911ms 750.394558ms 750.43005ms 750.471375ms 750.494885ms 750.500459ms 750.606058ms 750.608023ms 750.609407ms 750.655025ms 750.663563ms 750.728919ms 750.740928ms 750.759472ms 750.779178ms 750.812448ms 750.817472ms 750.846947ms 750.870661ms 750.929431ms 751.001851ms 751.069475ms 751.10981ms 751.348352ms 751.405774ms 751.413902ms 751.443643ms 751.488114ms 751.524985ms 751.549275ms 751.604633ms 751.698876ms 751.718971ms 751.750536ms 751.764262ms 751.797407ms 751.842575ms 752.017644ms 752.041412ms 752.11771ms 752.184802ms 752.223232ms 752.349975ms 752.370978ms 752.459024ms 752.483884ms 752.670411ms 752.724802ms 752.725888ms 753.127627ms 753.176727ms 753.187504ms 753.272525ms 753.365625ms 753.599144ms 753.664523ms 753.797927ms 753.896519ms 754.26088ms 754.490837ms 754.495979ms 754.783416ms 755.111081ms 755.431775ms 755.506826ms 756.98145ms 759.015838ms 759.893973ms 766.450077ms]
Jun  7 15:03:41.948: INFO: 50 %ile: 748.775853ms
Jun  7 15:03:41.948: INFO: 90 %ile: 753.127627ms
Jun  7 15:03:41.948: INFO: 99 %ile: 759.893973ms
Jun  7 15:03:41.948: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:03:41.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8531" for this suite.

• [SLOW TEST:10.812 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":333,"skipped":5806,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:03:41.976: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-4812
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4812 to expose endpoints map[]
Jun  7 15:03:42.030: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jun  7 15:03:43.044: INFO: successfully validated that service multi-endpoint-test in namespace services-4812 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4812
Jun  7 15:03:43.058: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 15:03:45.070: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4812 to expose endpoints map[pod1:[100]]
Jun  7 15:03:45.089: INFO: successfully validated that service multi-endpoint-test in namespace services-4812 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4812
Jun  7 15:03:45.102: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 15:03:47.112: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4812 to expose endpoints map[pod1:[100] pod2:[101]]
Jun  7 15:03:47.130: INFO: successfully validated that service multi-endpoint-test in namespace services-4812 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jun  7 15:03:47.130: INFO: Creating new exec pod
Jun  7 15:03:50.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-4812 exec execpod56hzx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jun  7 15:03:50.341: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jun  7 15:03:50.342: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 15:03:50.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-4812 exec execpod56hzx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.172.41 80'
Jun  7 15:03:50.523: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.172.41 80\nConnection to 10.107.172.41 80 port [tcp/http] succeeded!\n"
Jun  7 15:03:50.523: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 15:03:50.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-4812 exec execpod56hzx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jun  7 15:03:50.719: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jun  7 15:03:50.719: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  7 15:03:50.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-227657883 --namespace=services-4812 exec execpod56hzx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.172.41 81'
Jun  7 15:03:50.898: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.172.41 81\nConnection to 10.107.172.41 81 port [tcp/*] succeeded!\n"
Jun  7 15:03:50.898: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4812
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4812 to expose endpoints map[pod2:[101]]
Jun  7 15:03:54.927: INFO: Unexpected endpoints: found map[3a86fcb3-0e48-4b2b-8f7d-fb9f62c08fb3:[100] 4053d0fe-4b3f-488b-bcf3-2f0d2f631c21:[101]], expected map[pod2:[101]], will retry
Jun  7 15:03:59.922: INFO: Unexpected endpoints: found map[3a86fcb3-0e48-4b2b-8f7d-fb9f62c08fb3:[100] 4053d0fe-4b3f-488b-bcf3-2f0d2f631c21:[101]], expected map[pod2:[101]], will retry
Jun  7 15:04:01.933: INFO: successfully validated that service multi-endpoint-test in namespace services-4812 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4812
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4812 to expose endpoints map[]
Jun  7 15:04:02.987: INFO: successfully validated that service multi-endpoint-test in namespace services-4812 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:04:03.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4812" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:21.041 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":334,"skipped":5818,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:04:03.018: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 15:04:03.594: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 15:04:06.634: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 15:04:06.644: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7476-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:04:09.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5395" for this suite.
STEP: Destroying namespace "webhook-5395-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.858 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":335,"skipped":5830,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:04:09.886: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-078babbd-0e7b-4eb1-a9b3-f1af8ebbda6b
STEP: Creating a pod to test consume secrets
Jun  7 15:04:09.927: INFO: Waiting up to 5m0s for pod "pod-secrets-574a48b8-c876-4412-b202-eaa4df36f05f" in namespace "secrets-9856" to be "Succeeded or Failed"
Jun  7 15:04:09.929: INFO: Pod "pod-secrets-574a48b8-c876-4412-b202-eaa4df36f05f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368686ms
Jun  7 15:04:11.936: INFO: Pod "pod-secrets-574a48b8-c876-4412-b202-eaa4df36f05f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009002256s
STEP: Saw pod success
Jun  7 15:04:11.936: INFO: Pod "pod-secrets-574a48b8-c876-4412-b202-eaa4df36f05f" satisfied condition "Succeeded or Failed"
Jun  7 15:04:11.942: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-secrets-574a48b8-c876-4412-b202-eaa4df36f05f container secret-volume-test: <nil>
STEP: delete the pod
Jun  7 15:04:11.959: INFO: Waiting for pod pod-secrets-574a48b8-c876-4412-b202-eaa4df36f05f to disappear
Jun  7 15:04:11.961: INFO: Pod pod-secrets-574a48b8-c876-4412-b202-eaa4df36f05f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:04:11.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9856" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":5865,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:04:11.973: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 15:04:12.026: INFO: The status of Pod busybox-readonly-fs17d95feb-d7b1-439b-839f-9c432a07ab6f is Pending, waiting for it to be Running (with Ready = true)
Jun  7 15:04:14.036: INFO: The status of Pod busybox-readonly-fs17d95feb-d7b1-439b-839f-9c432a07ab6f is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:04:14.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1794" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":337,"skipped":5892,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:04:14.059: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-41fdbf1e-15ae-4d63-84f7-9c83fa266e58
STEP: Creating a pod to test consume secrets
Jun  7 15:04:14.107: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-df4fcd59-9fb9-4768-b28d-29373f55557a" in namespace "projected-769" to be "Succeeded or Failed"
Jun  7 15:04:14.110: INFO: Pod "pod-projected-secrets-df4fcd59-9fb9-4768-b28d-29373f55557a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.431136ms
Jun  7 15:04:16.117: INFO: Pod "pod-projected-secrets-df4fcd59-9fb9-4768-b28d-29373f55557a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009880699s
STEP: Saw pod success
Jun  7 15:04:16.117: INFO: Pod "pod-projected-secrets-df4fcd59-9fb9-4768-b28d-29373f55557a" satisfied condition "Succeeded or Failed"
Jun  7 15:04:16.122: INFO: Trying to get logs from node proact-prod01-wk002 pod pod-projected-secrets-df4fcd59-9fb9-4768-b28d-29373f55557a container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  7 15:04:16.147: INFO: Waiting for pod pod-projected-secrets-df4fcd59-9fb9-4768-b28d-29373f55557a to disappear
Jun  7 15:04:16.150: INFO: Pod pod-projected-secrets-df4fcd59-9fb9-4768-b28d-29373f55557a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:04:16.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-769" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":338,"skipped":5893,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:04:16.167: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun  7 15:04:16.218: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
Jun  7 15:04:21.765: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:04:40.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8753" for this suite.

• [SLOW TEST:24.683 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":339,"skipped":5904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:04:40.850: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 15:04:40.909: INFO: The status of Pod busybox-host-aliases7c6cb80c-0c4c-48f2-9398-926942d6b938 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 15:04:42.920: INFO: The status of Pod busybox-host-aliases7c6cb80c-0c4c-48f2-9398-926942d6b938 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:04:42.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-613" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":340,"skipped":5933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:04:42.953: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 15:04:45.026: INFO: Deleting pod "var-expansion-31878b95-1457-4d52-97a0-381585786a26" in namespace "var-expansion-1400"
Jun  7 15:04:45.036: INFO: Wait up to 5m0s for pod "var-expansion-31878b95-1457-4d52-97a0-381585786a26" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:04:47.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1400" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":341,"skipped":5993,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:04:47.068: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  7 15:04:47.425: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  7 15:04:50.449: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:04:50.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1291" for this suite.
STEP: Destroying namespace "webhook-1291-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":342,"skipped":6000,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:04:50.530: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jun  7 15:04:50.567: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca473eeb-d51e-46d4-aa51-762631b6bd6c" in namespace "downward-api-9607" to be "Succeeded or Failed"
Jun  7 15:04:50.569: INFO: Pod "downwardapi-volume-ca473eeb-d51e-46d4-aa51-762631b6bd6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.139266ms
Jun  7 15:04:52.577: INFO: Pod "downwardapi-volume-ca473eeb-d51e-46d4-aa51-762631b6bd6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010538324s
STEP: Saw pod success
Jun  7 15:04:52.577: INFO: Pod "downwardapi-volume-ca473eeb-d51e-46d4-aa51-762631b6bd6c" satisfied condition "Succeeded or Failed"
Jun  7 15:04:52.582: INFO: Trying to get logs from node proact-prod01-wk002 pod downwardapi-volume-ca473eeb-d51e-46d4-aa51-762631b6bd6c container client-container: <nil>
STEP: delete the pod
Jun  7 15:04:52.602: INFO: Waiting for pod downwardapi-volume-ca473eeb-d51e-46d4-aa51-762631b6bd6c to disappear
Jun  7 15:04:52.605: INFO: Pod downwardapi-volume-ca473eeb-d51e-46d4-aa51-762631b6bd6c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:04:52.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9607" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":343,"skipped":6016,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:04:52.621: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:04:58.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5850" for this suite.

• [SLOW TEST:6.151 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":344,"skipped":6034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:04:58.774: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jun  7 15:04:58.840: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  7 15:04:58.840: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  7 15:04:58.846: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  7 15:04:58.846: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  7 15:04:58.860: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  7 15:04:58.860: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  7 15:04:58.876: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  7 15:04:58.876: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  7 15:04:59.920: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun  7 15:04:59.920: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun  7 15:04:59.994: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jun  7 15:05:00.004: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jun  7 15:05:00.006: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0
Jun  7 15:05:00.006: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0
Jun  7 15:05:00.006: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0
Jun  7 15:05:00.006: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0
Jun  7 15:05:00.007: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0
Jun  7 15:05:00.007: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0
Jun  7 15:05:00.007: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0
Jun  7 15:05:00.007: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 0
Jun  7 15:05:00.007: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
Jun  7 15:05:00.007: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
Jun  7 15:05:00.007: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:00.008: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:00.009: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:00.009: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:00.014: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:00.014: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:00.030: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:00.030: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:00.037: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
Jun  7 15:05:00.037: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
Jun  7 15:05:00.054: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
Jun  7 15:05:00.054: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
Jun  7 15:05:01.018: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:01.018: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:01.045: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
STEP: listing Deployments
Jun  7 15:05:01.054: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jun  7 15:05:01.066: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jun  7 15:05:01.072: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  7 15:05:01.076: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  7 15:05:01.095: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  7 15:05:01.107: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  7 15:05:01.120: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  7 15:05:02.018: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun  7 15:05:02.048: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun  7 15:05:02.055: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun  7 15:05:03.429: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jun  7 15:05:03.460: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
Jun  7 15:05:03.460: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
Jun  7 15:05:03.460: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
Jun  7 15:05:03.460: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
Jun  7 15:05:03.460: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 1
Jun  7 15:05:03.460: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:03.460: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:03.461: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 2
Jun  7 15:05:03.461: INFO: observed Deployment test-deployment in namespace deployment-3211 with ReadyReplicas 3
STEP: deleting the Deployment
Jun  7 15:05:03.470: INFO: observed event type MODIFIED
Jun  7 15:05:03.470: INFO: observed event type MODIFIED
Jun  7 15:05:03.470: INFO: observed event type MODIFIED
Jun  7 15:05:03.470: INFO: observed event type MODIFIED
Jun  7 15:05:03.470: INFO: observed event type MODIFIED
Jun  7 15:05:03.470: INFO: observed event type MODIFIED
Jun  7 15:05:03.470: INFO: observed event type MODIFIED
Jun  7 15:05:03.470: INFO: observed event type MODIFIED
Jun  7 15:05:03.470: INFO: observed event type MODIFIED
Jun  7 15:05:03.470: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  7 15:05:03.472: INFO: Log out all the ReplicaSets if there is no deployment created
Jun  7 15:05:03.475: INFO: ReplicaSet "test-deployment-56c98d85f9":
&ReplicaSet{ObjectMeta:{test-deployment-56c98d85f9  deployment-3211  5473131d-62e3-43e0-9f6e-95214c4c161e 33608955 4 2022-06-07 15:05:00 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 6d2145eb-7b01-44b3-a03f-a57ef481e881 0xc008e0a927 0xc008e0a928}] []  [{kube-controller-manager Update apps/v1 2022-06-07 15:05:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d2145eb-7b01-44b3-a03f-a57ef481e881\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 15:05:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 56c98d85f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.5 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc008e0a9b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun  7 15:05:03.479: INFO: pod: "test-deployment-56c98d85f9-crzr8":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-crzr8 test-deployment-56c98d85f9- deployment-3211  58a3b48a-f2d6-4d8e-8a5c-671605ac7d43 33608936 0 2022-06-07 15:05:01 +0000 UTC 2022-06-07 15:05:03 +0000 UTC 0xc008e54048 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[cni.projectcalico.org/containerID:cf43260ef0b041246fccfce076a7918539203bc60847e16c4289463628968646 cni.projectcalico.org/podIP:192.168.56.193/32 cni.projectcalico.org/podIPs:192.168.56.193/32] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 5473131d-62e3-43e0-9f6e-95214c4c161e 0xc008e54077 0xc008e54078}] []  [{calico Update v1 2022-06-07 15:05:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 15:05:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5473131d-62e3-43e0-9f6e-95214c4c161e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 15:05:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.56.193\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r9n4g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r9n4g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.13,PodIP:192.168.56.193,StartTime:2022-06-07 15:05:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 15:05:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:cri-o://fb63f59e4772086e9da09049ba2c6c97d3d7378d6dd33f242e33242d95d8ec4b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.56.193,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  7 15:05:03.479: INFO: pod: "test-deployment-56c98d85f9-snlh6":
&Pod{ObjectMeta:{test-deployment-56c98d85f9-snlh6 test-deployment-56c98d85f9- deployment-3211  b9420eb2-d35f-4422-b450-f8611ae6b51f 33608950 0 2022-06-07 15:05:00 +0000 UTC 2022-06-07 15:05:04 +0000 UTC 0xc008e54258 map[pod-template-hash:56c98d85f9 test-deployment-static:true] map[cni.projectcalico.org/containerID:5a649abfd37047d8499eab9cdb9cc6fd9fee042500ce1f89d5ce38c3ba16db58 cni.projectcalico.org/podIP:192.168.39.197/32 cni.projectcalico.org/podIPs:192.168.39.197/32] [{apps/v1 ReplicaSet test-deployment-56c98d85f9 5473131d-62e3-43e0-9f6e-95214c4c161e 0xc008e54287 0xc008e54288}] []  [{calico Update v1 2022-06-07 15:05:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 15:05:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5473131d-62e3-43e0-9f6e-95214c4c161e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 15:05:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.39.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-46gqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.5,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-46gqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.14,PodIP:192.168.39.197,StartTime:2022-06-07 15:05:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 15:05:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.5,ImageID:k8s.gcr.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07,ContainerID:cri-o://dba908ce8cf03eafa3f305a8db9fb796f7534ee18f5b1aec2eab2fe8c0cfeb93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.39.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  7 15:05:03.479: INFO: ReplicaSet "test-deployment-855f7994f9":
&ReplicaSet{ObjectMeta:{test-deployment-855f7994f9  deployment-3211  186eed9d-70ba-427f-ad42-71f5df159097 33608855 3 2022-06-07 15:04:58 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6d2145eb-7b01-44b3-a03f-a57ef481e881 0xc008e0aa17 0xc008e0aa18}] []  [{kube-controller-manager Update apps/v1 2022-06-07 15:04:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d2145eb-7b01-44b3-a03f-a57ef481e881\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 15:05:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 855f7994f9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:855f7994f9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc008e0aaa0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun  7 15:05:03.484: INFO: ReplicaSet "test-deployment-d4dfddfbf":
&ReplicaSet{ObjectMeta:{test-deployment-d4dfddfbf  deployment-3211  4684b32f-9cf5-421f-be70-e83a931e12e2 33608947 2 2022-06-07 15:05:01 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6d2145eb-7b01-44b3-a03f-a57ef481e881 0xc008e0ab07 0xc008e0ab08}] []  [{kube-controller-manager Update apps/v1 2022-06-07 15:05:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d2145eb-7b01-44b3-a03f-a57ef481e881\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-07 15:05:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: d4dfddfbf,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc008e0ab90 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jun  7 15:05:03.488: INFO: pod: "test-deployment-d4dfddfbf-mwsgv":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-mwsgv test-deployment-d4dfddfbf- deployment-3211  27acc933-81ea-404c-b0f6-c781c693d939 33608900 0 2022-06-07 15:05:01 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:2015728b4e1a1d0f2d1758ff28fc8c57a9c62c61e79664f9e83c07799a6dc85a cni.projectcalico.org/podIP:192.168.39.249/32 cni.projectcalico.org/podIPs:192.168.39.249/32] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 4684b32f-9cf5-421f-be70-e83a931e12e2 0xc008e55ab7 0xc008e55ab8}] []  [{calico Update v1 2022-06-07 15:05:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 15:05:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4684b32f-9cf5-421f-be70-e83a931e12e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 15:05:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.39.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vzglg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vzglg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.14,PodIP:192.168.39.249,StartTime:2022-06-07 15:05:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 15:05:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://8a9e6dbc9a4b0fe535240d3daa894fe3059beda9020ff54f901ee91d31189a76,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.39.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  7 15:05:03.488: INFO: pod: "test-deployment-d4dfddfbf-q5m62":
&Pod{ObjectMeta:{test-deployment-d4dfddfbf-q5m62 test-deployment-d4dfddfbf- deployment-3211  3944cbc8-bb78-406b-9646-366216e5acb5 33608946 0 2022-06-07 15:05:02 +0000 UTC <nil> <nil> map[pod-template-hash:d4dfddfbf test-deployment-static:true] map[cni.projectcalico.org/containerID:10446eb1b824e6d0c8c051dd016e0e0771220e81d7d862b66ef3293ea7c00112 cni.projectcalico.org/podIP:192.168.7.13/32 cni.projectcalico.org/podIPs:192.168.7.13/32] [{apps/v1 ReplicaSet test-deployment-d4dfddfbf 4684b32f-9cf5-421f-be70-e83a931e12e2 0xc008e55d07 0xc008e55d08}] []  [{calico Update v1 2022-06-07 15:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-06-07 15:05:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4684b32f-9cf5-421f-be70-e83a931e12e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-07 15:05:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.7.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vjftc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vjftc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:proact-prod01-wk004,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-07 15:05:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.55.210.16,PodIP:192.168.7.13,StartTime:2022-06-07 15:05:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-07 15:05:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:cri-o://c1e83660993a58947d9db4df2332abb9354c0a39bb512ff61cd31d5a06b871b2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.7.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:05:03.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3211" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":345,"skipped":6057,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  7 15:05:03.500: INFO: >>> kubeConfig: /tmp/kubeconfig-227657883
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jun  7 15:05:03.542: INFO: The status of Pod test-webserver-caee9682-a050-4762-abf4-7970da46a111 is Pending, waiting for it to be Running (with Ready = true)
Jun  7 15:05:05.552: INFO: The status of Pod test-webserver-caee9682-a050-4762-abf4-7970da46a111 is Running (Ready = false)
Jun  7 15:05:07.558: INFO: The status of Pod test-webserver-caee9682-a050-4762-abf4-7970da46a111 is Running (Ready = false)
Jun  7 15:05:09.556: INFO: The status of Pod test-webserver-caee9682-a050-4762-abf4-7970da46a111 is Running (Ready = false)
Jun  7 15:05:11.561: INFO: The status of Pod test-webserver-caee9682-a050-4762-abf4-7970da46a111 is Running (Ready = false)
Jun  7 15:05:13.556: INFO: The status of Pod test-webserver-caee9682-a050-4762-abf4-7970da46a111 is Running (Ready = false)
Jun  7 15:05:15.549: INFO: The status of Pod test-webserver-caee9682-a050-4762-abf4-7970da46a111 is Running (Ready = false)
Jun  7 15:05:17.561: INFO: The status of Pod test-webserver-caee9682-a050-4762-abf4-7970da46a111 is Running (Ready = false)
Jun  7 15:05:19.551: INFO: The status of Pod test-webserver-caee9682-a050-4762-abf4-7970da46a111 is Running (Ready = false)
Jun  7 15:05:21.555: INFO: The status of Pod test-webserver-caee9682-a050-4762-abf4-7970da46a111 is Running (Ready = false)
Jun  7 15:05:23.553: INFO: The status of Pod test-webserver-caee9682-a050-4762-abf4-7970da46a111 is Running (Ready = false)
Jun  7 15:05:25.557: INFO: The status of Pod test-webserver-caee9682-a050-4762-abf4-7970da46a111 is Running (Ready = true)
Jun  7 15:05:25.561: INFO: Container started at 2022-06-07 15:05:04 +0000 UTC, pod became ready at 2022-06-07 15:05:23 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  7 15:05:25.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5628" for this suite.

• [SLOW TEST:22.075 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":346,"skipped":6075,"failed":0}
SSSSSSSSSSSSSJun  7 15:05:25.575: INFO: Running AfterSuite actions on all nodes
Jun  7 15:05:25.575: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func17.2
Jun  7 15:05:25.575: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jun  7 15:05:25.575: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Jun  7 15:05:25.576: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jun  7 15:05:25.576: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jun  7 15:05:25.576: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jun  7 15:05:25.576: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jun  7 15:05:25.576: INFO: Running AfterSuite actions on node 1
Jun  7 15:05:25.576: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6088,"failed":0}

Ran 346 of 6434 Specs in 5537.864 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6088 Skipped
PASS

Ginkgo ran 1 suite in 1h32m19.55464522s
Test Suite Passed
