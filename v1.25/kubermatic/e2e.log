I0227 11:19:34.640829      20 e2e.go:116] Starting e2e run "b7a633fd-337d-4a15-aac2-3c71ea508a0e" on Ginkgo node 1
Feb 27 11:19:34.654: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1677496774 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Feb 27 11:19:34.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 11:19:34.821: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 27 11:19:34.865: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 27 11:19:34.927: INFO: 27 / 27 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 27 11:19:34.927: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 27 11:19:34.927: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 27 11:19:34.958: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'aws-node-termination-handler' (0 seconds elapsed)
Feb 27 11:19:34.958: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Feb 27 11:19:34.958: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
Feb 27 11:19:34.958: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'envoy-agent' (0 seconds elapsed)
Feb 27 11:19:34.958: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 27 11:19:34.958: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Feb 27 11:19:34.958: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'user-ssh-keys-agent' (0 seconds elapsed)
Feb 27 11:19:34.958: INFO: e2e test version: v1.25.6
Feb 27 11:19:34.976: INFO: kube-apiserver version: v1.25.6
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Feb 27 11:19:34.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 11:19:35.009: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.190 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Feb 27 11:19:34.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 11:19:34.821: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Feb 27 11:19:34.865: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Feb 27 11:19:34.927: INFO: 27 / 27 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Feb 27 11:19:34.927: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
    Feb 27 11:19:34.927: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Feb 27 11:19:34.958: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'aws-node-termination-handler' (0 seconds elapsed)
    Feb 27 11:19:34.958: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
    Feb 27 11:19:34.958: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
    Feb 27 11:19:34.958: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'envoy-agent' (0 seconds elapsed)
    Feb 27 11:19:34.958: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Feb 27 11:19:34.958: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Feb 27 11:19:34.958: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'user-ssh-keys-agent' (0 seconds elapsed)
    Feb 27 11:19:34.958: INFO: e2e test version: v1.25.6
    Feb 27 11:19:34.976: INFO: kube-apiserver version: v1.25.6
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Feb 27 11:19:34.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 11:19:35.009: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:19:35.039
Feb 27 11:19:35.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 11:19:35.04
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:19:35.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:19:35.094
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-595893ba-0fe1-459d-b742-43601522e3c9 02/27/23 11:19:35.105
STEP: Creating a pod to test consume secrets 02/27/23 11:19:35.116
Feb 27 11:19:35.131: INFO: Waiting up to 5m0s for pod "pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031" in namespace "secrets-1416" to be "Succeeded or Failed"
Feb 27 11:19:35.141: INFO: Pod "pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031": Phase="Pending", Reason="", readiness=false. Elapsed: 9.202817ms
Feb 27 11:19:37.154: INFO: Pod "pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022777459s
Feb 27 11:19:39.150: INFO: Pod "pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01838631s
STEP: Saw pod success 02/27/23 11:19:39.15
Feb 27 11:19:39.150: INFO: Pod "pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031" satisfied condition "Succeeded or Failed"
Feb 27 11:19:39.184: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 11:19:39.22
Feb 27 11:19:39.254: INFO: Waiting for pod pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031 to disappear
Feb 27 11:19:39.270: INFO: Pod pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 27 11:19:39.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1416" for this suite. 02/27/23 11:19:39.36
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":1,"skipped":60,"failed":0}
------------------------------
• [4.337 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:19:35.039
    Feb 27 11:19:35.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 11:19:35.04
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:19:35.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:19:35.094
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-595893ba-0fe1-459d-b742-43601522e3c9 02/27/23 11:19:35.105
    STEP: Creating a pod to test consume secrets 02/27/23 11:19:35.116
    Feb 27 11:19:35.131: INFO: Waiting up to 5m0s for pod "pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031" in namespace "secrets-1416" to be "Succeeded or Failed"
    Feb 27 11:19:35.141: INFO: Pod "pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031": Phase="Pending", Reason="", readiness=false. Elapsed: 9.202817ms
    Feb 27 11:19:37.154: INFO: Pod "pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022777459s
    Feb 27 11:19:39.150: INFO: Pod "pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01838631s
    STEP: Saw pod success 02/27/23 11:19:39.15
    Feb 27 11:19:39.150: INFO: Pod "pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031" satisfied condition "Succeeded or Failed"
    Feb 27 11:19:39.184: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 11:19:39.22
    Feb 27 11:19:39.254: INFO: Waiting for pod pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031 to disappear
    Feb 27 11:19:39.270: INFO: Pod pod-secrets-7976a7f4-03ea-4e81-8e2a-61d0678d1031 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 11:19:39.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1416" for this suite. 02/27/23 11:19:39.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:19:39.38
Feb 27 11:19:39.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:19:39.381
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:19:39.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:19:39.428
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-57846362-a6c6-4894-80e3-7794ff58ef4f 02/27/23 11:19:39.44
STEP: Creating a pod to test consume configMaps 02/27/23 11:19:39.448
Feb 27 11:19:39.469: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a" in namespace "projected-1601" to be "Succeeded or Failed"
Feb 27 11:19:39.483: INFO: Pod "pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.733637ms
Feb 27 11:19:41.496: INFO: Pod "pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02729548s
Feb 27 11:19:43.497: INFO: Pod "pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0276158s
STEP: Saw pod success 02/27/23 11:19:43.497
Feb 27 11:19:43.497: INFO: Pod "pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a" satisfied condition "Succeeded or Failed"
Feb 27 11:19:43.507: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a container agnhost-container: <nil>
STEP: delete the pod 02/27/23 11:19:43.532
Feb 27 11:19:43.556: INFO: Waiting for pod pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a to disappear
Feb 27 11:19:43.565: INFO: Pod pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 27 11:19:43.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1601" for this suite. 02/27/23 11:19:43.585
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":2,"skipped":68,"failed":0}
------------------------------
• [4.222 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:19:39.38
    Feb 27 11:19:39.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:19:39.381
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:19:39.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:19:39.428
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-57846362-a6c6-4894-80e3-7794ff58ef4f 02/27/23 11:19:39.44
    STEP: Creating a pod to test consume configMaps 02/27/23 11:19:39.448
    Feb 27 11:19:39.469: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a" in namespace "projected-1601" to be "Succeeded or Failed"
    Feb 27 11:19:39.483: INFO: Pod "pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.733637ms
    Feb 27 11:19:41.496: INFO: Pod "pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02729548s
    Feb 27 11:19:43.497: INFO: Pod "pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0276158s
    STEP: Saw pod success 02/27/23 11:19:43.497
    Feb 27 11:19:43.497: INFO: Pod "pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a" satisfied condition "Succeeded or Failed"
    Feb 27 11:19:43.507: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 11:19:43.532
    Feb 27 11:19:43.556: INFO: Waiting for pod pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a to disappear
    Feb 27 11:19:43.565: INFO: Pod pod-projected-configmaps-67bad288-437d-4ece-ba41-8da9bcd2202a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 27 11:19:43.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1601" for this suite. 02/27/23 11:19:43.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:19:43.606
Feb 27 11:19:43.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 11:19:43.607
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:19:43.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:19:43.665
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/27/23 11:19:43.677
Feb 27 11:19:43.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/27/23 11:19:58.348
Feb 27 11:19:58.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 11:20:01.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:20:19.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8815" for this suite. 02/27/23 11:20:19.039
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":3,"skipped":79,"failed":0}
------------------------------
• [SLOW TEST] [35.455 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:19:43.606
    Feb 27 11:19:43.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 11:19:43.607
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:19:43.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:19:43.665
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/27/23 11:19:43.677
    Feb 27 11:19:43.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/27/23 11:19:58.348
    Feb 27 11:19:58.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 11:20:01.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:20:19.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8815" for this suite. 02/27/23 11:20:19.039
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:20:19.061
Feb 27 11:20:19.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename svcaccounts 02/27/23 11:20:19.063
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:20:19.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:20:19.115
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Feb 27 11:20:19.166: INFO: created pod
Feb 27 11:20:19.166: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5598" to be "Succeeded or Failed"
Feb 27 11:20:19.178: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.08668ms
Feb 27 11:20:21.196: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029654935s
Feb 27 11:20:23.190: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023241328s
STEP: Saw pod success 02/27/23 11:20:23.19
Feb 27 11:20:23.190: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Feb 27 11:20:53.192: INFO: polling logs
Feb 27 11:20:53.209: INFO: Pod logs: 
I0227 11:20:20.023534       1 log.go:195] OK: Got token
I0227 11:20:20.023732       1 log.go:195] validating with in-cluster discovery
I0227 11:20:20.024124       1 log.go:195] OK: got issuer https://w7c7vmqk6c.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443
I0227 11:20:20.024399       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://w7c7vmqk6c.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443", Subject:"system:serviceaccount:svcaccounts-5598:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677497419, NotBefore:1677496819, IssuedAt:1677496819, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5598", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"afc398e2-72a9-4c42-8109-57d88f288e6b"}}}
I0227 11:20:20.042193       1 log.go:195] OK: Constructed OIDC provider for issuer https://w7c7vmqk6c.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443
I0227 11:20:20.063526       1 log.go:195] OK: Validated signature on JWT
I0227 11:20:20.063813       1 log.go:195] OK: Got valid claims from token!
I0227 11:20:20.063915       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://w7c7vmqk6c.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443", Subject:"system:serviceaccount:svcaccounts-5598:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677497419, NotBefore:1677496819, IssuedAt:1677496819, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5598", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"afc398e2-72a9-4c42-8109-57d88f288e6b"}}}

Feb 27 11:20:53.209: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 27 11:20:53.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5598" for this suite. 02/27/23 11:20:53.239
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":4,"skipped":82,"failed":0}
------------------------------
• [SLOW TEST] [34.197 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:20:19.061
    Feb 27 11:20:19.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 11:20:19.063
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:20:19.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:20:19.115
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Feb 27 11:20:19.166: INFO: created pod
    Feb 27 11:20:19.166: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5598" to be "Succeeded or Failed"
    Feb 27 11:20:19.178: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.08668ms
    Feb 27 11:20:21.196: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029654935s
    Feb 27 11:20:23.190: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023241328s
    STEP: Saw pod success 02/27/23 11:20:23.19
    Feb 27 11:20:23.190: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Feb 27 11:20:53.192: INFO: polling logs
    Feb 27 11:20:53.209: INFO: Pod logs: 
    I0227 11:20:20.023534       1 log.go:195] OK: Got token
    I0227 11:20:20.023732       1 log.go:195] validating with in-cluster discovery
    I0227 11:20:20.024124       1 log.go:195] OK: got issuer https://w7c7vmqk6c.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443
    I0227 11:20:20.024399       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://w7c7vmqk6c.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443", Subject:"system:serviceaccount:svcaccounts-5598:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677497419, NotBefore:1677496819, IssuedAt:1677496819, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5598", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"afc398e2-72a9-4c42-8109-57d88f288e6b"}}}
    I0227 11:20:20.042193       1 log.go:195] OK: Constructed OIDC provider for issuer https://w7c7vmqk6c.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443
    I0227 11:20:20.063526       1 log.go:195] OK: Validated signature on JWT
    I0227 11:20:20.063813       1 log.go:195] OK: Got valid claims from token!
    I0227 11:20:20.063915       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://w7c7vmqk6c.kkp-qa-env.kkp.qa.lab.kubermatic.io:6443", Subject:"system:serviceaccount:svcaccounts-5598:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677497419, NotBefore:1677496819, IssuedAt:1677496819, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5598", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"afc398e2-72a9-4c42-8109-57d88f288e6b"}}}

    Feb 27 11:20:53.209: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 27 11:20:53.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5598" for this suite. 02/27/23 11:20:53.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:20:53.27
Feb 27 11:20:53.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename security-context 02/27/23 11:20:53.272
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:20:53.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:20:53.352
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/27/23 11:20:53.369
Feb 27 11:20:53.387: INFO: Waiting up to 5m0s for pod "security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598" in namespace "security-context-3423" to be "Succeeded or Failed"
Feb 27 11:20:53.405: INFO: Pod "security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598": Phase="Pending", Reason="", readiness=false. Elapsed: 18.155705ms
Feb 27 11:20:55.415: INFO: Pod "security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028485485s
Feb 27 11:20:57.415: INFO: Pod "security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028137701s
STEP: Saw pod success 02/27/23 11:20:57.415
Feb 27 11:20:57.415: INFO: Pod "security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598" satisfied condition "Succeeded or Failed"
Feb 27 11:20:57.424: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598 container test-container: <nil>
STEP: delete the pod 02/27/23 11:20:57.44
Feb 27 11:20:57.464: INFO: Waiting for pod security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598 to disappear
Feb 27 11:20:57.473: INFO: Pod security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 27 11:20:57.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3423" for this suite. 02/27/23 11:20:57.485
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":5,"skipped":107,"failed":0}
------------------------------
• [4.231 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:20:53.27
    Feb 27 11:20:53.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename security-context 02/27/23 11:20:53.272
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:20:53.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:20:53.352
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/27/23 11:20:53.369
    Feb 27 11:20:53.387: INFO: Waiting up to 5m0s for pod "security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598" in namespace "security-context-3423" to be "Succeeded or Failed"
    Feb 27 11:20:53.405: INFO: Pod "security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598": Phase="Pending", Reason="", readiness=false. Elapsed: 18.155705ms
    Feb 27 11:20:55.415: INFO: Pod "security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028485485s
    Feb 27 11:20:57.415: INFO: Pod "security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028137701s
    STEP: Saw pod success 02/27/23 11:20:57.415
    Feb 27 11:20:57.415: INFO: Pod "security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598" satisfied condition "Succeeded or Failed"
    Feb 27 11:20:57.424: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598 container test-container: <nil>
    STEP: delete the pod 02/27/23 11:20:57.44
    Feb 27 11:20:57.464: INFO: Waiting for pod security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598 to disappear
    Feb 27 11:20:57.473: INFO: Pod security-context-93c3de5a-1f35-4b69-94aa-f1843eed8598 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 27 11:20:57.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-3423" for this suite. 02/27/23 11:20:57.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:20:57.505
Feb 27 11:20:57.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 11:20:57.507
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:20:57.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:20:57.559
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 02/27/23 11:20:57.58
STEP: watching for the Service to be added 02/27/23 11:20:57.617
Feb 27 11:20:57.627: INFO: Found Service test-service-q7nq6 in namespace services-4821 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Feb 27 11:20:57.628: INFO: Service test-service-q7nq6 created
STEP: Getting /status 02/27/23 11:20:57.63
Feb 27 11:20:57.642: INFO: Service test-service-q7nq6 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 02/27/23 11:20:57.642
STEP: watching for the Service to be patched 02/27/23 11:20:57.666
Feb 27 11:20:57.672: INFO: observed Service test-service-q7nq6 in namespace services-4821 with annotations: map[] & LoadBalancer: {[]}
Feb 27 11:20:57.672: INFO: Found Service test-service-q7nq6 in namespace services-4821 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Feb 27 11:20:57.672: INFO: Service test-service-q7nq6 has service status patched
STEP: updating the ServiceStatus 02/27/23 11:20:57.672
Feb 27 11:20:57.700: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 02/27/23 11:20:57.7
Feb 27 11:20:57.712: INFO: Observed Service test-service-q7nq6 in namespace services-4821 with annotations: map[] & Conditions: {[]}
Feb 27 11:20:57.712: INFO: Observed event: &Service{ObjectMeta:{test-service-q7nq6  services-4821  92e3b7dc-b827-4d15-994d-7c37c42f7524 57618 0 2023-02-27 11:20:57 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-27 11:20:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-27 11:20:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.240.24.169,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.240.24.169],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Feb 27 11:20:57.712: INFO: Found Service test-service-q7nq6 in namespace services-4821 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 27 11:20:57.712: INFO: Service test-service-q7nq6 has service status updated
STEP: patching the service 02/27/23 11:20:57.712
STEP: watching for the Service to be patched 02/27/23 11:20:57.738
Feb 27 11:20:57.745: INFO: observed Service test-service-q7nq6 in namespace services-4821 with labels: map[test-service-static:true]
Feb 27 11:20:57.745: INFO: observed Service test-service-q7nq6 in namespace services-4821 with labels: map[test-service-static:true]
Feb 27 11:20:57.745: INFO: observed Service test-service-q7nq6 in namespace services-4821 with labels: map[test-service-static:true]
Feb 27 11:20:57.745: INFO: Found Service test-service-q7nq6 in namespace services-4821 with labels: map[test-service:patched test-service-static:true]
Feb 27 11:20:57.745: INFO: Service test-service-q7nq6 patched
STEP: deleting the service 02/27/23 11:20:57.745
STEP: watching for the Service to be deleted 02/27/23 11:20:57.789
Feb 27 11:20:57.801: INFO: Observed event: ADDED
Feb 27 11:20:57.802: INFO: Observed event: MODIFIED
Feb 27 11:20:57.802: INFO: Observed event: MODIFIED
Feb 27 11:20:57.805: INFO: Observed event: MODIFIED
Feb 27 11:20:57.805: INFO: Found Service test-service-q7nq6 in namespace services-4821 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Feb 27 11:20:57.805: INFO: Service test-service-q7nq6 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 11:20:57.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4821" for this suite. 02/27/23 11:20:57.82
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":6,"skipped":122,"failed":0}
------------------------------
• [0.332 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:20:57.505
    Feb 27 11:20:57.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 11:20:57.507
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:20:57.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:20:57.559
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 02/27/23 11:20:57.58
    STEP: watching for the Service to be added 02/27/23 11:20:57.617
    Feb 27 11:20:57.627: INFO: Found Service test-service-q7nq6 in namespace services-4821 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Feb 27 11:20:57.628: INFO: Service test-service-q7nq6 created
    STEP: Getting /status 02/27/23 11:20:57.63
    Feb 27 11:20:57.642: INFO: Service test-service-q7nq6 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 02/27/23 11:20:57.642
    STEP: watching for the Service to be patched 02/27/23 11:20:57.666
    Feb 27 11:20:57.672: INFO: observed Service test-service-q7nq6 in namespace services-4821 with annotations: map[] & LoadBalancer: {[]}
    Feb 27 11:20:57.672: INFO: Found Service test-service-q7nq6 in namespace services-4821 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Feb 27 11:20:57.672: INFO: Service test-service-q7nq6 has service status patched
    STEP: updating the ServiceStatus 02/27/23 11:20:57.672
    Feb 27 11:20:57.700: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 02/27/23 11:20:57.7
    Feb 27 11:20:57.712: INFO: Observed Service test-service-q7nq6 in namespace services-4821 with annotations: map[] & Conditions: {[]}
    Feb 27 11:20:57.712: INFO: Observed event: &Service{ObjectMeta:{test-service-q7nq6  services-4821  92e3b7dc-b827-4d15-994d-7c37c42f7524 57618 0 2023-02-27 11:20:57 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-27 11:20:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-27 11:20:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.240.24.169,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.240.24.169],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Feb 27 11:20:57.712: INFO: Found Service test-service-q7nq6 in namespace services-4821 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 27 11:20:57.712: INFO: Service test-service-q7nq6 has service status updated
    STEP: patching the service 02/27/23 11:20:57.712
    STEP: watching for the Service to be patched 02/27/23 11:20:57.738
    Feb 27 11:20:57.745: INFO: observed Service test-service-q7nq6 in namespace services-4821 with labels: map[test-service-static:true]
    Feb 27 11:20:57.745: INFO: observed Service test-service-q7nq6 in namespace services-4821 with labels: map[test-service-static:true]
    Feb 27 11:20:57.745: INFO: observed Service test-service-q7nq6 in namespace services-4821 with labels: map[test-service-static:true]
    Feb 27 11:20:57.745: INFO: Found Service test-service-q7nq6 in namespace services-4821 with labels: map[test-service:patched test-service-static:true]
    Feb 27 11:20:57.745: INFO: Service test-service-q7nq6 patched
    STEP: deleting the service 02/27/23 11:20:57.745
    STEP: watching for the Service to be deleted 02/27/23 11:20:57.789
    Feb 27 11:20:57.801: INFO: Observed event: ADDED
    Feb 27 11:20:57.802: INFO: Observed event: MODIFIED
    Feb 27 11:20:57.802: INFO: Observed event: MODIFIED
    Feb 27 11:20:57.805: INFO: Observed event: MODIFIED
    Feb 27 11:20:57.805: INFO: Found Service test-service-q7nq6 in namespace services-4821 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Feb 27 11:20:57.805: INFO: Service test-service-q7nq6 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 11:20:57.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4821" for this suite. 02/27/23 11:20:57.82
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:20:57.84
Feb 27 11:20:57.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:20:57.841
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:20:57.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:20:57.9
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 02/27/23 11:20:57.914
Feb 27 11:20:57.936: INFO: Waiting up to 5m0s for pod "annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618" in namespace "projected-1255" to be "running and ready"
Feb 27 11:20:57.957: INFO: Pod "annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618": Phase="Pending", Reason="", readiness=false. Elapsed: 21.464178ms
Feb 27 11:20:57.957: INFO: The phase of Pod annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:20:59.967: INFO: Pod "annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618": Phase="Running", Reason="", readiness=true. Elapsed: 2.030949312s
Feb 27 11:20:59.967: INFO: The phase of Pod annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618 is Running (Ready = true)
Feb 27 11:20:59.967: INFO: Pod "annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618" satisfied condition "running and ready"
Feb 27 11:21:00.522: INFO: Successfully updated pod "annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 27 11:21:04.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1255" for this suite. 02/27/23 11:21:04.622
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":7,"skipped":164,"failed":0}
------------------------------
• [SLOW TEST] [6.802 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:20:57.84
    Feb 27 11:20:57.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:20:57.841
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:20:57.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:20:57.9
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 02/27/23 11:20:57.914
    Feb 27 11:20:57.936: INFO: Waiting up to 5m0s for pod "annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618" in namespace "projected-1255" to be "running and ready"
    Feb 27 11:20:57.957: INFO: Pod "annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618": Phase="Pending", Reason="", readiness=false. Elapsed: 21.464178ms
    Feb 27 11:20:57.957: INFO: The phase of Pod annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:20:59.967: INFO: Pod "annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618": Phase="Running", Reason="", readiness=true. Elapsed: 2.030949312s
    Feb 27 11:20:59.967: INFO: The phase of Pod annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618 is Running (Ready = true)
    Feb 27 11:20:59.967: INFO: Pod "annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618" satisfied condition "running and ready"
    Feb 27 11:21:00.522: INFO: Successfully updated pod "annotationupdate201b7042-0206-41b6-b307-cfcdc8f6d618"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 27 11:21:04.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1255" for this suite. 02/27/23 11:21:04.622
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:21:04.646
Feb 27 11:21:04.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 11:21:04.65
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:04.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:04.7
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-42442de7-5e67-4783-a3e8-35d455895897 02/27/23 11:21:04.711
STEP: Creating a pod to test consume configMaps 02/27/23 11:21:04.726
Feb 27 11:21:04.747: INFO: Waiting up to 5m0s for pod "pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3" in namespace "configmap-4547" to be "Succeeded or Failed"
Feb 27 11:21:04.756: INFO: Pod "pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.604676ms
Feb 27 11:21:06.773: INFO: Pod "pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025165246s
Feb 27 11:21:08.791: INFO: Pod "pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04372909s
STEP: Saw pod success 02/27/23 11:21:08.792
Feb 27 11:21:08.792: INFO: Pod "pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3" satisfied condition "Succeeded or Failed"
Feb 27 11:21:08.806: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 11:21:08.823
Feb 27 11:21:08.853: INFO: Waiting for pod pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3 to disappear
Feb 27 11:21:08.869: INFO: Pod pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 11:21:08.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4547" for this suite. 02/27/23 11:21:08.887
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":8,"skipped":165,"failed":0}
------------------------------
• [4.256 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:21:04.646
    Feb 27 11:21:04.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 11:21:04.65
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:04.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:04.7
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-42442de7-5e67-4783-a3e8-35d455895897 02/27/23 11:21:04.711
    STEP: Creating a pod to test consume configMaps 02/27/23 11:21:04.726
    Feb 27 11:21:04.747: INFO: Waiting up to 5m0s for pod "pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3" in namespace "configmap-4547" to be "Succeeded or Failed"
    Feb 27 11:21:04.756: INFO: Pod "pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.604676ms
    Feb 27 11:21:06.773: INFO: Pod "pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025165246s
    Feb 27 11:21:08.791: INFO: Pod "pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04372909s
    STEP: Saw pod success 02/27/23 11:21:08.792
    Feb 27 11:21:08.792: INFO: Pod "pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3" satisfied condition "Succeeded or Failed"
    Feb 27 11:21:08.806: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 11:21:08.823
    Feb 27 11:21:08.853: INFO: Waiting for pod pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3 to disappear
    Feb 27 11:21:08.869: INFO: Pod pod-configmaps-a6ff3572-1c9f-4786-bb05-7a613daadcb3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 11:21:08.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4547" for this suite. 02/27/23 11:21:08.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:21:08.907
Feb 27 11:21:08.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 11:21:08.908
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:08.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:08.961
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 02/27/23 11:21:08.973
Feb 27 11:21:08.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 create -f -'
Feb 27 11:21:10.374: INFO: stderr: ""
Feb 27 11:21:10.374: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 11:21:10.374
Feb 27 11:21:10.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 11:21:10.523: INFO: stderr: ""
Feb 27 11:21:10.523: INFO: stdout: "update-demo-nautilus-527cv update-demo-nautilus-5ctwk "
Feb 27 11:21:10.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods update-demo-nautilus-527cv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 11:21:10.623: INFO: stderr: ""
Feb 27 11:21:10.623: INFO: stdout: ""
Feb 27 11:21:10.623: INFO: update-demo-nautilus-527cv is created but not running
Feb 27 11:21:15.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 11:21:15.730: INFO: stderr: ""
Feb 27 11:21:15.730: INFO: stdout: "update-demo-nautilus-527cv update-demo-nautilus-5ctwk "
Feb 27 11:21:15.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods update-demo-nautilus-527cv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 11:21:15.840: INFO: stderr: ""
Feb 27 11:21:15.840: INFO: stdout: "true"
Feb 27 11:21:15.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods update-demo-nautilus-527cv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 11:21:15.937: INFO: stderr: ""
Feb 27 11:21:15.937: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 27 11:21:15.937: INFO: validating pod update-demo-nautilus-527cv
Feb 27 11:21:15.955: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 11:21:15.955: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 11:21:15.955: INFO: update-demo-nautilus-527cv is verified up and running
Feb 27 11:21:15.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods update-demo-nautilus-5ctwk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 11:21:16.055: INFO: stderr: ""
Feb 27 11:21:16.055: INFO: stdout: "true"
Feb 27 11:21:16.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods update-demo-nautilus-5ctwk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 11:21:16.217: INFO: stderr: ""
Feb 27 11:21:16.217: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 27 11:21:16.217: INFO: validating pod update-demo-nautilus-5ctwk
Feb 27 11:21:16.234: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 11:21:16.234: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 11:21:16.234: INFO: update-demo-nautilus-5ctwk is verified up and running
STEP: using delete to clean up resources 02/27/23 11:21:16.234
Feb 27 11:21:16.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 delete --grace-period=0 --force -f -'
Feb 27 11:21:16.344: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 11:21:16.344: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 27 11:21:16.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get rc,svc -l name=update-demo --no-headers'
Feb 27 11:21:16.466: INFO: stderr: "No resources found in kubectl-1657 namespace.\n"
Feb 27 11:21:16.466: INFO: stdout: ""
Feb 27 11:21:16.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 27 11:21:16.577: INFO: stderr: ""
Feb 27 11:21:16.577: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 11:21:16.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1657" for this suite. 02/27/23 11:21:16.593
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":9,"skipped":186,"failed":0}
------------------------------
• [SLOW TEST] [7.699 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:21:08.907
    Feb 27 11:21:08.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 11:21:08.908
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:08.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:08.961
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 02/27/23 11:21:08.973
    Feb 27 11:21:08.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 create -f -'
    Feb 27 11:21:10.374: INFO: stderr: ""
    Feb 27 11:21:10.374: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 11:21:10.374
    Feb 27 11:21:10.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 11:21:10.523: INFO: stderr: ""
    Feb 27 11:21:10.523: INFO: stdout: "update-demo-nautilus-527cv update-demo-nautilus-5ctwk "
    Feb 27 11:21:10.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods update-demo-nautilus-527cv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 11:21:10.623: INFO: stderr: ""
    Feb 27 11:21:10.623: INFO: stdout: ""
    Feb 27 11:21:10.623: INFO: update-demo-nautilus-527cv is created but not running
    Feb 27 11:21:15.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 11:21:15.730: INFO: stderr: ""
    Feb 27 11:21:15.730: INFO: stdout: "update-demo-nautilus-527cv update-demo-nautilus-5ctwk "
    Feb 27 11:21:15.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods update-demo-nautilus-527cv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 11:21:15.840: INFO: stderr: ""
    Feb 27 11:21:15.840: INFO: stdout: "true"
    Feb 27 11:21:15.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods update-demo-nautilus-527cv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 11:21:15.937: INFO: stderr: ""
    Feb 27 11:21:15.937: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 27 11:21:15.937: INFO: validating pod update-demo-nautilus-527cv
    Feb 27 11:21:15.955: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 11:21:15.955: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 11:21:15.955: INFO: update-demo-nautilus-527cv is verified up and running
    Feb 27 11:21:15.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods update-demo-nautilus-5ctwk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 11:21:16.055: INFO: stderr: ""
    Feb 27 11:21:16.055: INFO: stdout: "true"
    Feb 27 11:21:16.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods update-demo-nautilus-5ctwk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 11:21:16.217: INFO: stderr: ""
    Feb 27 11:21:16.217: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 27 11:21:16.217: INFO: validating pod update-demo-nautilus-5ctwk
    Feb 27 11:21:16.234: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 11:21:16.234: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 11:21:16.234: INFO: update-demo-nautilus-5ctwk is verified up and running
    STEP: using delete to clean up resources 02/27/23 11:21:16.234
    Feb 27 11:21:16.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 delete --grace-period=0 --force -f -'
    Feb 27 11:21:16.344: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 11:21:16.344: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 27 11:21:16.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get rc,svc -l name=update-demo --no-headers'
    Feb 27 11:21:16.466: INFO: stderr: "No resources found in kubectl-1657 namespace.\n"
    Feb 27 11:21:16.466: INFO: stdout: ""
    Feb 27 11:21:16.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1657 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 27 11:21:16.577: INFO: stderr: ""
    Feb 27 11:21:16.577: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 11:21:16.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1657" for this suite. 02/27/23 11:21:16.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:21:16.607
Feb 27 11:21:16.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:21:16.608
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:16.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:16.642
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-b4a1fd3c-71dc-4fb5-a342-67804f9e3619 02/27/23 11:21:16.652
STEP: Creating secret with name secret-projected-all-test-volume-4e682cdf-682c-4dd4-8af4-8e2a1cabb971 02/27/23 11:21:16.661
STEP: Creating a pod to test Check all projections for projected volume plugin 02/27/23 11:21:16.675
Feb 27 11:21:16.690: INFO: Waiting up to 5m0s for pod "projected-volume-24ce239b-c049-49ee-b200-9d438148443e" in namespace "projected-8811" to be "Succeeded or Failed"
Feb 27 11:21:16.705: INFO: Pod "projected-volume-24ce239b-c049-49ee-b200-9d438148443e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.234219ms
Feb 27 11:21:18.720: INFO: Pod "projected-volume-24ce239b-c049-49ee-b200-9d438148443e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0298378s
Feb 27 11:21:20.716: INFO: Pod "projected-volume-24ce239b-c049-49ee-b200-9d438148443e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025214006s
Feb 27 11:21:22.716: INFO: Pod "projected-volume-24ce239b-c049-49ee-b200-9d438148443e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025953515s
STEP: Saw pod success 02/27/23 11:21:22.717
Feb 27 11:21:22.718: INFO: Pod "projected-volume-24ce239b-c049-49ee-b200-9d438148443e" satisfied condition "Succeeded or Failed"
Feb 27 11:21:22.727: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod projected-volume-24ce239b-c049-49ee-b200-9d438148443e container projected-all-volume-test: <nil>
STEP: delete the pod 02/27/23 11:21:22.744
Feb 27 11:21:22.770: INFO: Waiting for pod projected-volume-24ce239b-c049-49ee-b200-9d438148443e to disappear
Feb 27 11:21:22.785: INFO: Pod projected-volume-24ce239b-c049-49ee-b200-9d438148443e no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Feb 27 11:21:22.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8811" for this suite. 02/27/23 11:21:22.796
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":10,"skipped":194,"failed":0}
------------------------------
• [SLOW TEST] [6.213 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:21:16.607
    Feb 27 11:21:16.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:21:16.608
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:16.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:16.642
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-b4a1fd3c-71dc-4fb5-a342-67804f9e3619 02/27/23 11:21:16.652
    STEP: Creating secret with name secret-projected-all-test-volume-4e682cdf-682c-4dd4-8af4-8e2a1cabb971 02/27/23 11:21:16.661
    STEP: Creating a pod to test Check all projections for projected volume plugin 02/27/23 11:21:16.675
    Feb 27 11:21:16.690: INFO: Waiting up to 5m0s for pod "projected-volume-24ce239b-c049-49ee-b200-9d438148443e" in namespace "projected-8811" to be "Succeeded or Failed"
    Feb 27 11:21:16.705: INFO: Pod "projected-volume-24ce239b-c049-49ee-b200-9d438148443e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.234219ms
    Feb 27 11:21:18.720: INFO: Pod "projected-volume-24ce239b-c049-49ee-b200-9d438148443e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0298378s
    Feb 27 11:21:20.716: INFO: Pod "projected-volume-24ce239b-c049-49ee-b200-9d438148443e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025214006s
    Feb 27 11:21:22.716: INFO: Pod "projected-volume-24ce239b-c049-49ee-b200-9d438148443e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025953515s
    STEP: Saw pod success 02/27/23 11:21:22.717
    Feb 27 11:21:22.718: INFO: Pod "projected-volume-24ce239b-c049-49ee-b200-9d438148443e" satisfied condition "Succeeded or Failed"
    Feb 27 11:21:22.727: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod projected-volume-24ce239b-c049-49ee-b200-9d438148443e container projected-all-volume-test: <nil>
    STEP: delete the pod 02/27/23 11:21:22.744
    Feb 27 11:21:22.770: INFO: Waiting for pod projected-volume-24ce239b-c049-49ee-b200-9d438148443e to disappear
    Feb 27 11:21:22.785: INFO: Pod projected-volume-24ce239b-c049-49ee-b200-9d438148443e no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Feb 27 11:21:22.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8811" for this suite. 02/27/23 11:21:22.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:21:22.821
Feb 27 11:21:22.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:21:22.823
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:22.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:22.861
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-26b4bb3f-3de8-4ef9-b01b-dbb233913837 02/27/23 11:21:22.874
STEP: Creating a pod to test consume configMaps 02/27/23 11:21:22.884
Feb 27 11:21:22.897: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076" in namespace "projected-2783" to be "Succeeded or Failed"
Feb 27 11:21:22.909: INFO: Pod "pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076": Phase="Pending", Reason="", readiness=false. Elapsed: 11.892239ms
Feb 27 11:21:24.929: INFO: Pod "pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032108839s
Feb 27 11:21:26.917: INFO: Pod "pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020358129s
Feb 27 11:21:28.918: INFO: Pod "pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02178278s
STEP: Saw pod success 02/27/23 11:21:28.919
Feb 27 11:21:28.919: INFO: Pod "pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076" satisfied condition "Succeeded or Failed"
Feb 27 11:21:28.927: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 11:21:28.95
Feb 27 11:21:28.976: INFO: Waiting for pod pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076 to disappear
Feb 27 11:21:28.986: INFO: Pod pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 27 11:21:28.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2783" for this suite. 02/27/23 11:21:29.005
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":11,"skipped":217,"failed":0}
------------------------------
• [SLOW TEST] [6.198 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:21:22.821
    Feb 27 11:21:22.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:21:22.823
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:22.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:22.861
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-26b4bb3f-3de8-4ef9-b01b-dbb233913837 02/27/23 11:21:22.874
    STEP: Creating a pod to test consume configMaps 02/27/23 11:21:22.884
    Feb 27 11:21:22.897: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076" in namespace "projected-2783" to be "Succeeded or Failed"
    Feb 27 11:21:22.909: INFO: Pod "pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076": Phase="Pending", Reason="", readiness=false. Elapsed: 11.892239ms
    Feb 27 11:21:24.929: INFO: Pod "pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032108839s
    Feb 27 11:21:26.917: INFO: Pod "pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020358129s
    Feb 27 11:21:28.918: INFO: Pod "pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02178278s
    STEP: Saw pod success 02/27/23 11:21:28.919
    Feb 27 11:21:28.919: INFO: Pod "pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076" satisfied condition "Succeeded or Failed"
    Feb 27 11:21:28.927: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 11:21:28.95
    Feb 27 11:21:28.976: INFO: Waiting for pod pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076 to disappear
    Feb 27 11:21:28.986: INFO: Pod pod-projected-configmaps-f3395129-a196-4fa5-8184-9864c2d9c076 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 27 11:21:28.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2783" for this suite. 02/27/23 11:21:29.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:21:29.034
Feb 27 11:21:29.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename gc 02/27/23 11:21:29.035
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:29.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:29.073
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Feb 27 11:21:29.161: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"82eaaf72-b850-45a9-a9fc-ed79634f7767", Controller:(*bool)(0xc004ddcf16), BlockOwnerDeletion:(*bool)(0xc004ddcf17)}}
Feb 27 11:21:29.174: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d556918e-940b-4fdf-8629-6c1d5b783832", Controller:(*bool)(0xc004ddd2c6), BlockOwnerDeletion:(*bool)(0xc004ddd2c7)}}
Feb 27 11:21:29.197: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e7f8e87a-0cea-48ec-bfbf-594327044afd", Controller:(*bool)(0xc004ddd706), BlockOwnerDeletion:(*bool)(0xc004ddd707)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 27 11:21:34.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7837" for this suite. 02/27/23 11:21:34.251
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":12,"skipped":268,"failed":0}
------------------------------
• [SLOW TEST] [5.236 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:21:29.034
    Feb 27 11:21:29.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename gc 02/27/23 11:21:29.035
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:29.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:29.073
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Feb 27 11:21:29.161: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"82eaaf72-b850-45a9-a9fc-ed79634f7767", Controller:(*bool)(0xc004ddcf16), BlockOwnerDeletion:(*bool)(0xc004ddcf17)}}
    Feb 27 11:21:29.174: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d556918e-940b-4fdf-8629-6c1d5b783832", Controller:(*bool)(0xc004ddd2c6), BlockOwnerDeletion:(*bool)(0xc004ddd2c7)}}
    Feb 27 11:21:29.197: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e7f8e87a-0cea-48ec-bfbf-594327044afd", Controller:(*bool)(0xc004ddd706), BlockOwnerDeletion:(*bool)(0xc004ddd707)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 27 11:21:34.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7837" for this suite. 02/27/23 11:21:34.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:21:34.272
Feb 27 11:21:34.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename daemonsets 02/27/23 11:21:34.273
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:34.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:34.33
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 02/27/23 11:21:34.404
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 11:21:34.416
Feb 27 11:21:34.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:21:34.442: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:21:35.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:21:35.479: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:21:36.584: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 11:21:36.584: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:21:37.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 11:21:37.477: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/27/23 11:21:37.486
Feb 27 11:21:37.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 11:21:37.553: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:21:38.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 11:21:38.576: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:21:39.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 11:21:39.577: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 02/27/23 11:21:39.577
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/27/23 11:21:39.593
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6094, will wait for the garbage collector to delete the pods 02/27/23 11:21:39.593
Feb 27 11:21:39.669: INFO: Deleting DaemonSet.extensions daemon-set took: 16.793069ms
Feb 27 11:21:39.770: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.249096ms
Feb 27 11:21:42.179: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:21:42.179: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 11:21:42.190: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"58146"},"items":null}

Feb 27 11:21:42.201: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"58146"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 27 11:21:42.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6094" for this suite. 02/27/23 11:21:42.264
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":13,"skipped":274,"failed":0}
------------------------------
• [SLOW TEST] [8.007 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:21:34.272
    Feb 27 11:21:34.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename daemonsets 02/27/23 11:21:34.273
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:34.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:34.33
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 02/27/23 11:21:34.404
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 11:21:34.416
    Feb 27 11:21:34.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:21:34.442: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:21:35.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:21:35.479: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:21:36.584: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 11:21:36.584: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:21:37.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 11:21:37.477: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/27/23 11:21:37.486
    Feb 27 11:21:37.553: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 11:21:37.553: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:21:38.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 11:21:38.576: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:21:39.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 11:21:39.577: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 02/27/23 11:21:39.577
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 11:21:39.593
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6094, will wait for the garbage collector to delete the pods 02/27/23 11:21:39.593
    Feb 27 11:21:39.669: INFO: Deleting DaemonSet.extensions daemon-set took: 16.793069ms
    Feb 27 11:21:39.770: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.249096ms
    Feb 27 11:21:42.179: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:21:42.179: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 11:21:42.190: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"58146"},"items":null}

    Feb 27 11:21:42.201: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"58146"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 11:21:42.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6094" for this suite. 02/27/23 11:21:42.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:21:42.284
Feb 27 11:21:42.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 11:21:42.285
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:42.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:42.322
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 02/27/23 11:21:42.333
Feb 27 11:21:42.351: INFO: Waiting up to 5m0s for pod "pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4" in namespace "emptydir-6122" to be "Succeeded or Failed"
Feb 27 11:21:42.359: INFO: Pod "pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.942462ms
Feb 27 11:21:44.375: INFO: Pod "pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024078442s
Feb 27 11:21:46.370: INFO: Pod "pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019698689s
STEP: Saw pod success 02/27/23 11:21:46.37
Feb 27 11:21:46.370: INFO: Pod "pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4" satisfied condition "Succeeded or Failed"
Feb 27 11:21:46.377: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4 container test-container: <nil>
STEP: delete the pod 02/27/23 11:21:46.395
Feb 27 11:21:46.422: INFO: Waiting for pod pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4 to disappear
Feb 27 11:21:46.432: INFO: Pod pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 11:21:46.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6122" for this suite. 02/27/23 11:21:46.464
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":14,"skipped":282,"failed":0}
------------------------------
• [4.195 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:21:42.284
    Feb 27 11:21:42.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 11:21:42.285
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:42.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:42.322
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/27/23 11:21:42.333
    Feb 27 11:21:42.351: INFO: Waiting up to 5m0s for pod "pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4" in namespace "emptydir-6122" to be "Succeeded or Failed"
    Feb 27 11:21:42.359: INFO: Pod "pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.942462ms
    Feb 27 11:21:44.375: INFO: Pod "pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024078442s
    Feb 27 11:21:46.370: INFO: Pod "pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019698689s
    STEP: Saw pod success 02/27/23 11:21:46.37
    Feb 27 11:21:46.370: INFO: Pod "pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4" satisfied condition "Succeeded or Failed"
    Feb 27 11:21:46.377: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4 container test-container: <nil>
    STEP: delete the pod 02/27/23 11:21:46.395
    Feb 27 11:21:46.422: INFO: Waiting for pod pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4 to disappear
    Feb 27 11:21:46.432: INFO: Pod pod-933e84f8-5464-41e5-b0cc-99a092a3a9c4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 11:21:46.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6122" for this suite. 02/27/23 11:21:46.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:21:46.485
Feb 27 11:21:46.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename dns 02/27/23 11:21:46.494
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:46.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:46.555
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 02/27/23 11:21:46.565
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8084.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8084.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8084.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8084.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 82.23.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.23.82_udp@PTR;check="$$(dig +tcp +noall +answer +search 82.23.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.23.82_tcp@PTR;sleep 1; done
 02/27/23 11:21:46.612
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8084.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8084.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8084.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8084.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 82.23.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.23.82_udp@PTR;check="$$(dig +tcp +noall +answer +search 82.23.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.23.82_tcp@PTR;sleep 1; done
 02/27/23 11:21:46.612
STEP: creating a pod to probe DNS 02/27/23 11:21:46.612
STEP: submitting the pod to kubernetes 02/27/23 11:21:46.613
Feb 27 11:21:46.639: INFO: Waiting up to 15m0s for pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c" in namespace "dns-8084" to be "running"
Feb 27 11:21:46.650: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.246982ms
Feb 27 11:21:48.664: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024321407s
Feb 27 11:21:50.659: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01969091s
Feb 27 11:21:52.662: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022743738s
Feb 27 11:21:54.665: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025478206s
Feb 27 11:21:56.662: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022149827s
Feb 27 11:21:58.667: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.027555122s
Feb 27 11:22:00.659: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019760717s
Feb 27 11:22:02.661: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Running", Reason="", readiness=true. Elapsed: 16.021706239s
Feb 27 11:22:02.661: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c" satisfied condition "running"
STEP: retrieving the pod 02/27/23 11:22:02.661
STEP: looking for the results for each expected name from probers 02/27/23 11:22:02.674
Feb 27 11:22:02.702: INFO: Unable to read wheezy_udp@dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
Feb 27 11:22:02.715: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
Feb 27 11:22:02.729: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
Feb 27 11:22:02.742: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
Feb 27 11:22:02.849: INFO: Unable to read jessie_udp@dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
Feb 27 11:22:02.889: INFO: Unable to read jessie_tcp@dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
Feb 27 11:22:02.937: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
Feb 27 11:22:02.958: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
Feb 27 11:22:03.024: INFO: Lookups using dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c failed for: [wheezy_udp@dns-test-service.dns-8084.svc.cluster.local wheezy_tcp@dns-test-service.dns-8084.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local jessie_udp@dns-test-service.dns-8084.svc.cluster.local jessie_tcp@dns-test-service.dns-8084.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local]

Feb 27 11:22:08.241: INFO: DNS probes using dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c succeeded

STEP: deleting the pod 02/27/23 11:22:08.241
STEP: deleting the test service 02/27/23 11:22:08.266
STEP: deleting the test headless service 02/27/23 11:22:08.313
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 27 11:22:08.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8084" for this suite. 02/27/23 11:22:08.367
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":15,"skipped":291,"failed":0}
------------------------------
• [SLOW TEST] [21.901 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:21:46.485
    Feb 27 11:21:46.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename dns 02/27/23 11:21:46.494
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:21:46.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:21:46.555
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 02/27/23 11:21:46.565
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8084.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8084.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8084.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8084.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 82.23.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.23.82_udp@PTR;check="$$(dig +tcp +noall +answer +search 82.23.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.23.82_tcp@PTR;sleep 1; done
     02/27/23 11:21:46.612
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8084.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8084.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8084.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8084.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8084.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 82.23.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.23.82_udp@PTR;check="$$(dig +tcp +noall +answer +search 82.23.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.23.82_tcp@PTR;sleep 1; done
     02/27/23 11:21:46.612
    STEP: creating a pod to probe DNS 02/27/23 11:21:46.612
    STEP: submitting the pod to kubernetes 02/27/23 11:21:46.613
    Feb 27 11:21:46.639: INFO: Waiting up to 15m0s for pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c" in namespace "dns-8084" to be "running"
    Feb 27 11:21:46.650: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.246982ms
    Feb 27 11:21:48.664: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024321407s
    Feb 27 11:21:50.659: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01969091s
    Feb 27 11:21:52.662: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022743738s
    Feb 27 11:21:54.665: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025478206s
    Feb 27 11:21:56.662: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022149827s
    Feb 27 11:21:58.667: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.027555122s
    Feb 27 11:22:00.659: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019760717s
    Feb 27 11:22:02.661: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c": Phase="Running", Reason="", readiness=true. Elapsed: 16.021706239s
    Feb 27 11:22:02.661: INFO: Pod "dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 11:22:02.661
    STEP: looking for the results for each expected name from probers 02/27/23 11:22:02.674
    Feb 27 11:22:02.702: INFO: Unable to read wheezy_udp@dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
    Feb 27 11:22:02.715: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
    Feb 27 11:22:02.729: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
    Feb 27 11:22:02.742: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
    Feb 27 11:22:02.849: INFO: Unable to read jessie_udp@dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
    Feb 27 11:22:02.889: INFO: Unable to read jessie_tcp@dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
    Feb 27 11:22:02.937: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
    Feb 27 11:22:02.958: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local from pod dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c: the server could not find the requested resource (get pods dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c)
    Feb 27 11:22:03.024: INFO: Lookups using dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c failed for: [wheezy_udp@dns-test-service.dns-8084.svc.cluster.local wheezy_tcp@dns-test-service.dns-8084.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local jessie_udp@dns-test-service.dns-8084.svc.cluster.local jessie_tcp@dns-test-service.dns-8084.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8084.svc.cluster.local]

    Feb 27 11:22:08.241: INFO: DNS probes using dns-8084/dns-test-8f668b99-6091-4932-a9a9-c57dfb7ffd1c succeeded

    STEP: deleting the pod 02/27/23 11:22:08.241
    STEP: deleting the test service 02/27/23 11:22:08.266
    STEP: deleting the test headless service 02/27/23 11:22:08.313
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 27 11:22:08.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8084" for this suite. 02/27/23 11:22:08.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:22:08.387
Feb 27 11:22:08.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename runtimeclass 02/27/23 11:22:08.389
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:08.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:08.437
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 27 11:22:08.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4280" for this suite. 02/27/23 11:22:08.493
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":16,"skipped":300,"failed":0}
------------------------------
• [0.130 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:22:08.387
    Feb 27 11:22:08.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 11:22:08.389
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:08.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:08.437
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 27 11:22:08.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4280" for this suite. 02/27/23 11:22:08.493
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:22:08.52
Feb 27 11:22:08.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename crd-webhook 02/27/23 11:22:08.521
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:08.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:08.576
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/27/23 11:22:08.592
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/27/23 11:22:09.335
STEP: Deploying the custom resource conversion webhook pod 02/27/23 11:22:09.348
STEP: Wait for the deployment to be ready 02/27/23 11:22:09.384
Feb 27 11:22:09.410: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 27 11:22:11.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 11:22:13.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 11:22:15.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/27/23 11:22:17.565
STEP: Verifying the service has paired with the endpoint 02/27/23 11:22:17.583
Feb 27 11:22:18.584: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Feb 27 11:22:18.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Creating a v1 custom resource 02/27/23 11:22:21.643
STEP: v2 custom resource should be converted 02/27/23 11:22:21.659
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:22:22.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8948" for this suite. 02/27/23 11:22:22.217
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":17,"skipped":303,"failed":0}
------------------------------
• [SLOW TEST] [13.810 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:22:08.52
    Feb 27 11:22:08.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename crd-webhook 02/27/23 11:22:08.521
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:08.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:08.576
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/27/23 11:22:08.592
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/27/23 11:22:09.335
    STEP: Deploying the custom resource conversion webhook pod 02/27/23 11:22:09.348
    STEP: Wait for the deployment to be ready 02/27/23 11:22:09.384
    Feb 27 11:22:09.410: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Feb 27 11:22:11.557: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 11:22:13.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 11:22:15.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 22, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/27/23 11:22:17.565
    STEP: Verifying the service has paired with the endpoint 02/27/23 11:22:17.583
    Feb 27 11:22:18.584: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Feb 27 11:22:18.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Creating a v1 custom resource 02/27/23 11:22:21.643
    STEP: v2 custom resource should be converted 02/27/23 11:22:21.659
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:22:22.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-8948" for this suite. 02/27/23 11:22:22.217
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:22:22.334
Feb 27 11:22:22.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename resourcequota 02/27/23 11:22:22.336
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:22.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:22.384
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 02/27/23 11:22:22.406
STEP: Creating a ResourceQuota 02/27/23 11:22:27.416
STEP: Ensuring resource quota status is calculated 02/27/23 11:22:27.431
STEP: Creating a Service 02/27/23 11:22:29.446
STEP: Creating a NodePort Service 02/27/23 11:22:29.487
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/27/23 11:22:29.536
STEP: Ensuring resource quota status captures service creation 02/27/23 11:22:29.586
STEP: Deleting Services 02/27/23 11:22:31.603
STEP: Ensuring resource quota status released usage 02/27/23 11:22:31.742
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 27 11:22:33.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9788" for this suite. 02/27/23 11:22:33.764
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":18,"skipped":309,"failed":0}
------------------------------
• [SLOW TEST] [11.448 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:22:22.334
    Feb 27 11:22:22.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename resourcequota 02/27/23 11:22:22.336
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:22.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:22.384
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 02/27/23 11:22:22.406
    STEP: Creating a ResourceQuota 02/27/23 11:22:27.416
    STEP: Ensuring resource quota status is calculated 02/27/23 11:22:27.431
    STEP: Creating a Service 02/27/23 11:22:29.446
    STEP: Creating a NodePort Service 02/27/23 11:22:29.487
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/27/23 11:22:29.536
    STEP: Ensuring resource quota status captures service creation 02/27/23 11:22:29.586
    STEP: Deleting Services 02/27/23 11:22:31.603
    STEP: Ensuring resource quota status released usage 02/27/23 11:22:31.742
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 27 11:22:33.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9788" for this suite. 02/27/23 11:22:33.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:22:33.783
Feb 27 11:22:33.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:22:33.784
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:33.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:33.817
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-a24be0f8-9b93-474b-9819-169f3fa46fba 02/27/23 11:22:33.828
STEP: Creating a pod to test consume secrets 02/27/23 11:22:33.841
Feb 27 11:22:33.858: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f" in namespace "projected-2293" to be "Succeeded or Failed"
Feb 27 11:22:33.869: INFO: Pod "pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.044204ms
Feb 27 11:22:35.886: INFO: Pod "pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028129045s
Feb 27 11:22:37.881: INFO: Pod "pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022919523s
STEP: Saw pod success 02/27/23 11:22:37.881
Feb 27 11:22:37.881: INFO: Pod "pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f" satisfied condition "Succeeded or Failed"
Feb 27 11:22:37.896: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 11:22:37.913
Feb 27 11:22:37.934: INFO: Waiting for pod pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f to disappear
Feb 27 11:22:37.953: INFO: Pod pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 27 11:22:37.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2293" for this suite. 02/27/23 11:22:37.967
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":19,"skipped":323,"failed":0}
------------------------------
• [4.197 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:22:33.783
    Feb 27 11:22:33.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:22:33.784
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:33.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:33.817
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-a24be0f8-9b93-474b-9819-169f3fa46fba 02/27/23 11:22:33.828
    STEP: Creating a pod to test consume secrets 02/27/23 11:22:33.841
    Feb 27 11:22:33.858: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f" in namespace "projected-2293" to be "Succeeded or Failed"
    Feb 27 11:22:33.869: INFO: Pod "pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.044204ms
    Feb 27 11:22:35.886: INFO: Pod "pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028129045s
    Feb 27 11:22:37.881: INFO: Pod "pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022919523s
    STEP: Saw pod success 02/27/23 11:22:37.881
    Feb 27 11:22:37.881: INFO: Pod "pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f" satisfied condition "Succeeded or Failed"
    Feb 27 11:22:37.896: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 11:22:37.913
    Feb 27 11:22:37.934: INFO: Waiting for pod pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f to disappear
    Feb 27 11:22:37.953: INFO: Pod pod-projected-secrets-7b2cb7a7-78c0-4e6e-ab2f-81e0178c575f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 27 11:22:37.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2293" for this suite. 02/27/23 11:22:37.967
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:22:37.99
Feb 27 11:22:37.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 11:22:37.991
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:38.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:38.052
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 02/27/23 11:22:38.064
Feb 27 11:22:38.086: INFO: Waiting up to 5m0s for pod "downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f" in namespace "downward-api-3107" to be "Succeeded or Failed"
Feb 27 11:22:38.105: INFO: Pod "downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.452965ms
Feb 27 11:22:40.114: INFO: Pod "downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027418765s
Feb 27 11:22:42.119: INFO: Pod "downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032901726s
STEP: Saw pod success 02/27/23 11:22:42.119
Feb 27 11:22:42.120: INFO: Pod "downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f" satisfied condition "Succeeded or Failed"
Feb 27 11:22:42.131: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f container dapi-container: <nil>
STEP: delete the pod 02/27/23 11:22:42.152
Feb 27 11:22:42.175: INFO: Waiting for pod downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f to disappear
Feb 27 11:22:42.184: INFO: Pod downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 27 11:22:42.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3107" for this suite. 02/27/23 11:22:42.198
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":20,"skipped":323,"failed":0}
------------------------------
• [4.234 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:22:37.99
    Feb 27 11:22:37.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 11:22:37.991
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:38.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:38.052
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 02/27/23 11:22:38.064
    Feb 27 11:22:38.086: INFO: Waiting up to 5m0s for pod "downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f" in namespace "downward-api-3107" to be "Succeeded or Failed"
    Feb 27 11:22:38.105: INFO: Pod "downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.452965ms
    Feb 27 11:22:40.114: INFO: Pod "downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027418765s
    Feb 27 11:22:42.119: INFO: Pod "downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032901726s
    STEP: Saw pod success 02/27/23 11:22:42.119
    Feb 27 11:22:42.120: INFO: Pod "downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f" satisfied condition "Succeeded or Failed"
    Feb 27 11:22:42.131: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f container dapi-container: <nil>
    STEP: delete the pod 02/27/23 11:22:42.152
    Feb 27 11:22:42.175: INFO: Waiting for pod downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f to disappear
    Feb 27 11:22:42.184: INFO: Pod downward-api-5d18b1d8-d1ad-4743-aecb-0d85fd063a9f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 27 11:22:42.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3107" for this suite. 02/27/23 11:22:42.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:22:42.233
Feb 27 11:22:42.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename job 02/27/23 11:22:42.234
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:42.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:42.274
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 02/27/23 11:22:42.291
STEP: Ensuring active pods == parallelism 02/27/23 11:22:42.307
STEP: Orphaning one of the Job's Pods 02/27/23 11:22:46.317
Feb 27 11:22:46.867: INFO: Successfully updated pod "adopt-release-jjr26"
STEP: Checking that the Job readopts the Pod 02/27/23 11:22:46.867
Feb 27 11:22:46.867: INFO: Waiting up to 15m0s for pod "adopt-release-jjr26" in namespace "job-1580" to be "adopted"
Feb 27 11:22:46.893: INFO: Pod "adopt-release-jjr26": Phase="Running", Reason="", readiness=true. Elapsed: 26.317738ms
Feb 27 11:22:48.904: INFO: Pod "adopt-release-jjr26": Phase="Running", Reason="", readiness=true. Elapsed: 2.036758281s
Feb 27 11:22:48.904: INFO: Pod "adopt-release-jjr26" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 02/27/23 11:22:48.904
Feb 27 11:22:49.429: INFO: Successfully updated pod "adopt-release-jjr26"
STEP: Checking that the Job releases the Pod 02/27/23 11:22:49.429
Feb 27 11:22:49.430: INFO: Waiting up to 15m0s for pod "adopt-release-jjr26" in namespace "job-1580" to be "released"
Feb 27 11:22:49.439: INFO: Pod "adopt-release-jjr26": Phase="Running", Reason="", readiness=true. Elapsed: 9.103548ms
Feb 27 11:22:51.452: INFO: Pod "adopt-release-jjr26": Phase="Running", Reason="", readiness=true. Elapsed: 2.021824389s
Feb 27 11:22:51.452: INFO: Pod "adopt-release-jjr26" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 27 11:22:51.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1580" for this suite. 02/27/23 11:22:51.464
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":21,"skipped":364,"failed":0}
------------------------------
• [SLOW TEST] [9.246 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:22:42.233
    Feb 27 11:22:42.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename job 02/27/23 11:22:42.234
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:42.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:42.274
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 02/27/23 11:22:42.291
    STEP: Ensuring active pods == parallelism 02/27/23 11:22:42.307
    STEP: Orphaning one of the Job's Pods 02/27/23 11:22:46.317
    Feb 27 11:22:46.867: INFO: Successfully updated pod "adopt-release-jjr26"
    STEP: Checking that the Job readopts the Pod 02/27/23 11:22:46.867
    Feb 27 11:22:46.867: INFO: Waiting up to 15m0s for pod "adopt-release-jjr26" in namespace "job-1580" to be "adopted"
    Feb 27 11:22:46.893: INFO: Pod "adopt-release-jjr26": Phase="Running", Reason="", readiness=true. Elapsed: 26.317738ms
    Feb 27 11:22:48.904: INFO: Pod "adopt-release-jjr26": Phase="Running", Reason="", readiness=true. Elapsed: 2.036758281s
    Feb 27 11:22:48.904: INFO: Pod "adopt-release-jjr26" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 02/27/23 11:22:48.904
    Feb 27 11:22:49.429: INFO: Successfully updated pod "adopt-release-jjr26"
    STEP: Checking that the Job releases the Pod 02/27/23 11:22:49.429
    Feb 27 11:22:49.430: INFO: Waiting up to 15m0s for pod "adopt-release-jjr26" in namespace "job-1580" to be "released"
    Feb 27 11:22:49.439: INFO: Pod "adopt-release-jjr26": Phase="Running", Reason="", readiness=true. Elapsed: 9.103548ms
    Feb 27 11:22:51.452: INFO: Pod "adopt-release-jjr26": Phase="Running", Reason="", readiness=true. Elapsed: 2.021824389s
    Feb 27 11:22:51.452: INFO: Pod "adopt-release-jjr26" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 27 11:22:51.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1580" for this suite. 02/27/23 11:22:51.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:22:51.48
Feb 27 11:22:51.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 11:22:51.481
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:51.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:51.523
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 02/27/23 11:22:51.537
Feb 27 11:22:51.563: INFO: Waiting up to 5m0s for pod "downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f" in namespace "downward-api-4260" to be "Succeeded or Failed"
Feb 27 11:22:51.573: INFO: Pod "downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.993361ms
Feb 27 11:22:53.582: INFO: Pod "downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019871562s
Feb 27 11:22:55.585: INFO: Pod "downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022900724s
STEP: Saw pod success 02/27/23 11:22:55.586
Feb 27 11:22:55.586: INFO: Pod "downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f" satisfied condition "Succeeded or Failed"
Feb 27 11:22:55.595: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f container client-container: <nil>
STEP: delete the pod 02/27/23 11:22:55.617
Feb 27 11:22:55.657: INFO: Waiting for pod downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f to disappear
Feb 27 11:22:55.671: INFO: Pod downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 27 11:22:55.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4260" for this suite. 02/27/23 11:22:55.688
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":22,"skipped":371,"failed":0}
------------------------------
• [4.225 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:22:51.48
    Feb 27 11:22:51.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 11:22:51.481
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:51.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:51.523
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 02/27/23 11:22:51.537
    Feb 27 11:22:51.563: INFO: Waiting up to 5m0s for pod "downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f" in namespace "downward-api-4260" to be "Succeeded or Failed"
    Feb 27 11:22:51.573: INFO: Pod "downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.993361ms
    Feb 27 11:22:53.582: INFO: Pod "downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019871562s
    Feb 27 11:22:55.585: INFO: Pod "downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022900724s
    STEP: Saw pod success 02/27/23 11:22:55.586
    Feb 27 11:22:55.586: INFO: Pod "downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f" satisfied condition "Succeeded or Failed"
    Feb 27 11:22:55.595: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f container client-container: <nil>
    STEP: delete the pod 02/27/23 11:22:55.617
    Feb 27 11:22:55.657: INFO: Waiting for pod downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f to disappear
    Feb 27 11:22:55.671: INFO: Pod downwardapi-volume-64c2866a-f520-48f0-91d1-cc72b9b4478f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 27 11:22:55.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4260" for this suite. 02/27/23 11:22:55.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:22:55.705
Feb 27 11:22:55.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 11:22:55.707
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:55.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:55.752
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Feb 27 11:22:55.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:22:56.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8654" for this suite. 02/27/23 11:22:56.375
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":23,"skipped":378,"failed":0}
------------------------------
• [0.685 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:22:55.705
    Feb 27 11:22:55.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 11:22:55.707
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:55.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:55.752
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Feb 27 11:22:55.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:22:56.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8654" for this suite. 02/27/23 11:22:56.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:22:56.393
Feb 27 11:22:56.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 11:22:56.394
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:56.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:56.433
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 11:22:56.469
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:22:57.183
STEP: Deploying the webhook pod 02/27/23 11:22:57.203
STEP: Wait for the deployment to be ready 02/27/23 11:22:57.237
Feb 27 11:22:57.262: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 11:22:59.295
STEP: Verifying the service has paired with the endpoint 02/27/23 11:22:59.321
Feb 27 11:23:00.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 02/27/23 11:23:00.331
STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/27/23 11:23:00.388
STEP: Creating a configMap that should not be mutated 02/27/23 11:23:00.403
STEP: Patching a mutating webhook configuration's rules to include the create operation 02/27/23 11:23:00.437
STEP: Creating a configMap that should be mutated 02/27/23 11:23:00.461
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:23:00.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8041" for this suite. 02/27/23 11:23:00.55
STEP: Destroying namespace "webhook-8041-markers" for this suite. 02/27/23 11:23:00.576
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":24,"skipped":394,"failed":0}
------------------------------
• [4.298 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:22:56.393
    Feb 27 11:22:56.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 11:22:56.394
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:22:56.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:22:56.433
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 11:22:56.469
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:22:57.183
    STEP: Deploying the webhook pod 02/27/23 11:22:57.203
    STEP: Wait for the deployment to be ready 02/27/23 11:22:57.237
    Feb 27 11:22:57.262: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 11:22:59.295
    STEP: Verifying the service has paired with the endpoint 02/27/23 11:22:59.321
    Feb 27 11:23:00.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 02/27/23 11:23:00.331
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/27/23 11:23:00.388
    STEP: Creating a configMap that should not be mutated 02/27/23 11:23:00.403
    STEP: Patching a mutating webhook configuration's rules to include the create operation 02/27/23 11:23:00.437
    STEP: Creating a configMap that should be mutated 02/27/23 11:23:00.461
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:23:00.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8041" for this suite. 02/27/23 11:23:00.55
    STEP: Destroying namespace "webhook-8041-markers" for this suite. 02/27/23 11:23:00.576
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:00.713
Feb 27 11:23:00.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename csistoragecapacity 02/27/23 11:23:00.715
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:00.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:00.757
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 02/27/23 11:23:00.781
STEP: getting /apis/storage.k8s.io 02/27/23 11:23:00.805
STEP: getting /apis/storage.k8s.io/v1 02/27/23 11:23:00.828
STEP: creating 02/27/23 11:23:00.842
STEP: watching 02/27/23 11:23:00.931
Feb 27 11:23:00.932: INFO: starting watch
STEP: getting 02/27/23 11:23:00.955
STEP: listing in namespace 02/27/23 11:23:00.978
STEP: listing across namespaces 02/27/23 11:23:00.997
STEP: patching 02/27/23 11:23:01.008
STEP: updating 02/27/23 11:23:01.033
Feb 27 11:23:01.063: INFO: waiting for watch events with expected annotations in namespace
Feb 27 11:23:01.064: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 02/27/23 11:23:01.064
STEP: deleting a collection 02/27/23 11:23:01.112
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Feb 27 11:23:01.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-4916" for this suite. 02/27/23 11:23:01.179
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":25,"skipped":402,"failed":0}
------------------------------
• [0.491 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:00.713
    Feb 27 11:23:00.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename csistoragecapacity 02/27/23 11:23:00.715
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:00.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:00.757
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 02/27/23 11:23:00.781
    STEP: getting /apis/storage.k8s.io 02/27/23 11:23:00.805
    STEP: getting /apis/storage.k8s.io/v1 02/27/23 11:23:00.828
    STEP: creating 02/27/23 11:23:00.842
    STEP: watching 02/27/23 11:23:00.931
    Feb 27 11:23:00.932: INFO: starting watch
    STEP: getting 02/27/23 11:23:00.955
    STEP: listing in namespace 02/27/23 11:23:00.978
    STEP: listing across namespaces 02/27/23 11:23:00.997
    STEP: patching 02/27/23 11:23:01.008
    STEP: updating 02/27/23 11:23:01.033
    Feb 27 11:23:01.063: INFO: waiting for watch events with expected annotations in namespace
    Feb 27 11:23:01.064: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 02/27/23 11:23:01.064
    STEP: deleting a collection 02/27/23 11:23:01.112
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Feb 27 11:23:01.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-4916" for this suite. 02/27/23 11:23:01.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:01.209
Feb 27 11:23:01.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 11:23:01.21
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:01.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:01.248
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 11:23:01.286
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:23:02.073
STEP: Deploying the webhook pod 02/27/23 11:23:02.097
STEP: Wait for the deployment to be ready 02/27/23 11:23:02.125
Feb 27 11:23:02.158: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 11:23:04.199
STEP: Verifying the service has paired with the endpoint 02/27/23 11:23:04.226
Feb 27 11:23:05.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Feb 27 11:23:05.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3485-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 11:23:05.773
STEP: Creating a custom resource that should be mutated by the webhook 02/27/23 11:23:05.864
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:23:08.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9862" for this suite. 02/27/23 11:23:08.484
STEP: Destroying namespace "webhook-9862-markers" for this suite. 02/27/23 11:23:08.502
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":26,"skipped":418,"failed":0}
------------------------------
• [SLOW TEST] [7.414 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:01.209
    Feb 27 11:23:01.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 11:23:01.21
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:01.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:01.248
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 11:23:01.286
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:23:02.073
    STEP: Deploying the webhook pod 02/27/23 11:23:02.097
    STEP: Wait for the deployment to be ready 02/27/23 11:23:02.125
    Feb 27 11:23:02.158: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 11:23:04.199
    STEP: Verifying the service has paired with the endpoint 02/27/23 11:23:04.226
    Feb 27 11:23:05.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Feb 27 11:23:05.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3485-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 11:23:05.773
    STEP: Creating a custom resource that should be mutated by the webhook 02/27/23 11:23:05.864
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:23:08.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9862" for this suite. 02/27/23 11:23:08.484
    STEP: Destroying namespace "webhook-9862-markers" for this suite. 02/27/23 11:23:08.502
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:08.644
Feb 27 11:23:08.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 11:23:08.646
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:08.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:08.705
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/27/23 11:23:08.717
Feb 27 11:23:08.744: INFO: Waiting up to 5m0s for pod "pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b" in namespace "emptydir-346" to be "Succeeded or Failed"
Feb 27 11:23:08.755: INFO: Pod "pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.079084ms
Feb 27 11:23:10.764: INFO: Pod "pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019940047s
Feb 27 11:23:12.766: INFO: Pod "pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021979388s
STEP: Saw pod success 02/27/23 11:23:12.766
Feb 27 11:23:12.767: INFO: Pod "pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b" satisfied condition "Succeeded or Failed"
Feb 27 11:23:12.778: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b container test-container: <nil>
STEP: delete the pod 02/27/23 11:23:12.824
Feb 27 11:23:13.044: INFO: Waiting for pod pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b to disappear
Feb 27 11:23:13.054: INFO: Pod pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 11:23:13.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-346" for this suite. 02/27/23 11:23:13.065
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":27,"skipped":474,"failed":0}
------------------------------
• [4.435 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:08.644
    Feb 27 11:23:08.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 11:23:08.646
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:08.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:08.705
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/27/23 11:23:08.717
    Feb 27 11:23:08.744: INFO: Waiting up to 5m0s for pod "pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b" in namespace "emptydir-346" to be "Succeeded or Failed"
    Feb 27 11:23:08.755: INFO: Pod "pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.079084ms
    Feb 27 11:23:10.764: INFO: Pod "pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019940047s
    Feb 27 11:23:12.766: INFO: Pod "pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021979388s
    STEP: Saw pod success 02/27/23 11:23:12.766
    Feb 27 11:23:12.767: INFO: Pod "pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b" satisfied condition "Succeeded or Failed"
    Feb 27 11:23:12.778: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b container test-container: <nil>
    STEP: delete the pod 02/27/23 11:23:12.824
    Feb 27 11:23:13.044: INFO: Waiting for pod pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b to disappear
    Feb 27 11:23:13.054: INFO: Pod pod-ac994b2c-0d01-4abd-beff-f0baa3dcae0b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 11:23:13.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-346" for this suite. 02/27/23 11:23:13.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:13.084
Feb 27 11:23:13.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-runtime 02/27/23 11:23:13.085
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:13.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:13.125
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 02/27/23 11:23:13.138
STEP: wait for the container to reach Succeeded 02/27/23 11:23:13.159
STEP: get the container status 02/27/23 11:23:17.213
STEP: the container should be terminated 02/27/23 11:23:17.222
STEP: the termination message should be set 02/27/23 11:23:17.222
Feb 27 11:23:17.222: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 02/27/23 11:23:17.222
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 27 11:23:17.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1037" for this suite. 02/27/23 11:23:17.286
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":28,"skipped":510,"failed":0}
------------------------------
• [4.225 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:13.084
    Feb 27 11:23:13.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-runtime 02/27/23 11:23:13.085
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:13.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:13.125
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 02/27/23 11:23:13.138
    STEP: wait for the container to reach Succeeded 02/27/23 11:23:13.159
    STEP: get the container status 02/27/23 11:23:17.213
    STEP: the container should be terminated 02/27/23 11:23:17.222
    STEP: the termination message should be set 02/27/23 11:23:17.222
    Feb 27 11:23:17.222: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 02/27/23 11:23:17.222
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 27 11:23:17.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1037" for this suite. 02/27/23 11:23:17.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:17.314
Feb 27 11:23:17.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename controllerrevisions 02/27/23 11:23:17.315
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:17.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:17.361
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-ghvtn-daemon-set" 02/27/23 11:23:17.448
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 11:23:17.466
Feb 27 11:23:17.498: INFO: Number of nodes with available pods controlled by daemonset e2e-ghvtn-daemon-set: 0
Feb 27 11:23:17.498: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:23:18.521: INFO: Number of nodes with available pods controlled by daemonset e2e-ghvtn-daemon-set: 2
Feb 27 11:23:18.521: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:23:19.525: INFO: Number of nodes with available pods controlled by daemonset e2e-ghvtn-daemon-set: 3
Feb 27 11:23:19.525: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-ghvtn-daemon-set
STEP: Confirm DaemonSet "e2e-ghvtn-daemon-set" successfully created with "daemonset-name=e2e-ghvtn-daemon-set" label 02/27/23 11:23:19.532
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-ghvtn-daemon-set" 02/27/23 11:23:19.551
Feb 27 11:23:19.562: INFO: Located ControllerRevision: "e2e-ghvtn-daemon-set-fd467c5bb"
STEP: Patching ControllerRevision "e2e-ghvtn-daemon-set-fd467c5bb" 02/27/23 11:23:19.569
Feb 27 11:23:19.588: INFO: e2e-ghvtn-daemon-set-fd467c5bb has been patched
STEP: Create a new ControllerRevision 02/27/23 11:23:19.588
Feb 27 11:23:19.615: INFO: Created ControllerRevision: e2e-ghvtn-daemon-set-59554cd648
STEP: Confirm that there are two ControllerRevisions 02/27/23 11:23:19.615
Feb 27 11:23:19.615: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 11:23:19.623: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-ghvtn-daemon-set-fd467c5bb" 02/27/23 11:23:19.623
STEP: Confirm that there is only one ControllerRevision 02/27/23 11:23:19.658
Feb 27 11:23:19.658: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 11:23:19.672: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-ghvtn-daemon-set-59554cd648" 02/27/23 11:23:19.681
Feb 27 11:23:19.705: INFO: e2e-ghvtn-daemon-set-59554cd648 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 02/27/23 11:23:19.705
W0227 11:23:19.720057      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 02/27/23 11:23:19.72
Feb 27 11:23:19.720: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 11:23:20.732: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 11:23:20.742: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-ghvtn-daemon-set-59554cd648=updated" 02/27/23 11:23:20.742
STEP: Confirm that there is only one ControllerRevision 02/27/23 11:23:20.765
Feb 27 11:23:20.766: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 11:23:20.786: INFO: Found 1 ControllerRevisions
Feb 27 11:23:20.801: INFO: ControllerRevision "e2e-ghvtn-daemon-set-7b946f4b97" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-ghvtn-daemon-set" 02/27/23 11:23:20.813
STEP: deleting DaemonSet.extensions e2e-ghvtn-daemon-set in namespace controllerrevisions-4219, will wait for the garbage collector to delete the pods 02/27/23 11:23:20.814
Feb 27 11:23:20.915: INFO: Deleting DaemonSet.extensions e2e-ghvtn-daemon-set took: 32.558117ms
Feb 27 11:23:21.017: INFO: Terminating DaemonSet.extensions e2e-ghvtn-daemon-set pods took: 101.854601ms
Feb 27 11:23:22.636: INFO: Number of nodes with available pods controlled by daemonset e2e-ghvtn-daemon-set: 0
Feb 27 11:23:22.636: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-ghvtn-daemon-set
Feb 27 11:23:22.644: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"59293"},"items":null}

Feb 27 11:23:22.653: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"59293"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Feb 27 11:23:22.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-4219" for this suite. 02/27/23 11:23:22.727
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":29,"skipped":516,"failed":0}
------------------------------
• [SLOW TEST] [5.431 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:17.314
    Feb 27 11:23:17.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename controllerrevisions 02/27/23 11:23:17.315
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:17.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:17.361
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-ghvtn-daemon-set" 02/27/23 11:23:17.448
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 11:23:17.466
    Feb 27 11:23:17.498: INFO: Number of nodes with available pods controlled by daemonset e2e-ghvtn-daemon-set: 0
    Feb 27 11:23:17.498: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:23:18.521: INFO: Number of nodes with available pods controlled by daemonset e2e-ghvtn-daemon-set: 2
    Feb 27 11:23:18.521: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:23:19.525: INFO: Number of nodes with available pods controlled by daemonset e2e-ghvtn-daemon-set: 3
    Feb 27 11:23:19.525: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-ghvtn-daemon-set
    STEP: Confirm DaemonSet "e2e-ghvtn-daemon-set" successfully created with "daemonset-name=e2e-ghvtn-daemon-set" label 02/27/23 11:23:19.532
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-ghvtn-daemon-set" 02/27/23 11:23:19.551
    Feb 27 11:23:19.562: INFO: Located ControllerRevision: "e2e-ghvtn-daemon-set-fd467c5bb"
    STEP: Patching ControllerRevision "e2e-ghvtn-daemon-set-fd467c5bb" 02/27/23 11:23:19.569
    Feb 27 11:23:19.588: INFO: e2e-ghvtn-daemon-set-fd467c5bb has been patched
    STEP: Create a new ControllerRevision 02/27/23 11:23:19.588
    Feb 27 11:23:19.615: INFO: Created ControllerRevision: e2e-ghvtn-daemon-set-59554cd648
    STEP: Confirm that there are two ControllerRevisions 02/27/23 11:23:19.615
    Feb 27 11:23:19.615: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 11:23:19.623: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-ghvtn-daemon-set-fd467c5bb" 02/27/23 11:23:19.623
    STEP: Confirm that there is only one ControllerRevision 02/27/23 11:23:19.658
    Feb 27 11:23:19.658: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 11:23:19.672: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-ghvtn-daemon-set-59554cd648" 02/27/23 11:23:19.681
    Feb 27 11:23:19.705: INFO: e2e-ghvtn-daemon-set-59554cd648 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 02/27/23 11:23:19.705
    W0227 11:23:19.720057      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 02/27/23 11:23:19.72
    Feb 27 11:23:19.720: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 11:23:20.732: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 11:23:20.742: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-ghvtn-daemon-set-59554cd648=updated" 02/27/23 11:23:20.742
    STEP: Confirm that there is only one ControllerRevision 02/27/23 11:23:20.765
    Feb 27 11:23:20.766: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 11:23:20.786: INFO: Found 1 ControllerRevisions
    Feb 27 11:23:20.801: INFO: ControllerRevision "e2e-ghvtn-daemon-set-7b946f4b97" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-ghvtn-daemon-set" 02/27/23 11:23:20.813
    STEP: deleting DaemonSet.extensions e2e-ghvtn-daemon-set in namespace controllerrevisions-4219, will wait for the garbage collector to delete the pods 02/27/23 11:23:20.814
    Feb 27 11:23:20.915: INFO: Deleting DaemonSet.extensions e2e-ghvtn-daemon-set took: 32.558117ms
    Feb 27 11:23:21.017: INFO: Terminating DaemonSet.extensions e2e-ghvtn-daemon-set pods took: 101.854601ms
    Feb 27 11:23:22.636: INFO: Number of nodes with available pods controlled by daemonset e2e-ghvtn-daemon-set: 0
    Feb 27 11:23:22.636: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-ghvtn-daemon-set
    Feb 27 11:23:22.644: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"59293"},"items":null}

    Feb 27 11:23:22.653: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"59293"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 11:23:22.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-4219" for this suite. 02/27/23 11:23:22.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:22.748
Feb 27 11:23:22.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename namespaces 02/27/23 11:23:22.749
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:22.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:22.843
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 02/27/23 11:23:22.874
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:22.91
STEP: Creating a service in the namespace 02/27/23 11:23:22.943
STEP: Deleting the namespace 02/27/23 11:23:22.999
STEP: Waiting for the namespace to be removed. 02/27/23 11:23:23.024
STEP: Recreating the namespace 02/27/23 11:23:30.032
STEP: Verifying there is no service in the namespace 02/27/23 11:23:30.064
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 27 11:23:30.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6959" for this suite. 02/27/23 11:23:30.09
STEP: Destroying namespace "nsdeletetest-882" for this suite. 02/27/23 11:23:30.103
Feb 27 11:23:30.112: INFO: Namespace nsdeletetest-882 was already deleted
STEP: Destroying namespace "nsdeletetest-8433" for this suite. 02/27/23 11:23:30.112
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":30,"skipped":528,"failed":0}
------------------------------
• [SLOW TEST] [7.383 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:22.748
    Feb 27 11:23:22.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename namespaces 02/27/23 11:23:22.749
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:22.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:22.843
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 02/27/23 11:23:22.874
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:22.91
    STEP: Creating a service in the namespace 02/27/23 11:23:22.943
    STEP: Deleting the namespace 02/27/23 11:23:22.999
    STEP: Waiting for the namespace to be removed. 02/27/23 11:23:23.024
    STEP: Recreating the namespace 02/27/23 11:23:30.032
    STEP: Verifying there is no service in the namespace 02/27/23 11:23:30.064
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 11:23:30.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6959" for this suite. 02/27/23 11:23:30.09
    STEP: Destroying namespace "nsdeletetest-882" for this suite. 02/27/23 11:23:30.103
    Feb 27 11:23:30.112: INFO: Namespace nsdeletetest-882 was already deleted
    STEP: Destroying namespace "nsdeletetest-8433" for this suite. 02/27/23 11:23:30.112
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:30.134
Feb 27 11:23:30.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 11:23:30.136
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:30.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:30.18
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 02/27/23 11:23:30.191
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/27/23 11:23:30.202
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/27/23 11:23:30.202
STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/27/23 11:23:30.202
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/27/23 11:23:30.211
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/27/23 11:23:30.211
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/27/23 11:23:30.216
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:23:30.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8542" for this suite. 02/27/23 11:23:30.233
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":31,"skipped":531,"failed":0}
------------------------------
• [0.116 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:30.134
    Feb 27 11:23:30.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 11:23:30.136
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:30.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:30.18
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 02/27/23 11:23:30.191
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/27/23 11:23:30.202
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/27/23 11:23:30.202
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/27/23 11:23:30.202
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/27/23 11:23:30.211
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/27/23 11:23:30.211
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/27/23 11:23:30.216
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:23:30.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8542" for this suite. 02/27/23 11:23:30.233
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:30.25
Feb 27 11:23:30.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename deployment 02/27/23 11:23:30.252
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:30.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:30.313
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Feb 27 11:23:30.326: INFO: Creating deployment "test-recreate-deployment"
Feb 27 11:23:30.341: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 27 11:23:30.367: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 27 11:23:32.394: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 27 11:23:32.411: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 27 11:23:32.443: INFO: Updating deployment test-recreate-deployment
Feb 27 11:23:32.443: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 11:23:32.606: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-167  18447b07-9a0f-4496-98f3-49024a2f6b2a 59472 2 2023-02-27 11:23:30 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048b9a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 11:23:32 +0000 UTC,LastTransitionTime:2023-02-27 11:23:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-02-27 11:23:32 +0000 UTC,LastTransitionTime:2023-02-27 11:23:30 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 27 11:23:32.619: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-167  defdac95-3940-4668-86a9-0177ce038794 59469 1 2023-02-27 11:23:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 18447b07-9a0f-4496-98f3-49024a2f6b2a 0xc0048b9f70 0xc0048b9f71}] [] [{kube-controller-manager Update apps/v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18447b07-9a0f-4496-98f3-49024a2f6b2a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a7c008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 11:23:32.619: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 27 11:23:32.619: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-167  1f53f345-6caa-491c-b4ec-376be10ffa29 59461 2 2023-02-27 11:23:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 18447b07-9a0f-4496-98f3-49024a2f6b2a 0xc0048b9e57 0xc0048b9e58}] [] [{kube-controller-manager Update apps/v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18447b07-9a0f-4496-98f3-49024a2f6b2a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048b9f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 11:23:32.628: INFO: Pod "test-recreate-deployment-9d58999df-cv4dq" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-cv4dq test-recreate-deployment-9d58999df- deployment-167  61e2a4d5-d844-4728-8baf-88a84488c690 59473 0 2023-02-27 11:23:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df defdac95-3940-4668-86a9-0177ce038794 0xc004a7c460 0xc004a7c461}] [] [{kube-controller-manager Update v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"defdac95-3940-4668-86a9-0177ce038794\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lrjc8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lrjc8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:23:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:23:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:23:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 11:23:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 27 11:23:32.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-167" for this suite. 02/27/23 11:23:32.64
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":32,"skipped":531,"failed":0}
------------------------------
• [2.407 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:30.25
    Feb 27 11:23:30.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename deployment 02/27/23 11:23:30.252
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:30.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:30.313
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Feb 27 11:23:30.326: INFO: Creating deployment "test-recreate-deployment"
    Feb 27 11:23:30.341: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Feb 27 11:23:30.367: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Feb 27 11:23:32.394: INFO: Waiting deployment "test-recreate-deployment" to complete
    Feb 27 11:23:32.411: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Feb 27 11:23:32.443: INFO: Updating deployment test-recreate-deployment
    Feb 27 11:23:32.443: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 11:23:32.606: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-167  18447b07-9a0f-4496-98f3-49024a2f6b2a 59472 2 2023-02-27 11:23:30 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048b9a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 11:23:32 +0000 UTC,LastTransitionTime:2023-02-27 11:23:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-02-27 11:23:32 +0000 UTC,LastTransitionTime:2023-02-27 11:23:30 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Feb 27 11:23:32.619: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-167  defdac95-3940-4668-86a9-0177ce038794 59469 1 2023-02-27 11:23:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 18447b07-9a0f-4496-98f3-49024a2f6b2a 0xc0048b9f70 0xc0048b9f71}] [] [{kube-controller-manager Update apps/v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18447b07-9a0f-4496-98f3-49024a2f6b2a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a7c008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 11:23:32.619: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Feb 27 11:23:32.619: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-167  1f53f345-6caa-491c-b4ec-376be10ffa29 59461 2 2023-02-27 11:23:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 18447b07-9a0f-4496-98f3-49024a2f6b2a 0xc0048b9e57 0xc0048b9e58}] [] [{kube-controller-manager Update apps/v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18447b07-9a0f-4496-98f3-49024a2f6b2a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048b9f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 11:23:32.628: INFO: Pod "test-recreate-deployment-9d58999df-cv4dq" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-cv4dq test-recreate-deployment-9d58999df- deployment-167  61e2a4d5-d844-4728-8baf-88a84488c690 59473 0 2023-02-27 11:23:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df defdac95-3940-4668-86a9-0177ce038794 0xc004a7c460 0xc004a7c461}] [] [{kube-controller-manager Update v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"defdac95-3940-4668-86a9-0177ce038794\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 11:23:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lrjc8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lrjc8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:23:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:23:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:23:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 11:23:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 27 11:23:32.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-167" for this suite. 02/27/23 11:23:32.64
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:32.661
Feb 27 11:23:32.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 11:23:32.662
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:32.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:32.7
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 11:23:32.738
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:23:33.244
STEP: Deploying the webhook pod 02/27/23 11:23:33.271
STEP: Wait for the deployment to be ready 02/27/23 11:23:33.301
Feb 27 11:23:33.325: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 11:23:35.359
STEP: Verifying the service has paired with the endpoint 02/27/23 11:23:35.383
Feb 27 11:23:36.383: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 02/27/23 11:23:36.397
STEP: create a pod 02/27/23 11:23:36.45
Feb 27 11:23:36.460: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1603" to be "running"
Feb 27 11:23:36.469: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.230373ms
Feb 27 11:23:38.482: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021523788s
Feb 27 11:23:38.482: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 02/27/23 11:23:38.482
Feb 27 11:23:38.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=webhook-1603 attach --namespace=webhook-1603 to-be-attached-pod -i -c=container1'
Feb 27 11:23:38.680: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:23:38.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1603" for this suite. 02/27/23 11:23:38.703
STEP: Destroying namespace "webhook-1603-markers" for this suite. 02/27/23 11:23:38.713
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":33,"skipped":531,"failed":0}
------------------------------
• [SLOW TEST] [6.179 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:32.661
    Feb 27 11:23:32.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 11:23:32.662
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:32.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:32.7
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 11:23:32.738
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:23:33.244
    STEP: Deploying the webhook pod 02/27/23 11:23:33.271
    STEP: Wait for the deployment to be ready 02/27/23 11:23:33.301
    Feb 27 11:23:33.325: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 11:23:35.359
    STEP: Verifying the service has paired with the endpoint 02/27/23 11:23:35.383
    Feb 27 11:23:36.383: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 02/27/23 11:23:36.397
    STEP: create a pod 02/27/23 11:23:36.45
    Feb 27 11:23:36.460: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1603" to be "running"
    Feb 27 11:23:36.469: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.230373ms
    Feb 27 11:23:38.482: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021523788s
    Feb 27 11:23:38.482: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 02/27/23 11:23:38.482
    Feb 27 11:23:38.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=webhook-1603 attach --namespace=webhook-1603 to-be-attached-pod -i -c=container1'
    Feb 27 11:23:38.680: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:23:38.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1603" for this suite. 02/27/23 11:23:38.703
    STEP: Destroying namespace "webhook-1603-markers" for this suite. 02/27/23 11:23:38.713
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:38.841
Feb 27 11:23:38.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pods 02/27/23 11:23:38.854
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:38.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:38.914
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 02/27/23 11:23:38.923
STEP: setting up watch 02/27/23 11:23:38.923
STEP: submitting the pod to kubernetes 02/27/23 11:23:39.031
STEP: verifying the pod is in kubernetes 02/27/23 11:23:39.051
STEP: verifying pod creation was observed 02/27/23 11:23:39.067
Feb 27 11:23:39.067: INFO: Waiting up to 5m0s for pod "pod-submit-remove-7d1793c8-6d35-49dc-af8b-97448b28ce8c" in namespace "pods-3809" to be "running"
Feb 27 11:23:39.075: INFO: Pod "pod-submit-remove-7d1793c8-6d35-49dc-af8b-97448b28ce8c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.401183ms
Feb 27 11:23:41.087: INFO: Pod "pod-submit-remove-7d1793c8-6d35-49dc-af8b-97448b28ce8c": Phase="Running", Reason="", readiness=true. Elapsed: 2.01946185s
Feb 27 11:23:41.087: INFO: Pod "pod-submit-remove-7d1793c8-6d35-49dc-af8b-97448b28ce8c" satisfied condition "running"
STEP: deleting the pod gracefully 02/27/23 11:23:41.099
STEP: verifying pod deletion was observed 02/27/23 11:23:41.115
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 27 11:23:43.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3809" for this suite. 02/27/23 11:23:43.632
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":34,"skipped":549,"failed":0}
------------------------------
• [4.805 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:38.841
    Feb 27 11:23:38.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pods 02/27/23 11:23:38.854
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:38.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:38.914
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 02/27/23 11:23:38.923
    STEP: setting up watch 02/27/23 11:23:38.923
    STEP: submitting the pod to kubernetes 02/27/23 11:23:39.031
    STEP: verifying the pod is in kubernetes 02/27/23 11:23:39.051
    STEP: verifying pod creation was observed 02/27/23 11:23:39.067
    Feb 27 11:23:39.067: INFO: Waiting up to 5m0s for pod "pod-submit-remove-7d1793c8-6d35-49dc-af8b-97448b28ce8c" in namespace "pods-3809" to be "running"
    Feb 27 11:23:39.075: INFO: Pod "pod-submit-remove-7d1793c8-6d35-49dc-af8b-97448b28ce8c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.401183ms
    Feb 27 11:23:41.087: INFO: Pod "pod-submit-remove-7d1793c8-6d35-49dc-af8b-97448b28ce8c": Phase="Running", Reason="", readiness=true. Elapsed: 2.01946185s
    Feb 27 11:23:41.087: INFO: Pod "pod-submit-remove-7d1793c8-6d35-49dc-af8b-97448b28ce8c" satisfied condition "running"
    STEP: deleting the pod gracefully 02/27/23 11:23:41.099
    STEP: verifying pod deletion was observed 02/27/23 11:23:41.115
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 27 11:23:43.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3809" for this suite. 02/27/23 11:23:43.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:43.657
Feb 27 11:23:43.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 11:23:43.658
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:43.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:43.708
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/27/23 11:23:43.74
Feb 27 11:23:43.761: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3535" to be "running and ready"
Feb 27 11:23:43.793: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 32.284328ms
Feb 27 11:23:43.793: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:23:45.804: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042629644s
Feb 27 11:23:45.804: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:23:47.802: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.040773338s
Feb 27 11:23:47.802: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 27 11:23:47.802: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 02/27/23 11:23:47.821
Feb 27 11:23:47.836: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-3535" to be "running and ready"
Feb 27 11:23:47.852: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 16.401763ms
Feb 27 11:23:47.853: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:23:49.862: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.02625424s
Feb 27 11:23:49.862: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Feb 27 11:23:49.862: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/27/23 11:23:49.87
STEP: delete the pod with lifecycle hook 02/27/23 11:23:49.888
Feb 27 11:23:49.903: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 27 11:23:49.914: INFO: Pod pod-with-poststart-http-hook still exists
Feb 27 11:23:51.915: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 27 11:23:51.927: INFO: Pod pod-with-poststart-http-hook still exists
Feb 27 11:23:53.915: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 27 11:23:53.924: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 27 11:23:53.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3535" for this suite. 02/27/23 11:23:53.945
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":35,"skipped":558,"failed":0}
------------------------------
• [SLOW TEST] [10.317 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:43.657
    Feb 27 11:23:43.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 11:23:43.658
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:43.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:43.708
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/27/23 11:23:43.74
    Feb 27 11:23:43.761: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3535" to be "running and ready"
    Feb 27 11:23:43.793: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 32.284328ms
    Feb 27 11:23:43.793: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:23:45.804: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042629644s
    Feb 27 11:23:45.804: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:23:47.802: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.040773338s
    Feb 27 11:23:47.802: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 27 11:23:47.802: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 02/27/23 11:23:47.821
    Feb 27 11:23:47.836: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-3535" to be "running and ready"
    Feb 27 11:23:47.852: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 16.401763ms
    Feb 27 11:23:47.853: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:23:49.862: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.02625424s
    Feb 27 11:23:49.862: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Feb 27 11:23:49.862: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/27/23 11:23:49.87
    STEP: delete the pod with lifecycle hook 02/27/23 11:23:49.888
    Feb 27 11:23:49.903: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 27 11:23:49.914: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 27 11:23:51.915: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 27 11:23:51.927: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 27 11:23:53.915: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 27 11:23:53.924: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 27 11:23:53.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-3535" for this suite. 02/27/23 11:23:53.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:53.995
Feb 27 11:23:53.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename replicaset 02/27/23 11:23:53.996
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:54.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:54.038
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 02/27/23 11:23:54.049
STEP: Verify that the required pods have come up 02/27/23 11:23:54.06
Feb 27 11:23:54.070: INFO: Pod name sample-pod: Found 0 pods out of 3
Feb 27 11:23:59.084: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 02/27/23 11:23:59.084
Feb 27 11:23:59.097: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 02/27/23 11:23:59.098
STEP: DeleteCollection of the ReplicaSets 02/27/23 11:23:59.11
STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/27/23 11:23:59.122
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 27 11:23:59.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9432" for this suite. 02/27/23 11:23:59.149
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":36,"skipped":620,"failed":0}
------------------------------
• [SLOW TEST] [5.174 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:53.995
    Feb 27 11:23:53.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename replicaset 02/27/23 11:23:53.996
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:54.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:54.038
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 02/27/23 11:23:54.049
    STEP: Verify that the required pods have come up 02/27/23 11:23:54.06
    Feb 27 11:23:54.070: INFO: Pod name sample-pod: Found 0 pods out of 3
    Feb 27 11:23:59.084: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 02/27/23 11:23:59.084
    Feb 27 11:23:59.097: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 02/27/23 11:23:59.098
    STEP: DeleteCollection of the ReplicaSets 02/27/23 11:23:59.11
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/27/23 11:23:59.122
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 27 11:23:59.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9432" for this suite. 02/27/23 11:23:59.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:23:59.18
Feb 27 11:23:59.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pods 02/27/23 11:23:59.181
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:59.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:59.275
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 02/27/23 11:23:59.292
Feb 27 11:23:59.308: INFO: Waiting up to 5m0s for pod "pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f" in namespace "pods-4680" to be "running and ready"
Feb 27 11:23:59.319: INFO: Pod "pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.890682ms
Feb 27 11:23:59.320: INFO: The phase of Pod pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:24:01.332: INFO: Pod "pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f": Phase="Running", Reason="", readiness=true. Elapsed: 2.023501376s
Feb 27 11:24:01.332: INFO: The phase of Pod pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f is Running (Ready = true)
Feb 27 11:24:01.332: INFO: Pod "pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f" satisfied condition "running and ready"
Feb 27 11:24:01.361: INFO: Pod pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f has hostIP: 172.31.7.167
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 27 11:24:01.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4680" for this suite. 02/27/23 11:24:01.379
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":37,"skipped":638,"failed":0}
------------------------------
• [2.216 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:23:59.18
    Feb 27 11:23:59.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pods 02/27/23 11:23:59.181
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:23:59.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:23:59.275
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 02/27/23 11:23:59.292
    Feb 27 11:23:59.308: INFO: Waiting up to 5m0s for pod "pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f" in namespace "pods-4680" to be "running and ready"
    Feb 27 11:23:59.319: INFO: Pod "pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.890682ms
    Feb 27 11:23:59.320: INFO: The phase of Pod pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:24:01.332: INFO: Pod "pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f": Phase="Running", Reason="", readiness=true. Elapsed: 2.023501376s
    Feb 27 11:24:01.332: INFO: The phase of Pod pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f is Running (Ready = true)
    Feb 27 11:24:01.332: INFO: Pod "pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f" satisfied condition "running and ready"
    Feb 27 11:24:01.361: INFO: Pod pod-hostip-5d5485a4-9c57-4a90-a105-d9c879df118f has hostIP: 172.31.7.167
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 27 11:24:01.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4680" for this suite. 02/27/23 11:24:01.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:24:01.398
Feb 27 11:24:01.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename dns 02/27/23 11:24:01.4
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:01.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:01.46
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4794.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4794.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 02/27/23 11:24:01.473
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4794.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4794.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 02/27/23 11:24:01.473
STEP: creating a pod to probe /etc/hosts 02/27/23 11:24:01.474
STEP: submitting the pod to kubernetes 02/27/23 11:24:01.474
Feb 27 11:24:01.506: INFO: Waiting up to 15m0s for pod "dns-test-9642265d-df06-4342-a85a-007197367787" in namespace "dns-4794" to be "running"
Feb 27 11:24:01.521: INFO: Pod "dns-test-9642265d-df06-4342-a85a-007197367787": Phase="Pending", Reason="", readiness=false. Elapsed: 15.066121ms
Feb 27 11:24:03.532: INFO: Pod "dns-test-9642265d-df06-4342-a85a-007197367787": Phase="Running", Reason="", readiness=true. Elapsed: 2.026231007s
Feb 27 11:24:03.532: INFO: Pod "dns-test-9642265d-df06-4342-a85a-007197367787" satisfied condition "running"
STEP: retrieving the pod 02/27/23 11:24:03.532
STEP: looking for the results for each expected name from probers 02/27/23 11:24:03.542
Feb 27 11:24:03.600: INFO: DNS probes using dns-4794/dns-test-9642265d-df06-4342-a85a-007197367787 succeeded

STEP: deleting the pod 02/27/23 11:24:03.601
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 27 11:24:03.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4794" for this suite. 02/27/23 11:24:03.639
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":38,"skipped":659,"failed":0}
------------------------------
• [2.253 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:24:01.398
    Feb 27 11:24:01.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename dns 02/27/23 11:24:01.4
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:01.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:01.46
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4794.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4794.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     02/27/23 11:24:01.473
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4794.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4794.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     02/27/23 11:24:01.473
    STEP: creating a pod to probe /etc/hosts 02/27/23 11:24:01.474
    STEP: submitting the pod to kubernetes 02/27/23 11:24:01.474
    Feb 27 11:24:01.506: INFO: Waiting up to 15m0s for pod "dns-test-9642265d-df06-4342-a85a-007197367787" in namespace "dns-4794" to be "running"
    Feb 27 11:24:01.521: INFO: Pod "dns-test-9642265d-df06-4342-a85a-007197367787": Phase="Pending", Reason="", readiness=false. Elapsed: 15.066121ms
    Feb 27 11:24:03.532: INFO: Pod "dns-test-9642265d-df06-4342-a85a-007197367787": Phase="Running", Reason="", readiness=true. Elapsed: 2.026231007s
    Feb 27 11:24:03.532: INFO: Pod "dns-test-9642265d-df06-4342-a85a-007197367787" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 11:24:03.532
    STEP: looking for the results for each expected name from probers 02/27/23 11:24:03.542
    Feb 27 11:24:03.600: INFO: DNS probes using dns-4794/dns-test-9642265d-df06-4342-a85a-007197367787 succeeded

    STEP: deleting the pod 02/27/23 11:24:03.601
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 27 11:24:03.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4794" for this suite. 02/27/23 11:24:03.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:24:03.658
Feb 27 11:24:03.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 11:24:03.659
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:03.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:03.695
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/27/23 11:24:03.708
Feb 27 11:24:03.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-9745 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Feb 27 11:24:03.815: INFO: stderr: ""
Feb 27 11:24:03.815: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 02/27/23 11:24:03.815
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Feb 27 11:24:03.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-9745 delete pods e2e-test-httpd-pod'
Feb 27 11:24:06.843: INFO: stderr: ""
Feb 27 11:24:06.843: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 11:24:06.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9745" for this suite. 02/27/23 11:24:06.856
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":39,"skipped":672,"failed":0}
------------------------------
• [3.219 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:24:03.658
    Feb 27 11:24:03.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 11:24:03.659
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:03.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:03.695
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/27/23 11:24:03.708
    Feb 27 11:24:03.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-9745 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Feb 27 11:24:03.815: INFO: stderr: ""
    Feb 27 11:24:03.815: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 02/27/23 11:24:03.815
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Feb 27 11:24:03.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-9745 delete pods e2e-test-httpd-pod'
    Feb 27 11:24:06.843: INFO: stderr: ""
    Feb 27 11:24:06.843: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 11:24:06.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9745" for this suite. 02/27/23 11:24:06.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:24:06.88
Feb 27 11:24:06.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 11:24:06.881
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:06.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:06.948
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 02/27/23 11:24:06.967
Feb 27 11:24:06.997: INFO: Waiting up to 5m0s for pod "pod-e40b9acd-c165-4aa3-8212-a3642c4c9628" in namespace "emptydir-3515" to be "Succeeded or Failed"
Feb 27 11:24:07.009: INFO: Pod "pod-e40b9acd-c165-4aa3-8212-a3642c4c9628": Phase="Pending", Reason="", readiness=false. Elapsed: 11.138395ms
Feb 27 11:24:09.025: INFO: Pod "pod-e40b9acd-c165-4aa3-8212-a3642c4c9628": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026941721s
Feb 27 11:24:11.018: INFO: Pod "pod-e40b9acd-c165-4aa3-8212-a3642c4c9628": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020616286s
STEP: Saw pod success 02/27/23 11:24:11.018
Feb 27 11:24:11.019: INFO: Pod "pod-e40b9acd-c165-4aa3-8212-a3642c4c9628" satisfied condition "Succeeded or Failed"
Feb 27 11:24:11.029: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-e40b9acd-c165-4aa3-8212-a3642c4c9628 container test-container: <nil>
STEP: delete the pod 02/27/23 11:24:11.051
Feb 27 11:24:11.070: INFO: Waiting for pod pod-e40b9acd-c165-4aa3-8212-a3642c4c9628 to disappear
Feb 27 11:24:11.244: INFO: Pod pod-e40b9acd-c165-4aa3-8212-a3642c4c9628 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 11:24:11.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3515" for this suite. 02/27/23 11:24:11.258
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":40,"skipped":691,"failed":0}
------------------------------
• [4.394 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:24:06.88
    Feb 27 11:24:06.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 11:24:06.881
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:06.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:06.948
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/27/23 11:24:06.967
    Feb 27 11:24:06.997: INFO: Waiting up to 5m0s for pod "pod-e40b9acd-c165-4aa3-8212-a3642c4c9628" in namespace "emptydir-3515" to be "Succeeded or Failed"
    Feb 27 11:24:07.009: INFO: Pod "pod-e40b9acd-c165-4aa3-8212-a3642c4c9628": Phase="Pending", Reason="", readiness=false. Elapsed: 11.138395ms
    Feb 27 11:24:09.025: INFO: Pod "pod-e40b9acd-c165-4aa3-8212-a3642c4c9628": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026941721s
    Feb 27 11:24:11.018: INFO: Pod "pod-e40b9acd-c165-4aa3-8212-a3642c4c9628": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020616286s
    STEP: Saw pod success 02/27/23 11:24:11.018
    Feb 27 11:24:11.019: INFO: Pod "pod-e40b9acd-c165-4aa3-8212-a3642c4c9628" satisfied condition "Succeeded or Failed"
    Feb 27 11:24:11.029: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-e40b9acd-c165-4aa3-8212-a3642c4c9628 container test-container: <nil>
    STEP: delete the pod 02/27/23 11:24:11.051
    Feb 27 11:24:11.070: INFO: Waiting for pod pod-e40b9acd-c165-4aa3-8212-a3642c4c9628 to disappear
    Feb 27 11:24:11.244: INFO: Pod pod-e40b9acd-c165-4aa3-8212-a3642c4c9628 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 11:24:11.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3515" for this suite. 02/27/23 11:24:11.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:24:11.281
Feb 27 11:24:11.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename job 02/27/23 11:24:11.282
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:11.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:11.322
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 02/27/23 11:24:11.334
STEP: Ensuring job reaches completions 02/27/23 11:24:11.345
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 27 11:24:23.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2261" for this suite. 02/27/23 11:24:23.385
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":41,"skipped":703,"failed":0}
------------------------------
• [SLOW TEST] [12.126 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:24:11.281
    Feb 27 11:24:11.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename job 02/27/23 11:24:11.282
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:11.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:11.322
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 02/27/23 11:24:11.334
    STEP: Ensuring job reaches completions 02/27/23 11:24:11.345
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 27 11:24:23.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2261" for this suite. 02/27/23 11:24:23.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:24:23.412
Feb 27 11:24:23.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 11:24:23.414
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:23.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:23.465
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 11:24:23.507
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:24:24.01
STEP: Deploying the webhook pod 02/27/23 11:24:24.027
STEP: Wait for the deployment to be ready 02/27/23 11:24:24.052
Feb 27 11:24:24.081: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 11:24:26.117
STEP: Verifying the service has paired with the endpoint 02/27/23 11:24:26.135
Feb 27 11:24:27.136: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 02/27/23 11:24:27.281
STEP: Creating a configMap that should be mutated 02/27/23 11:24:27.327
STEP: Deleting the collection of validation webhooks 02/27/23 11:24:27.422
STEP: Creating a configMap that should not be mutated 02/27/23 11:24:27.499
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:24:27.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-933" for this suite. 02/27/23 11:24:27.533
STEP: Destroying namespace "webhook-933-markers" for this suite. 02/27/23 11:24:27.55
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":42,"skipped":716,"failed":0}
------------------------------
• [4.232 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:24:23.412
    Feb 27 11:24:23.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 11:24:23.414
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:23.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:23.465
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 11:24:23.507
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:24:24.01
    STEP: Deploying the webhook pod 02/27/23 11:24:24.027
    STEP: Wait for the deployment to be ready 02/27/23 11:24:24.052
    Feb 27 11:24:24.081: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 11:24:26.117
    STEP: Verifying the service has paired with the endpoint 02/27/23 11:24:26.135
    Feb 27 11:24:27.136: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 02/27/23 11:24:27.281
    STEP: Creating a configMap that should be mutated 02/27/23 11:24:27.327
    STEP: Deleting the collection of validation webhooks 02/27/23 11:24:27.422
    STEP: Creating a configMap that should not be mutated 02/27/23 11:24:27.499
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:24:27.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-933" for this suite. 02/27/23 11:24:27.533
    STEP: Destroying namespace "webhook-933-markers" for this suite. 02/27/23 11:24:27.55
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:24:27.646
Feb 27 11:24:27.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-runtime 02/27/23 11:24:27.647
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:27.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:27.689
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 02/27/23 11:24:27.704
STEP: wait for the container to reach Failed 02/27/23 11:24:27.72
STEP: get the container status 02/27/23 11:24:31.036
STEP: the container should be terminated 02/27/23 11:24:31.054
STEP: the termination message should be set 02/27/23 11:24:31.055
Feb 27 11:24:31.055: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/27/23 11:24:31.055
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 27 11:24:31.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5639" for this suite. 02/27/23 11:24:31.109
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":43,"skipped":720,"failed":0}
------------------------------
• [3.478 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:24:27.646
    Feb 27 11:24:27.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-runtime 02/27/23 11:24:27.647
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:27.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:27.689
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 02/27/23 11:24:27.704
    STEP: wait for the container to reach Failed 02/27/23 11:24:27.72
    STEP: get the container status 02/27/23 11:24:31.036
    STEP: the container should be terminated 02/27/23 11:24:31.054
    STEP: the termination message should be set 02/27/23 11:24:31.055
    Feb 27 11:24:31.055: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/27/23 11:24:31.055
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 27 11:24:31.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5639" for this suite. 02/27/23 11:24:31.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:24:31.134
Feb 27 11:24:31.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename watch 02/27/23 11:24:31.136
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:31.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:31.188
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 02/27/23 11:24:31.202
STEP: starting a background goroutine to produce watch events 02/27/23 11:24:31.211
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/27/23 11:24:31.212
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 27 11:24:33.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-505" for this suite. 02/27/23 11:24:34.001
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":44,"skipped":765,"failed":0}
------------------------------
• [2.915 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:24:31.134
    Feb 27 11:24:31.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename watch 02/27/23 11:24:31.136
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:31.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:31.188
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 02/27/23 11:24:31.202
    STEP: starting a background goroutine to produce watch events 02/27/23 11:24:31.211
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/27/23 11:24:31.212
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 27 11:24:33.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-505" for this suite. 02/27/23 11:24:34.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:24:34.054
Feb 27 11:24:34.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename security-context 02/27/23 11:24:34.055
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:34.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:34.108
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/27/23 11:24:34.125
Feb 27 11:24:34.153: INFO: Waiting up to 5m0s for pod "security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af" in namespace "security-context-6080" to be "Succeeded or Failed"
Feb 27 11:24:34.167: INFO: Pod "security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af": Phase="Pending", Reason="", readiness=false. Elapsed: 14.050545ms
Feb 27 11:24:36.177: INFO: Pod "security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024455204s
Feb 27 11:24:38.179: INFO: Pod "security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025988238s
STEP: Saw pod success 02/27/23 11:24:38.179
Feb 27 11:24:38.179: INFO: Pod "security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af" satisfied condition "Succeeded or Failed"
Feb 27 11:24:38.191: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af container test-container: <nil>
STEP: delete the pod 02/27/23 11:24:38.206
Feb 27 11:24:38.229: INFO: Waiting for pod security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af to disappear
Feb 27 11:24:38.237: INFO: Pod security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 27 11:24:38.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6080" for this suite. 02/27/23 11:24:38.251
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":45,"skipped":779,"failed":0}
------------------------------
• [4.227 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:24:34.054
    Feb 27 11:24:34.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename security-context 02/27/23 11:24:34.055
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:34.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:34.108
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/27/23 11:24:34.125
    Feb 27 11:24:34.153: INFO: Waiting up to 5m0s for pod "security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af" in namespace "security-context-6080" to be "Succeeded or Failed"
    Feb 27 11:24:34.167: INFO: Pod "security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af": Phase="Pending", Reason="", readiness=false. Elapsed: 14.050545ms
    Feb 27 11:24:36.177: INFO: Pod "security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024455204s
    Feb 27 11:24:38.179: INFO: Pod "security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025988238s
    STEP: Saw pod success 02/27/23 11:24:38.179
    Feb 27 11:24:38.179: INFO: Pod "security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af" satisfied condition "Succeeded or Failed"
    Feb 27 11:24:38.191: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af container test-container: <nil>
    STEP: delete the pod 02/27/23 11:24:38.206
    Feb 27 11:24:38.229: INFO: Waiting for pod security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af to disappear
    Feb 27 11:24:38.237: INFO: Pod security-context-c80f5d68-4e31-4ebe-b1ba-c398fe9839af no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 27 11:24:38.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-6080" for this suite. 02/27/23 11:24:38.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:24:38.286
Feb 27 11:24:38.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename dns 02/27/23 11:24:38.288
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:38.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:38.331
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 02/27/23 11:24:38.349
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7618.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7618.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 02/27/23 11:24:38.36
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7618.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7618.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 02/27/23 11:24:38.36
STEP: creating a pod to probe DNS 02/27/23 11:24:38.361
STEP: submitting the pod to kubernetes 02/27/23 11:24:38.362
Feb 27 11:24:38.384: INFO: Waiting up to 15m0s for pod "dns-test-b462416d-bc8e-43d5-9b24-15b05c1d5bb2" in namespace "dns-7618" to be "running"
Feb 27 11:24:38.393: INFO: Pod "dns-test-b462416d-bc8e-43d5-9b24-15b05c1d5bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.059045ms
Feb 27 11:24:40.403: INFO: Pod "dns-test-b462416d-bc8e-43d5-9b24-15b05c1d5bb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.019462604s
Feb 27 11:24:40.403: INFO: Pod "dns-test-b462416d-bc8e-43d5-9b24-15b05c1d5bb2" satisfied condition "running"
STEP: retrieving the pod 02/27/23 11:24:40.403
STEP: looking for the results for each expected name from probers 02/27/23 11:24:40.415
Feb 27 11:24:40.469: INFO: DNS probes using dns-7618/dns-test-b462416d-bc8e-43d5-9b24-15b05c1d5bb2 succeeded

STEP: deleting the pod 02/27/23 11:24:40.47
STEP: deleting the test headless service 02/27/23 11:24:40.499
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 27 11:24:40.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7618" for this suite. 02/27/23 11:24:40.531
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":46,"skipped":794,"failed":0}
------------------------------
• [2.258 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:24:38.286
    Feb 27 11:24:38.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename dns 02/27/23 11:24:38.288
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:38.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:38.331
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 02/27/23 11:24:38.349
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7618.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7618.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     02/27/23 11:24:38.36
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7618.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7618.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     02/27/23 11:24:38.36
    STEP: creating a pod to probe DNS 02/27/23 11:24:38.361
    STEP: submitting the pod to kubernetes 02/27/23 11:24:38.362
    Feb 27 11:24:38.384: INFO: Waiting up to 15m0s for pod "dns-test-b462416d-bc8e-43d5-9b24-15b05c1d5bb2" in namespace "dns-7618" to be "running"
    Feb 27 11:24:38.393: INFO: Pod "dns-test-b462416d-bc8e-43d5-9b24-15b05c1d5bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.059045ms
    Feb 27 11:24:40.403: INFO: Pod "dns-test-b462416d-bc8e-43d5-9b24-15b05c1d5bb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.019462604s
    Feb 27 11:24:40.403: INFO: Pod "dns-test-b462416d-bc8e-43d5-9b24-15b05c1d5bb2" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 11:24:40.403
    STEP: looking for the results for each expected name from probers 02/27/23 11:24:40.415
    Feb 27 11:24:40.469: INFO: DNS probes using dns-7618/dns-test-b462416d-bc8e-43d5-9b24-15b05c1d5bb2 succeeded

    STEP: deleting the pod 02/27/23 11:24:40.47
    STEP: deleting the test headless service 02/27/23 11:24:40.499
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 27 11:24:40.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7618" for this suite. 02/27/23 11:24:40.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:24:40.545
Feb 27 11:24:40.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename resourcequota 02/27/23 11:24:40.547
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:40.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:40.594
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 02/27/23 11:24:57.614
STEP: Creating a ResourceQuota 02/27/23 11:25:02.635
STEP: Ensuring resource quota status is calculated 02/27/23 11:25:02.646
STEP: Creating a ConfigMap 02/27/23 11:25:04.659
STEP: Ensuring resource quota status captures configMap creation 02/27/23 11:25:04.684
STEP: Deleting a ConfigMap 02/27/23 11:25:06.697
STEP: Ensuring resource quota status released usage 02/27/23 11:25:06.709
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 27 11:25:08.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7868" for this suite. 02/27/23 11:25:08.735
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":47,"skipped":811,"failed":0}
------------------------------
• [SLOW TEST] [28.200 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:24:40.545
    Feb 27 11:24:40.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename resourcequota 02/27/23 11:24:40.547
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:24:40.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:24:40.594
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 02/27/23 11:24:57.614
    STEP: Creating a ResourceQuota 02/27/23 11:25:02.635
    STEP: Ensuring resource quota status is calculated 02/27/23 11:25:02.646
    STEP: Creating a ConfigMap 02/27/23 11:25:04.659
    STEP: Ensuring resource quota status captures configMap creation 02/27/23 11:25:04.684
    STEP: Deleting a ConfigMap 02/27/23 11:25:06.697
    STEP: Ensuring resource quota status released usage 02/27/23 11:25:06.709
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 27 11:25:08.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7868" for this suite. 02/27/23 11:25:08.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:25:08.751
Feb 27 11:25:08.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename events 02/27/23 11:25:08.752
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:08.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:08.811
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 02/27/23 11:25:08.838
STEP: listing all events in all namespaces 02/27/23 11:25:08.855
STEP: patching the test event 02/27/23 11:25:08.87
STEP: fetching the test event 02/27/23 11:25:08.884
STEP: updating the test event 02/27/23 11:25:08.892
STEP: getting the test event 02/27/23 11:25:08.91
STEP: deleting the test event 02/27/23 11:25:08.917
STEP: listing all events in all namespaces 02/27/23 11:25:08.934
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Feb 27 11:25:08.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8986" for this suite. 02/27/23 11:25:08.979
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":48,"skipped":829,"failed":0}
------------------------------
• [0.241 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:25:08.751
    Feb 27 11:25:08.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename events 02/27/23 11:25:08.752
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:08.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:08.811
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 02/27/23 11:25:08.838
    STEP: listing all events in all namespaces 02/27/23 11:25:08.855
    STEP: patching the test event 02/27/23 11:25:08.87
    STEP: fetching the test event 02/27/23 11:25:08.884
    STEP: updating the test event 02/27/23 11:25:08.892
    STEP: getting the test event 02/27/23 11:25:08.91
    STEP: deleting the test event 02/27/23 11:25:08.917
    STEP: listing all events in all namespaces 02/27/23 11:25:08.934
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Feb 27 11:25:08.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8986" for this suite. 02/27/23 11:25:08.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:25:09.012
Feb 27 11:25:09.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 11:25:09.013
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:09.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:09.069
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 11:25:09.124
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:25:09.995
STEP: Deploying the webhook pod 02/27/23 11:25:10.013
STEP: Wait for the deployment to be ready 02/27/23 11:25:10.035
Feb 27 11:25:10.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 11:25:12.092
STEP: Verifying the service has paired with the endpoint 02/27/23 11:25:12.122
Feb 27 11:25:13.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/27/23 11:25:13.143
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/27/23 11:25:13.182
STEP: Creating a dummy validating-webhook-configuration object 02/27/23 11:25:13.239
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/27/23 11:25:13.283
STEP: Creating a dummy mutating-webhook-configuration object 02/27/23 11:25:13.338
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/27/23 11:25:13.388
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:25:13.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7880" for this suite. 02/27/23 11:25:13.495
STEP: Destroying namespace "webhook-7880-markers" for this suite. 02/27/23 11:25:13.51
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":49,"skipped":905,"failed":0}
------------------------------
• [4.625 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:25:09.012
    Feb 27 11:25:09.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 11:25:09.013
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:09.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:09.069
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 11:25:09.124
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:25:09.995
    STEP: Deploying the webhook pod 02/27/23 11:25:10.013
    STEP: Wait for the deployment to be ready 02/27/23 11:25:10.035
    Feb 27 11:25:10.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 11:25:12.092
    STEP: Verifying the service has paired with the endpoint 02/27/23 11:25:12.122
    Feb 27 11:25:13.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/27/23 11:25:13.143
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/27/23 11:25:13.182
    STEP: Creating a dummy validating-webhook-configuration object 02/27/23 11:25:13.239
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/27/23 11:25:13.283
    STEP: Creating a dummy mutating-webhook-configuration object 02/27/23 11:25:13.338
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/27/23 11:25:13.388
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:25:13.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7880" for this suite. 02/27/23 11:25:13.495
    STEP: Destroying namespace "webhook-7880-markers" for this suite. 02/27/23 11:25:13.51
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:25:13.661
Feb 27 11:25:13.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 11:25:13.668
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:13.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:13.738
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/27/23 11:25:13.764
Feb 27 11:25:13.797: INFO: Waiting up to 5m0s for pod "pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6" in namespace "emptydir-2779" to be "Succeeded or Failed"
Feb 27 11:25:13.817: INFO: Pod "pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.258576ms
Feb 27 11:25:15.829: INFO: Pod "pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031526601s
Feb 27 11:25:17.830: INFO: Pod "pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03256372s
STEP: Saw pod success 02/27/23 11:25:17.83
Feb 27 11:25:17.830: INFO: Pod "pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6" satisfied condition "Succeeded or Failed"
Feb 27 11:25:17.865: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6 container test-container: <nil>
STEP: delete the pod 02/27/23 11:25:17.91
Feb 27 11:25:17.946: INFO: Waiting for pod pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6 to disappear
Feb 27 11:25:17.961: INFO: Pod pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 11:25:17.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2779" for this suite. 02/27/23 11:25:17.978
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":50,"skipped":934,"failed":0}
------------------------------
• [4.331 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:25:13.661
    Feb 27 11:25:13.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 11:25:13.668
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:13.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:13.738
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/27/23 11:25:13.764
    Feb 27 11:25:13.797: INFO: Waiting up to 5m0s for pod "pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6" in namespace "emptydir-2779" to be "Succeeded or Failed"
    Feb 27 11:25:13.817: INFO: Pod "pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.258576ms
    Feb 27 11:25:15.829: INFO: Pod "pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031526601s
    Feb 27 11:25:17.830: INFO: Pod "pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03256372s
    STEP: Saw pod success 02/27/23 11:25:17.83
    Feb 27 11:25:17.830: INFO: Pod "pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6" satisfied condition "Succeeded or Failed"
    Feb 27 11:25:17.865: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6 container test-container: <nil>
    STEP: delete the pod 02/27/23 11:25:17.91
    Feb 27 11:25:17.946: INFO: Waiting for pod pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6 to disappear
    Feb 27 11:25:17.961: INFO: Pod pod-e4b9f5f8-5f54-4df8-b422-7ed99d2e71f6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 11:25:17.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2779" for this suite. 02/27/23 11:25:17.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:25:17.996
Feb 27 11:25:17.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 11:25:17.997
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:18.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:18.062
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3384 02/27/23 11:25:18.082
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/27/23 11:25:18.129
STEP: creating service externalsvc in namespace services-3384 02/27/23 11:25:18.129
STEP: creating replication controller externalsvc in namespace services-3384 02/27/23 11:25:18.156
I0227 11:25:18.167299      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3384, replica count: 2
I0227 11:25:21.219217      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 02/27/23 11:25:21.227
Feb 27 11:25:21.293: INFO: Creating new exec pod
Feb 27 11:25:21.316: INFO: Waiting up to 5m0s for pod "execpodzbl2s" in namespace "services-3384" to be "running"
Feb 27 11:25:21.334: INFO: Pod "execpodzbl2s": Phase="Pending", Reason="", readiness=false. Elapsed: 16.655795ms
Feb 27 11:25:23.342: INFO: Pod "execpodzbl2s": Phase="Running", Reason="", readiness=true. Elapsed: 2.025016897s
Feb 27 11:25:23.342: INFO: Pod "execpodzbl2s" satisfied condition "running"
Feb 27 11:25:23.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-3384 exec execpodzbl2s -- /bin/sh -x -c nslookup nodeport-service.services-3384.svc.cluster.local'
Feb 27 11:25:24.013: INFO: stderr: "+ nslookup nodeport-service.services-3384.svc.cluster.local\n"
Feb 27 11:25:24.013: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-3384.svc.cluster.local\tcanonical name = externalsvc.services-3384.svc.cluster.local.\nName:\texternalsvc.services-3384.svc.cluster.local\nAddress: 10.240.26.66\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3384, will wait for the garbage collector to delete the pods 02/27/23 11:25:24.013
Feb 27 11:25:24.086: INFO: Deleting ReplicationController externalsvc took: 13.908836ms
Feb 27 11:25:24.188: INFO: Terminating ReplicationController externalsvc pods took: 101.987961ms
Feb 27 11:25:26.347: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 11:25:26.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3384" for this suite. 02/27/23 11:25:26.436
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":51,"skipped":985,"failed":0}
------------------------------
• [SLOW TEST] [8.458 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:25:17.996
    Feb 27 11:25:17.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 11:25:17.997
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:18.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:18.062
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3384 02/27/23 11:25:18.082
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/27/23 11:25:18.129
    STEP: creating service externalsvc in namespace services-3384 02/27/23 11:25:18.129
    STEP: creating replication controller externalsvc in namespace services-3384 02/27/23 11:25:18.156
    I0227 11:25:18.167299      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3384, replica count: 2
    I0227 11:25:21.219217      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 02/27/23 11:25:21.227
    Feb 27 11:25:21.293: INFO: Creating new exec pod
    Feb 27 11:25:21.316: INFO: Waiting up to 5m0s for pod "execpodzbl2s" in namespace "services-3384" to be "running"
    Feb 27 11:25:21.334: INFO: Pod "execpodzbl2s": Phase="Pending", Reason="", readiness=false. Elapsed: 16.655795ms
    Feb 27 11:25:23.342: INFO: Pod "execpodzbl2s": Phase="Running", Reason="", readiness=true. Elapsed: 2.025016897s
    Feb 27 11:25:23.342: INFO: Pod "execpodzbl2s" satisfied condition "running"
    Feb 27 11:25:23.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-3384 exec execpodzbl2s -- /bin/sh -x -c nslookup nodeport-service.services-3384.svc.cluster.local'
    Feb 27 11:25:24.013: INFO: stderr: "+ nslookup nodeport-service.services-3384.svc.cluster.local\n"
    Feb 27 11:25:24.013: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-3384.svc.cluster.local\tcanonical name = externalsvc.services-3384.svc.cluster.local.\nName:\texternalsvc.services-3384.svc.cluster.local\nAddress: 10.240.26.66\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3384, will wait for the garbage collector to delete the pods 02/27/23 11:25:24.013
    Feb 27 11:25:24.086: INFO: Deleting ReplicationController externalsvc took: 13.908836ms
    Feb 27 11:25:24.188: INFO: Terminating ReplicationController externalsvc pods took: 101.987961ms
    Feb 27 11:25:26.347: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 11:25:26.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3384" for this suite. 02/27/23 11:25:26.436
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:25:26.455
Feb 27 11:25:26.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 11:25:26.457
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:26.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:26.52
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-7529 02/27/23 11:25:26.536
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7529 to expose endpoints map[] 02/27/23 11:25:27.018
Feb 27 11:25:27.029: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Feb 27 11:25:28.048: INFO: successfully validated that service endpoint-test2 in namespace services-7529 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7529 02/27/23 11:25:28.049
Feb 27 11:25:28.063: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7529" to be "running and ready"
Feb 27 11:25:28.076: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.4515ms
Feb 27 11:25:28.076: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:25:30.086: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.022252195s
Feb 27 11:25:30.086: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 27 11:25:30.086: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7529 to expose endpoints map[pod1:[80]] 02/27/23 11:25:30.1
Feb 27 11:25:30.131: INFO: successfully validated that service endpoint-test2 in namespace services-7529 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 02/27/23 11:25:30.132
Feb 27 11:25:30.132: INFO: Creating new exec pod
Feb 27 11:25:30.148: INFO: Waiting up to 5m0s for pod "execpodzcbnm" in namespace "services-7529" to be "running"
Feb 27 11:25:30.155: INFO: Pod "execpodzcbnm": Phase="Pending", Reason="", readiness=false. Elapsed: 7.111079ms
Feb 27 11:25:32.164: INFO: Pod "execpodzcbnm": Phase="Running", Reason="", readiness=true. Elapsed: 2.015900498s
Feb 27 11:25:32.164: INFO: Pod "execpodzcbnm" satisfied condition "running"
Feb 27 11:25:33.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-7529 exec execpodzcbnm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 27 11:25:33.471: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 27 11:25:33.471: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:25:33.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-7529 exec execpodzcbnm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.18.123 80'
Feb 27 11:25:33.701: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.18.123 80\nConnection to 10.240.18.123 80 port [tcp/http] succeeded!\n"
Feb 27 11:25:33.701: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-7529 02/27/23 11:25:33.701
Feb 27 11:25:33.713: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7529" to be "running and ready"
Feb 27 11:25:33.722: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.120147ms
Feb 27 11:25:33.722: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:25:35.732: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.019277072s
Feb 27 11:25:35.732: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 27 11:25:35.732: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7529 to expose endpoints map[pod1:[80] pod2:[80]] 02/27/23 11:25:35.739
Feb 27 11:25:35.779: INFO: successfully validated that service endpoint-test2 in namespace services-7529 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 02/27/23 11:25:35.779
Feb 27 11:25:36.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-7529 exec execpodzcbnm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 27 11:25:37.093: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 27 11:25:37.093: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:25:37.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-7529 exec execpodzcbnm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.18.123 80'
Feb 27 11:25:37.319: INFO: stderr: "+ nc -v -t -w 2 10.240.18.123 80\nConnection to 10.240.18.123 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Feb 27 11:25:37.319: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7529 02/27/23 11:25:37.319
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7529 to expose endpoints map[pod2:[80]] 02/27/23 11:25:37.345
Feb 27 11:25:37.388: INFO: successfully validated that service endpoint-test2 in namespace services-7529 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 02/27/23 11:25:37.388
Feb 27 11:25:38.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-7529 exec execpodzcbnm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 27 11:25:38.623: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 27 11:25:38.623: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:25:38.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-7529 exec execpodzcbnm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.18.123 80'
Feb 27 11:25:38.880: INFO: stderr: "+ nc -v -t -w 2 10.240.18.123 80\n+ echo hostName\nConnection to 10.240.18.123 80 port [tcp/http] succeeded!\n"
Feb 27 11:25:38.880: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-7529 02/27/23 11:25:38.88
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7529 to expose endpoints map[] 02/27/23 11:25:38.916
Feb 27 11:25:38.938: INFO: successfully validated that service endpoint-test2 in namespace services-7529 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 11:25:38.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7529" for this suite. 02/27/23 11:25:38.982
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":52,"skipped":987,"failed":0}
------------------------------
• [SLOW TEST] [12.542 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:25:26.455
    Feb 27 11:25:26.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 11:25:26.457
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:26.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:26.52
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-7529 02/27/23 11:25:26.536
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7529 to expose endpoints map[] 02/27/23 11:25:27.018
    Feb 27 11:25:27.029: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Feb 27 11:25:28.048: INFO: successfully validated that service endpoint-test2 in namespace services-7529 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7529 02/27/23 11:25:28.049
    Feb 27 11:25:28.063: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7529" to be "running and ready"
    Feb 27 11:25:28.076: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.4515ms
    Feb 27 11:25:28.076: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:25:30.086: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.022252195s
    Feb 27 11:25:30.086: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 27 11:25:30.086: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7529 to expose endpoints map[pod1:[80]] 02/27/23 11:25:30.1
    Feb 27 11:25:30.131: INFO: successfully validated that service endpoint-test2 in namespace services-7529 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 02/27/23 11:25:30.132
    Feb 27 11:25:30.132: INFO: Creating new exec pod
    Feb 27 11:25:30.148: INFO: Waiting up to 5m0s for pod "execpodzcbnm" in namespace "services-7529" to be "running"
    Feb 27 11:25:30.155: INFO: Pod "execpodzcbnm": Phase="Pending", Reason="", readiness=false. Elapsed: 7.111079ms
    Feb 27 11:25:32.164: INFO: Pod "execpodzcbnm": Phase="Running", Reason="", readiness=true. Elapsed: 2.015900498s
    Feb 27 11:25:32.164: INFO: Pod "execpodzcbnm" satisfied condition "running"
    Feb 27 11:25:33.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-7529 exec execpodzcbnm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb 27 11:25:33.471: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 27 11:25:33.471: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:25:33.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-7529 exec execpodzcbnm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.18.123 80'
    Feb 27 11:25:33.701: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.18.123 80\nConnection to 10.240.18.123 80 port [tcp/http] succeeded!\n"
    Feb 27 11:25:33.701: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-7529 02/27/23 11:25:33.701
    Feb 27 11:25:33.713: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7529" to be "running and ready"
    Feb 27 11:25:33.722: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.120147ms
    Feb 27 11:25:33.722: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:25:35.732: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.019277072s
    Feb 27 11:25:35.732: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 27 11:25:35.732: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7529 to expose endpoints map[pod1:[80] pod2:[80]] 02/27/23 11:25:35.739
    Feb 27 11:25:35.779: INFO: successfully validated that service endpoint-test2 in namespace services-7529 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 02/27/23 11:25:35.779
    Feb 27 11:25:36.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-7529 exec execpodzcbnm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb 27 11:25:37.093: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 27 11:25:37.093: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:25:37.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-7529 exec execpodzcbnm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.18.123 80'
    Feb 27 11:25:37.319: INFO: stderr: "+ nc -v -t -w 2 10.240.18.123 80\nConnection to 10.240.18.123 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Feb 27 11:25:37.319: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-7529 02/27/23 11:25:37.319
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7529 to expose endpoints map[pod2:[80]] 02/27/23 11:25:37.345
    Feb 27 11:25:37.388: INFO: successfully validated that service endpoint-test2 in namespace services-7529 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 02/27/23 11:25:37.388
    Feb 27 11:25:38.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-7529 exec execpodzcbnm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb 27 11:25:38.623: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 27 11:25:38.623: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:25:38.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-7529 exec execpodzcbnm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.18.123 80'
    Feb 27 11:25:38.880: INFO: stderr: "+ nc -v -t -w 2 10.240.18.123 80\n+ echo hostName\nConnection to 10.240.18.123 80 port [tcp/http] succeeded!\n"
    Feb 27 11:25:38.880: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-7529 02/27/23 11:25:38.88
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7529 to expose endpoints map[] 02/27/23 11:25:38.916
    Feb 27 11:25:38.938: INFO: successfully validated that service endpoint-test2 in namespace services-7529 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 11:25:38.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7529" for this suite. 02/27/23 11:25:38.982
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:25:39.004
Feb 27 11:25:39.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pods 02/27/23 11:25:39.005
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:39.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:39.065
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 02/27/23 11:25:39.077
STEP: submitting the pod to kubernetes 02/27/23 11:25:39.078
Feb 27 11:25:39.093: INFO: Waiting up to 5m0s for pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07" in namespace "pods-8578" to be "running and ready"
Feb 27 11:25:39.101: INFO: Pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07": Phase="Pending", Reason="", readiness=false. Elapsed: 7.85905ms
Feb 27 11:25:39.101: INFO: The phase of Pod pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:25:41.113: INFO: Pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07": Phase="Running", Reason="", readiness=true. Elapsed: 2.019999029s
Feb 27 11:25:41.113: INFO: The phase of Pod pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07 is Running (Ready = true)
Feb 27 11:25:41.113: INFO: Pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/27/23 11:25:41.129
STEP: updating the pod 02/27/23 11:25:41.14
Feb 27 11:25:41.691: INFO: Successfully updated pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07"
Feb 27 11:25:41.691: INFO: Waiting up to 5m0s for pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07" in namespace "pods-8578" to be "running"
Feb 27 11:25:41.699: INFO: Pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07": Phase="Running", Reason="", readiness=true. Elapsed: 7.449958ms
Feb 27 11:25:41.699: INFO: Pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 02/27/23 11:25:41.699
Feb 27 11:25:41.726: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 27 11:25:41.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8578" for this suite. 02/27/23 11:25:41.74
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":53,"skipped":1023,"failed":0}
------------------------------
• [2.759 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:25:39.004
    Feb 27 11:25:39.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pods 02/27/23 11:25:39.005
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:39.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:39.065
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 02/27/23 11:25:39.077
    STEP: submitting the pod to kubernetes 02/27/23 11:25:39.078
    Feb 27 11:25:39.093: INFO: Waiting up to 5m0s for pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07" in namespace "pods-8578" to be "running and ready"
    Feb 27 11:25:39.101: INFO: Pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07": Phase="Pending", Reason="", readiness=false. Elapsed: 7.85905ms
    Feb 27 11:25:39.101: INFO: The phase of Pod pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:25:41.113: INFO: Pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07": Phase="Running", Reason="", readiness=true. Elapsed: 2.019999029s
    Feb 27 11:25:41.113: INFO: The phase of Pod pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07 is Running (Ready = true)
    Feb 27 11:25:41.113: INFO: Pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/27/23 11:25:41.129
    STEP: updating the pod 02/27/23 11:25:41.14
    Feb 27 11:25:41.691: INFO: Successfully updated pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07"
    Feb 27 11:25:41.691: INFO: Waiting up to 5m0s for pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07" in namespace "pods-8578" to be "running"
    Feb 27 11:25:41.699: INFO: Pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07": Phase="Running", Reason="", readiness=true. Elapsed: 7.449958ms
    Feb 27 11:25:41.699: INFO: Pod "pod-update-6890e5f5-9e3f-428e-815b-c1886a36ee07" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 02/27/23 11:25:41.699
    Feb 27 11:25:41.726: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 27 11:25:41.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8578" for this suite. 02/27/23 11:25:41.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:25:41.768
Feb 27 11:25:41.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename sysctl 02/27/23 11:25:41.769
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:41.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:41.817
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 02/27/23 11:25:41.83
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 27 11:25:41.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-42" for this suite. 02/27/23 11:25:41.861
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":54,"skipped":1028,"failed":0}
------------------------------
• [0.111 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:25:41.768
    Feb 27 11:25:41.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename sysctl 02/27/23 11:25:41.769
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:41.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:41.817
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 02/27/23 11:25:41.83
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 27 11:25:41.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-42" for this suite. 02/27/23 11:25:41.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:25:41.888
Feb 27 11:25:41.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename discovery 02/27/23 11:25:41.889
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:41.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:41.924
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 02/27/23 11:25:41.941
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Feb 27 11:25:42.563: INFO: Checking APIGroup: apiregistration.k8s.io
Feb 27 11:25:42.571: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb 27 11:25:42.571: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Feb 27 11:25:42.571: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb 27 11:25:42.571: INFO: Checking APIGroup: apps
Feb 27 11:25:42.575: INFO: PreferredVersion.GroupVersion: apps/v1
Feb 27 11:25:42.575: INFO: Versions found [{apps/v1 v1}]
Feb 27 11:25:42.575: INFO: apps/v1 matches apps/v1
Feb 27 11:25:42.575: INFO: Checking APIGroup: events.k8s.io
Feb 27 11:25:42.580: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb 27 11:25:42.580: INFO: Versions found [{events.k8s.io/v1 v1}]
Feb 27 11:25:42.580: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb 27 11:25:42.580: INFO: Checking APIGroup: authentication.k8s.io
Feb 27 11:25:42.586: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb 27 11:25:42.586: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Feb 27 11:25:42.586: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb 27 11:25:42.586: INFO: Checking APIGroup: authorization.k8s.io
Feb 27 11:25:42.591: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb 27 11:25:42.591: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Feb 27 11:25:42.591: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb 27 11:25:42.591: INFO: Checking APIGroup: autoscaling
Feb 27 11:25:42.596: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Feb 27 11:25:42.596: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Feb 27 11:25:42.596: INFO: autoscaling/v2 matches autoscaling/v2
Feb 27 11:25:42.596: INFO: Checking APIGroup: batch
Feb 27 11:25:42.602: INFO: PreferredVersion.GroupVersion: batch/v1
Feb 27 11:25:42.602: INFO: Versions found [{batch/v1 v1}]
Feb 27 11:25:42.602: INFO: batch/v1 matches batch/v1
Feb 27 11:25:42.602: INFO: Checking APIGroup: certificates.k8s.io
Feb 27 11:25:42.606: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb 27 11:25:42.606: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Feb 27 11:25:42.606: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb 27 11:25:42.606: INFO: Checking APIGroup: networking.k8s.io
Feb 27 11:25:42.610: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb 27 11:25:42.610: INFO: Versions found [{networking.k8s.io/v1 v1}]
Feb 27 11:25:42.610: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb 27 11:25:42.610: INFO: Checking APIGroup: policy
Feb 27 11:25:42.617: INFO: PreferredVersion.GroupVersion: policy/v1
Feb 27 11:25:42.617: INFO: Versions found [{policy/v1 v1}]
Feb 27 11:25:42.617: INFO: policy/v1 matches policy/v1
Feb 27 11:25:42.617: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb 27 11:25:42.621: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb 27 11:25:42.621: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Feb 27 11:25:42.621: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb 27 11:25:42.621: INFO: Checking APIGroup: storage.k8s.io
Feb 27 11:25:42.626: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb 27 11:25:42.626: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Feb 27 11:25:42.626: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb 27 11:25:42.626: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb 27 11:25:42.631: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb 27 11:25:42.631: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Feb 27 11:25:42.631: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb 27 11:25:42.631: INFO: Checking APIGroup: apiextensions.k8s.io
Feb 27 11:25:42.637: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb 27 11:25:42.637: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Feb 27 11:25:42.637: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb 27 11:25:42.637: INFO: Checking APIGroup: scheduling.k8s.io
Feb 27 11:25:42.641: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb 27 11:25:42.641: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Feb 27 11:25:42.641: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb 27 11:25:42.641: INFO: Checking APIGroup: coordination.k8s.io
Feb 27 11:25:42.647: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb 27 11:25:42.647: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Feb 27 11:25:42.647: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb 27 11:25:42.647: INFO: Checking APIGroup: node.k8s.io
Feb 27 11:25:42.652: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb 27 11:25:42.652: INFO: Versions found [{node.k8s.io/v1 v1}]
Feb 27 11:25:42.652: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb 27 11:25:42.652: INFO: Checking APIGroup: discovery.k8s.io
Feb 27 11:25:42.656: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Feb 27 11:25:42.656: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Feb 27 11:25:42.656: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Feb 27 11:25:42.656: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb 27 11:25:42.661: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Feb 27 11:25:42.661: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Feb 27 11:25:42.661: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Feb 27 11:25:42.661: INFO: Checking APIGroup: apps.kubermatic.k8c.io
Feb 27 11:25:42.666: INFO: PreferredVersion.GroupVersion: apps.kubermatic.k8c.io/v1
Feb 27 11:25:42.666: INFO: Versions found [{apps.kubermatic.k8c.io/v1 v1}]
Feb 27 11:25:42.666: INFO: apps.kubermatic.k8c.io/v1 matches apps.kubermatic.k8c.io/v1
Feb 27 11:25:42.666: INFO: Checking APIGroup: crd.projectcalico.org
Feb 27 11:25:42.670: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Feb 27 11:25:42.670: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Feb 27 11:25:42.670: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Feb 27 11:25:42.670: INFO: Checking APIGroup: cluster.k8s.io
Feb 27 11:25:42.675: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
Feb 27 11:25:42.675: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
Feb 27 11:25:42.675: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
Feb 27 11:25:42.675: INFO: Checking APIGroup: operatingsystemmanager.k8c.io
Feb 27 11:25:42.682: INFO: PreferredVersion.GroupVersion: operatingsystemmanager.k8c.io/v1alpha1
Feb 27 11:25:42.682: INFO: Versions found [{operatingsystemmanager.k8c.io/v1alpha1 v1alpha1}]
Feb 27 11:25:42.682: INFO: operatingsystemmanager.k8c.io/v1alpha1 matches operatingsystemmanager.k8c.io/v1alpha1
Feb 27 11:25:42.682: INFO: Checking APIGroup: metrics.k8s.io
Feb 27 11:25:42.687: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Feb 27 11:25:42.687: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Feb 27 11:25:42.687: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Feb 27 11:25:42.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3812" for this suite. 02/27/23 11:25:42.699
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":55,"skipped":1056,"failed":0}
------------------------------
• [0.828 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:25:41.888
    Feb 27 11:25:41.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename discovery 02/27/23 11:25:41.889
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:41.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:41.924
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 02/27/23 11:25:41.941
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Feb 27 11:25:42.563: INFO: Checking APIGroup: apiregistration.k8s.io
    Feb 27 11:25:42.571: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Feb 27 11:25:42.571: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Feb 27 11:25:42.571: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Feb 27 11:25:42.571: INFO: Checking APIGroup: apps
    Feb 27 11:25:42.575: INFO: PreferredVersion.GroupVersion: apps/v1
    Feb 27 11:25:42.575: INFO: Versions found [{apps/v1 v1}]
    Feb 27 11:25:42.575: INFO: apps/v1 matches apps/v1
    Feb 27 11:25:42.575: INFO: Checking APIGroup: events.k8s.io
    Feb 27 11:25:42.580: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Feb 27 11:25:42.580: INFO: Versions found [{events.k8s.io/v1 v1}]
    Feb 27 11:25:42.580: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Feb 27 11:25:42.580: INFO: Checking APIGroup: authentication.k8s.io
    Feb 27 11:25:42.586: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Feb 27 11:25:42.586: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Feb 27 11:25:42.586: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Feb 27 11:25:42.586: INFO: Checking APIGroup: authorization.k8s.io
    Feb 27 11:25:42.591: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Feb 27 11:25:42.591: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Feb 27 11:25:42.591: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Feb 27 11:25:42.591: INFO: Checking APIGroup: autoscaling
    Feb 27 11:25:42.596: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Feb 27 11:25:42.596: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Feb 27 11:25:42.596: INFO: autoscaling/v2 matches autoscaling/v2
    Feb 27 11:25:42.596: INFO: Checking APIGroup: batch
    Feb 27 11:25:42.602: INFO: PreferredVersion.GroupVersion: batch/v1
    Feb 27 11:25:42.602: INFO: Versions found [{batch/v1 v1}]
    Feb 27 11:25:42.602: INFO: batch/v1 matches batch/v1
    Feb 27 11:25:42.602: INFO: Checking APIGroup: certificates.k8s.io
    Feb 27 11:25:42.606: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Feb 27 11:25:42.606: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Feb 27 11:25:42.606: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Feb 27 11:25:42.606: INFO: Checking APIGroup: networking.k8s.io
    Feb 27 11:25:42.610: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Feb 27 11:25:42.610: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Feb 27 11:25:42.610: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Feb 27 11:25:42.610: INFO: Checking APIGroup: policy
    Feb 27 11:25:42.617: INFO: PreferredVersion.GroupVersion: policy/v1
    Feb 27 11:25:42.617: INFO: Versions found [{policy/v1 v1}]
    Feb 27 11:25:42.617: INFO: policy/v1 matches policy/v1
    Feb 27 11:25:42.617: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Feb 27 11:25:42.621: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Feb 27 11:25:42.621: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Feb 27 11:25:42.621: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Feb 27 11:25:42.621: INFO: Checking APIGroup: storage.k8s.io
    Feb 27 11:25:42.626: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Feb 27 11:25:42.626: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Feb 27 11:25:42.626: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Feb 27 11:25:42.626: INFO: Checking APIGroup: admissionregistration.k8s.io
    Feb 27 11:25:42.631: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Feb 27 11:25:42.631: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Feb 27 11:25:42.631: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Feb 27 11:25:42.631: INFO: Checking APIGroup: apiextensions.k8s.io
    Feb 27 11:25:42.637: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Feb 27 11:25:42.637: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Feb 27 11:25:42.637: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Feb 27 11:25:42.637: INFO: Checking APIGroup: scheduling.k8s.io
    Feb 27 11:25:42.641: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Feb 27 11:25:42.641: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Feb 27 11:25:42.641: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Feb 27 11:25:42.641: INFO: Checking APIGroup: coordination.k8s.io
    Feb 27 11:25:42.647: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Feb 27 11:25:42.647: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Feb 27 11:25:42.647: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Feb 27 11:25:42.647: INFO: Checking APIGroup: node.k8s.io
    Feb 27 11:25:42.652: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Feb 27 11:25:42.652: INFO: Versions found [{node.k8s.io/v1 v1}]
    Feb 27 11:25:42.652: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Feb 27 11:25:42.652: INFO: Checking APIGroup: discovery.k8s.io
    Feb 27 11:25:42.656: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Feb 27 11:25:42.656: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Feb 27 11:25:42.656: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Feb 27 11:25:42.656: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Feb 27 11:25:42.661: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Feb 27 11:25:42.661: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Feb 27 11:25:42.661: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Feb 27 11:25:42.661: INFO: Checking APIGroup: apps.kubermatic.k8c.io
    Feb 27 11:25:42.666: INFO: PreferredVersion.GroupVersion: apps.kubermatic.k8c.io/v1
    Feb 27 11:25:42.666: INFO: Versions found [{apps.kubermatic.k8c.io/v1 v1}]
    Feb 27 11:25:42.666: INFO: apps.kubermatic.k8c.io/v1 matches apps.kubermatic.k8c.io/v1
    Feb 27 11:25:42.666: INFO: Checking APIGroup: crd.projectcalico.org
    Feb 27 11:25:42.670: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Feb 27 11:25:42.670: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Feb 27 11:25:42.670: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Feb 27 11:25:42.670: INFO: Checking APIGroup: cluster.k8s.io
    Feb 27 11:25:42.675: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
    Feb 27 11:25:42.675: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
    Feb 27 11:25:42.675: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
    Feb 27 11:25:42.675: INFO: Checking APIGroup: operatingsystemmanager.k8c.io
    Feb 27 11:25:42.682: INFO: PreferredVersion.GroupVersion: operatingsystemmanager.k8c.io/v1alpha1
    Feb 27 11:25:42.682: INFO: Versions found [{operatingsystemmanager.k8c.io/v1alpha1 v1alpha1}]
    Feb 27 11:25:42.682: INFO: operatingsystemmanager.k8c.io/v1alpha1 matches operatingsystemmanager.k8c.io/v1alpha1
    Feb 27 11:25:42.682: INFO: Checking APIGroup: metrics.k8s.io
    Feb 27 11:25:42.687: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Feb 27 11:25:42.687: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Feb 27 11:25:42.687: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Feb 27 11:25:42.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-3812" for this suite. 02/27/23 11:25:42.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:25:42.718
Feb 27 11:25:42.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-probe 02/27/23 11:25:42.72
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:42.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:42.759
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Feb 27 11:25:42.804: INFO: Waiting up to 5m0s for pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e" in namespace "container-probe-9396" to be "running and ready"
Feb 27 11:25:42.813: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.822464ms
Feb 27 11:25:42.814: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:25:44.849: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 2.044106709s
Feb 27 11:25:44.849: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
Feb 27 11:25:46.824: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 4.019369028s
Feb 27 11:25:46.824: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
Feb 27 11:25:48.822: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 6.017460804s
Feb 27 11:25:48.822: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
Feb 27 11:25:50.823: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 8.019044782s
Feb 27 11:25:50.824: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
Feb 27 11:25:52.824: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 10.019149573s
Feb 27 11:25:52.824: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
Feb 27 11:25:54.824: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 12.0198292s
Feb 27 11:25:54.824: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
Feb 27 11:25:56.825: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 14.020406718s
Feb 27 11:25:56.825: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
Feb 27 11:25:58.830: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 16.025692414s
Feb 27 11:25:58.830: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
Feb 27 11:26:00.822: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 18.017980101s
Feb 27 11:26:00.822: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
Feb 27 11:26:02.830: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 20.025178467s
Feb 27 11:26:02.830: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
Feb 27 11:26:04.822: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=true. Elapsed: 22.017593843s
Feb 27 11:26:04.822: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = true)
Feb 27 11:26:04.822: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e" satisfied condition "running and ready"
Feb 27 11:26:04.830: INFO: Container started at 2023-02-27 11:25:43 +0000 UTC, pod became ready at 2023-02-27 11:26:03 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 27 11:26:04.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9396" for this suite. 02/27/23 11:26:04.842
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":56,"skipped":1073,"failed":0}
------------------------------
• [SLOW TEST] [22.135 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:25:42.718
    Feb 27 11:25:42.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-probe 02/27/23 11:25:42.72
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:25:42.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:25:42.759
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Feb 27 11:25:42.804: INFO: Waiting up to 5m0s for pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e" in namespace "container-probe-9396" to be "running and ready"
    Feb 27 11:25:42.813: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.822464ms
    Feb 27 11:25:42.814: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:25:44.849: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 2.044106709s
    Feb 27 11:25:44.849: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
    Feb 27 11:25:46.824: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 4.019369028s
    Feb 27 11:25:46.824: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
    Feb 27 11:25:48.822: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 6.017460804s
    Feb 27 11:25:48.822: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
    Feb 27 11:25:50.823: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 8.019044782s
    Feb 27 11:25:50.824: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
    Feb 27 11:25:52.824: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 10.019149573s
    Feb 27 11:25:52.824: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
    Feb 27 11:25:54.824: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 12.0198292s
    Feb 27 11:25:54.824: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
    Feb 27 11:25:56.825: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 14.020406718s
    Feb 27 11:25:56.825: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
    Feb 27 11:25:58.830: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 16.025692414s
    Feb 27 11:25:58.830: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
    Feb 27 11:26:00.822: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 18.017980101s
    Feb 27 11:26:00.822: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
    Feb 27 11:26:02.830: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=false. Elapsed: 20.025178467s
    Feb 27 11:26:02.830: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = false)
    Feb 27 11:26:04.822: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e": Phase="Running", Reason="", readiness=true. Elapsed: 22.017593843s
    Feb 27 11:26:04.822: INFO: The phase of Pod test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e is Running (Ready = true)
    Feb 27 11:26:04.822: INFO: Pod "test-webserver-499aa2aa-fd12-4d28-830d-f66c30ae969e" satisfied condition "running and ready"
    Feb 27 11:26:04.830: INFO: Container started at 2023-02-27 11:25:43 +0000 UTC, pod became ready at 2023-02-27 11:26:03 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 27 11:26:04.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9396" for this suite. 02/27/23 11:26:04.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:26:04.859
Feb 27 11:26:04.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 11:26:04.868
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:26:04.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:26:04.921
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/27/23 11:26:04.994
Feb 27 11:26:05.032: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5726" to be "running and ready"
Feb 27 11:26:05.064: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 32.347888ms
Feb 27 11:26:05.064: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:26:07.074: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.042786221s
Feb 27 11:26:07.074: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 27 11:26:07.074: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 02/27/23 11:26:07.086
Feb 27 11:26:07.098: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5726" to be "running and ready"
Feb 27 11:26:07.116: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 17.58065ms
Feb 27 11:26:07.116: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:26:09.128: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.029246392s
Feb 27 11:26:09.128: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Feb 27 11:26:09.128: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/27/23 11:26:09.15
Feb 27 11:26:09.166: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 27 11:26:09.177: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 27 11:26:11.178: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 27 11:26:11.189: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 27 11:26:13.178: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 27 11:26:13.190: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 02/27/23 11:26:13.19
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 27 11:26:13.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5726" for this suite. 02/27/23 11:26:13.233
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":57,"skipped":1091,"failed":0}
------------------------------
• [SLOW TEST] [8.395 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:26:04.859
    Feb 27 11:26:04.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 11:26:04.868
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:26:04.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:26:04.921
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/27/23 11:26:04.994
    Feb 27 11:26:05.032: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5726" to be "running and ready"
    Feb 27 11:26:05.064: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 32.347888ms
    Feb 27 11:26:05.064: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:26:07.074: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.042786221s
    Feb 27 11:26:07.074: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 27 11:26:07.074: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 02/27/23 11:26:07.086
    Feb 27 11:26:07.098: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5726" to be "running and ready"
    Feb 27 11:26:07.116: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 17.58065ms
    Feb 27 11:26:07.116: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:26:09.128: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.029246392s
    Feb 27 11:26:09.128: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Feb 27 11:26:09.128: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/27/23 11:26:09.15
    Feb 27 11:26:09.166: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 27 11:26:09.177: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 27 11:26:11.178: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 27 11:26:11.189: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 27 11:26:13.178: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 27 11:26:13.190: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 02/27/23 11:26:13.19
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 27 11:26:13.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5726" for this suite. 02/27/23 11:26:13.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:26:13.269
Feb 27 11:26:13.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 11:26:13.271
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:26:13.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:26:13.351
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 02/27/23 11:26:13.382
STEP: fetching the ConfigMap 02/27/23 11:26:13.4
STEP: patching the ConfigMap 02/27/23 11:26:13.41
STEP: listing all ConfigMaps in all namespaces with a label selector 02/27/23 11:26:13.436
STEP: deleting the ConfigMap by collection with a label selector 02/27/23 11:26:13.446
STEP: listing all ConfigMaps in test namespace 02/27/23 11:26:13.464
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 11:26:13.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4343" for this suite. 02/27/23 11:26:13.489
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":58,"skipped":1139,"failed":0}
------------------------------
• [0.234 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:26:13.269
    Feb 27 11:26:13.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 11:26:13.271
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:26:13.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:26:13.351
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 02/27/23 11:26:13.382
    STEP: fetching the ConfigMap 02/27/23 11:26:13.4
    STEP: patching the ConfigMap 02/27/23 11:26:13.41
    STEP: listing all ConfigMaps in all namespaces with a label selector 02/27/23 11:26:13.436
    STEP: deleting the ConfigMap by collection with a label selector 02/27/23 11:26:13.446
    STEP: listing all ConfigMaps in test namespace 02/27/23 11:26:13.464
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 11:26:13.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4343" for this suite. 02/27/23 11:26:13.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:26:13.53
Feb 27 11:26:13.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename daemonsets 02/27/23 11:26:13.534
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:26:13.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:26:13.593
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 02/27/23 11:26:13.701
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 11:26:13.72
Feb 27 11:26:13.750: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:26:13.751: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:26:14.771: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:26:14.771: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:26:15.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 11:26:15.775: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 02/27/23 11:26:15.787
Feb 27 11:26:15.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 11:26:15.838: INFO: Node ip-172-31-7-167.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:26:16.857: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 11:26:16.857: INFO: Node ip-172-31-7-167.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:26:17.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 11:26:17.870: INFO: Node ip-172-31-7-167.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:26:18.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 11:26:18.863: INFO: Node ip-172-31-7-167.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:26:19.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 11:26:19.861: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/27/23 11:26:19.868
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7206, will wait for the garbage collector to delete the pods 02/27/23 11:26:19.869
Feb 27 11:26:19.947: INFO: Deleting DaemonSet.extensions daemon-set took: 17.100929ms
Feb 27 11:26:20.047: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.123085ms
Feb 27 11:26:22.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:26:22.559: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 11:26:22.566: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"61586"},"items":null}

Feb 27 11:26:22.575: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"61586"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 27 11:26:22.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7206" for this suite. 02/27/23 11:26:22.63
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":59,"skipped":1228,"failed":0}
------------------------------
• [SLOW TEST] [9.112 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:26:13.53
    Feb 27 11:26:13.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename daemonsets 02/27/23 11:26:13.534
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:26:13.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:26:13.593
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 02/27/23 11:26:13.701
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 11:26:13.72
    Feb 27 11:26:13.750: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:26:13.751: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:26:14.771: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:26:14.771: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:26:15.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 11:26:15.775: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 02/27/23 11:26:15.787
    Feb 27 11:26:15.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 11:26:15.838: INFO: Node ip-172-31-7-167.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:26:16.857: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 11:26:16.857: INFO: Node ip-172-31-7-167.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:26:17.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 11:26:17.870: INFO: Node ip-172-31-7-167.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:26:18.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 11:26:18.863: INFO: Node ip-172-31-7-167.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:26:19.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 11:26:19.861: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 11:26:19.868
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7206, will wait for the garbage collector to delete the pods 02/27/23 11:26:19.869
    Feb 27 11:26:19.947: INFO: Deleting DaemonSet.extensions daemon-set took: 17.100929ms
    Feb 27 11:26:20.047: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.123085ms
    Feb 27 11:26:22.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:26:22.559: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 11:26:22.566: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"61586"},"items":null}

    Feb 27 11:26:22.575: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"61586"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 11:26:22.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7206" for this suite. 02/27/23 11:26:22.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:26:22.652
Feb 27 11:26:22.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 11:26:22.653
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:26:22.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:26:22.691
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 02/27/23 11:26:22.7
Feb 27 11:26:22.717: INFO: Waiting up to 5m0s for pod "downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69" in namespace "downward-api-5167" to be "Succeeded or Failed"
Feb 27 11:26:22.726: INFO: Pod "downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69": Phase="Pending", Reason="", readiness=false. Elapsed: 9.272778ms
Feb 27 11:26:24.736: INFO: Pod "downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019078122s
Feb 27 11:26:26.736: INFO: Pod "downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019091823s
STEP: Saw pod success 02/27/23 11:26:26.737
Feb 27 11:26:26.737: INFO: Pod "downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69" satisfied condition "Succeeded or Failed"
Feb 27 11:26:26.746: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69 container dapi-container: <nil>
STEP: delete the pod 02/27/23 11:26:26.764
Feb 27 11:26:26.792: INFO: Waiting for pod downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69 to disappear
Feb 27 11:26:26.805: INFO: Pod downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 27 11:26:26.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5167" for this suite. 02/27/23 11:26:26.819
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":60,"skipped":1236,"failed":0}
------------------------------
• [4.182 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:26:22.652
    Feb 27 11:26:22.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 11:26:22.653
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:26:22.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:26:22.691
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 02/27/23 11:26:22.7
    Feb 27 11:26:22.717: INFO: Waiting up to 5m0s for pod "downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69" in namespace "downward-api-5167" to be "Succeeded or Failed"
    Feb 27 11:26:22.726: INFO: Pod "downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69": Phase="Pending", Reason="", readiness=false. Elapsed: 9.272778ms
    Feb 27 11:26:24.736: INFO: Pod "downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019078122s
    Feb 27 11:26:26.736: INFO: Pod "downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019091823s
    STEP: Saw pod success 02/27/23 11:26:26.737
    Feb 27 11:26:26.737: INFO: Pod "downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69" satisfied condition "Succeeded or Failed"
    Feb 27 11:26:26.746: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 11:26:26.764
    Feb 27 11:26:26.792: INFO: Waiting for pod downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69 to disappear
    Feb 27 11:26:26.805: INFO: Pod downward-api-e9f95b9f-7af5-46a3-ae48-603310255f69 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 27 11:26:26.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5167" for this suite. 02/27/23 11:26:26.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:26:26.838
Feb 27 11:26:26.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:26:26.84
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:26:26.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:26:26.894
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 02/27/23 11:26:26.907
Feb 27 11:26:26.924: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7" in namespace "projected-9091" to be "Succeeded or Failed"
Feb 27 11:26:26.932: INFO: Pod "downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.577645ms
Feb 27 11:26:28.941: INFO: Pod "downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017224576s
Feb 27 11:26:30.950: INFO: Pod "downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02617828s
STEP: Saw pod success 02/27/23 11:26:30.95
Feb 27 11:26:30.950: INFO: Pod "downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7" satisfied condition "Succeeded or Failed"
Feb 27 11:26:30.960: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7 container client-container: <nil>
STEP: delete the pod 02/27/23 11:26:30.978
Feb 27 11:26:30.999: INFO: Waiting for pod downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7 to disappear
Feb 27 11:26:31.009: INFO: Pod downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 27 11:26:31.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9091" for this suite. 02/27/23 11:26:31.021
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":61,"skipped":1251,"failed":0}
------------------------------
• [4.205 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:26:26.838
    Feb 27 11:26:26.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:26:26.84
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:26:26.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:26:26.894
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 02/27/23 11:26:26.907
    Feb 27 11:26:26.924: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7" in namespace "projected-9091" to be "Succeeded or Failed"
    Feb 27 11:26:26.932: INFO: Pod "downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.577645ms
    Feb 27 11:26:28.941: INFO: Pod "downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017224576s
    Feb 27 11:26:30.950: INFO: Pod "downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02617828s
    STEP: Saw pod success 02/27/23 11:26:30.95
    Feb 27 11:26:30.950: INFO: Pod "downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7" satisfied condition "Succeeded or Failed"
    Feb 27 11:26:30.960: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7 container client-container: <nil>
    STEP: delete the pod 02/27/23 11:26:30.978
    Feb 27 11:26:30.999: INFO: Waiting for pod downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7 to disappear
    Feb 27 11:26:31.009: INFO: Pod downwardapi-volume-d4efb136-79d6-4a04-b133-c41a8b948be7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 27 11:26:31.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9091" for this suite. 02/27/23 11:26:31.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:26:31.046
Feb 27 11:26:31.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename endpointslice 02/27/23 11:26:31.048
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:26:31.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:26:31.092
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 02/27/23 11:26:36.259
STEP: referencing matching pods with named port 02/27/23 11:26:41.277
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/27/23 11:26:46.305
STEP: recreating EndpointSlices after they've been deleted 02/27/23 11:26:51.326
Feb 27 11:26:51.378: INFO: EndpointSlice for Service endpointslice-1372/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 27 11:27:01.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1372" for this suite. 02/27/23 11:27:01.422
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":62,"skipped":1264,"failed":0}
------------------------------
• [SLOW TEST] [30.395 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:26:31.046
    Feb 27 11:26:31.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename endpointslice 02/27/23 11:26:31.048
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:26:31.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:26:31.092
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 02/27/23 11:26:36.259
    STEP: referencing matching pods with named port 02/27/23 11:26:41.277
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/27/23 11:26:46.305
    STEP: recreating EndpointSlices after they've been deleted 02/27/23 11:26:51.326
    Feb 27 11:26:51.378: INFO: EndpointSlice for Service endpointslice-1372/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 27 11:27:01.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-1372" for this suite. 02/27/23 11:27:01.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:27:01.445
Feb 27 11:27:01.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename taint-single-pod 02/27/23 11:27:01.447
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:27:01.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:27:01.506
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Feb 27 11:27:01.521: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 11:28:01.604: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Feb 27 11:28:01.619: INFO: Starting informer...
STEP: Starting pod... 02/27/23 11:28:01.619
Feb 27 11:28:01.859: INFO: Pod is running on ip-172-31-7-167.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 02/27/23 11:28:01.859
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 11:28:01.914
STEP: Waiting short time to make sure Pod is queued for deletion 02/27/23 11:28:01.933
Feb 27 11:28:01.933: INFO: Pod wasn't evicted. Proceeding
Feb 27 11:28:01.933: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 11:28:02.011
STEP: Waiting some time to make sure that toleration time passed. 02/27/23 11:28:02.041
Feb 27 11:29:17.043: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Feb 27 11:29:17.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7905" for this suite. 02/27/23 11:29:17.062
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":63,"skipped":1281,"failed":0}
------------------------------
• [SLOW TEST] [135.637 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:27:01.445
    Feb 27 11:27:01.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename taint-single-pod 02/27/23 11:27:01.447
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:27:01.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:27:01.506
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Feb 27 11:27:01.521: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 11:28:01.604: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Feb 27 11:28:01.619: INFO: Starting informer...
    STEP: Starting pod... 02/27/23 11:28:01.619
    Feb 27 11:28:01.859: INFO: Pod is running on ip-172-31-7-167.eu-central-1.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 02/27/23 11:28:01.859
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 11:28:01.914
    STEP: Waiting short time to make sure Pod is queued for deletion 02/27/23 11:28:01.933
    Feb 27 11:28:01.933: INFO: Pod wasn't evicted. Proceeding
    Feb 27 11:28:01.933: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 11:28:02.011
    STEP: Waiting some time to make sure that toleration time passed. 02/27/23 11:28:02.041
    Feb 27 11:29:17.043: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 11:29:17.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-7905" for this suite. 02/27/23 11:29:17.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:29:17.084
Feb 27 11:29:17.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 11:29:17.085
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:17.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:17.131
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Feb 27 11:29:17.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-5114 version'
Feb 27 11:29:17.230: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Feb 27 11:29:17.230: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:15:26Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 11:29:17.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5114" for this suite. 02/27/23 11:29:17.239
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":64,"skipped":1287,"failed":0}
------------------------------
• [0.168 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:29:17.084
    Feb 27 11:29:17.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 11:29:17.085
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:17.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:17.131
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Feb 27 11:29:17.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-5114 version'
    Feb 27 11:29:17.230: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Feb 27 11:29:17.230: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:15:26Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 11:29:17.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5114" for this suite. 02/27/23 11:29:17.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:29:17.255
Feb 27 11:29:17.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:29:17.256
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:17.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:17.301
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-5462b28a-509b-4202-b975-56d45213bd1b 02/27/23 11:29:17.311
STEP: Creating a pod to test consume secrets 02/27/23 11:29:17.321
Feb 27 11:29:17.336: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b" in namespace "projected-5731" to be "Succeeded or Failed"
Feb 27 11:29:17.349: INFO: Pod "pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.27556ms
Feb 27 11:29:19.358: INFO: Pod "pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022036548s
Feb 27 11:29:21.360: INFO: Pod "pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023475575s
STEP: Saw pod success 02/27/23 11:29:21.36
Feb 27 11:29:21.360: INFO: Pod "pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b" satisfied condition "Succeeded or Failed"
Feb 27 11:29:21.369: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 11:29:21.391
Feb 27 11:29:21.586: INFO: Waiting for pod pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b to disappear
Feb 27 11:29:21.594: INFO: Pod pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 27 11:29:21.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5731" for this suite. 02/27/23 11:29:21.61
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":65,"skipped":1348,"failed":0}
------------------------------
• [4.371 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:29:17.255
    Feb 27 11:29:17.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:29:17.256
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:17.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:17.301
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-5462b28a-509b-4202-b975-56d45213bd1b 02/27/23 11:29:17.311
    STEP: Creating a pod to test consume secrets 02/27/23 11:29:17.321
    Feb 27 11:29:17.336: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b" in namespace "projected-5731" to be "Succeeded or Failed"
    Feb 27 11:29:17.349: INFO: Pod "pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.27556ms
    Feb 27 11:29:19.358: INFO: Pod "pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022036548s
    Feb 27 11:29:21.360: INFO: Pod "pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023475575s
    STEP: Saw pod success 02/27/23 11:29:21.36
    Feb 27 11:29:21.360: INFO: Pod "pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b" satisfied condition "Succeeded or Failed"
    Feb 27 11:29:21.369: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 11:29:21.391
    Feb 27 11:29:21.586: INFO: Waiting for pod pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b to disappear
    Feb 27 11:29:21.594: INFO: Pod pod-projected-secrets-36c620c4-b184-4ba3-895d-59c0920d997b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 27 11:29:21.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5731" for this suite. 02/27/23 11:29:21.61
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:29:21.63
Feb 27 11:29:21.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename endpointslicemirroring 02/27/23 11:29:21.632
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:21.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:21.68
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 02/27/23 11:29:21.71
STEP: mirroring an update to a custom Endpoint 02/27/23 11:29:21.753
STEP: mirroring deletion of a custom Endpoint 02/27/23 11:29:21.775
Feb 27 11:29:21.800: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Feb 27 11:29:23.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-1739" for this suite. 02/27/23 11:29:23.821
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":66,"skipped":1352,"failed":0}
------------------------------
• [2.203 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:29:21.63
    Feb 27 11:29:21.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename endpointslicemirroring 02/27/23 11:29:21.632
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:21.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:21.68
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 02/27/23 11:29:21.71
    STEP: mirroring an update to a custom Endpoint 02/27/23 11:29:21.753
    STEP: mirroring deletion of a custom Endpoint 02/27/23 11:29:21.775
    Feb 27 11:29:21.800: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Feb 27 11:29:23.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-1739" for this suite. 02/27/23 11:29:23.821
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:29:23.839
Feb 27 11:29:23.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 11:29:23.84
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:23.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:23.877
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 11:29:23.917
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:29:24.265
STEP: Deploying the webhook pod 02/27/23 11:29:24.277
STEP: Wait for the deployment to be ready 02/27/23 11:29:24.299
Feb 27 11:29:24.330: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 11:29:26.359
STEP: Verifying the service has paired with the endpoint 02/27/23 11:29:26.378
Feb 27 11:29:27.379: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 02/27/23 11:29:27.396
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 11:29:27.438
STEP: Updating a validating webhook configuration's rules to not include the create operation 02/27/23 11:29:27.462
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 11:29:27.482
STEP: Patching a validating webhook configuration's rules to include the create operation 02/27/23 11:29:27.506
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 11:29:27.519
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:29:27.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9903" for this suite. 02/27/23 11:29:27.553
STEP: Destroying namespace "webhook-9903-markers" for this suite. 02/27/23 11:29:27.566
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":67,"skipped":1414,"failed":0}
------------------------------
• [3.822 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:29:23.839
    Feb 27 11:29:23.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 11:29:23.84
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:23.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:23.877
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 11:29:23.917
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:29:24.265
    STEP: Deploying the webhook pod 02/27/23 11:29:24.277
    STEP: Wait for the deployment to be ready 02/27/23 11:29:24.299
    Feb 27 11:29:24.330: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 11:29:26.359
    STEP: Verifying the service has paired with the endpoint 02/27/23 11:29:26.378
    Feb 27 11:29:27.379: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 02/27/23 11:29:27.396
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 11:29:27.438
    STEP: Updating a validating webhook configuration's rules to not include the create operation 02/27/23 11:29:27.462
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 11:29:27.482
    STEP: Patching a validating webhook configuration's rules to include the create operation 02/27/23 11:29:27.506
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 11:29:27.519
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:29:27.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9903" for this suite. 02/27/23 11:29:27.553
    STEP: Destroying namespace "webhook-9903-markers" for this suite. 02/27/23 11:29:27.566
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:29:27.663
Feb 27 11:29:27.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename resourcequota 02/27/23 11:29:27.664
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:27.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:27.713
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 02/27/23 11:29:27.723
STEP: Creating a ResourceQuota 02/27/23 11:29:32.739
STEP: Ensuring resource quota status is calculated 02/27/23 11:29:32.755
STEP: Creating a Pod that fits quota 02/27/23 11:29:34.772
STEP: Ensuring ResourceQuota status captures the pod usage 02/27/23 11:29:34.802
STEP: Not allowing a pod to be created that exceeds remaining quota 02/27/23 11:29:36.81
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/27/23 11:29:36.816
STEP: Ensuring a pod cannot update its resource requirements 02/27/23 11:29:36.824
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/27/23 11:29:36.833
STEP: Deleting the pod 02/27/23 11:29:38.842
STEP: Ensuring resource quota status released the pod usage 02/27/23 11:29:38.876
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 27 11:29:40.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8886" for this suite. 02/27/23 11:29:40.904
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":68,"skipped":1421,"failed":0}
------------------------------
• [SLOW TEST] [13.253 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:29:27.663
    Feb 27 11:29:27.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename resourcequota 02/27/23 11:29:27.664
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:27.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:27.713
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 02/27/23 11:29:27.723
    STEP: Creating a ResourceQuota 02/27/23 11:29:32.739
    STEP: Ensuring resource quota status is calculated 02/27/23 11:29:32.755
    STEP: Creating a Pod that fits quota 02/27/23 11:29:34.772
    STEP: Ensuring ResourceQuota status captures the pod usage 02/27/23 11:29:34.802
    STEP: Not allowing a pod to be created that exceeds remaining quota 02/27/23 11:29:36.81
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/27/23 11:29:36.816
    STEP: Ensuring a pod cannot update its resource requirements 02/27/23 11:29:36.824
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/27/23 11:29:36.833
    STEP: Deleting the pod 02/27/23 11:29:38.842
    STEP: Ensuring resource quota status released the pod usage 02/27/23 11:29:38.876
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 27 11:29:40.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8886" for this suite. 02/27/23 11:29:40.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:29:40.926
Feb 27 11:29:40.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename resourcequota 02/27/23 11:29:40.927
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:40.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:40.972
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 02/27/23 11:29:40.984
STEP: Creating a ResourceQuota 02/27/23 11:29:45.991
STEP: Ensuring resource quota status is calculated 02/27/23 11:29:46.004
STEP: Creating a ReplicationController 02/27/23 11:29:48.017
STEP: Ensuring resource quota status captures replication controller creation 02/27/23 11:29:48.042
STEP: Deleting a ReplicationController 02/27/23 11:29:50.054
STEP: Ensuring resource quota status released usage 02/27/23 11:29:50.067
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 27 11:29:52.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2753" for this suite. 02/27/23 11:29:52.092
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":69,"skipped":1454,"failed":0}
------------------------------
• [SLOW TEST] [11.182 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:29:40.926
    Feb 27 11:29:40.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename resourcequota 02/27/23 11:29:40.927
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:40.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:40.972
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 02/27/23 11:29:40.984
    STEP: Creating a ResourceQuota 02/27/23 11:29:45.991
    STEP: Ensuring resource quota status is calculated 02/27/23 11:29:46.004
    STEP: Creating a ReplicationController 02/27/23 11:29:48.017
    STEP: Ensuring resource quota status captures replication controller creation 02/27/23 11:29:48.042
    STEP: Deleting a ReplicationController 02/27/23 11:29:50.054
    STEP: Ensuring resource quota status released usage 02/27/23 11:29:50.067
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 27 11:29:52.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2753" for this suite. 02/27/23 11:29:52.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:29:52.109
Feb 27 11:29:52.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename disruption 02/27/23 11:29:52.111
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:52.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:52.165
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 02/27/23 11:29:52.173
STEP: Waiting for the pdb to be processed 02/27/23 11:29:52.183
STEP: updating the pdb 02/27/23 11:29:52.193
STEP: Waiting for the pdb to be processed 02/27/23 11:29:52.223
STEP: patching the pdb 02/27/23 11:29:52.234
STEP: Waiting for the pdb to be processed 02/27/23 11:29:52.252
STEP: Waiting for the pdb to be deleted 02/27/23 11:29:52.286
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 27 11:29:52.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6734" for this suite. 02/27/23 11:29:52.306
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":70,"skipped":1459,"failed":0}
------------------------------
• [0.215 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:29:52.109
    Feb 27 11:29:52.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename disruption 02/27/23 11:29:52.111
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:52.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:52.165
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 02/27/23 11:29:52.173
    STEP: Waiting for the pdb to be processed 02/27/23 11:29:52.183
    STEP: updating the pdb 02/27/23 11:29:52.193
    STEP: Waiting for the pdb to be processed 02/27/23 11:29:52.223
    STEP: patching the pdb 02/27/23 11:29:52.234
    STEP: Waiting for the pdb to be processed 02/27/23 11:29:52.252
    STEP: Waiting for the pdb to be deleted 02/27/23 11:29:52.286
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 27 11:29:52.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6734" for this suite. 02/27/23 11:29:52.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:29:52.33
Feb 27 11:29:52.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename resourcequota 02/27/23 11:29:52.331
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:52.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:52.371
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 02/27/23 11:29:52.383
STEP: Ensuring ResourceQuota status is calculated 02/27/23 11:29:52.408
STEP: Creating a ResourceQuota with not best effort scope 02/27/23 11:29:54.426
STEP: Ensuring ResourceQuota status is calculated 02/27/23 11:29:54.437
STEP: Creating a best-effort pod 02/27/23 11:29:56.451
STEP: Ensuring resource quota with best effort scope captures the pod usage 02/27/23 11:29:56.483
STEP: Ensuring resource quota with not best effort ignored the pod usage 02/27/23 11:29:58.501
STEP: Deleting the pod 02/27/23 11:30:00.511
STEP: Ensuring resource quota status released the pod usage 02/27/23 11:30:00.998
STEP: Creating a not best-effort pod 02/27/23 11:30:03.011
STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/27/23 11:30:03.033
STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/27/23 11:30:05.055
STEP: Deleting the pod 02/27/23 11:30:07.065
STEP: Ensuring resource quota status released the pod usage 02/27/23 11:30:07.092
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 27 11:30:09.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9441" for this suite. 02/27/23 11:30:09.113
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":71,"skipped":1494,"failed":0}
------------------------------
• [SLOW TEST] [16.803 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:29:52.33
    Feb 27 11:29:52.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename resourcequota 02/27/23 11:29:52.331
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:29:52.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:29:52.371
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 02/27/23 11:29:52.383
    STEP: Ensuring ResourceQuota status is calculated 02/27/23 11:29:52.408
    STEP: Creating a ResourceQuota with not best effort scope 02/27/23 11:29:54.426
    STEP: Ensuring ResourceQuota status is calculated 02/27/23 11:29:54.437
    STEP: Creating a best-effort pod 02/27/23 11:29:56.451
    STEP: Ensuring resource quota with best effort scope captures the pod usage 02/27/23 11:29:56.483
    STEP: Ensuring resource quota with not best effort ignored the pod usage 02/27/23 11:29:58.501
    STEP: Deleting the pod 02/27/23 11:30:00.511
    STEP: Ensuring resource quota status released the pod usage 02/27/23 11:30:00.998
    STEP: Creating a not best-effort pod 02/27/23 11:30:03.011
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/27/23 11:30:03.033
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/27/23 11:30:05.055
    STEP: Deleting the pod 02/27/23 11:30:07.065
    STEP: Ensuring resource quota status released the pod usage 02/27/23 11:30:07.092
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 27 11:30:09.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9441" for this suite. 02/27/23 11:30:09.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:30:09.137
Feb 27 11:30:09.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 11:30:09.139
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:30:09.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:30:09.194
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-5951 02/27/23 11:30:09.207
Feb 27 11:30:09.226: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-5951" to be "running and ready"
Feb 27 11:30:09.248: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 22.23597ms
Feb 27 11:30:09.248: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:30:11.259: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.032551722s
Feb 27 11:30:11.259: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 27 11:30:11.259: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Feb 27 11:30:11.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-5951 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 27 11:30:12.072: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 27 11:30:12.072: INFO: stdout: "ipvs"
Feb 27 11:30:12.072: INFO: proxyMode: ipvs
Feb 27 11:30:12.096: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 27 11:30:12.104: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-5951 02/27/23 11:30:12.104
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5951 02/27/23 11:30:12.136
I0227 11:30:12.152690      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5951, replica count: 3
I0227 11:30:15.205060      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 11:30:15.225: INFO: Creating new exec pod
Feb 27 11:30:15.241: INFO: Waiting up to 5m0s for pod "execpod-affinitytlstz" in namespace "services-5951" to be "running"
Feb 27 11:30:15.249: INFO: Pod "execpod-affinitytlstz": Phase="Pending", Reason="", readiness=false. Elapsed: 7.57456ms
Feb 27 11:30:17.259: INFO: Pod "execpod-affinitytlstz": Phase="Running", Reason="", readiness=true. Elapsed: 2.01774061s
Feb 27 11:30:17.259: INFO: Pod "execpod-affinitytlstz" satisfied condition "running"
Feb 27 11:30:18.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-5951 exec execpod-affinitytlstz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Feb 27 11:30:18.655: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Feb 27 11:30:18.655: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:30:18.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-5951 exec execpod-affinitytlstz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.212 80'
Feb 27 11:30:19.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.20.212 80\nConnection to 10.240.20.212 80 port [tcp/http] succeeded!\n"
Feb 27 11:30:19.369: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:30:19.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-5951 exec execpod-affinitytlstz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.20.212:80/ ; done'
Feb 27 11:30:19.747: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n"
Feb 27 11:30:19.748: INFO: stdout: "\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r"
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
Feb 27 11:30:19.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-5951 exec execpod-affinitytlstz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.20.212:80/'
Feb 27 11:30:19.992: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n"
Feb 27 11:30:19.992: INFO: stdout: "affinity-clusterip-timeout-76c2r"
Feb 27 11:32:29.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-5951 exec execpod-affinitytlstz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.20.212:80/'
Feb 27 11:32:30.456: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n"
Feb 27 11:32:30.456: INFO: stdout: "affinity-clusterip-timeout-4f795"
Feb 27 11:32:30.456: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5951, will wait for the garbage collector to delete the pods 02/27/23 11:32:30.481
Feb 27 11:32:30.565: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 14.151397ms
Feb 27 11:32:30.665: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.255926ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 11:32:32.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5951" for this suite. 02/27/23 11:32:32.826
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":72,"skipped":1501,"failed":0}
------------------------------
• [SLOW TEST] [143.701 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:30:09.137
    Feb 27 11:30:09.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 11:30:09.139
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:30:09.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:30:09.194
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-5951 02/27/23 11:30:09.207
    Feb 27 11:30:09.226: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-5951" to be "running and ready"
    Feb 27 11:30:09.248: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 22.23597ms
    Feb 27 11:30:09.248: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:30:11.259: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.032551722s
    Feb 27 11:30:11.259: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Feb 27 11:30:11.259: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Feb 27 11:30:11.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-5951 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Feb 27 11:30:12.072: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Feb 27 11:30:12.072: INFO: stdout: "ipvs"
    Feb 27 11:30:12.072: INFO: proxyMode: ipvs
    Feb 27 11:30:12.096: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Feb 27 11:30:12.104: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-5951 02/27/23 11:30:12.104
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-5951 02/27/23 11:30:12.136
    I0227 11:30:12.152690      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5951, replica count: 3
    I0227 11:30:15.205060      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 11:30:15.225: INFO: Creating new exec pod
    Feb 27 11:30:15.241: INFO: Waiting up to 5m0s for pod "execpod-affinitytlstz" in namespace "services-5951" to be "running"
    Feb 27 11:30:15.249: INFO: Pod "execpod-affinitytlstz": Phase="Pending", Reason="", readiness=false. Elapsed: 7.57456ms
    Feb 27 11:30:17.259: INFO: Pod "execpod-affinitytlstz": Phase="Running", Reason="", readiness=true. Elapsed: 2.01774061s
    Feb 27 11:30:17.259: INFO: Pod "execpod-affinitytlstz" satisfied condition "running"
    Feb 27 11:30:18.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-5951 exec execpod-affinitytlstz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Feb 27 11:30:18.655: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Feb 27 11:30:18.655: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:30:18.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-5951 exec execpod-affinitytlstz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.212 80'
    Feb 27 11:30:19.369: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.20.212 80\nConnection to 10.240.20.212 80 port [tcp/http] succeeded!\n"
    Feb 27 11:30:19.369: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:30:19.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-5951 exec execpod-affinitytlstz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.20.212:80/ ; done'
    Feb 27 11:30:19.747: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n"
    Feb 27 11:30:19.748: INFO: stdout: "\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r\naffinity-clusterip-timeout-76c2r"
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Received response from host: affinity-clusterip-timeout-76c2r
    Feb 27 11:30:19.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-5951 exec execpod-affinitytlstz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.20.212:80/'
    Feb 27 11:30:19.992: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n"
    Feb 27 11:30:19.992: INFO: stdout: "affinity-clusterip-timeout-76c2r"
    Feb 27 11:32:29.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-5951 exec execpod-affinitytlstz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.20.212:80/'
    Feb 27 11:32:30.456: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.20.212:80/\n"
    Feb 27 11:32:30.456: INFO: stdout: "affinity-clusterip-timeout-4f795"
    Feb 27 11:32:30.456: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5951, will wait for the garbage collector to delete the pods 02/27/23 11:32:30.481
    Feb 27 11:32:30.565: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 14.151397ms
    Feb 27 11:32:30.665: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.255926ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 11:32:32.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5951" for this suite. 02/27/23 11:32:32.826
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:32:32.84
Feb 27 11:32:32.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename svcaccounts 02/27/23 11:32:32.841
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:32:32.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:32:32.931
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Feb 27 11:32:33.001: INFO: created pod pod-service-account-defaultsa
Feb 27 11:32:33.001: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 27 11:32:33.016: INFO: created pod pod-service-account-mountsa
Feb 27 11:32:33.016: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 27 11:32:33.221: INFO: created pod pod-service-account-nomountsa
Feb 27 11:32:33.221: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 27 11:32:33.250: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 27 11:32:33.250: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 27 11:32:33.285: INFO: created pod pod-service-account-mountsa-mountspec
Feb 27 11:32:33.285: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 27 11:32:33.323: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 27 11:32:33.336: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 27 11:32:33.356: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 27 11:32:33.356: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 27 11:32:33.385: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 27 11:32:33.385: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 27 11:32:33.406: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 27 11:32:33.407: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 27 11:32:33.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-517" for this suite. 02/27/23 11:32:33.435
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":73,"skipped":1529,"failed":0}
------------------------------
• [0.620 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:32:32.84
    Feb 27 11:32:32.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 11:32:32.841
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:32:32.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:32:32.931
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Feb 27 11:32:33.001: INFO: created pod pod-service-account-defaultsa
    Feb 27 11:32:33.001: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Feb 27 11:32:33.016: INFO: created pod pod-service-account-mountsa
    Feb 27 11:32:33.016: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Feb 27 11:32:33.221: INFO: created pod pod-service-account-nomountsa
    Feb 27 11:32:33.221: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Feb 27 11:32:33.250: INFO: created pod pod-service-account-defaultsa-mountspec
    Feb 27 11:32:33.250: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Feb 27 11:32:33.285: INFO: created pod pod-service-account-mountsa-mountspec
    Feb 27 11:32:33.285: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Feb 27 11:32:33.323: INFO: created pod pod-service-account-nomountsa-mountspec
    Feb 27 11:32:33.336: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Feb 27 11:32:33.356: INFO: created pod pod-service-account-defaultsa-nomountspec
    Feb 27 11:32:33.356: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Feb 27 11:32:33.385: INFO: created pod pod-service-account-mountsa-nomountspec
    Feb 27 11:32:33.385: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Feb 27 11:32:33.406: INFO: created pod pod-service-account-nomountsa-nomountspec
    Feb 27 11:32:33.407: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 27 11:32:33.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-517" for this suite. 02/27/23 11:32:33.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:32:33.466
Feb 27 11:32:33.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-probe 02/27/23 11:32:33.468
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:32:33.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:32:33.535
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-727efddd-4241-4a0e-86dc-0196019e80fc in namespace container-probe-3594 02/27/23 11:32:33.544
Feb 27 11:32:33.563: INFO: Waiting up to 5m0s for pod "liveness-727efddd-4241-4a0e-86dc-0196019e80fc" in namespace "container-probe-3594" to be "not pending"
Feb 27 11:32:33.575: INFO: Pod "liveness-727efddd-4241-4a0e-86dc-0196019e80fc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.562946ms
Feb 27 11:32:35.583: INFO: Pod "liveness-727efddd-4241-4a0e-86dc-0196019e80fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0194914s
Feb 27 11:32:37.586: INFO: Pod "liveness-727efddd-4241-4a0e-86dc-0196019e80fc": Phase="Running", Reason="", readiness=true. Elapsed: 4.022622066s
Feb 27 11:32:37.586: INFO: Pod "liveness-727efddd-4241-4a0e-86dc-0196019e80fc" satisfied condition "not pending"
Feb 27 11:32:37.586: INFO: Started pod liveness-727efddd-4241-4a0e-86dc-0196019e80fc in namespace container-probe-3594
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 11:32:37.586
Feb 27 11:32:37.597: INFO: Initial restart count of pod liveness-727efddd-4241-4a0e-86dc-0196019e80fc is 0
Feb 27 11:32:55.709: INFO: Restart count of pod container-probe-3594/liveness-727efddd-4241-4a0e-86dc-0196019e80fc is now 1 (18.112046171s elapsed)
STEP: deleting the pod 02/27/23 11:32:55.709
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 27 11:32:55.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3594" for this suite. 02/27/23 11:32:55.765
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":74,"skipped":1537,"failed":0}
------------------------------
• [SLOW TEST] [22.331 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:32:33.466
    Feb 27 11:32:33.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-probe 02/27/23 11:32:33.468
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:32:33.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:32:33.535
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-727efddd-4241-4a0e-86dc-0196019e80fc in namespace container-probe-3594 02/27/23 11:32:33.544
    Feb 27 11:32:33.563: INFO: Waiting up to 5m0s for pod "liveness-727efddd-4241-4a0e-86dc-0196019e80fc" in namespace "container-probe-3594" to be "not pending"
    Feb 27 11:32:33.575: INFO: Pod "liveness-727efddd-4241-4a0e-86dc-0196019e80fc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.562946ms
    Feb 27 11:32:35.583: INFO: Pod "liveness-727efddd-4241-4a0e-86dc-0196019e80fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0194914s
    Feb 27 11:32:37.586: INFO: Pod "liveness-727efddd-4241-4a0e-86dc-0196019e80fc": Phase="Running", Reason="", readiness=true. Elapsed: 4.022622066s
    Feb 27 11:32:37.586: INFO: Pod "liveness-727efddd-4241-4a0e-86dc-0196019e80fc" satisfied condition "not pending"
    Feb 27 11:32:37.586: INFO: Started pod liveness-727efddd-4241-4a0e-86dc-0196019e80fc in namespace container-probe-3594
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 11:32:37.586
    Feb 27 11:32:37.597: INFO: Initial restart count of pod liveness-727efddd-4241-4a0e-86dc-0196019e80fc is 0
    Feb 27 11:32:55.709: INFO: Restart count of pod container-probe-3594/liveness-727efddd-4241-4a0e-86dc-0196019e80fc is now 1 (18.112046171s elapsed)
    STEP: deleting the pod 02/27/23 11:32:55.709
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 27 11:32:55.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3594" for this suite. 02/27/23 11:32:55.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:32:55.806
Feb 27 11:32:55.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename namespaces 02/27/23 11:32:55.809
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:32:55.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:32:55.859
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 02/27/23 11:32:55.869
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:32:55.919
STEP: Creating a pod in the namespace 02/27/23 11:32:55.941
STEP: Waiting for the pod to have running status 02/27/23 11:32:55.964
Feb 27 11:32:55.964: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6268" to be "running"
Feb 27 11:32:55.978: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.654196ms
Feb 27 11:32:57.990: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026010026s
Feb 27 11:32:57.990: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 02/27/23 11:32:57.99
STEP: Waiting for the namespace to be removed. 02/27/23 11:32:58.004
STEP: Recreating the namespace 02/27/23 11:33:10.014
STEP: Verifying there are no pods in the namespace 02/27/23 11:33:10.047
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 27 11:33:10.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9216" for this suite. 02/27/23 11:33:10.08
STEP: Destroying namespace "nsdeletetest-6268" for this suite. 02/27/23 11:33:10.092
Feb 27 11:33:10.099: INFO: Namespace nsdeletetest-6268 was already deleted
STEP: Destroying namespace "nsdeletetest-1953" for this suite. 02/27/23 11:33:10.099
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":75,"skipped":1547,"failed":0}
------------------------------
• [SLOW TEST] [14.307 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:32:55.806
    Feb 27 11:32:55.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename namespaces 02/27/23 11:32:55.809
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:32:55.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:32:55.859
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 02/27/23 11:32:55.869
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:32:55.919
    STEP: Creating a pod in the namespace 02/27/23 11:32:55.941
    STEP: Waiting for the pod to have running status 02/27/23 11:32:55.964
    Feb 27 11:32:55.964: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6268" to be "running"
    Feb 27 11:32:55.978: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.654196ms
    Feb 27 11:32:57.990: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026010026s
    Feb 27 11:32:57.990: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 02/27/23 11:32:57.99
    STEP: Waiting for the namespace to be removed. 02/27/23 11:32:58.004
    STEP: Recreating the namespace 02/27/23 11:33:10.014
    STEP: Verifying there are no pods in the namespace 02/27/23 11:33:10.047
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 11:33:10.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9216" for this suite. 02/27/23 11:33:10.08
    STEP: Destroying namespace "nsdeletetest-6268" for this suite. 02/27/23 11:33:10.092
    Feb 27 11:33:10.099: INFO: Namespace nsdeletetest-6268 was already deleted
    STEP: Destroying namespace "nsdeletetest-1953" for this suite. 02/27/23 11:33:10.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:33:10.119
Feb 27 11:33:10.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 11:33:10.121
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:10.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:10.196
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 02/27/23 11:33:10.211
Feb 27 11:33:10.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32" in namespace "downward-api-2759" to be "Succeeded or Failed"
Feb 27 11:33:10.246: INFO: Pod "downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32": Phase="Pending", Reason="", readiness=false. Elapsed: 16.443058ms
Feb 27 11:33:12.257: INFO: Pod "downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027301627s
Feb 27 11:33:14.255: INFO: Pod "downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025004515s
STEP: Saw pod success 02/27/23 11:33:14.255
Feb 27 11:33:14.255: INFO: Pod "downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32" satisfied condition "Succeeded or Failed"
Feb 27 11:33:14.263: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32 container client-container: <nil>
STEP: delete the pod 02/27/23 11:33:14.284
Feb 27 11:33:14.305: INFO: Waiting for pod downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32 to disappear
Feb 27 11:33:14.311: INFO: Pod downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 27 11:33:14.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2759" for this suite. 02/27/23 11:33:14.325
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":76,"skipped":1587,"failed":0}
------------------------------
• [4.217 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:33:10.119
    Feb 27 11:33:10.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 11:33:10.121
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:10.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:10.196
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 02/27/23 11:33:10.211
    Feb 27 11:33:10.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32" in namespace "downward-api-2759" to be "Succeeded or Failed"
    Feb 27 11:33:10.246: INFO: Pod "downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32": Phase="Pending", Reason="", readiness=false. Elapsed: 16.443058ms
    Feb 27 11:33:12.257: INFO: Pod "downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027301627s
    Feb 27 11:33:14.255: INFO: Pod "downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025004515s
    STEP: Saw pod success 02/27/23 11:33:14.255
    Feb 27 11:33:14.255: INFO: Pod "downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32" satisfied condition "Succeeded or Failed"
    Feb 27 11:33:14.263: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32 container client-container: <nil>
    STEP: delete the pod 02/27/23 11:33:14.284
    Feb 27 11:33:14.305: INFO: Waiting for pod downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32 to disappear
    Feb 27 11:33:14.311: INFO: Pod downwardapi-volume-31a3a1ac-1621-4cc8-820a-4365f2030e32 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 27 11:33:14.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2759" for this suite. 02/27/23 11:33:14.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:33:14.346
Feb 27 11:33:14.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 11:33:14.348
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:14.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:14.397
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 02/27/23 11:33:14.407
Feb 27 11:33:14.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 create -f -'
Feb 27 11:33:14.765: INFO: stderr: ""
Feb 27 11:33:14.765: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 11:33:14.765
Feb 27 11:33:14.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 11:33:14.901: INFO: stderr: ""
Feb 27 11:33:14.901: INFO: stdout: "update-demo-nautilus-j76cj update-demo-nautilus-x67wk "
Feb 27 11:33:14.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 11:33:15.015: INFO: stderr: ""
Feb 27 11:33:15.015: INFO: stdout: ""
Feb 27 11:33:15.015: INFO: update-demo-nautilus-j76cj is created but not running
Feb 27 11:33:20.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 11:33:20.136: INFO: stderr: ""
Feb 27 11:33:20.136: INFO: stdout: "update-demo-nautilus-j76cj update-demo-nautilus-x67wk "
Feb 27 11:33:20.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 11:33:20.243: INFO: stderr: ""
Feb 27 11:33:20.243: INFO: stdout: "true"
Feb 27 11:33:20.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 11:33:20.352: INFO: stderr: ""
Feb 27 11:33:20.352: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 27 11:33:20.352: INFO: validating pod update-demo-nautilus-j76cj
Feb 27 11:33:20.368: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 11:33:20.368: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 11:33:20.368: INFO: update-demo-nautilus-j76cj is verified up and running
Feb 27 11:33:20.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-x67wk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 11:33:20.465: INFO: stderr: ""
Feb 27 11:33:20.466: INFO: stdout: "true"
Feb 27 11:33:20.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-x67wk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 11:33:20.572: INFO: stderr: ""
Feb 27 11:33:20.572: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 27 11:33:20.572: INFO: validating pod update-demo-nautilus-x67wk
Feb 27 11:33:20.605: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 11:33:20.605: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 11:33:20.605: INFO: update-demo-nautilus-x67wk is verified up and running
STEP: scaling down the replication controller 02/27/23 11:33:20.605
Feb 27 11:33:20.607: INFO: scanned /root for discovery docs: <nil>
Feb 27 11:33:20.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb 27 11:33:21.763: INFO: stderr: ""
Feb 27 11:33:21.763: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 11:33:21.763
Feb 27 11:33:21.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 11:33:21.872: INFO: stderr: ""
Feb 27 11:33:21.872: INFO: stdout: "update-demo-nautilus-j76cj "
Feb 27 11:33:21.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 11:33:21.961: INFO: stderr: ""
Feb 27 11:33:21.961: INFO: stdout: "true"
Feb 27 11:33:21.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 11:33:22.056: INFO: stderr: ""
Feb 27 11:33:22.056: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 27 11:33:22.056: INFO: validating pod update-demo-nautilus-j76cj
Feb 27 11:33:22.072: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 11:33:22.072: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 11:33:22.072: INFO: update-demo-nautilus-j76cj is verified up and running
STEP: scaling up the replication controller 02/27/23 11:33:22.072
Feb 27 11:33:22.074: INFO: scanned /root for discovery docs: <nil>
Feb 27 11:33:22.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb 27 11:33:23.206: INFO: stderr: ""
Feb 27 11:33:23.206: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 11:33:23.206
Feb 27 11:33:23.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 11:33:23.349: INFO: stderr: ""
Feb 27 11:33:23.349: INFO: stdout: "update-demo-nautilus-2kqln update-demo-nautilus-j76cj "
Feb 27 11:33:23.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-2kqln -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 11:33:23.439: INFO: stderr: ""
Feb 27 11:33:23.439: INFO: stdout: ""
Feb 27 11:33:23.439: INFO: update-demo-nautilus-2kqln is created but not running
Feb 27 11:33:28.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 11:33:28.557: INFO: stderr: ""
Feb 27 11:33:28.557: INFO: stdout: "update-demo-nautilus-2kqln update-demo-nautilus-j76cj "
Feb 27 11:33:28.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-2kqln -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 11:33:28.657: INFO: stderr: ""
Feb 27 11:33:28.657: INFO: stdout: "true"
Feb 27 11:33:28.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-2kqln -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 11:33:28.763: INFO: stderr: ""
Feb 27 11:33:28.763: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 27 11:33:28.763: INFO: validating pod update-demo-nautilus-2kqln
Feb 27 11:33:28.780: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 11:33:28.780: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 11:33:28.780: INFO: update-demo-nautilus-2kqln is verified up and running
Feb 27 11:33:28.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 11:33:28.874: INFO: stderr: ""
Feb 27 11:33:28.874: INFO: stdout: "true"
Feb 27 11:33:28.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 11:33:28.968: INFO: stderr: ""
Feb 27 11:33:28.969: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 27 11:33:28.969: INFO: validating pod update-demo-nautilus-j76cj
Feb 27 11:33:28.983: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 11:33:28.983: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 11:33:28.983: INFO: update-demo-nautilus-j76cj is verified up and running
STEP: using delete to clean up resources 02/27/23 11:33:28.983
Feb 27 11:33:28.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 delete --grace-period=0 --force -f -'
Feb 27 11:33:29.087: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 11:33:29.087: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 27 11:33:29.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get rc,svc -l name=update-demo --no-headers'
Feb 27 11:33:29.272: INFO: stderr: "No resources found in kubectl-7951 namespace.\n"
Feb 27 11:33:29.272: INFO: stdout: ""
Feb 27 11:33:29.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 27 11:33:29.432: INFO: stderr: ""
Feb 27 11:33:29.432: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 11:33:29.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7951" for this suite. 02/27/23 11:33:29.446
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":77,"skipped":1615,"failed":0}
------------------------------
• [SLOW TEST] [15.132 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:33:14.346
    Feb 27 11:33:14.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 11:33:14.348
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:14.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:14.397
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 02/27/23 11:33:14.407
    Feb 27 11:33:14.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 create -f -'
    Feb 27 11:33:14.765: INFO: stderr: ""
    Feb 27 11:33:14.765: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 11:33:14.765
    Feb 27 11:33:14.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 11:33:14.901: INFO: stderr: ""
    Feb 27 11:33:14.901: INFO: stdout: "update-demo-nautilus-j76cj update-demo-nautilus-x67wk "
    Feb 27 11:33:14.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 11:33:15.015: INFO: stderr: ""
    Feb 27 11:33:15.015: INFO: stdout: ""
    Feb 27 11:33:15.015: INFO: update-demo-nautilus-j76cj is created but not running
    Feb 27 11:33:20.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 11:33:20.136: INFO: stderr: ""
    Feb 27 11:33:20.136: INFO: stdout: "update-demo-nautilus-j76cj update-demo-nautilus-x67wk "
    Feb 27 11:33:20.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 11:33:20.243: INFO: stderr: ""
    Feb 27 11:33:20.243: INFO: stdout: "true"
    Feb 27 11:33:20.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 11:33:20.352: INFO: stderr: ""
    Feb 27 11:33:20.352: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 27 11:33:20.352: INFO: validating pod update-demo-nautilus-j76cj
    Feb 27 11:33:20.368: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 11:33:20.368: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 11:33:20.368: INFO: update-demo-nautilus-j76cj is verified up and running
    Feb 27 11:33:20.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-x67wk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 11:33:20.465: INFO: stderr: ""
    Feb 27 11:33:20.466: INFO: stdout: "true"
    Feb 27 11:33:20.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-x67wk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 11:33:20.572: INFO: stderr: ""
    Feb 27 11:33:20.572: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 27 11:33:20.572: INFO: validating pod update-demo-nautilus-x67wk
    Feb 27 11:33:20.605: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 11:33:20.605: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 11:33:20.605: INFO: update-demo-nautilus-x67wk is verified up and running
    STEP: scaling down the replication controller 02/27/23 11:33:20.605
    Feb 27 11:33:20.607: INFO: scanned /root for discovery docs: <nil>
    Feb 27 11:33:20.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Feb 27 11:33:21.763: INFO: stderr: ""
    Feb 27 11:33:21.763: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 11:33:21.763
    Feb 27 11:33:21.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 11:33:21.872: INFO: stderr: ""
    Feb 27 11:33:21.872: INFO: stdout: "update-demo-nautilus-j76cj "
    Feb 27 11:33:21.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 11:33:21.961: INFO: stderr: ""
    Feb 27 11:33:21.961: INFO: stdout: "true"
    Feb 27 11:33:21.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 11:33:22.056: INFO: stderr: ""
    Feb 27 11:33:22.056: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 27 11:33:22.056: INFO: validating pod update-demo-nautilus-j76cj
    Feb 27 11:33:22.072: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 11:33:22.072: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 11:33:22.072: INFO: update-demo-nautilus-j76cj is verified up and running
    STEP: scaling up the replication controller 02/27/23 11:33:22.072
    Feb 27 11:33:22.074: INFO: scanned /root for discovery docs: <nil>
    Feb 27 11:33:22.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Feb 27 11:33:23.206: INFO: stderr: ""
    Feb 27 11:33:23.206: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 11:33:23.206
    Feb 27 11:33:23.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 11:33:23.349: INFO: stderr: ""
    Feb 27 11:33:23.349: INFO: stdout: "update-demo-nautilus-2kqln update-demo-nautilus-j76cj "
    Feb 27 11:33:23.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-2kqln -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 11:33:23.439: INFO: stderr: ""
    Feb 27 11:33:23.439: INFO: stdout: ""
    Feb 27 11:33:23.439: INFO: update-demo-nautilus-2kqln is created but not running
    Feb 27 11:33:28.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 11:33:28.557: INFO: stderr: ""
    Feb 27 11:33:28.557: INFO: stdout: "update-demo-nautilus-2kqln update-demo-nautilus-j76cj "
    Feb 27 11:33:28.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-2kqln -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 11:33:28.657: INFO: stderr: ""
    Feb 27 11:33:28.657: INFO: stdout: "true"
    Feb 27 11:33:28.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-2kqln -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 11:33:28.763: INFO: stderr: ""
    Feb 27 11:33:28.763: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 27 11:33:28.763: INFO: validating pod update-demo-nautilus-2kqln
    Feb 27 11:33:28.780: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 11:33:28.780: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 11:33:28.780: INFO: update-demo-nautilus-2kqln is verified up and running
    Feb 27 11:33:28.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 11:33:28.874: INFO: stderr: ""
    Feb 27 11:33:28.874: INFO: stdout: "true"
    Feb 27 11:33:28.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods update-demo-nautilus-j76cj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 11:33:28.968: INFO: stderr: ""
    Feb 27 11:33:28.969: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 27 11:33:28.969: INFO: validating pod update-demo-nautilus-j76cj
    Feb 27 11:33:28.983: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 11:33:28.983: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 11:33:28.983: INFO: update-demo-nautilus-j76cj is verified up and running
    STEP: using delete to clean up resources 02/27/23 11:33:28.983
    Feb 27 11:33:28.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 delete --grace-period=0 --force -f -'
    Feb 27 11:33:29.087: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 11:33:29.087: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 27 11:33:29.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get rc,svc -l name=update-demo --no-headers'
    Feb 27 11:33:29.272: INFO: stderr: "No resources found in kubectl-7951 namespace.\n"
    Feb 27 11:33:29.272: INFO: stdout: ""
    Feb 27 11:33:29.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7951 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 27 11:33:29.432: INFO: stderr: ""
    Feb 27 11:33:29.432: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 11:33:29.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7951" for this suite. 02/27/23 11:33:29.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:33:29.512
Feb 27 11:33:29.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename runtimeclass 02/27/23 11:33:29.513
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:29.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:29.564
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Feb 27 11:33:29.600: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3460 to be scheduled
Feb 27 11:33:29.610: INFO: 1 pods are not scheduled: [runtimeclass-3460/test-runtimeclass-runtimeclass-3460-preconfigured-handler-8mzzn(cb343872-176c-4079-87e3-7fe6a8fd6a24)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 27 11:33:31.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3460" for this suite. 02/27/23 11:33:31.641
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":78,"skipped":1658,"failed":0}
------------------------------
• [2.143 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:33:29.512
    Feb 27 11:33:29.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 11:33:29.513
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:29.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:29.564
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Feb 27 11:33:29.600: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3460 to be scheduled
    Feb 27 11:33:29.610: INFO: 1 pods are not scheduled: [runtimeclass-3460/test-runtimeclass-runtimeclass-3460-preconfigured-handler-8mzzn(cb343872-176c-4079-87e3-7fe6a8fd6a24)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 27 11:33:31.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3460" for this suite. 02/27/23 11:33:31.641
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:33:31.658
Feb 27 11:33:31.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 11:33:31.66
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:32.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:32.038
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 02/27/23 11:33:32.048
Feb 27 11:33:32.064: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8" in namespace "downward-api-5550" to be "Succeeded or Failed"
Feb 27 11:33:32.072: INFO: Pod "downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.453915ms
Feb 27 11:33:34.080: INFO: Pod "downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015876611s
Feb 27 11:33:36.081: INFO: Pod "downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017008588s
STEP: Saw pod success 02/27/23 11:33:36.081
Feb 27 11:33:36.081: INFO: Pod "downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8" satisfied condition "Succeeded or Failed"
Feb 27 11:33:36.089: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8 container client-container: <nil>
STEP: delete the pod 02/27/23 11:33:36.11
Feb 27 11:33:36.131: INFO: Waiting for pod downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8 to disappear
Feb 27 11:33:36.140: INFO: Pod downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 27 11:33:36.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5550" for this suite. 02/27/23 11:33:36.158
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":79,"skipped":1662,"failed":0}
------------------------------
• [4.519 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:33:31.658
    Feb 27 11:33:31.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 11:33:31.66
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:32.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:32.038
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 02/27/23 11:33:32.048
    Feb 27 11:33:32.064: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8" in namespace "downward-api-5550" to be "Succeeded or Failed"
    Feb 27 11:33:32.072: INFO: Pod "downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.453915ms
    Feb 27 11:33:34.080: INFO: Pod "downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015876611s
    Feb 27 11:33:36.081: INFO: Pod "downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017008588s
    STEP: Saw pod success 02/27/23 11:33:36.081
    Feb 27 11:33:36.081: INFO: Pod "downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8" satisfied condition "Succeeded or Failed"
    Feb 27 11:33:36.089: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8 container client-container: <nil>
    STEP: delete the pod 02/27/23 11:33:36.11
    Feb 27 11:33:36.131: INFO: Waiting for pod downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8 to disappear
    Feb 27 11:33:36.140: INFO: Pod downwardapi-volume-04ffec03-6e81-4022-8a85-38c6a366eca8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 27 11:33:36.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5550" for this suite. 02/27/23 11:33:36.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:33:36.185
Feb 27 11:33:36.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 11:33:36.186
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:36.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:36.228
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 02/27/23 11:33:36.237
Feb 27 11:33:36.252: INFO: Waiting up to 5m0s for pod "labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca" in namespace "downward-api-1965" to be "running and ready"
Feb 27 11:33:36.261: INFO: Pod "labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca": Phase="Pending", Reason="", readiness=false. Elapsed: 9.250478ms
Feb 27 11:33:36.262: INFO: The phase of Pod labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:33:38.269: INFO: Pod "labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca": Phase="Running", Reason="", readiness=true. Elapsed: 2.016762168s
Feb 27 11:33:38.269: INFO: The phase of Pod labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca is Running (Ready = true)
Feb 27 11:33:38.269: INFO: Pod "labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca" satisfied condition "running and ready"
Feb 27 11:33:38.817: INFO: Successfully updated pod "labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 27 11:33:42.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1965" for this suite. 02/27/23 11:33:42.896
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":80,"skipped":1686,"failed":0}
------------------------------
• [SLOW TEST] [6.726 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:33:36.185
    Feb 27 11:33:36.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 11:33:36.186
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:36.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:36.228
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 02/27/23 11:33:36.237
    Feb 27 11:33:36.252: INFO: Waiting up to 5m0s for pod "labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca" in namespace "downward-api-1965" to be "running and ready"
    Feb 27 11:33:36.261: INFO: Pod "labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca": Phase="Pending", Reason="", readiness=false. Elapsed: 9.250478ms
    Feb 27 11:33:36.262: INFO: The phase of Pod labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:33:38.269: INFO: Pod "labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca": Phase="Running", Reason="", readiness=true. Elapsed: 2.016762168s
    Feb 27 11:33:38.269: INFO: The phase of Pod labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca is Running (Ready = true)
    Feb 27 11:33:38.269: INFO: Pod "labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca" satisfied condition "running and ready"
    Feb 27 11:33:38.817: INFO: Successfully updated pod "labelsupdate7236f3d6-ed49-41a0-aa30-db264f3403ca"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 27 11:33:42.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1965" for this suite. 02/27/23 11:33:42.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:33:42.915
Feb 27 11:33:42.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 11:33:42.917
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:42.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:42.952
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-6777 02/27/23 11:33:42.961
STEP: creating service affinity-nodeport-transition in namespace services-6777 02/27/23 11:33:42.963
STEP: creating replication controller affinity-nodeport-transition in namespace services-6777 02/27/23 11:33:42.989
I0227 11:33:43.001463      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6777, replica count: 3
I0227 11:33:46.053154      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 11:33:46.085: INFO: Creating new exec pod
Feb 27 11:33:46.102: INFO: Waiting up to 5m0s for pod "execpod-affinityhlhgb" in namespace "services-6777" to be "running"
Feb 27 11:33:46.128: INFO: Pod "execpod-affinityhlhgb": Phase="Pending", Reason="", readiness=false. Elapsed: 25.034334ms
Feb 27 11:33:48.137: INFO: Pod "execpod-affinityhlhgb": Phase="Running", Reason="", readiness=true. Elapsed: 2.034766959s
Feb 27 11:33:48.137: INFO: Pod "execpod-affinityhlhgb" satisfied condition "running"
Feb 27 11:33:49.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-6777 exec execpod-affinityhlhgb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Feb 27 11:33:49.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb 27 11:33:49.363: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:33:49.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-6777 exec execpod-affinityhlhgb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.4 80'
Feb 27 11:33:49.572: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.23.4 80\nConnection to 10.240.23.4 80 port [tcp/http] succeeded!\n"
Feb 27 11:33:49.572: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:33:49.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-6777 exec execpod-affinityhlhgb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.159 31319'
Feb 27 11:33:49.819: INFO: stderr: "+ nc -v -t -w 2 172.31.11.159 31319\nConnection to 172.31.11.159 31319 port [tcp/*] succeeded!\n+ echo hostName\n"
Feb 27 11:33:49.819: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:33:49.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-6777 exec execpod-affinityhlhgb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.167 31319'
Feb 27 11:33:50.035: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.7.167 31319\nConnection to 172.31.7.167 31319 port [tcp/*] succeeded!\n"
Feb 27 11:33:50.035: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:33:50.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-6777 exec execpod-affinityhlhgb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.159:31319/ ; done'
Feb 27 11:33:50.484: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n"
Feb 27 11:33:50.484: INFO: stdout: "\naffinity-nodeport-transition-gch8n\naffinity-nodeport-transition-8wltv\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-gch8n\naffinity-nodeport-transition-8wltv\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-gch8n\naffinity-nodeport-transition-8wltv\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-gch8n\naffinity-nodeport-transition-8wltv\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-gch8n\naffinity-nodeport-transition-8wltv\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-gch8n"
Feb 27 11:33:50.484: INFO: Received response from host: affinity-nodeport-transition-gch8n
Feb 27 11:33:50.484: INFO: Received response from host: affinity-nodeport-transition-8wltv
Feb 27 11:33:50.484: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.484: INFO: Received response from host: affinity-nodeport-transition-gch8n
Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-8wltv
Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-gch8n
Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-8wltv
Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-gch8n
Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-8wltv
Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-gch8n
Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-8wltv
Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-gch8n
Feb 27 11:33:50.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-6777 exec execpod-affinityhlhgb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.159:31319/ ; done'
Feb 27 11:33:50.964: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n"
Feb 27 11:33:50.964: INFO: stdout: "\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl"
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
Feb 27 11:33:50.964: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6777, will wait for the garbage collector to delete the pods 02/27/23 11:33:51.01
Feb 27 11:33:51.098: INFO: Deleting ReplicationController affinity-nodeport-transition took: 15.713852ms
Feb 27 11:33:51.199: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.624851ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 11:33:53.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6777" for this suite. 02/27/23 11:33:53.143
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":81,"skipped":1705,"failed":0}
------------------------------
• [SLOW TEST] [10.241 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:33:42.915
    Feb 27 11:33:42.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 11:33:42.917
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:42.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:42.952
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-6777 02/27/23 11:33:42.961
    STEP: creating service affinity-nodeport-transition in namespace services-6777 02/27/23 11:33:42.963
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6777 02/27/23 11:33:42.989
    I0227 11:33:43.001463      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6777, replica count: 3
    I0227 11:33:46.053154      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 11:33:46.085: INFO: Creating new exec pod
    Feb 27 11:33:46.102: INFO: Waiting up to 5m0s for pod "execpod-affinityhlhgb" in namespace "services-6777" to be "running"
    Feb 27 11:33:46.128: INFO: Pod "execpod-affinityhlhgb": Phase="Pending", Reason="", readiness=false. Elapsed: 25.034334ms
    Feb 27 11:33:48.137: INFO: Pod "execpod-affinityhlhgb": Phase="Running", Reason="", readiness=true. Elapsed: 2.034766959s
    Feb 27 11:33:48.137: INFO: Pod "execpod-affinityhlhgb" satisfied condition "running"
    Feb 27 11:33:49.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-6777 exec execpod-affinityhlhgb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Feb 27 11:33:49.363: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Feb 27 11:33:49.363: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:33:49.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-6777 exec execpod-affinityhlhgb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.4 80'
    Feb 27 11:33:49.572: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.23.4 80\nConnection to 10.240.23.4 80 port [tcp/http] succeeded!\n"
    Feb 27 11:33:49.572: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:33:49.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-6777 exec execpod-affinityhlhgb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.159 31319'
    Feb 27 11:33:49.819: INFO: stderr: "+ nc -v -t -w 2 172.31.11.159 31319\nConnection to 172.31.11.159 31319 port [tcp/*] succeeded!\n+ echo hostName\n"
    Feb 27 11:33:49.819: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:33:49.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-6777 exec execpod-affinityhlhgb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.167 31319'
    Feb 27 11:33:50.035: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.7.167 31319\nConnection to 172.31.7.167 31319 port [tcp/*] succeeded!\n"
    Feb 27 11:33:50.035: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:33:50.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-6777 exec execpod-affinityhlhgb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.159:31319/ ; done'
    Feb 27 11:33:50.484: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n"
    Feb 27 11:33:50.484: INFO: stdout: "\naffinity-nodeport-transition-gch8n\naffinity-nodeport-transition-8wltv\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-gch8n\naffinity-nodeport-transition-8wltv\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-gch8n\naffinity-nodeport-transition-8wltv\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-gch8n\naffinity-nodeport-transition-8wltv\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-gch8n\naffinity-nodeport-transition-8wltv\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-gch8n"
    Feb 27 11:33:50.484: INFO: Received response from host: affinity-nodeport-transition-gch8n
    Feb 27 11:33:50.484: INFO: Received response from host: affinity-nodeport-transition-8wltv
    Feb 27 11:33:50.484: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.484: INFO: Received response from host: affinity-nodeport-transition-gch8n
    Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-8wltv
    Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-gch8n
    Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-8wltv
    Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-gch8n
    Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-8wltv
    Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-gch8n
    Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-8wltv
    Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.485: INFO: Received response from host: affinity-nodeport-transition-gch8n
    Feb 27 11:33:50.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-6777 exec execpod-affinityhlhgb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.159:31319/ ; done'
    Feb 27 11:33:50.964: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31319/\n"
    Feb 27 11:33:50.964: INFO: stdout: "\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl\naffinity-nodeport-transition-n54gl"
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Received response from host: affinity-nodeport-transition-n54gl
    Feb 27 11:33:50.964: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6777, will wait for the garbage collector to delete the pods 02/27/23 11:33:51.01
    Feb 27 11:33:51.098: INFO: Deleting ReplicationController affinity-nodeport-transition took: 15.713852ms
    Feb 27 11:33:51.199: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.624851ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 11:33:53.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6777" for this suite. 02/27/23 11:33:53.143
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:33:53.168
Feb 27 11:33:53.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:33:53.169
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:53.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:53.228
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 02/27/23 11:33:53.238
Feb 27 11:33:53.259: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85" in namespace "projected-9261" to be "Succeeded or Failed"
Feb 27 11:33:53.272: INFO: Pod "downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85": Phase="Pending", Reason="", readiness=false. Elapsed: 12.603918ms
Feb 27 11:33:55.291: INFO: Pod "downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031320051s
Feb 27 11:33:57.281: INFO: Pod "downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021528178s
STEP: Saw pod success 02/27/23 11:33:57.281
Feb 27 11:33:57.281: INFO: Pod "downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85" satisfied condition "Succeeded or Failed"
Feb 27 11:33:57.300: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85 container client-container: <nil>
STEP: delete the pod 02/27/23 11:33:57.351
Feb 27 11:33:57.375: INFO: Waiting for pod downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85 to disappear
Feb 27 11:33:57.382: INFO: Pod downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 27 11:33:57.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9261" for this suite. 02/27/23 11:33:57.393
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":82,"skipped":1736,"failed":0}
------------------------------
• [4.241 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:33:53.168
    Feb 27 11:33:53.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:33:53.169
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:53.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:53.228
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 02/27/23 11:33:53.238
    Feb 27 11:33:53.259: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85" in namespace "projected-9261" to be "Succeeded or Failed"
    Feb 27 11:33:53.272: INFO: Pod "downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85": Phase="Pending", Reason="", readiness=false. Elapsed: 12.603918ms
    Feb 27 11:33:55.291: INFO: Pod "downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031320051s
    Feb 27 11:33:57.281: INFO: Pod "downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021528178s
    STEP: Saw pod success 02/27/23 11:33:57.281
    Feb 27 11:33:57.281: INFO: Pod "downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85" satisfied condition "Succeeded or Failed"
    Feb 27 11:33:57.300: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85 container client-container: <nil>
    STEP: delete the pod 02/27/23 11:33:57.351
    Feb 27 11:33:57.375: INFO: Waiting for pod downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85 to disappear
    Feb 27 11:33:57.382: INFO: Pod downwardapi-volume-c950d70e-7c4f-4f33-aab8-379e3b130d85 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 27 11:33:57.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9261" for this suite. 02/27/23 11:33:57.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:33:57.411
Feb 27 11:33:57.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:33:57.413
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:57.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:57.471
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 02/27/23 11:33:57.481
Feb 27 11:33:57.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9" in namespace "projected-134" to be "Succeeded or Failed"
Feb 27 11:33:57.504: INFO: Pod "downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.454883ms
Feb 27 11:33:59.512: INFO: Pod "downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017290525s
Feb 27 11:34:01.514: INFO: Pod "downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019594286s
STEP: Saw pod success 02/27/23 11:34:01.514
Feb 27 11:34:01.515: INFO: Pod "downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9" satisfied condition "Succeeded or Failed"
Feb 27 11:34:01.522: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9 container client-container: <nil>
STEP: delete the pod 02/27/23 11:34:01.536
Feb 27 11:34:01.557: INFO: Waiting for pod downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9 to disappear
Feb 27 11:34:01.569: INFO: Pod downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 27 11:34:01.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-134" for this suite. 02/27/23 11:34:01.583
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":83,"skipped":1757,"failed":0}
------------------------------
• [4.185 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:33:57.411
    Feb 27 11:33:57.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:33:57.413
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:33:57.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:33:57.471
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 02/27/23 11:33:57.481
    Feb 27 11:33:57.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9" in namespace "projected-134" to be "Succeeded or Failed"
    Feb 27 11:33:57.504: INFO: Pod "downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.454883ms
    Feb 27 11:33:59.512: INFO: Pod "downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017290525s
    Feb 27 11:34:01.514: INFO: Pod "downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019594286s
    STEP: Saw pod success 02/27/23 11:34:01.514
    Feb 27 11:34:01.515: INFO: Pod "downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9" satisfied condition "Succeeded or Failed"
    Feb 27 11:34:01.522: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9 container client-container: <nil>
    STEP: delete the pod 02/27/23 11:34:01.536
    Feb 27 11:34:01.557: INFO: Waiting for pod downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9 to disappear
    Feb 27 11:34:01.569: INFO: Pod downwardapi-volume-61ef4be5-a7ab-4f73-9604-9667c6a462a9 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 27 11:34:01.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-134" for this suite. 02/27/23 11:34:01.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:34:01.61
Feb 27 11:34:01.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 11:34:01.611
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:01.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:01.655
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 02/27/23 11:34:01.674
Feb 27 11:34:01.687: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9" in namespace "downward-api-7123" to be "Succeeded or Failed"
Feb 27 11:34:01.697: INFO: Pod "downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.851821ms
Feb 27 11:34:03.704: INFO: Pod "downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01692834s
Feb 27 11:34:05.707: INFO: Pod "downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0197281s
Feb 27 11:34:07.706: INFO: Pod "downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018836588s
STEP: Saw pod success 02/27/23 11:34:07.706
Feb 27 11:34:07.707: INFO: Pod "downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9" satisfied condition "Succeeded or Failed"
Feb 27 11:34:07.713: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9 container client-container: <nil>
STEP: delete the pod 02/27/23 11:34:07.734
Feb 27 11:34:07.767: INFO: Waiting for pod downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9 to disappear
Feb 27 11:34:07.773: INFO: Pod downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 27 11:34:07.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7123" for this suite. 02/27/23 11:34:07.784
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":84,"skipped":1795,"failed":0}
------------------------------
• [SLOW TEST] [6.185 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:34:01.61
    Feb 27 11:34:01.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 11:34:01.611
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:01.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:01.655
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 02/27/23 11:34:01.674
    Feb 27 11:34:01.687: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9" in namespace "downward-api-7123" to be "Succeeded or Failed"
    Feb 27 11:34:01.697: INFO: Pod "downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.851821ms
    Feb 27 11:34:03.704: INFO: Pod "downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01692834s
    Feb 27 11:34:05.707: INFO: Pod "downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0197281s
    Feb 27 11:34:07.706: INFO: Pod "downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018836588s
    STEP: Saw pod success 02/27/23 11:34:07.706
    Feb 27 11:34:07.707: INFO: Pod "downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9" satisfied condition "Succeeded or Failed"
    Feb 27 11:34:07.713: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9 container client-container: <nil>
    STEP: delete the pod 02/27/23 11:34:07.734
    Feb 27 11:34:07.767: INFO: Waiting for pod downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9 to disappear
    Feb 27 11:34:07.773: INFO: Pod downwardapi-volume-5eed9842-384e-41aa-9804-f4d9ee0b0dd9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 27 11:34:07.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7123" for this suite. 02/27/23 11:34:07.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:34:07.803
Feb 27 11:34:07.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pods 02/27/23 11:34:07.804
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:07.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:07.847
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Feb 27 11:34:07.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: creating the pod 02/27/23 11:34:07.857
STEP: submitting the pod to kubernetes 02/27/23 11:34:07.857
Feb 27 11:34:07.871: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-67aeede2-eef5-4ce5-9918-514fec0cb25e" in namespace "pods-4921" to be "running and ready"
Feb 27 11:34:07.885: INFO: Pod "pod-exec-websocket-67aeede2-eef5-4ce5-9918-514fec0cb25e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.250004ms
Feb 27 11:34:07.885: INFO: The phase of Pod pod-exec-websocket-67aeede2-eef5-4ce5-9918-514fec0cb25e is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:34:09.893: INFO: Pod "pod-exec-websocket-67aeede2-eef5-4ce5-9918-514fec0cb25e": Phase="Running", Reason="", readiness=true. Elapsed: 2.021142538s
Feb 27 11:34:09.893: INFO: The phase of Pod pod-exec-websocket-67aeede2-eef5-4ce5-9918-514fec0cb25e is Running (Ready = true)
Feb 27 11:34:09.893: INFO: Pod "pod-exec-websocket-67aeede2-eef5-4ce5-9918-514fec0cb25e" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 27 11:34:10.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4921" for this suite. 02/27/23 11:34:10.055
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":85,"skipped":1848,"failed":0}
------------------------------
• [2.262 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:34:07.803
    Feb 27 11:34:07.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pods 02/27/23 11:34:07.804
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:07.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:07.847
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Feb 27 11:34:07.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: creating the pod 02/27/23 11:34:07.857
    STEP: submitting the pod to kubernetes 02/27/23 11:34:07.857
    Feb 27 11:34:07.871: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-67aeede2-eef5-4ce5-9918-514fec0cb25e" in namespace "pods-4921" to be "running and ready"
    Feb 27 11:34:07.885: INFO: Pod "pod-exec-websocket-67aeede2-eef5-4ce5-9918-514fec0cb25e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.250004ms
    Feb 27 11:34:07.885: INFO: The phase of Pod pod-exec-websocket-67aeede2-eef5-4ce5-9918-514fec0cb25e is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:34:09.893: INFO: Pod "pod-exec-websocket-67aeede2-eef5-4ce5-9918-514fec0cb25e": Phase="Running", Reason="", readiness=true. Elapsed: 2.021142538s
    Feb 27 11:34:09.893: INFO: The phase of Pod pod-exec-websocket-67aeede2-eef5-4ce5-9918-514fec0cb25e is Running (Ready = true)
    Feb 27 11:34:09.893: INFO: Pod "pod-exec-websocket-67aeede2-eef5-4ce5-9918-514fec0cb25e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 27 11:34:10.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4921" for this suite. 02/27/23 11:34:10.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:34:10.071
Feb 27 11:34:10.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 11:34:10.072
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:10.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:10.117
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/27/23 11:34:10.137
Feb 27 11:34:10.155: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7940" to be "running and ready"
Feb 27 11:34:10.163: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.450924ms
Feb 27 11:34:10.163: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:34:12.172: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016595881s
Feb 27 11:34:12.172: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 27 11:34:12.172: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 02/27/23 11:34:12.185
Feb 27 11:34:12.194: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-7940" to be "running and ready"
Feb 27 11:34:12.207: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.195289ms
Feb 27 11:34:12.207: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:34:14.214: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.019504441s
Feb 27 11:34:14.214: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Feb 27 11:34:14.214: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/27/23 11:34:14.222
STEP: delete the pod with lifecycle hook 02/27/23 11:34:14.239
Feb 27 11:34:14.254: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 27 11:34:14.262: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 27 11:34:16.263: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 27 11:34:16.271: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 27 11:34:18.263: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 27 11:34:18.275: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 27 11:34:18.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7940" for this suite. 02/27/23 11:34:18.287
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":86,"skipped":1859,"failed":0}
------------------------------
• [SLOW TEST] [8.239 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:34:10.071
    Feb 27 11:34:10.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 11:34:10.072
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:10.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:10.117
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/27/23 11:34:10.137
    Feb 27 11:34:10.155: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7940" to be "running and ready"
    Feb 27 11:34:10.163: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.450924ms
    Feb 27 11:34:10.163: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:34:12.172: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016595881s
    Feb 27 11:34:12.172: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 27 11:34:12.172: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 02/27/23 11:34:12.185
    Feb 27 11:34:12.194: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-7940" to be "running and ready"
    Feb 27 11:34:12.207: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.195289ms
    Feb 27 11:34:12.207: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:34:14.214: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.019504441s
    Feb 27 11:34:14.214: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Feb 27 11:34:14.214: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/27/23 11:34:14.222
    STEP: delete the pod with lifecycle hook 02/27/23 11:34:14.239
    Feb 27 11:34:14.254: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 27 11:34:14.262: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 27 11:34:16.263: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 27 11:34:16.271: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 27 11:34:18.263: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 27 11:34:18.275: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 27 11:34:18.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7940" for this suite. 02/27/23 11:34:18.287
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:34:18.313
Feb 27 11:34:18.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 11:34:18.315
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:18.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:18.369
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Feb 27 11:34:18.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/27/23 11:34:21.799
Feb 27 11:34:21.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 create -f -'
Feb 27 11:34:22.623: INFO: stderr: ""
Feb 27 11:34:22.623: INFO: stdout: "e2e-test-crd-publish-openapi-2894-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 27 11:34:22.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 delete e2e-test-crd-publish-openapi-2894-crds test-foo'
Feb 27 11:34:22.757: INFO: stderr: ""
Feb 27 11:34:22.757: INFO: stdout: "e2e-test-crd-publish-openapi-2894-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 27 11:34:22.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 apply -f -'
Feb 27 11:34:23.091: INFO: stderr: ""
Feb 27 11:34:23.091: INFO: stdout: "e2e-test-crd-publish-openapi-2894-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 27 11:34:23.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 delete e2e-test-crd-publish-openapi-2894-crds test-foo'
Feb 27 11:34:23.211: INFO: stderr: ""
Feb 27 11:34:23.211: INFO: stdout: "e2e-test-crd-publish-openapi-2894-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/27/23 11:34:23.211
Feb 27 11:34:23.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 create -f -'
Feb 27 11:34:24.092: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/27/23 11:34:24.092
Feb 27 11:34:24.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 create -f -'
Feb 27 11:34:24.423: INFO: rc: 1
Feb 27 11:34:24.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 apply -f -'
Feb 27 11:34:24.796: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/27/23 11:34:24.797
Feb 27 11:34:24.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 create -f -'
Feb 27 11:34:25.082: INFO: rc: 1
Feb 27 11:34:25.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 apply -f -'
Feb 27 11:34:25.395: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 02/27/23 11:34:25.395
Feb 27 11:34:25.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 explain e2e-test-crd-publish-openapi-2894-crds'
Feb 27 11:34:25.716: INFO: stderr: ""
Feb 27 11:34:25.716: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2894-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 02/27/23 11:34:25.716
Feb 27 11:34:25.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 explain e2e-test-crd-publish-openapi-2894-crds.metadata'
Feb 27 11:34:25.983: INFO: stderr: ""
Feb 27 11:34:25.983: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2894-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 27 11:34:25.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 explain e2e-test-crd-publish-openapi-2894-crds.spec'
Feb 27 11:34:26.256: INFO: stderr: ""
Feb 27 11:34:26.256: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2894-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 27 11:34:26.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 explain e2e-test-crd-publish-openapi-2894-crds.spec.bars'
Feb 27 11:34:27.297: INFO: stderr: ""
Feb 27 11:34:27.297: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2894-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/27/23 11:34:27.297
Feb 27 11:34:27.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 explain e2e-test-crd-publish-openapi-2894-crds.spec.bars2'
Feb 27 11:34:28.221: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:34:32.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7318" for this suite. 02/27/23 11:34:32.964
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":87,"skipped":1861,"failed":0}
------------------------------
• [SLOW TEST] [14.661 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:34:18.313
    Feb 27 11:34:18.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 11:34:18.315
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:18.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:18.369
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Feb 27 11:34:18.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/27/23 11:34:21.799
    Feb 27 11:34:21.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 create -f -'
    Feb 27 11:34:22.623: INFO: stderr: ""
    Feb 27 11:34:22.623: INFO: stdout: "e2e-test-crd-publish-openapi-2894-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 27 11:34:22.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 delete e2e-test-crd-publish-openapi-2894-crds test-foo'
    Feb 27 11:34:22.757: INFO: stderr: ""
    Feb 27 11:34:22.757: INFO: stdout: "e2e-test-crd-publish-openapi-2894-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Feb 27 11:34:22.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 apply -f -'
    Feb 27 11:34:23.091: INFO: stderr: ""
    Feb 27 11:34:23.091: INFO: stdout: "e2e-test-crd-publish-openapi-2894-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 27 11:34:23.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 delete e2e-test-crd-publish-openapi-2894-crds test-foo'
    Feb 27 11:34:23.211: INFO: stderr: ""
    Feb 27 11:34:23.211: INFO: stdout: "e2e-test-crd-publish-openapi-2894-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/27/23 11:34:23.211
    Feb 27 11:34:23.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 create -f -'
    Feb 27 11:34:24.092: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/27/23 11:34:24.092
    Feb 27 11:34:24.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 create -f -'
    Feb 27 11:34:24.423: INFO: rc: 1
    Feb 27 11:34:24.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 apply -f -'
    Feb 27 11:34:24.796: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/27/23 11:34:24.797
    Feb 27 11:34:24.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 create -f -'
    Feb 27 11:34:25.082: INFO: rc: 1
    Feb 27 11:34:25.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 --namespace=crd-publish-openapi-7318 apply -f -'
    Feb 27 11:34:25.395: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 02/27/23 11:34:25.395
    Feb 27 11:34:25.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 explain e2e-test-crd-publish-openapi-2894-crds'
    Feb 27 11:34:25.716: INFO: stderr: ""
    Feb 27 11:34:25.716: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2894-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 02/27/23 11:34:25.716
    Feb 27 11:34:25.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 explain e2e-test-crd-publish-openapi-2894-crds.metadata'
    Feb 27 11:34:25.983: INFO: stderr: ""
    Feb 27 11:34:25.983: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2894-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Feb 27 11:34:25.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 explain e2e-test-crd-publish-openapi-2894-crds.spec'
    Feb 27 11:34:26.256: INFO: stderr: ""
    Feb 27 11:34:26.256: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2894-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Feb 27 11:34:26.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 explain e2e-test-crd-publish-openapi-2894-crds.spec.bars'
    Feb 27 11:34:27.297: INFO: stderr: ""
    Feb 27 11:34:27.297: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2894-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/27/23 11:34:27.297
    Feb 27 11:34:27.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-7318 explain e2e-test-crd-publish-openapi-2894-crds.spec.bars2'
    Feb 27 11:34:28.221: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:34:32.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7318" for this suite. 02/27/23 11:34:32.964
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:34:32.975
Feb 27 11:34:32.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 11:34:32.977
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:33.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:33.023
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/27/23 11:34:33.031
Feb 27 11:34:33.053: INFO: Waiting up to 5m0s for pod "pod-932fbd57-d5a4-4268-946b-491455a28cc0" in namespace "emptydir-9162" to be "Succeeded or Failed"
Feb 27 11:34:33.061: INFO: Pod "pod-932fbd57-d5a4-4268-946b-491455a28cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.492071ms
Feb 27 11:34:35.070: INFO: Pod "pod-932fbd57-d5a4-4268-946b-491455a28cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017097188s
Feb 27 11:34:37.103: INFO: Pod "pod-932fbd57-d5a4-4268-946b-491455a28cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049831171s
Feb 27 11:34:39.069: INFO: Pod "pod-932fbd57-d5a4-4268-946b-491455a28cc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016121084s
STEP: Saw pod success 02/27/23 11:34:39.069
Feb 27 11:34:39.070: INFO: Pod "pod-932fbd57-d5a4-4268-946b-491455a28cc0" satisfied condition "Succeeded or Failed"
Feb 27 11:34:39.078: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-932fbd57-d5a4-4268-946b-491455a28cc0 container test-container: <nil>
STEP: delete the pod 02/27/23 11:34:39.094
Feb 27 11:34:39.114: INFO: Waiting for pod pod-932fbd57-d5a4-4268-946b-491455a28cc0 to disappear
Feb 27 11:34:39.121: INFO: Pod pod-932fbd57-d5a4-4268-946b-491455a28cc0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 11:34:39.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9162" for this suite. 02/27/23 11:34:39.136
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":88,"skipped":1863,"failed":0}
------------------------------
• [SLOW TEST] [6.191 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:34:32.975
    Feb 27 11:34:32.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 11:34:32.977
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:33.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:33.023
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/27/23 11:34:33.031
    Feb 27 11:34:33.053: INFO: Waiting up to 5m0s for pod "pod-932fbd57-d5a4-4268-946b-491455a28cc0" in namespace "emptydir-9162" to be "Succeeded or Failed"
    Feb 27 11:34:33.061: INFO: Pod "pod-932fbd57-d5a4-4268-946b-491455a28cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.492071ms
    Feb 27 11:34:35.070: INFO: Pod "pod-932fbd57-d5a4-4268-946b-491455a28cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017097188s
    Feb 27 11:34:37.103: INFO: Pod "pod-932fbd57-d5a4-4268-946b-491455a28cc0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049831171s
    Feb 27 11:34:39.069: INFO: Pod "pod-932fbd57-d5a4-4268-946b-491455a28cc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016121084s
    STEP: Saw pod success 02/27/23 11:34:39.069
    Feb 27 11:34:39.070: INFO: Pod "pod-932fbd57-d5a4-4268-946b-491455a28cc0" satisfied condition "Succeeded or Failed"
    Feb 27 11:34:39.078: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-932fbd57-d5a4-4268-946b-491455a28cc0 container test-container: <nil>
    STEP: delete the pod 02/27/23 11:34:39.094
    Feb 27 11:34:39.114: INFO: Waiting for pod pod-932fbd57-d5a4-4268-946b-491455a28cc0 to disappear
    Feb 27 11:34:39.121: INFO: Pod pod-932fbd57-d5a4-4268-946b-491455a28cc0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 11:34:39.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9162" for this suite. 02/27/23 11:34:39.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:34:39.167
Feb 27 11:34:39.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 11:34:39.169
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:39.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:39.234
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 11:34:39.277
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:34:39.9
STEP: Deploying the webhook pod 02/27/23 11:34:39.914
STEP: Wait for the deployment to be ready 02/27/23 11:34:39.933
Feb 27 11:34:39.958: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 27 11:34:41.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 11, 34, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 34, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 11, 34, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 34, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/27/23 11:34:44.006
STEP: Verifying the service has paired with the endpoint 02/27/23 11:34:44.026
Feb 27 11:34:45.026: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 02/27/23 11:34:45.035
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/27/23 11:34:45.043
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/27/23 11:34:45.043
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/27/23 11:34:45.043
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/27/23 11:34:45.048
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/27/23 11:34:45.048
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/27/23 11:34:45.052
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:34:45.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2726" for this suite. 02/27/23 11:34:45.065
STEP: Destroying namespace "webhook-2726-markers" for this suite. 02/27/23 11:34:45.078
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":89,"skipped":1872,"failed":0}
------------------------------
• [SLOW TEST] [5.998 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:34:39.167
    Feb 27 11:34:39.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 11:34:39.169
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:39.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:39.234
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 11:34:39.277
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:34:39.9
    STEP: Deploying the webhook pod 02/27/23 11:34:39.914
    STEP: Wait for the deployment to be ready 02/27/23 11:34:39.933
    Feb 27 11:34:39.958: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 27 11:34:41.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 11, 34, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 34, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 11, 34, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 11, 34, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/27/23 11:34:44.006
    STEP: Verifying the service has paired with the endpoint 02/27/23 11:34:44.026
    Feb 27 11:34:45.026: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 02/27/23 11:34:45.035
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/27/23 11:34:45.043
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/27/23 11:34:45.043
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/27/23 11:34:45.043
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/27/23 11:34:45.048
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/27/23 11:34:45.048
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/27/23 11:34:45.052
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:34:45.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2726" for this suite. 02/27/23 11:34:45.065
    STEP: Destroying namespace "webhook-2726-markers" for this suite. 02/27/23 11:34:45.078
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:34:45.17
Feb 27 11:34:45.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 11:34:45.171
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:45.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:45.27
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 02/27/23 11:34:45.312
Feb 27 11:34:45.312: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-253 proxy --unix-socket=/tmp/kubectl-proxy-unix895116829/test'
STEP: retrieving proxy /api/ output 02/27/23 11:34:45.492
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 11:34:45.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-253" for this suite. 02/27/23 11:34:45.51
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":90,"skipped":1880,"failed":0}
------------------------------
• [0.371 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:34:45.17
    Feb 27 11:34:45.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 11:34:45.171
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:45.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:45.27
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 02/27/23 11:34:45.312
    Feb 27 11:34:45.312: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-253 proxy --unix-socket=/tmp/kubectl-proxy-unix895116829/test'
    STEP: retrieving proxy /api/ output 02/27/23 11:34:45.492
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 11:34:45.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-253" for this suite. 02/27/23 11:34:45.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:34:45.542
Feb 27 11:34:45.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 11:34:45.544
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:45.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:45.585
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 02/27/23 11:34:45.62
Feb 27 11:34:45.636: INFO: Waiting up to 5m0s for pod "downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b" in namespace "downward-api-2928" to be "Succeeded or Failed"
Feb 27 11:34:45.655: INFO: Pod "downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.220428ms
Feb 27 11:34:47.662: INFO: Pod "downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02190524s
Feb 27 11:34:49.663: INFO: Pod "downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022456401s
STEP: Saw pod success 02/27/23 11:34:49.663
Feb 27 11:34:49.663: INFO: Pod "downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b" satisfied condition "Succeeded or Failed"
Feb 27 11:34:49.683: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b container dapi-container: <nil>
STEP: delete the pod 02/27/23 11:34:49.705
Feb 27 11:34:49.738: INFO: Waiting for pod downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b to disappear
Feb 27 11:34:49.745: INFO: Pod downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 27 11:34:49.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2928" for this suite. 02/27/23 11:34:49.759
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":91,"skipped":1903,"failed":0}
------------------------------
• [4.229 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:34:45.542
    Feb 27 11:34:45.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 11:34:45.544
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:45.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:45.585
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 02/27/23 11:34:45.62
    Feb 27 11:34:45.636: INFO: Waiting up to 5m0s for pod "downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b" in namespace "downward-api-2928" to be "Succeeded or Failed"
    Feb 27 11:34:45.655: INFO: Pod "downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.220428ms
    Feb 27 11:34:47.662: INFO: Pod "downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02190524s
    Feb 27 11:34:49.663: INFO: Pod "downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022456401s
    STEP: Saw pod success 02/27/23 11:34:49.663
    Feb 27 11:34:49.663: INFO: Pod "downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b" satisfied condition "Succeeded or Failed"
    Feb 27 11:34:49.683: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b container dapi-container: <nil>
    STEP: delete the pod 02/27/23 11:34:49.705
    Feb 27 11:34:49.738: INFO: Waiting for pod downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b to disappear
    Feb 27 11:34:49.745: INFO: Pod downward-api-89838e28-6a64-476e-8c27-4cae427f5a6b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 27 11:34:49.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2928" for this suite. 02/27/23 11:34:49.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:34:49.778
Feb 27 11:34:49.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename init-container 02/27/23 11:34:49.78
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:49.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:49.829
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 02/27/23 11:34:49.841
Feb 27 11:34:49.842: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 27 11:34:56.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7877" for this suite. 02/27/23 11:34:56.052
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":92,"skipped":1912,"failed":0}
------------------------------
• [SLOW TEST] [6.295 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:34:49.778
    Feb 27 11:34:49.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename init-container 02/27/23 11:34:49.78
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:49.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:49.829
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 02/27/23 11:34:49.841
    Feb 27 11:34:49.842: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 27 11:34:56.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-7877" for this suite. 02/27/23 11:34:56.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:34:56.074
Feb 27 11:34:56.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename sched-preemption 02/27/23 11:34:56.076
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:56.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:56.126
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 27 11:34:56.159: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 11:35:56.236: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:35:56.243
Feb 27 11:35:56.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename sched-preemption-path 02/27/23 11:35:56.244
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:35:56.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:35:56.288
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 02/27/23 11:35:56.296
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 11:35:56.297
Feb 27 11:35:56.312: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7902" to be "running"
Feb 27 11:35:56.322: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.759869ms
Feb 27 11:35:58.331: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.018387088s
Feb 27 11:35:58.331: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 11:35:58.337
Feb 27 11:35:58.363: INFO: found a healthy node: ip-172-31-15-17.eu-central-1.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Feb 27 11:36:12.547: INFO: pods created so far: [1 1 1]
Feb 27 11:36:12.547: INFO: length of pods created so far: 3
Feb 27 11:36:14.567: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Feb 27 11:36:21.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7902" for this suite. 02/27/23 11:36:21.583
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 27 11:36:21.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-10" for this suite. 02/27/23 11:36:21.696
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":93,"skipped":1925,"failed":0}
------------------------------
• [SLOW TEST] [85.737 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:34:56.074
    Feb 27 11:34:56.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename sched-preemption 02/27/23 11:34:56.076
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:34:56.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:34:56.126
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 27 11:34:56.159: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 11:35:56.236: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:35:56.243
    Feb 27 11:35:56.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename sched-preemption-path 02/27/23 11:35:56.244
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:35:56.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:35:56.288
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 02/27/23 11:35:56.296
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 11:35:56.297
    Feb 27 11:35:56.312: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7902" to be "running"
    Feb 27 11:35:56.322: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.759869ms
    Feb 27 11:35:58.331: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.018387088s
    Feb 27 11:35:58.331: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 11:35:58.337
    Feb 27 11:35:58.363: INFO: found a healthy node: ip-172-31-15-17.eu-central-1.compute.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Feb 27 11:36:12.547: INFO: pods created so far: [1 1 1]
    Feb 27 11:36:12.547: INFO: length of pods created so far: 3
    Feb 27 11:36:14.567: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Feb 27 11:36:21.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-7902" for this suite. 02/27/23 11:36:21.583
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 11:36:21.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-10" for this suite. 02/27/23 11:36:21.696
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:36:21.811
Feb 27 11:36:21.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename job 02/27/23 11:36:21.812
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:21.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:21.85
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 02/27/23 11:36:21.865
STEP: Ensure pods equal to paralellism count is attached to the job 02/27/23 11:36:21.881
STEP: patching /status 02/27/23 11:36:23.89
STEP: updating /status 02/27/23 11:36:23.902
STEP: get /status 02/27/23 11:36:23.961
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 27 11:36:23.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3944" for this suite. 02/27/23 11:36:23.984
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":94,"skipped":1925,"failed":0}
------------------------------
• [2.192 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:36:21.811
    Feb 27 11:36:21.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename job 02/27/23 11:36:21.812
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:21.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:21.85
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 02/27/23 11:36:21.865
    STEP: Ensure pods equal to paralellism count is attached to the job 02/27/23 11:36:21.881
    STEP: patching /status 02/27/23 11:36:23.89
    STEP: updating /status 02/27/23 11:36:23.902
    STEP: get /status 02/27/23 11:36:23.961
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 27 11:36:23.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3944" for this suite. 02/27/23 11:36:23.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:36:24.01
Feb 27 11:36:24.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename sched-pred 02/27/23 11:36:24.011
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:24.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:24.071
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 27 11:36:24.094: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 27 11:36:24.137: INFO: Waiting for terminating namespaces to be deleted...
Feb 27 11:36:24.145: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-11-159.eu-central-1.compute.internal before test
Feb 27 11:36:24.179: INFO: suspend-false-to-true-6d49c from job-3944 started at 2023-02-27 11:36:21 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.179: INFO: 	Container c ready: true, restart count 0
Feb 27 11:36:24.189: INFO: suspend-false-to-true-9n6hs from job-3944 started at 2023-02-27 11:36:21 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.190: INFO: 	Container c ready: true, restart count 0
Feb 27 11:36:24.190: INFO: canal-4q9m8 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
Feb 27 11:36:24.190: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 11:36:24.190: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 27 11:36:24.190: INFO: coredns-bf8668b4f-nbc9z from kube-system started at 2023-02-27 11:28:02 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.191: INFO: 	Container coredns ready: true, restart count 0
Feb 27 11:36:24.191: INFO: ebs-csi-node-b6z5h from kube-system started at 2023-02-27 09:22:11 +0000 UTC (3 container statuses recorded)
Feb 27 11:36:24.191: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 11:36:24.191: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 11:36:24.191: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 11:36:24.192: INFO: envoy-agent-2wwht from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
Feb 27 11:36:24.192: INFO: 	Container assign-address ready: true, restart count 0
Feb 27 11:36:24.192: INFO: 	Container envoy-agent ready: true, restart count 0
Feb 27 11:36:24.192: INFO: konnectivity-agent-76c848fdd6-fr8fm from kube-system started at 2023-02-27 11:28:02 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.192: INFO: 	Container konnectivity-agent ready: true, restart count 0
Feb 27 11:36:24.192: INFO: kube-proxy-cz9zt from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.193: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 27 11:36:24.193: INFO: metrics-server-5f7c5d4b9-qh6s5 from kube-system started at 2023-02-27 11:28:02 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.193: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 11:36:24.193: INFO: node-local-dns-r6r4k from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.193: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 11:36:24.193: INFO: user-ssh-keys-agent-rk5t9 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.194: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 27 11:36:24.194: INFO: dashboard-metrics-scraper-85f6dd84d5-c7wsq from kubernetes-dashboard started at 2023-02-27 11:08:24 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.194: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 27 11:36:24.194: INFO: sonobuoy from sonobuoy started at 2023-02-27 11:19:26 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.194: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 27 11:36:24.194: INFO: sonobuoy-e2e-job-18131847dfcd49d5 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 11:36:24.195: INFO: 	Container e2e ready: true, restart count 0
Feb 27 11:36:24.195: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 11:36:24.195: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-rlhml from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 11:36:24.195: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 11:36:24.195: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 11:36:24.196: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-15-17.eu-central-1.compute.internal before test
Feb 27 11:36:24.217: INFO: calico-kube-controllers-55d99d998f-f5ngs from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 27 11:36:24.217: INFO: canal-bxq4m from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 11:36:24.217: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 27 11:36:24.217: INFO: coredns-bf8668b4f-5h5v9 from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container coredns ready: true, restart count 0
Feb 27 11:36:24.217: INFO: ebs-csi-node-z9l5x from kube-system started at 2023-02-27 09:22:52 +0000 UTC (3 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 11:36:24.217: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 11:36:24.217: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 11:36:24.217: INFO: envoy-agent-7xhs5 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container assign-address ready: true, restart count 0
Feb 27 11:36:24.217: INFO: 	Container envoy-agent ready: true, restart count 0
Feb 27 11:36:24.217: INFO: konnectivity-agent-76c848fdd6-56hgq from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container konnectivity-agent ready: true, restart count 0
Feb 27 11:36:24.217: INFO: kube-proxy-hxmbw from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 27 11:36:24.217: INFO: metrics-server-5f7c5d4b9-xlr2c from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 11:36:24.217: INFO: node-local-dns-8z787 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 11:36:24.217: INFO: user-ssh-keys-agent-fvhdn from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 27 11:36:24.217: INFO: dashboard-metrics-scraper-85f6dd84d5-2qtjs from kubernetes-dashboard started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 27 11:36:24.217: INFO: pod4 from sched-preemption-path-7902 started at 2023-02-27 11:36:13 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container pod4 ready: true, restart count 0
Feb 27 11:36:24.217: INFO: rs-pod3-g75r8 from sched-preemption-path-7902 started at 2023-02-27 11:36:10 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container pod3 ready: true, restart count 0
Feb 27 11:36:24.217: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-94vkz from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 11:36:24.217: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 11:36:24.217: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 11:36:24.217: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-7-167.eu-central-1.compute.internal before test
Feb 27 11:36:24.233: INFO: canal-mbg4r from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
Feb 27 11:36:24.234: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 11:36:24.234: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 27 11:36:24.234: INFO: ebs-csi-controller-54c5c66b84-mkj54 from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
Feb 27 11:36:24.234: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 27 11:36:24.234: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 27 11:36:24.234: INFO: 	Container csi-resizer ready: true, restart count 0
Feb 27 11:36:24.234: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 11:36:24.234: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 11:36:24.234: INFO: ebs-csi-controller-54c5c66b84-rxhdh from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
Feb 27 11:36:24.234: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 27 11:36:24.234: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 27 11:36:24.234: INFO: 	Container csi-resizer ready: true, restart count 0
Feb 27 11:36:24.234: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 11:36:24.234: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 11:36:24.234: INFO: ebs-csi-node-7dvrp from kube-system started at 2023-02-27 09:22:08 +0000 UTC (3 container statuses recorded)
Feb 27 11:36:24.234: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 11:36:24.234: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 11:36:24.234: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 11:36:24.234: INFO: envoy-agent-scd88 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
Feb 27 11:36:24.234: INFO: 	Container assign-address ready: true, restart count 0
Feb 27 11:36:24.234: INFO: 	Container envoy-agent ready: true, restart count 0
Feb 27 11:36:24.235: INFO: kube-proxy-ghd44 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.235: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 27 11:36:24.235: INFO: node-local-dns-kfv2k from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.235: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 11:36:24.235: INFO: user-ssh-keys-agent-gjs99 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
Feb 27 11:36:24.235: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 27 11:36:24.235: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-k5l74 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 11:36:24.235: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 11:36:24.235: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 02/27/23 11:36:24.235
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1747ab59c983b2d5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 02/27/23 11:36:28.351
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 27 11:36:29.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2297" for this suite. 02/27/23 11:36:29.37
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":95,"skipped":1949,"failed":0}
------------------------------
• [SLOW TEST] [5.388 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:36:24.01
    Feb 27 11:36:24.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename sched-pred 02/27/23 11:36:24.011
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:24.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:24.071
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 27 11:36:24.094: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 27 11:36:24.137: INFO: Waiting for terminating namespaces to be deleted...
    Feb 27 11:36:24.145: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-11-159.eu-central-1.compute.internal before test
    Feb 27 11:36:24.179: INFO: suspend-false-to-true-6d49c from job-3944 started at 2023-02-27 11:36:21 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.179: INFO: 	Container c ready: true, restart count 0
    Feb 27 11:36:24.189: INFO: suspend-false-to-true-9n6hs from job-3944 started at 2023-02-27 11:36:21 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.190: INFO: 	Container c ready: true, restart count 0
    Feb 27 11:36:24.190: INFO: canal-4q9m8 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
    Feb 27 11:36:24.190: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 11:36:24.190: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 27 11:36:24.190: INFO: coredns-bf8668b4f-nbc9z from kube-system started at 2023-02-27 11:28:02 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.191: INFO: 	Container coredns ready: true, restart count 0
    Feb 27 11:36:24.191: INFO: ebs-csi-node-b6z5h from kube-system started at 2023-02-27 09:22:11 +0000 UTC (3 container statuses recorded)
    Feb 27 11:36:24.191: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 11:36:24.191: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 11:36:24.191: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 11:36:24.192: INFO: envoy-agent-2wwht from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
    Feb 27 11:36:24.192: INFO: 	Container assign-address ready: true, restart count 0
    Feb 27 11:36:24.192: INFO: 	Container envoy-agent ready: true, restart count 0
    Feb 27 11:36:24.192: INFO: konnectivity-agent-76c848fdd6-fr8fm from kube-system started at 2023-02-27 11:28:02 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.192: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Feb 27 11:36:24.192: INFO: kube-proxy-cz9zt from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.193: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 27 11:36:24.193: INFO: metrics-server-5f7c5d4b9-qh6s5 from kube-system started at 2023-02-27 11:28:02 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.193: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 11:36:24.193: INFO: node-local-dns-r6r4k from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.193: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 11:36:24.193: INFO: user-ssh-keys-agent-rk5t9 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.194: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Feb 27 11:36:24.194: INFO: dashboard-metrics-scraper-85f6dd84d5-c7wsq from kubernetes-dashboard started at 2023-02-27 11:08:24 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.194: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Feb 27 11:36:24.194: INFO: sonobuoy from sonobuoy started at 2023-02-27 11:19:26 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.194: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 27 11:36:24.194: INFO: sonobuoy-e2e-job-18131847dfcd49d5 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 11:36:24.195: INFO: 	Container e2e ready: true, restart count 0
    Feb 27 11:36:24.195: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 11:36:24.195: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-rlhml from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 11:36:24.195: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 11:36:24.195: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 11:36:24.196: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-15-17.eu-central-1.compute.internal before test
    Feb 27 11:36:24.217: INFO: calico-kube-controllers-55d99d998f-f5ngs from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: canal-bxq4m from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: coredns-bf8668b4f-5h5v9 from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container coredns ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: ebs-csi-node-z9l5x from kube-system started at 2023-02-27 09:22:52 +0000 UTC (3 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: envoy-agent-7xhs5 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container assign-address ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: 	Container envoy-agent ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: konnectivity-agent-76c848fdd6-56hgq from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: kube-proxy-hxmbw from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: metrics-server-5f7c5d4b9-xlr2c from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: node-local-dns-8z787 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: user-ssh-keys-agent-fvhdn from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: dashboard-metrics-scraper-85f6dd84d5-2qtjs from kubernetes-dashboard started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: pod4 from sched-preemption-path-7902 started at 2023-02-27 11:36:13 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container pod4 ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: rs-pod3-g75r8 from sched-preemption-path-7902 started at 2023-02-27 11:36:10 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container pod3 ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-94vkz from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 11:36:24.217: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 11:36:24.217: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-7-167.eu-central-1.compute.internal before test
    Feb 27 11:36:24.233: INFO: canal-mbg4r from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
    Feb 27 11:36:24.234: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: ebs-csi-controller-54c5c66b84-mkj54 from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
    Feb 27 11:36:24.234: INFO: 	Container csi-attacher ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: 	Container csi-provisioner ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: 	Container csi-resizer ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: ebs-csi-controller-54c5c66b84-rxhdh from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
    Feb 27 11:36:24.234: INFO: 	Container csi-attacher ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: 	Container csi-provisioner ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: 	Container csi-resizer ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: ebs-csi-node-7dvrp from kube-system started at 2023-02-27 09:22:08 +0000 UTC (3 container statuses recorded)
    Feb 27 11:36:24.234: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: envoy-agent-scd88 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
    Feb 27 11:36:24.234: INFO: 	Container assign-address ready: true, restart count 0
    Feb 27 11:36:24.234: INFO: 	Container envoy-agent ready: true, restart count 0
    Feb 27 11:36:24.235: INFO: kube-proxy-ghd44 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.235: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 27 11:36:24.235: INFO: node-local-dns-kfv2k from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.235: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 11:36:24.235: INFO: user-ssh-keys-agent-gjs99 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
    Feb 27 11:36:24.235: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Feb 27 11:36:24.235: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-k5l74 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 11:36:24.235: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 11:36:24.235: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 02/27/23 11:36:24.235
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1747ab59c983b2d5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 02/27/23 11:36:28.351
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 11:36:29.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2297" for this suite. 02/27/23 11:36:29.37
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:36:29.402
Feb 27 11:36:29.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:36:29.404
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:29.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:29.453
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-eade7570-88e1-4489-bc55-da13c8e1e73f 02/27/23 11:36:29.462
STEP: Creating a pod to test consume configMaps 02/27/23 11:36:29.475
Feb 27 11:36:29.509: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21" in namespace "projected-4253" to be "Succeeded or Failed"
Feb 27 11:36:29.543: INFO: Pod "pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21": Phase="Pending", Reason="", readiness=false. Elapsed: 34.133059ms
Feb 27 11:36:31.558: INFO: Pod "pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048702205s
Feb 27 11:36:33.561: INFO: Pod "pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051175661s
STEP: Saw pod success 02/27/23 11:36:33.561
Feb 27 11:36:33.561: INFO: Pod "pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21" satisfied condition "Succeeded or Failed"
Feb 27 11:36:33.567: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 11:36:33.588
Feb 27 11:36:33.627: INFO: Waiting for pod pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21 to disappear
Feb 27 11:36:33.633: INFO: Pod pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 27 11:36:33.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4253" for this suite. 02/27/23 11:36:33.647
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":96,"skipped":1949,"failed":0}
------------------------------
• [4.263 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:36:29.402
    Feb 27 11:36:29.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:36:29.404
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:29.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:29.453
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-eade7570-88e1-4489-bc55-da13c8e1e73f 02/27/23 11:36:29.462
    STEP: Creating a pod to test consume configMaps 02/27/23 11:36:29.475
    Feb 27 11:36:29.509: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21" in namespace "projected-4253" to be "Succeeded or Failed"
    Feb 27 11:36:29.543: INFO: Pod "pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21": Phase="Pending", Reason="", readiness=false. Elapsed: 34.133059ms
    Feb 27 11:36:31.558: INFO: Pod "pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048702205s
    Feb 27 11:36:33.561: INFO: Pod "pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051175661s
    STEP: Saw pod success 02/27/23 11:36:33.561
    Feb 27 11:36:33.561: INFO: Pod "pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21" satisfied condition "Succeeded or Failed"
    Feb 27 11:36:33.567: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 11:36:33.588
    Feb 27 11:36:33.627: INFO: Waiting for pod pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21 to disappear
    Feb 27 11:36:33.633: INFO: Pod pod-projected-configmaps-261d4c1c-1f42-4e34-8e0b-3adc68962d21 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 27 11:36:33.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4253" for this suite. 02/27/23 11:36:33.647
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:36:33.666
Feb 27 11:36:33.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 11:36:33.667
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:33.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:33.717
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 02/27/23 11:36:33.737
Feb 27 11:36:33.753: INFO: Waiting up to 5m0s for pod "pod-23db442e-6f46-45c6-bdce-29df9ae53ef2" in namespace "emptydir-4020" to be "Succeeded or Failed"
Feb 27 11:36:33.767: INFO: Pod "pod-23db442e-6f46-45c6-bdce-29df9ae53ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.520175ms
Feb 27 11:36:35.775: INFO: Pod "pod-23db442e-6f46-45c6-bdce-29df9ae53ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021019534s
Feb 27 11:36:37.780: INFO: Pod "pod-23db442e-6f46-45c6-bdce-29df9ae53ef2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02668718s
STEP: Saw pod success 02/27/23 11:36:37.78
Feb 27 11:36:37.781: INFO: Pod "pod-23db442e-6f46-45c6-bdce-29df9ae53ef2" satisfied condition "Succeeded or Failed"
Feb 27 11:36:37.791: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-23db442e-6f46-45c6-bdce-29df9ae53ef2 container test-container: <nil>
STEP: delete the pod 02/27/23 11:36:37.813
Feb 27 11:36:37.838: INFO: Waiting for pod pod-23db442e-6f46-45c6-bdce-29df9ae53ef2 to disappear
Feb 27 11:36:37.860: INFO: Pod pod-23db442e-6f46-45c6-bdce-29df9ae53ef2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 11:36:37.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4020" for this suite. 02/27/23 11:36:37.871
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":97,"skipped":1950,"failed":0}
------------------------------
• [4.218 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:36:33.666
    Feb 27 11:36:33.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 11:36:33.667
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:33.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:33.717
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/27/23 11:36:33.737
    Feb 27 11:36:33.753: INFO: Waiting up to 5m0s for pod "pod-23db442e-6f46-45c6-bdce-29df9ae53ef2" in namespace "emptydir-4020" to be "Succeeded or Failed"
    Feb 27 11:36:33.767: INFO: Pod "pod-23db442e-6f46-45c6-bdce-29df9ae53ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.520175ms
    Feb 27 11:36:35.775: INFO: Pod "pod-23db442e-6f46-45c6-bdce-29df9ae53ef2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021019534s
    Feb 27 11:36:37.780: INFO: Pod "pod-23db442e-6f46-45c6-bdce-29df9ae53ef2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02668718s
    STEP: Saw pod success 02/27/23 11:36:37.78
    Feb 27 11:36:37.781: INFO: Pod "pod-23db442e-6f46-45c6-bdce-29df9ae53ef2" satisfied condition "Succeeded or Failed"
    Feb 27 11:36:37.791: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-23db442e-6f46-45c6-bdce-29df9ae53ef2 container test-container: <nil>
    STEP: delete the pod 02/27/23 11:36:37.813
    Feb 27 11:36:37.838: INFO: Waiting for pod pod-23db442e-6f46-45c6-bdce-29df9ae53ef2 to disappear
    Feb 27 11:36:37.860: INFO: Pod pod-23db442e-6f46-45c6-bdce-29df9ae53ef2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 11:36:37.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4020" for this suite. 02/27/23 11:36:37.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:36:37.89
Feb 27 11:36:37.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename disruption 02/27/23 11:36:37.891
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:37.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:37.937
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 02/27/23 11:36:37.956
STEP: Updating PodDisruptionBudget status 02/27/23 11:36:39.972
STEP: Waiting for all pods to be running 02/27/23 11:36:39.999
Feb 27 11:36:40.017: INFO: running pods: 0 < 1
STEP: locating a running pod 02/27/23 11:36:42.026
STEP: Waiting for the pdb to be processed 02/27/23 11:36:42.053
STEP: Patching PodDisruptionBudget status 02/27/23 11:36:42.069
STEP: Waiting for the pdb to be processed 02/27/23 11:36:42.087
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 27 11:36:42.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8150" for this suite. 02/27/23 11:36:42.107
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":98,"skipped":1967,"failed":0}
------------------------------
• [4.235 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:36:37.89
    Feb 27 11:36:37.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename disruption 02/27/23 11:36:37.891
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:37.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:37.937
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 02/27/23 11:36:37.956
    STEP: Updating PodDisruptionBudget status 02/27/23 11:36:39.972
    STEP: Waiting for all pods to be running 02/27/23 11:36:39.999
    Feb 27 11:36:40.017: INFO: running pods: 0 < 1
    STEP: locating a running pod 02/27/23 11:36:42.026
    STEP: Waiting for the pdb to be processed 02/27/23 11:36:42.053
    STEP: Patching PodDisruptionBudget status 02/27/23 11:36:42.069
    STEP: Waiting for the pdb to be processed 02/27/23 11:36:42.087
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 27 11:36:42.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8150" for this suite. 02/27/23 11:36:42.107
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:36:42.125
Feb 27 11:36:42.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:36:42.126
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:42.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:42.188
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-8992ff7d-b07a-4600-b74d-191c01c86ef5 02/27/23 11:36:42.217
STEP: Creating a pod to test consume secrets 02/27/23 11:36:42.229
Feb 27 11:36:42.252: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e" in namespace "projected-5463" to be "Succeeded or Failed"
Feb 27 11:36:42.262: INFO: Pod "pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.64484ms
Feb 27 11:36:44.272: INFO: Pod "pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019675688s
Feb 27 11:36:46.271: INFO: Pod "pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018157241s
STEP: Saw pod success 02/27/23 11:36:46.271
Feb 27 11:36:46.271: INFO: Pod "pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e" satisfied condition "Succeeded or Failed"
Feb 27 11:36:46.284: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 11:36:46.304
Feb 27 11:36:46.330: INFO: Waiting for pod pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e to disappear
Feb 27 11:36:46.338: INFO: Pod pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 27 11:36:46.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5463" for this suite. 02/27/23 11:36:46.353
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":99,"skipped":1970,"failed":0}
------------------------------
• [4.249 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:36:42.125
    Feb 27 11:36:42.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:36:42.126
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:42.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:42.188
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-8992ff7d-b07a-4600-b74d-191c01c86ef5 02/27/23 11:36:42.217
    STEP: Creating a pod to test consume secrets 02/27/23 11:36:42.229
    Feb 27 11:36:42.252: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e" in namespace "projected-5463" to be "Succeeded or Failed"
    Feb 27 11:36:42.262: INFO: Pod "pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.64484ms
    Feb 27 11:36:44.272: INFO: Pod "pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019675688s
    Feb 27 11:36:46.271: INFO: Pod "pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018157241s
    STEP: Saw pod success 02/27/23 11:36:46.271
    Feb 27 11:36:46.271: INFO: Pod "pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e" satisfied condition "Succeeded or Failed"
    Feb 27 11:36:46.284: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 11:36:46.304
    Feb 27 11:36:46.330: INFO: Waiting for pod pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e to disappear
    Feb 27 11:36:46.338: INFO: Pod pod-projected-secrets-0996f32d-7243-4e5e-8c06-ca9acd72779e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 27 11:36:46.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5463" for this suite. 02/27/23 11:36:46.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:36:46.378
Feb 27 11:36:46.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename var-expansion 02/27/23 11:36:46.38
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:46.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:46.426
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 02/27/23 11:36:46.439
Feb 27 11:36:46.469: INFO: Waiting up to 5m0s for pod "var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4" in namespace "var-expansion-1983" to be "Succeeded or Failed"
Feb 27 11:36:46.477: INFO: Pod "var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.583715ms
Feb 27 11:36:48.520: INFO: Pod "var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051323993s
Feb 27 11:36:50.486: INFO: Pod "var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017346852s
STEP: Saw pod success 02/27/23 11:36:50.486
Feb 27 11:36:50.487: INFO: Pod "var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4" satisfied condition "Succeeded or Failed"
Feb 27 11:36:50.498: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4 container dapi-container: <nil>
STEP: delete the pod 02/27/23 11:36:50.511
Feb 27 11:36:50.532: INFO: Waiting for pod var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4 to disappear
Feb 27 11:36:50.543: INFO: Pod var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 27 11:36:50.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1983" for this suite. 02/27/23 11:36:50.575
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":100,"skipped":1977,"failed":0}
------------------------------
• [4.216 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:36:46.378
    Feb 27 11:36:46.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename var-expansion 02/27/23 11:36:46.38
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:46.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:46.426
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 02/27/23 11:36:46.439
    Feb 27 11:36:46.469: INFO: Waiting up to 5m0s for pod "var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4" in namespace "var-expansion-1983" to be "Succeeded or Failed"
    Feb 27 11:36:46.477: INFO: Pod "var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.583715ms
    Feb 27 11:36:48.520: INFO: Pod "var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051323993s
    Feb 27 11:36:50.486: INFO: Pod "var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017346852s
    STEP: Saw pod success 02/27/23 11:36:50.486
    Feb 27 11:36:50.487: INFO: Pod "var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4" satisfied condition "Succeeded or Failed"
    Feb 27 11:36:50.498: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 11:36:50.511
    Feb 27 11:36:50.532: INFO: Waiting for pod var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4 to disappear
    Feb 27 11:36:50.543: INFO: Pod var-expansion-04e17b17-00d8-4cd6-a0f1-b955f1417da4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 27 11:36:50.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1983" for this suite. 02/27/23 11:36:50.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:36:50.598
Feb 27 11:36:50.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename cronjob 02/27/23 11:36:50.599
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:50.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:50.654
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 02/27/23 11:36:50.667
STEP: Ensuring no jobs are scheduled 02/27/23 11:36:50.68
STEP: Ensuring no job exists by listing jobs explicitly 02/27/23 11:41:50.703
STEP: Removing cronjob 02/27/23 11:41:50.72
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 27 11:41:50.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1945" for this suite. 02/27/23 11:41:50.747
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":101,"skipped":1983,"failed":0}
------------------------------
• [SLOW TEST] [300.165 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:36:50.598
    Feb 27 11:36:50.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename cronjob 02/27/23 11:36:50.599
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:36:50.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:36:50.654
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 02/27/23 11:36:50.667
    STEP: Ensuring no jobs are scheduled 02/27/23 11:36:50.68
    STEP: Ensuring no job exists by listing jobs explicitly 02/27/23 11:41:50.703
    STEP: Removing cronjob 02/27/23 11:41:50.72
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 27 11:41:50.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1945" for this suite. 02/27/23 11:41:50.747
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:41:50.763
Feb 27 11:41:50.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 11:41:50.764
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:41:50.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:41:50.83
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-3669/configmap-test-8f8d9d18-533c-44f9-b0ce-19c57a19f134 02/27/23 11:41:50.852
STEP: Creating a pod to test consume configMaps 02/27/23 11:41:50.876
Feb 27 11:41:50.895: INFO: Waiting up to 5m0s for pod "pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4" in namespace "configmap-3669" to be "Succeeded or Failed"
Feb 27 11:41:50.911: INFO: Pod "pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.953147ms
Feb 27 11:41:52.919: INFO: Pod "pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023711451s
Feb 27 11:41:54.920: INFO: Pod "pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025164349s
STEP: Saw pod success 02/27/23 11:41:54.921
Feb 27 11:41:54.921: INFO: Pod "pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4" satisfied condition "Succeeded or Failed"
Feb 27 11:41:54.928: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4 container env-test: <nil>
STEP: delete the pod 02/27/23 11:41:54.944
Feb 27 11:41:54.966: INFO: Waiting for pod pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4 to disappear
Feb 27 11:41:54.974: INFO: Pod pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 11:41:54.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3669" for this suite. 02/27/23 11:41:54.983
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":102,"skipped":1984,"failed":0}
------------------------------
• [4.234 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:41:50.763
    Feb 27 11:41:50.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 11:41:50.764
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:41:50.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:41:50.83
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-3669/configmap-test-8f8d9d18-533c-44f9-b0ce-19c57a19f134 02/27/23 11:41:50.852
    STEP: Creating a pod to test consume configMaps 02/27/23 11:41:50.876
    Feb 27 11:41:50.895: INFO: Waiting up to 5m0s for pod "pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4" in namespace "configmap-3669" to be "Succeeded or Failed"
    Feb 27 11:41:50.911: INFO: Pod "pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.953147ms
    Feb 27 11:41:52.919: INFO: Pod "pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023711451s
    Feb 27 11:41:54.920: INFO: Pod "pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025164349s
    STEP: Saw pod success 02/27/23 11:41:54.921
    Feb 27 11:41:54.921: INFO: Pod "pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4" satisfied condition "Succeeded or Failed"
    Feb 27 11:41:54.928: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4 container env-test: <nil>
    STEP: delete the pod 02/27/23 11:41:54.944
    Feb 27 11:41:54.966: INFO: Waiting for pod pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4 to disappear
    Feb 27 11:41:54.974: INFO: Pod pod-configmaps-f84ce4f2-f010-44d4-b718-745b456d3ce4 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 11:41:54.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3669" for this suite. 02/27/23 11:41:54.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:41:55
Feb 27 11:41:55.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 11:41:55.001
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:41:55.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:41:55.043
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Feb 27 11:41:55.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 11:41:59.586
Feb 27 11:41:59.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-3506 --namespace=crd-publish-openapi-3506 create -f -'
Feb 27 11:42:00.940: INFO: stderr: ""
Feb 27 11:42:00.940: INFO: stdout: "e2e-test-crd-publish-openapi-5221-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 27 11:42:00.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-3506 --namespace=crd-publish-openapi-3506 delete e2e-test-crd-publish-openapi-5221-crds test-cr'
Feb 27 11:42:01.122: INFO: stderr: ""
Feb 27 11:42:01.127: INFO: stdout: "e2e-test-crd-publish-openapi-5221-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 27 11:42:01.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-3506 --namespace=crd-publish-openapi-3506 apply -f -'
Feb 27 11:42:01.747: INFO: stderr: ""
Feb 27 11:42:01.747: INFO: stdout: "e2e-test-crd-publish-openapi-5221-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 27 11:42:01.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-3506 --namespace=crd-publish-openapi-3506 delete e2e-test-crd-publish-openapi-5221-crds test-cr'
Feb 27 11:42:01.942: INFO: stderr: ""
Feb 27 11:42:01.942: INFO: stdout: "e2e-test-crd-publish-openapi-5221-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/27/23 11:42:01.942
Feb 27 11:42:01.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-3506 explain e2e-test-crd-publish-openapi-5221-crds'
Feb 27 11:42:02.638: INFO: stderr: ""
Feb 27 11:42:02.638: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5221-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:42:07.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3506" for this suite. 02/27/23 11:42:07.248
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":103,"skipped":2040,"failed":0}
------------------------------
• [SLOW TEST] [12.261 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:41:55
    Feb 27 11:41:55.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 11:41:55.001
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:41:55.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:41:55.043
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Feb 27 11:41:55.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 11:41:59.586
    Feb 27 11:41:59.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-3506 --namespace=crd-publish-openapi-3506 create -f -'
    Feb 27 11:42:00.940: INFO: stderr: ""
    Feb 27 11:42:00.940: INFO: stdout: "e2e-test-crd-publish-openapi-5221-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 27 11:42:00.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-3506 --namespace=crd-publish-openapi-3506 delete e2e-test-crd-publish-openapi-5221-crds test-cr'
    Feb 27 11:42:01.122: INFO: stderr: ""
    Feb 27 11:42:01.127: INFO: stdout: "e2e-test-crd-publish-openapi-5221-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Feb 27 11:42:01.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-3506 --namespace=crd-publish-openapi-3506 apply -f -'
    Feb 27 11:42:01.747: INFO: stderr: ""
    Feb 27 11:42:01.747: INFO: stdout: "e2e-test-crd-publish-openapi-5221-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 27 11:42:01.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-3506 --namespace=crd-publish-openapi-3506 delete e2e-test-crd-publish-openapi-5221-crds test-cr'
    Feb 27 11:42:01.942: INFO: stderr: ""
    Feb 27 11:42:01.942: INFO: stdout: "e2e-test-crd-publish-openapi-5221-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/27/23 11:42:01.942
    Feb 27 11:42:01.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-3506 explain e2e-test-crd-publish-openapi-5221-crds'
    Feb 27 11:42:02.638: INFO: stderr: ""
    Feb 27 11:42:02.638: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5221-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:42:07.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3506" for this suite. 02/27/23 11:42:07.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:42:07.262
Feb 27 11:42:07.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:42:07.264
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:07.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:07.302
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-c1466c1d-3c4a-4661-bddc-6b8a4bb68be2 02/27/23 11:42:07.31
STEP: Creating a pod to test consume secrets 02/27/23 11:42:07.32
Feb 27 11:42:07.336: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75" in namespace "projected-5907" to be "Succeeded or Failed"
Feb 27 11:42:07.347: INFO: Pod "pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75": Phase="Pending", Reason="", readiness=false. Elapsed: 11.376459ms
Feb 27 11:42:09.357: INFO: Pod "pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75": Phase="Running", Reason="", readiness=true. Elapsed: 2.021221801s
Feb 27 11:42:11.356: INFO: Pod "pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75": Phase="Running", Reason="", readiness=false. Elapsed: 4.019776493s
Feb 27 11:42:13.355: INFO: Pod "pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019324272s
STEP: Saw pod success 02/27/23 11:42:13.355
Feb 27 11:42:13.356: INFO: Pod "pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75" satisfied condition "Succeeded or Failed"
Feb 27 11:42:13.367: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 11:42:13.389
Feb 27 11:42:13.418: INFO: Waiting for pod pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75 to disappear
Feb 27 11:42:13.425: INFO: Pod pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 27 11:42:13.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5907" for this suite. 02/27/23 11:42:13.438
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":104,"skipped":2049,"failed":0}
------------------------------
• [SLOW TEST] [6.191 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:42:07.262
    Feb 27 11:42:07.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:42:07.264
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:07.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:07.302
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-c1466c1d-3c4a-4661-bddc-6b8a4bb68be2 02/27/23 11:42:07.31
    STEP: Creating a pod to test consume secrets 02/27/23 11:42:07.32
    Feb 27 11:42:07.336: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75" in namespace "projected-5907" to be "Succeeded or Failed"
    Feb 27 11:42:07.347: INFO: Pod "pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75": Phase="Pending", Reason="", readiness=false. Elapsed: 11.376459ms
    Feb 27 11:42:09.357: INFO: Pod "pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75": Phase="Running", Reason="", readiness=true. Elapsed: 2.021221801s
    Feb 27 11:42:11.356: INFO: Pod "pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75": Phase="Running", Reason="", readiness=false. Elapsed: 4.019776493s
    Feb 27 11:42:13.355: INFO: Pod "pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019324272s
    STEP: Saw pod success 02/27/23 11:42:13.355
    Feb 27 11:42:13.356: INFO: Pod "pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75" satisfied condition "Succeeded or Failed"
    Feb 27 11:42:13.367: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 11:42:13.389
    Feb 27 11:42:13.418: INFO: Waiting for pod pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75 to disappear
    Feb 27 11:42:13.425: INFO: Pod pod-projected-secrets-eb42a6c8-371b-4700-abbd-da91d6aeda75 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 27 11:42:13.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5907" for this suite. 02/27/23 11:42:13.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:42:13.46
Feb 27 11:42:13.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 11:42:13.461
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:13.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:13.508
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/27/23 11:42:13.52
Feb 27 11:42:13.540: INFO: Waiting up to 5m0s for pod "pod-b1f78739-6533-4d10-a93a-543f5911e851" in namespace "emptydir-6206" to be "Succeeded or Failed"
Feb 27 11:42:13.556: INFO: Pod "pod-b1f78739-6533-4d10-a93a-543f5911e851": Phase="Pending", Reason="", readiness=false. Elapsed: 13.037155ms
Feb 27 11:42:15.579: INFO: Pod "pod-b1f78739-6533-4d10-a93a-543f5911e851": Phase="Running", Reason="", readiness=true. Elapsed: 2.036141834s
Feb 27 11:42:17.584: INFO: Pod "pod-b1f78739-6533-4d10-a93a-543f5911e851": Phase="Running", Reason="", readiness=false. Elapsed: 4.041103976s
Feb 27 11:42:19.578: INFO: Pod "pod-b1f78739-6533-4d10-a93a-543f5911e851": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034426868s
STEP: Saw pod success 02/27/23 11:42:19.578
Feb 27 11:42:19.578: INFO: Pod "pod-b1f78739-6533-4d10-a93a-543f5911e851" satisfied condition "Succeeded or Failed"
Feb 27 11:42:19.588: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-b1f78739-6533-4d10-a93a-543f5911e851 container test-container: <nil>
STEP: delete the pod 02/27/23 11:42:19.632
Feb 27 11:42:19.657: INFO: Waiting for pod pod-b1f78739-6533-4d10-a93a-543f5911e851 to disappear
Feb 27 11:42:19.664: INFO: Pod pod-b1f78739-6533-4d10-a93a-543f5911e851 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 11:42:19.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6206" for this suite. 02/27/23 11:42:19.68
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":105,"skipped":2141,"failed":0}
------------------------------
• [SLOW TEST] [6.235 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:42:13.46
    Feb 27 11:42:13.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 11:42:13.461
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:13.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:13.508
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/27/23 11:42:13.52
    Feb 27 11:42:13.540: INFO: Waiting up to 5m0s for pod "pod-b1f78739-6533-4d10-a93a-543f5911e851" in namespace "emptydir-6206" to be "Succeeded or Failed"
    Feb 27 11:42:13.556: INFO: Pod "pod-b1f78739-6533-4d10-a93a-543f5911e851": Phase="Pending", Reason="", readiness=false. Elapsed: 13.037155ms
    Feb 27 11:42:15.579: INFO: Pod "pod-b1f78739-6533-4d10-a93a-543f5911e851": Phase="Running", Reason="", readiness=true. Elapsed: 2.036141834s
    Feb 27 11:42:17.584: INFO: Pod "pod-b1f78739-6533-4d10-a93a-543f5911e851": Phase="Running", Reason="", readiness=false. Elapsed: 4.041103976s
    Feb 27 11:42:19.578: INFO: Pod "pod-b1f78739-6533-4d10-a93a-543f5911e851": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034426868s
    STEP: Saw pod success 02/27/23 11:42:19.578
    Feb 27 11:42:19.578: INFO: Pod "pod-b1f78739-6533-4d10-a93a-543f5911e851" satisfied condition "Succeeded or Failed"
    Feb 27 11:42:19.588: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-b1f78739-6533-4d10-a93a-543f5911e851 container test-container: <nil>
    STEP: delete the pod 02/27/23 11:42:19.632
    Feb 27 11:42:19.657: INFO: Waiting for pod pod-b1f78739-6533-4d10-a93a-543f5911e851 to disappear
    Feb 27 11:42:19.664: INFO: Pod pod-b1f78739-6533-4d10-a93a-543f5911e851 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 11:42:19.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6206" for this suite. 02/27/23 11:42:19.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:42:19.704
Feb 27 11:42:19.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename endpointslice 02/27/23 11:42:19.705
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:19.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:19.75
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Feb 27 11:42:19.783: INFO: Endpoints addresses: [100.64.30.10] , ports: [6443]
Feb 27 11:42:19.783: INFO: EndpointSlices addresses: [100.64.30.10] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 27 11:42:19.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6019" for this suite. 02/27/23 11:42:19.798
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":106,"skipped":2156,"failed":0}
------------------------------
• [0.107 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:42:19.704
    Feb 27 11:42:19.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename endpointslice 02/27/23 11:42:19.705
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:19.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:19.75
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Feb 27 11:42:19.783: INFO: Endpoints addresses: [100.64.30.10] , ports: [6443]
    Feb 27 11:42:19.783: INFO: EndpointSlices addresses: [100.64.30.10] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 27 11:42:19.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6019" for this suite. 02/27/23 11:42:19.798
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:42:19.817
Feb 27 11:42:19.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 11:42:19.818
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:19.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:19.849
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 11:42:19.88
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:42:20.444
STEP: Deploying the webhook pod 02/27/23 11:42:20.458
STEP: Wait for the deployment to be ready 02/27/23 11:42:20.75
Feb 27 11:42:20.773: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 11:42:22.81
STEP: Verifying the service has paired with the endpoint 02/27/23 11:42:22.835
Feb 27 11:42:23.836: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Feb 27 11:42:23.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9511-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 11:42:24.368
STEP: Creating a custom resource while v1 is storage version 02/27/23 11:42:24.506
STEP: Patching Custom Resource Definition to set v2 as storage 02/27/23 11:42:26.604
STEP: Patching the custom resource while v2 is storage version 02/27/23 11:42:26.649
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:42:27.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-11" for this suite. 02/27/23 11:42:27.469
STEP: Destroying namespace "webhook-11-markers" for this suite. 02/27/23 11:42:27.486
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":107,"skipped":2160,"failed":0}
------------------------------
• [SLOW TEST] [7.791 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:42:19.817
    Feb 27 11:42:19.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 11:42:19.818
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:19.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:19.849
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 11:42:19.88
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:42:20.444
    STEP: Deploying the webhook pod 02/27/23 11:42:20.458
    STEP: Wait for the deployment to be ready 02/27/23 11:42:20.75
    Feb 27 11:42:20.773: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 11:42:22.81
    STEP: Verifying the service has paired with the endpoint 02/27/23 11:42:22.835
    Feb 27 11:42:23.836: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Feb 27 11:42:23.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9511-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 11:42:24.368
    STEP: Creating a custom resource while v1 is storage version 02/27/23 11:42:24.506
    STEP: Patching Custom Resource Definition to set v2 as storage 02/27/23 11:42:26.604
    STEP: Patching the custom resource while v2 is storage version 02/27/23 11:42:26.649
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:42:27.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-11" for this suite. 02/27/23 11:42:27.469
    STEP: Destroying namespace "webhook-11-markers" for this suite. 02/27/23 11:42:27.486
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:42:27.615
Feb 27 11:42:27.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename subpath 02/27/23 11:42:27.616
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:27.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:27.668
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 11:42:27.68
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-qvh4 02/27/23 11:42:27.705
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 11:42:27.705
Feb 27 11:42:27.722: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qvh4" in namespace "subpath-5992" to be "Succeeded or Failed"
Feb 27 11:42:27.732: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.714516ms
Feb 27 11:42:29.741: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018952765s
Feb 27 11:42:31.742: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 4.020333714s
Feb 27 11:42:33.745: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 6.022939303s
Feb 27 11:42:35.740: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 8.01864494s
Feb 27 11:42:37.740: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 10.018689934s
Feb 27 11:42:39.742: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 12.020526109s
Feb 27 11:42:41.743: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 14.021058517s
Feb 27 11:42:43.742: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 16.02032958s
Feb 27 11:42:45.742: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 18.020705599s
Feb 27 11:42:47.744: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 20.022057653s
Feb 27 11:42:49.741: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=false. Elapsed: 22.01971446s
Feb 27 11:42:51.786: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.064132251s
STEP: Saw pod success 02/27/23 11:42:51.786
Feb 27 11:42:51.786: INFO: Pod "pod-subpath-test-downwardapi-qvh4" satisfied condition "Succeeded or Failed"
Feb 27 11:42:51.797: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-subpath-test-downwardapi-qvh4 container test-container-subpath-downwardapi-qvh4: <nil>
STEP: delete the pod 02/27/23 11:42:51.82
Feb 27 11:42:51.844: INFO: Waiting for pod pod-subpath-test-downwardapi-qvh4 to disappear
Feb 27 11:42:51.851: INFO: Pod pod-subpath-test-downwardapi-qvh4 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qvh4 02/27/23 11:42:51.851
Feb 27 11:42:51.856: INFO: Deleting pod "pod-subpath-test-downwardapi-qvh4" in namespace "subpath-5992"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 27 11:42:51.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5992" for this suite. 02/27/23 11:42:51.878
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":108,"skipped":2171,"failed":0}
------------------------------
• [SLOW TEST] [24.276 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:42:27.615
    Feb 27 11:42:27.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename subpath 02/27/23 11:42:27.616
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:27.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:27.668
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 11:42:27.68
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-qvh4 02/27/23 11:42:27.705
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 11:42:27.705
    Feb 27 11:42:27.722: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qvh4" in namespace "subpath-5992" to be "Succeeded or Failed"
    Feb 27 11:42:27.732: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.714516ms
    Feb 27 11:42:29.741: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018952765s
    Feb 27 11:42:31.742: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 4.020333714s
    Feb 27 11:42:33.745: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 6.022939303s
    Feb 27 11:42:35.740: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 8.01864494s
    Feb 27 11:42:37.740: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 10.018689934s
    Feb 27 11:42:39.742: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 12.020526109s
    Feb 27 11:42:41.743: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 14.021058517s
    Feb 27 11:42:43.742: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 16.02032958s
    Feb 27 11:42:45.742: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 18.020705599s
    Feb 27 11:42:47.744: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=true. Elapsed: 20.022057653s
    Feb 27 11:42:49.741: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Running", Reason="", readiness=false. Elapsed: 22.01971446s
    Feb 27 11:42:51.786: INFO: Pod "pod-subpath-test-downwardapi-qvh4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.064132251s
    STEP: Saw pod success 02/27/23 11:42:51.786
    Feb 27 11:42:51.786: INFO: Pod "pod-subpath-test-downwardapi-qvh4" satisfied condition "Succeeded or Failed"
    Feb 27 11:42:51.797: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-subpath-test-downwardapi-qvh4 container test-container-subpath-downwardapi-qvh4: <nil>
    STEP: delete the pod 02/27/23 11:42:51.82
    Feb 27 11:42:51.844: INFO: Waiting for pod pod-subpath-test-downwardapi-qvh4 to disappear
    Feb 27 11:42:51.851: INFO: Pod pod-subpath-test-downwardapi-qvh4 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-qvh4 02/27/23 11:42:51.851
    Feb 27 11:42:51.856: INFO: Deleting pod "pod-subpath-test-downwardapi-qvh4" in namespace "subpath-5992"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 27 11:42:51.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5992" for this suite. 02/27/23 11:42:51.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:42:51.907
Feb 27 11:42:51.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pods 02/27/23 11:42:51.908
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:51.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:51.948
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 02/27/23 11:42:51.956
STEP: submitting the pod to kubernetes 02/27/23 11:42:51.956
STEP: verifying QOS class is set on the pod 02/27/23 11:42:51.975
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Feb 27 11:42:51.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6617" for this suite. 02/27/23 11:42:52.001
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":109,"skipped":2255,"failed":0}
------------------------------
• [0.109 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:42:51.907
    Feb 27 11:42:51.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pods 02/27/23 11:42:51.908
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:51.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:51.948
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 02/27/23 11:42:51.956
    STEP: submitting the pod to kubernetes 02/27/23 11:42:51.956
    STEP: verifying QOS class is set on the pod 02/27/23 11:42:51.975
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Feb 27 11:42:51.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6617" for this suite. 02/27/23 11:42:52.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:42:52.021
Feb 27 11:42:52.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 11:42:52.023
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:52.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:52.056
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Feb 27 11:42:52.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:42:55.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-70" for this suite. 02/27/23 11:42:55.318
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":110,"skipped":2309,"failed":0}
------------------------------
• [3.317 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:42:52.021
    Feb 27 11:42:52.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 11:42:52.023
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:52.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:52.056
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Feb 27 11:42:52.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:42:55.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-70" for this suite. 02/27/23 11:42:55.318
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:42:55.339
Feb 27 11:42:55.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename deployment 02/27/23 11:42:55.341
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:55.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:55.39
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Feb 27 11:42:55.435: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 27 11:43:00.445: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 11:43:00.445
Feb 27 11:43:00.445: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/27/23 11:43:00.477
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 11:43:02.531: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5549  de4b9be1-ad19-41a8-84a1-df04363de276 67814 1 2023-02-27 11:43:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-27 11:43:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 11:43:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c51ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 11:43:00 +0000 UTC,LastTransitionTime:2023-02-27 11:43:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-02-27 11:43:01 +0000 UTC,LastTransitionTime:2023-02-27 11:43:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 11:43:02.541: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-5549  5a2a34a5-dbc0-4631-a6cb-b8e0f1355e4f 67804 1 2023-02-27 11:43:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment de4b9be1-ad19-41a8-84a1-df04363de276 0xc003ce6fd7 0xc003ce6fd8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 11:43:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de4b9be1-ad19-41a8-84a1-df04363de276\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 11:43:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ce70c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 11:43:02.551: INFO: Pod "test-cleanup-deployment-69cb9c5497-vvm75" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-vvm75 test-cleanup-deployment-69cb9c5497- deployment-5549  272c21e7-48f7-4784-88ac-4f818cc140dc 67803 0 2023-02-27 11:43:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:ba593bf3522f06fe912d9085cc8fdead418492403b68a8c860a826e09578d1b1 cni.projectcalico.org/podIP:172.25.2.85/32 cni.projectcalico.org/podIPs:172.25.2.85/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 5a2a34a5-dbc0-4631-a6cb-b8e0f1355e4f 0xc003d123e7 0xc003d123e8}] [] [{kube-controller-manager Update v1 2023-02-27 11:43:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a2a34a5-dbc0-4631-a6cb-b8e0f1355e4f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 11:43:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 11:43:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggblr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggblr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:43:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:43:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:43:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:43:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.85,StartTime:2023-02-27 11:43:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 11:43:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://14dcb6246c72dfbf92d5e5a9cc94ae3944032fce3f811a19c41c35dd61b10d05,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 27 11:43:02.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5549" for this suite. 02/27/23 11:43:02.564
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":111,"skipped":2310,"failed":0}
------------------------------
• [SLOW TEST] [7.238 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:42:55.339
    Feb 27 11:42:55.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename deployment 02/27/23 11:42:55.341
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:42:55.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:42:55.39
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Feb 27 11:42:55.435: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Feb 27 11:43:00.445: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 11:43:00.445
    Feb 27 11:43:00.445: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/27/23 11:43:00.477
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 11:43:02.531: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5549  de4b9be1-ad19-41a8-84a1-df04363de276 67814 1 2023-02-27 11:43:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-27 11:43:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 11:43:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c51ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 11:43:00 +0000 UTC,LastTransitionTime:2023-02-27 11:43:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-02-27 11:43:01 +0000 UTC,LastTransitionTime:2023-02-27 11:43:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 11:43:02.541: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-5549  5a2a34a5-dbc0-4631-a6cb-b8e0f1355e4f 67804 1 2023-02-27 11:43:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment de4b9be1-ad19-41a8-84a1-df04363de276 0xc003ce6fd7 0xc003ce6fd8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 11:43:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de4b9be1-ad19-41a8-84a1-df04363de276\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 11:43:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ce70c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 11:43:02.551: INFO: Pod "test-cleanup-deployment-69cb9c5497-vvm75" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-vvm75 test-cleanup-deployment-69cb9c5497- deployment-5549  272c21e7-48f7-4784-88ac-4f818cc140dc 67803 0 2023-02-27 11:43:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:ba593bf3522f06fe912d9085cc8fdead418492403b68a8c860a826e09578d1b1 cni.projectcalico.org/podIP:172.25.2.85/32 cni.projectcalico.org/podIPs:172.25.2.85/32] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 5a2a34a5-dbc0-4631-a6cb-b8e0f1355e4f 0xc003d123e7 0xc003d123e8}] [] [{kube-controller-manager Update v1 2023-02-27 11:43:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a2a34a5-dbc0-4631-a6cb-b8e0f1355e4f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 11:43:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 11:43:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggblr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggblr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:43:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:43:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:43:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 11:43:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.85,StartTime:2023-02-27 11:43:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 11:43:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://14dcb6246c72dfbf92d5e5a9cc94ae3944032fce3f811a19c41c35dd61b10d05,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 27 11:43:02.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5549" for this suite. 02/27/23 11:43:02.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:43:02.579
Feb 27 11:43:02.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename replicaset 02/27/23 11:43:02.58
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:02.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:02.657
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Feb 27 11:43:02.698: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 27 11:43:07.707: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 11:43:07.707
STEP: Scaling up "test-rs" replicaset  02/27/23 11:43:07.708
Feb 27 11:43:07.738: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 02/27/23 11:43:07.738
W0227 11:43:07.752922      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 27 11:43:07.763: INFO: observed ReplicaSet test-rs in namespace replicaset-263 with ReadyReplicas 1, AvailableReplicas 1
Feb 27 11:43:07.784: INFO: observed ReplicaSet test-rs in namespace replicaset-263 with ReadyReplicas 1, AvailableReplicas 1
Feb 27 11:43:07.804: INFO: observed ReplicaSet test-rs in namespace replicaset-263 with ReadyReplicas 1, AvailableReplicas 1
Feb 27 11:43:07.816: INFO: observed ReplicaSet test-rs in namespace replicaset-263 with ReadyReplicas 1, AvailableReplicas 1
Feb 27 11:43:08.694: INFO: observed ReplicaSet test-rs in namespace replicaset-263 with ReadyReplicas 2, AvailableReplicas 2
Feb 27 11:43:09.800: INFO: observed Replicaset test-rs in namespace replicaset-263 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 27 11:43:09.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-263" for this suite. 02/27/23 11:43:09.826
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":112,"skipped":2318,"failed":0}
------------------------------
• [SLOW TEST] [7.271 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:43:02.579
    Feb 27 11:43:02.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename replicaset 02/27/23 11:43:02.58
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:02.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:02.657
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Feb 27 11:43:02.698: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 27 11:43:07.707: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 11:43:07.707
    STEP: Scaling up "test-rs" replicaset  02/27/23 11:43:07.708
    Feb 27 11:43:07.738: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 02/27/23 11:43:07.738
    W0227 11:43:07.752922      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 27 11:43:07.763: INFO: observed ReplicaSet test-rs in namespace replicaset-263 with ReadyReplicas 1, AvailableReplicas 1
    Feb 27 11:43:07.784: INFO: observed ReplicaSet test-rs in namespace replicaset-263 with ReadyReplicas 1, AvailableReplicas 1
    Feb 27 11:43:07.804: INFO: observed ReplicaSet test-rs in namespace replicaset-263 with ReadyReplicas 1, AvailableReplicas 1
    Feb 27 11:43:07.816: INFO: observed ReplicaSet test-rs in namespace replicaset-263 with ReadyReplicas 1, AvailableReplicas 1
    Feb 27 11:43:08.694: INFO: observed ReplicaSet test-rs in namespace replicaset-263 with ReadyReplicas 2, AvailableReplicas 2
    Feb 27 11:43:09.800: INFO: observed Replicaset test-rs in namespace replicaset-263 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 27 11:43:09.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-263" for this suite. 02/27/23 11:43:09.826
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:43:09.853
Feb 27 11:43:09.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename certificates 02/27/23 11:43:09.854
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:09.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:09.894
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 02/27/23 11:43:11.042
STEP: getting /apis/certificates.k8s.io 02/27/23 11:43:11.05
STEP: getting /apis/certificates.k8s.io/v1 02/27/23 11:43:11.054
STEP: creating 02/27/23 11:43:11.058
STEP: getting 02/27/23 11:43:11.102
STEP: listing 02/27/23 11:43:11.109
STEP: watching 02/27/23 11:43:11.116
Feb 27 11:43:11.117: INFO: starting watch
STEP: patching 02/27/23 11:43:11.121
STEP: updating 02/27/23 11:43:11.133
Feb 27 11:43:11.145: INFO: waiting for watch events with expected annotations
Feb 27 11:43:11.145: INFO: saw patched and updated annotations
STEP: getting /approval 02/27/23 11:43:11.145
STEP: patching /approval 02/27/23 11:43:11.156
STEP: updating /approval 02/27/23 11:43:11.17
STEP: getting /status 02/27/23 11:43:11.18
STEP: patching /status 02/27/23 11:43:11.187
STEP: updating /status 02/27/23 11:43:11.203
STEP: deleting 02/27/23 11:43:11.216
STEP: deleting a collection 02/27/23 11:43:11.247
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:43:11.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7060" for this suite. 02/27/23 11:43:11.292
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":113,"skipped":2321,"failed":0}
------------------------------
• [1.459 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:43:09.853
    Feb 27 11:43:09.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename certificates 02/27/23 11:43:09.854
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:09.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:09.894
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 02/27/23 11:43:11.042
    STEP: getting /apis/certificates.k8s.io 02/27/23 11:43:11.05
    STEP: getting /apis/certificates.k8s.io/v1 02/27/23 11:43:11.054
    STEP: creating 02/27/23 11:43:11.058
    STEP: getting 02/27/23 11:43:11.102
    STEP: listing 02/27/23 11:43:11.109
    STEP: watching 02/27/23 11:43:11.116
    Feb 27 11:43:11.117: INFO: starting watch
    STEP: patching 02/27/23 11:43:11.121
    STEP: updating 02/27/23 11:43:11.133
    Feb 27 11:43:11.145: INFO: waiting for watch events with expected annotations
    Feb 27 11:43:11.145: INFO: saw patched and updated annotations
    STEP: getting /approval 02/27/23 11:43:11.145
    STEP: patching /approval 02/27/23 11:43:11.156
    STEP: updating /approval 02/27/23 11:43:11.17
    STEP: getting /status 02/27/23 11:43:11.18
    STEP: patching /status 02/27/23 11:43:11.187
    STEP: updating /status 02/27/23 11:43:11.203
    STEP: deleting 02/27/23 11:43:11.216
    STEP: deleting a collection 02/27/23 11:43:11.247
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:43:11.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-7060" for this suite. 02/27/23 11:43:11.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:43:11.314
Feb 27 11:43:11.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:43:11.315
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:11.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:11.362
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 02/27/23 11:43:11.371
Feb 27 11:43:11.389: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c" in namespace "projected-5029" to be "Succeeded or Failed"
Feb 27 11:43:11.399: INFO: Pod "downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031052ms
Feb 27 11:43:13.408: INFO: Pod "downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018929533s
Feb 27 11:43:15.407: INFO: Pod "downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01857799s
STEP: Saw pod success 02/27/23 11:43:15.408
Feb 27 11:43:15.408: INFO: Pod "downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c" satisfied condition "Succeeded or Failed"
Feb 27 11:43:15.416: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c container client-container: <nil>
STEP: delete the pod 02/27/23 11:43:15.437
Feb 27 11:43:15.463: INFO: Waiting for pod downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c to disappear
Feb 27 11:43:15.470: INFO: Pod downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 27 11:43:15.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5029" for this suite. 02/27/23 11:43:15.479
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":114,"skipped":2346,"failed":0}
------------------------------
• [4.184 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:43:11.314
    Feb 27 11:43:11.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:43:11.315
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:11.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:11.362
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 02/27/23 11:43:11.371
    Feb 27 11:43:11.389: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c" in namespace "projected-5029" to be "Succeeded or Failed"
    Feb 27 11:43:11.399: INFO: Pod "downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031052ms
    Feb 27 11:43:13.408: INFO: Pod "downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018929533s
    Feb 27 11:43:15.407: INFO: Pod "downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01857799s
    STEP: Saw pod success 02/27/23 11:43:15.408
    Feb 27 11:43:15.408: INFO: Pod "downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c" satisfied condition "Succeeded or Failed"
    Feb 27 11:43:15.416: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c container client-container: <nil>
    STEP: delete the pod 02/27/23 11:43:15.437
    Feb 27 11:43:15.463: INFO: Waiting for pod downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c to disappear
    Feb 27 11:43:15.470: INFO: Pod downwardapi-volume-bd160353-c851-4986-b6f5-60d33ad5ba6c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 27 11:43:15.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5029" for this suite. 02/27/23 11:43:15.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:43:15.5
Feb 27 11:43:15.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 11:43:15.501
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:15.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:15.546
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-83d31b99-e20a-47e1-9203-e5b8cd3b92cc 02/27/23 11:43:15.555
STEP: Creating a pod to test consume secrets 02/27/23 11:43:15.564
Feb 27 11:43:15.580: INFO: Waiting up to 5m0s for pod "pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a" in namespace "secrets-5176" to be "Succeeded or Failed"
Feb 27 11:43:15.592: INFO: Pod "pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.404487ms
Feb 27 11:43:17.616: INFO: Pod "pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.035847929s
Feb 27 11:43:19.617: INFO: Pod "pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a": Phase="Running", Reason="", readiness=false. Elapsed: 4.036101261s
Feb 27 11:43:21.742: INFO: Pod "pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.161819208s
STEP: Saw pod success 02/27/23 11:43:21.742
Feb 27 11:43:21.743: INFO: Pod "pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a" satisfied condition "Succeeded or Failed"
Feb 27 11:43:21.750: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 11:43:21.769
Feb 27 11:43:21.792: INFO: Waiting for pod pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a to disappear
Feb 27 11:43:21.803: INFO: Pod pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 27 11:43:21.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5176" for this suite. 02/27/23 11:43:21.816
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":115,"skipped":2352,"failed":0}
------------------------------
• [SLOW TEST] [6.333 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:43:15.5
    Feb 27 11:43:15.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 11:43:15.501
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:15.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:15.546
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-83d31b99-e20a-47e1-9203-e5b8cd3b92cc 02/27/23 11:43:15.555
    STEP: Creating a pod to test consume secrets 02/27/23 11:43:15.564
    Feb 27 11:43:15.580: INFO: Waiting up to 5m0s for pod "pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a" in namespace "secrets-5176" to be "Succeeded or Failed"
    Feb 27 11:43:15.592: INFO: Pod "pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.404487ms
    Feb 27 11:43:17.616: INFO: Pod "pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.035847929s
    Feb 27 11:43:19.617: INFO: Pod "pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a": Phase="Running", Reason="", readiness=false. Elapsed: 4.036101261s
    Feb 27 11:43:21.742: INFO: Pod "pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.161819208s
    STEP: Saw pod success 02/27/23 11:43:21.742
    Feb 27 11:43:21.743: INFO: Pod "pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a" satisfied condition "Succeeded or Failed"
    Feb 27 11:43:21.750: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 11:43:21.769
    Feb 27 11:43:21.792: INFO: Waiting for pod pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a to disappear
    Feb 27 11:43:21.803: INFO: Pod pod-secrets-0c27faf7-d895-4dd8-8cc4-7a2f15a0df7a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 11:43:21.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5176" for this suite. 02/27/23 11:43:21.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:43:21.837
Feb 27 11:43:21.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pod-network-test 02/27/23 11:43:21.838
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:21.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:21.873
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7262 02/27/23 11:43:21.882
STEP: creating a selector 02/27/23 11:43:21.882
STEP: Creating the service pods in kubernetes 02/27/23 11:43:21.882
Feb 27 11:43:21.882: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 27 11:43:21.946: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7262" to be "running and ready"
Feb 27 11:43:21.956: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.550013ms
Feb 27 11:43:21.957: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:43:23.966: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.019706572s
Feb 27 11:43:23.966: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 11:43:25.966: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01976774s
Feb 27 11:43:25.966: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 11:43:27.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.021159623s
Feb 27 11:43:27.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 11:43:29.965: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018750802s
Feb 27 11:43:29.965: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 11:43:31.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.026906196s
Feb 27 11:43:31.973: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 11:43:33.964: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.018303048s
Feb 27 11:43:33.964: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 27 11:43:33.964: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 27 11:43:33.974: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7262" to be "running and ready"
Feb 27 11:43:33.983: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.306109ms
Feb 27 11:43:33.983: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 27 11:43:33.983: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 27 11:43:33.990: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7262" to be "running and ready"
Feb 27 11:43:33.997: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.14152ms
Feb 27 11:43:33.997: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 27 11:43:33.997: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/27/23 11:43:34.006
Feb 27 11:43:34.016: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7262" to be "running"
Feb 27 11:43:34.026: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.886701ms
Feb 27 11:43:36.035: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018425946s
Feb 27 11:43:36.035: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 27 11:43:36.042: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 27 11:43:36.042: INFO: Breadth first check of 172.25.1.73 on host 172.31.11.159...
Feb 27 11:43:36.052: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.74:9080/dial?request=hostname&protocol=http&host=172.25.1.73&port=8083&tries=1'] Namespace:pod-network-test-7262 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 11:43:36.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 11:43:36.053: INFO: ExecWithOptions: Clientset creation
Feb 27 11:43:36.053: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7262/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.1.73%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 11:43:36.871: INFO: Waiting for responses: map[]
Feb 27 11:43:36.871: INFO: reached 172.25.1.73 after 0/1 tries
Feb 27 11:43:36.871: INFO: Breadth first check of 172.25.2.87 on host 172.31.15.17...
Feb 27 11:43:36.881: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.74:9080/dial?request=hostname&protocol=http&host=172.25.2.87&port=8083&tries=1'] Namespace:pod-network-test-7262 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 11:43:36.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 11:43:36.882: INFO: ExecWithOptions: Clientset creation
Feb 27 11:43:36.882: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7262/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.2.87%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 11:43:37.025: INFO: Waiting for responses: map[]
Feb 27 11:43:37.025: INFO: reached 172.25.2.87 after 0/1 tries
Feb 27 11:43:37.025: INFO: Breadth first check of 172.25.0.19 on host 172.31.7.167...
Feb 27 11:43:37.033: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.74:9080/dial?request=hostname&protocol=http&host=172.25.0.19&port=8083&tries=1'] Namespace:pod-network-test-7262 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 11:43:37.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 11:43:37.033: INFO: ExecWithOptions: Clientset creation
Feb 27 11:43:37.034: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7262/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.0.19%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 11:43:37.149: INFO: Waiting for responses: map[]
Feb 27 11:43:37.149: INFO: reached 172.25.0.19 after 0/1 tries
Feb 27 11:43:37.149: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 27 11:43:37.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7262" for this suite. 02/27/23 11:43:37.164
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":116,"skipped":2391,"failed":0}
------------------------------
• [SLOW TEST] [15.345 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:43:21.837
    Feb 27 11:43:21.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pod-network-test 02/27/23 11:43:21.838
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:21.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:21.873
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7262 02/27/23 11:43:21.882
    STEP: creating a selector 02/27/23 11:43:21.882
    STEP: Creating the service pods in kubernetes 02/27/23 11:43:21.882
    Feb 27 11:43:21.882: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 27 11:43:21.946: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7262" to be "running and ready"
    Feb 27 11:43:21.956: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.550013ms
    Feb 27 11:43:21.957: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:43:23.966: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.019706572s
    Feb 27 11:43:23.966: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 11:43:25.966: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01976774s
    Feb 27 11:43:25.966: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 11:43:27.967: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.021159623s
    Feb 27 11:43:27.967: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 11:43:29.965: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018750802s
    Feb 27 11:43:29.965: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 11:43:31.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.026906196s
    Feb 27 11:43:31.973: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 11:43:33.964: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.018303048s
    Feb 27 11:43:33.964: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 27 11:43:33.964: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 27 11:43:33.974: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7262" to be "running and ready"
    Feb 27 11:43:33.983: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.306109ms
    Feb 27 11:43:33.983: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 27 11:43:33.983: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 27 11:43:33.990: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7262" to be "running and ready"
    Feb 27 11:43:33.997: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.14152ms
    Feb 27 11:43:33.997: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 27 11:43:33.997: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/27/23 11:43:34.006
    Feb 27 11:43:34.016: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7262" to be "running"
    Feb 27 11:43:34.026: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.886701ms
    Feb 27 11:43:36.035: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018425946s
    Feb 27 11:43:36.035: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 27 11:43:36.042: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 27 11:43:36.042: INFO: Breadth first check of 172.25.1.73 on host 172.31.11.159...
    Feb 27 11:43:36.052: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.74:9080/dial?request=hostname&protocol=http&host=172.25.1.73&port=8083&tries=1'] Namespace:pod-network-test-7262 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 11:43:36.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 11:43:36.053: INFO: ExecWithOptions: Clientset creation
    Feb 27 11:43:36.053: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7262/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.1.73%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 11:43:36.871: INFO: Waiting for responses: map[]
    Feb 27 11:43:36.871: INFO: reached 172.25.1.73 after 0/1 tries
    Feb 27 11:43:36.871: INFO: Breadth first check of 172.25.2.87 on host 172.31.15.17...
    Feb 27 11:43:36.881: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.74:9080/dial?request=hostname&protocol=http&host=172.25.2.87&port=8083&tries=1'] Namespace:pod-network-test-7262 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 11:43:36.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 11:43:36.882: INFO: ExecWithOptions: Clientset creation
    Feb 27 11:43:36.882: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7262/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.2.87%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 11:43:37.025: INFO: Waiting for responses: map[]
    Feb 27 11:43:37.025: INFO: reached 172.25.2.87 after 0/1 tries
    Feb 27 11:43:37.025: INFO: Breadth first check of 172.25.0.19 on host 172.31.7.167...
    Feb 27 11:43:37.033: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.74:9080/dial?request=hostname&protocol=http&host=172.25.0.19&port=8083&tries=1'] Namespace:pod-network-test-7262 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 11:43:37.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 11:43:37.033: INFO: ExecWithOptions: Clientset creation
    Feb 27 11:43:37.034: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-7262/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.0.19%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 11:43:37.149: INFO: Waiting for responses: map[]
    Feb 27 11:43:37.149: INFO: reached 172.25.0.19 after 0/1 tries
    Feb 27 11:43:37.149: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 27 11:43:37.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-7262" for this suite. 02/27/23 11:43:37.164
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:43:37.183
Feb 27 11:43:37.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename var-expansion 02/27/23 11:43:37.184
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:37.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:37.23
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Feb 27 11:43:37.274: INFO: Waiting up to 2m0s for pod "var-expansion-6643c170-b6a0-46bb-9c29-0b13623ce120" in namespace "var-expansion-9667" to be "container 0 failed with reason CreateContainerConfigError"
Feb 27 11:43:37.285: INFO: Pod "var-expansion-6643c170-b6a0-46bb-9c29-0b13623ce120": Phase="Pending", Reason="", readiness=false. Elapsed: 11.87584ms
Feb 27 11:43:39.294: INFO: Pod "var-expansion-6643c170-b6a0-46bb-9c29-0b13623ce120": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020063184s
Feb 27 11:43:39.294: INFO: Pod "var-expansion-6643c170-b6a0-46bb-9c29-0b13623ce120" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 27 11:43:39.294: INFO: Deleting pod "var-expansion-6643c170-b6a0-46bb-9c29-0b13623ce120" in namespace "var-expansion-9667"
Feb 27 11:43:39.311: INFO: Wait up to 5m0s for pod "var-expansion-6643c170-b6a0-46bb-9c29-0b13623ce120" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 27 11:43:43.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9667" for this suite. 02/27/23 11:43:43.444
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":117,"skipped":2391,"failed":0}
------------------------------
• [SLOW TEST] [6.284 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:43:37.183
    Feb 27 11:43:37.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename var-expansion 02/27/23 11:43:37.184
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:37.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:37.23
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Feb 27 11:43:37.274: INFO: Waiting up to 2m0s for pod "var-expansion-6643c170-b6a0-46bb-9c29-0b13623ce120" in namespace "var-expansion-9667" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 27 11:43:37.285: INFO: Pod "var-expansion-6643c170-b6a0-46bb-9c29-0b13623ce120": Phase="Pending", Reason="", readiness=false. Elapsed: 11.87584ms
    Feb 27 11:43:39.294: INFO: Pod "var-expansion-6643c170-b6a0-46bb-9c29-0b13623ce120": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020063184s
    Feb 27 11:43:39.294: INFO: Pod "var-expansion-6643c170-b6a0-46bb-9c29-0b13623ce120" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 27 11:43:39.294: INFO: Deleting pod "var-expansion-6643c170-b6a0-46bb-9c29-0b13623ce120" in namespace "var-expansion-9667"
    Feb 27 11:43:39.311: INFO: Wait up to 5m0s for pod "var-expansion-6643c170-b6a0-46bb-9c29-0b13623ce120" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 27 11:43:43.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9667" for this suite. 02/27/23 11:43:43.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:43:43.478
Feb 27 11:43:43.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename prestop 02/27/23 11:43:43.48
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:43.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:43.522
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-1127 02/27/23 11:43:43.532
STEP: Waiting for pods to come up. 02/27/23 11:43:43.548
Feb 27 11:43:43.548: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1127" to be "running"
Feb 27 11:43:43.558: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 9.53522ms
Feb 27 11:43:45.569: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.020351585s
Feb 27 11:43:45.569: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-1127 02/27/23 11:43:45.575
Feb 27 11:43:45.585: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1127" to be "running"
Feb 27 11:43:45.595: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 10.599484ms
Feb 27 11:43:47.604: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.019746382s
Feb 27 11:43:47.604: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 02/27/23 11:43:47.604
Feb 27 11:43:52.636: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 02/27/23 11:43:52.636
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Feb 27 11:43:52.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1127" for this suite. 02/27/23 11:43:52.701
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":118,"skipped":2409,"failed":0}
------------------------------
• [SLOW TEST] [9.249 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:43:43.478
    Feb 27 11:43:43.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename prestop 02/27/23 11:43:43.48
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:43.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:43.522
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-1127 02/27/23 11:43:43.532
    STEP: Waiting for pods to come up. 02/27/23 11:43:43.548
    Feb 27 11:43:43.548: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1127" to be "running"
    Feb 27 11:43:43.558: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 9.53522ms
    Feb 27 11:43:45.569: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.020351585s
    Feb 27 11:43:45.569: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-1127 02/27/23 11:43:45.575
    Feb 27 11:43:45.585: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1127" to be "running"
    Feb 27 11:43:45.595: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 10.599484ms
    Feb 27 11:43:47.604: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.019746382s
    Feb 27 11:43:47.604: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 02/27/23 11:43:47.604
    Feb 27 11:43:52.636: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 02/27/23 11:43:52.636
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Feb 27 11:43:52.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-1127" for this suite. 02/27/23 11:43:52.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:43:52.743
Feb 27 11:43:52.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:43:52.746
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:52.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:52.812
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 02/27/23 11:43:52.827
Feb 27 11:43:52.842: INFO: Waiting up to 5m0s for pod "labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13" in namespace "projected-3136" to be "running and ready"
Feb 27 11:43:52.855: INFO: Pod "labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13": Phase="Pending", Reason="", readiness=false. Elapsed: 11.800659ms
Feb 27 11:43:52.855: INFO: The phase of Pod labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:43:54.871: INFO: Pod "labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13": Phase="Running", Reason="", readiness=true. Elapsed: 2.027944656s
Feb 27 11:43:54.871: INFO: The phase of Pod labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13 is Running (Ready = true)
Feb 27 11:43:54.871: INFO: Pod "labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13" satisfied condition "running and ready"
Feb 27 11:43:55.420: INFO: Successfully updated pod "labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 27 11:43:59.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3136" for this suite. 02/27/23 11:43:59.529
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":119,"skipped":2417,"failed":0}
------------------------------
• [SLOW TEST] [6.800 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:43:52.743
    Feb 27 11:43:52.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:43:52.746
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:52.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:52.812
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 02/27/23 11:43:52.827
    Feb 27 11:43:52.842: INFO: Waiting up to 5m0s for pod "labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13" in namespace "projected-3136" to be "running and ready"
    Feb 27 11:43:52.855: INFO: Pod "labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13": Phase="Pending", Reason="", readiness=false. Elapsed: 11.800659ms
    Feb 27 11:43:52.855: INFO: The phase of Pod labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:43:54.871: INFO: Pod "labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13": Phase="Running", Reason="", readiness=true. Elapsed: 2.027944656s
    Feb 27 11:43:54.871: INFO: The phase of Pod labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13 is Running (Ready = true)
    Feb 27 11:43:54.871: INFO: Pod "labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13" satisfied condition "running and ready"
    Feb 27 11:43:55.420: INFO: Successfully updated pod "labelsupdate60b36527-3840-4cfe-aab2-f5924a4feb13"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 27 11:43:59.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3136" for this suite. 02/27/23 11:43:59.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:43:59.546
Feb 27 11:43:59.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-probe 02/27/23 11:43:59.548
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:59.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:59.603
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040 in namespace container-probe-5651 02/27/23 11:43:59.619
Feb 27 11:43:59.638: INFO: Waiting up to 5m0s for pod "test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040" in namespace "container-probe-5651" to be "not pending"
Feb 27 11:43:59.648: INFO: Pod "test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040": Phase="Pending", Reason="", readiness=false. Elapsed: 9.825741ms
Feb 27 11:44:01.658: INFO: Pod "test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040": Phase="Running", Reason="", readiness=true. Elapsed: 2.019989339s
Feb 27 11:44:01.658: INFO: Pod "test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040" satisfied condition "not pending"
Feb 27 11:44:01.658: INFO: Started pod test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040 in namespace container-probe-5651
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 11:44:01.658
Feb 27 11:44:01.666: INFO: Initial restart count of pod test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040 is 0
STEP: deleting the pod 02/27/23 11:48:03.1
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 27 11:48:03.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5651" for this suite. 02/27/23 11:48:03.161
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":120,"skipped":2450,"failed":0}
------------------------------
• [SLOW TEST] [243.631 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:43:59.546
    Feb 27 11:43:59.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-probe 02/27/23 11:43:59.548
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:43:59.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:43:59.603
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040 in namespace container-probe-5651 02/27/23 11:43:59.619
    Feb 27 11:43:59.638: INFO: Waiting up to 5m0s for pod "test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040" in namespace "container-probe-5651" to be "not pending"
    Feb 27 11:43:59.648: INFO: Pod "test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040": Phase="Pending", Reason="", readiness=false. Elapsed: 9.825741ms
    Feb 27 11:44:01.658: INFO: Pod "test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040": Phase="Running", Reason="", readiness=true. Elapsed: 2.019989339s
    Feb 27 11:44:01.658: INFO: Pod "test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040" satisfied condition "not pending"
    Feb 27 11:44:01.658: INFO: Started pod test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040 in namespace container-probe-5651
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 11:44:01.658
    Feb 27 11:44:01.666: INFO: Initial restart count of pod test-webserver-67b48af1-9f05-41f2-b66b-f00ec9d07040 is 0
    STEP: deleting the pod 02/27/23 11:48:03.1
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 27 11:48:03.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5651" for this suite. 02/27/23 11:48:03.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:48:03.182
Feb 27 11:48:03.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 11:48:03.187
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:48:03.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:48:03.311
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-1077 02/27/23 11:48:03.347
STEP: creating service affinity-clusterip-transition in namespace services-1077 02/27/23 11:48:03.348
STEP: creating replication controller affinity-clusterip-transition in namespace services-1077 02/27/23 11:48:03.381
I0227 11:48:03.394167      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1077, replica count: 3
I0227 11:48:06.451355      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 11:48:06.466: INFO: Creating new exec pod
Feb 27 11:48:06.477: INFO: Waiting up to 5m0s for pod "execpod-affinity2s9s8" in namespace "services-1077" to be "running"
Feb 27 11:48:06.491: INFO: Pod "execpod-affinity2s9s8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.402435ms
Feb 27 11:48:08.500: INFO: Pod "execpod-affinity2s9s8": Phase="Running", Reason="", readiness=true. Elapsed: 2.022335881s
Feb 27 11:48:08.500: INFO: Pod "execpod-affinity2s9s8" satisfied condition "running"
Feb 27 11:48:09.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-1077 exec execpod-affinity2s9s8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Feb 27 11:48:09.844: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb 27 11:48:09.844: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:48:09.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-1077 exec execpod-affinity2s9s8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.52 80'
Feb 27 11:48:10.066: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.19.52 80\nConnection to 10.240.19.52 80 port [tcp/http] succeeded!\n"
Feb 27 11:48:10.066: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:48:10.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-1077 exec execpod-affinity2s9s8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.19.52:80/ ; done'
Feb 27 11:48:10.533: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n"
Feb 27 11:48:10.533: INFO: stdout: "\naffinity-clusterip-transition-5lk5s\naffinity-clusterip-transition-p5nsp\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-5lk5s\naffinity-clusterip-transition-p5nsp\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-5lk5s\naffinity-clusterip-transition-p5nsp\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-5lk5s\naffinity-clusterip-transition-p5nsp\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-5lk5s\naffinity-clusterip-transition-p5nsp\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-5lk5s"
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-5lk5s
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-p5nsp
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-5lk5s
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-p5nsp
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-5lk5s
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-p5nsp
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-5lk5s
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-p5nsp
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-5lk5s
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-p5nsp
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-5lk5s
Feb 27 11:48:10.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-1077 exec execpod-affinity2s9s8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.19.52:80/ ; done'
Feb 27 11:48:10.946: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n"
Feb 27 11:48:10.946: INFO: stdout: "\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c"
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
Feb 27 11:48:10.946: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1077, will wait for the garbage collector to delete the pods 02/27/23 11:48:10.98
Feb 27 11:48:11.064: INFO: Deleting ReplicationController affinity-clusterip-transition took: 10.967625ms
Feb 27 11:48:11.166: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 102.082451ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 11:48:13.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1077" for this suite. 02/27/23 11:48:13.514
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":121,"skipped":2463,"failed":0}
------------------------------
• [SLOW TEST] [10.348 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:48:03.182
    Feb 27 11:48:03.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 11:48:03.187
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:48:03.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:48:03.311
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-1077 02/27/23 11:48:03.347
    STEP: creating service affinity-clusterip-transition in namespace services-1077 02/27/23 11:48:03.348
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1077 02/27/23 11:48:03.381
    I0227 11:48:03.394167      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1077, replica count: 3
    I0227 11:48:06.451355      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 11:48:06.466: INFO: Creating new exec pod
    Feb 27 11:48:06.477: INFO: Waiting up to 5m0s for pod "execpod-affinity2s9s8" in namespace "services-1077" to be "running"
    Feb 27 11:48:06.491: INFO: Pod "execpod-affinity2s9s8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.402435ms
    Feb 27 11:48:08.500: INFO: Pod "execpod-affinity2s9s8": Phase="Running", Reason="", readiness=true. Elapsed: 2.022335881s
    Feb 27 11:48:08.500: INFO: Pod "execpod-affinity2s9s8" satisfied condition "running"
    Feb 27 11:48:09.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-1077 exec execpod-affinity2s9s8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Feb 27 11:48:09.844: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Feb 27 11:48:09.844: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:48:09.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-1077 exec execpod-affinity2s9s8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.52 80'
    Feb 27 11:48:10.066: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.19.52 80\nConnection to 10.240.19.52 80 port [tcp/http] succeeded!\n"
    Feb 27 11:48:10.066: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:48:10.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-1077 exec execpod-affinity2s9s8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.19.52:80/ ; done'
    Feb 27 11:48:10.533: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n"
    Feb 27 11:48:10.533: INFO: stdout: "\naffinity-clusterip-transition-5lk5s\naffinity-clusterip-transition-p5nsp\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-5lk5s\naffinity-clusterip-transition-p5nsp\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-5lk5s\naffinity-clusterip-transition-p5nsp\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-5lk5s\naffinity-clusterip-transition-p5nsp\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-5lk5s\naffinity-clusterip-transition-p5nsp\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-5lk5s"
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-5lk5s
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-p5nsp
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-5lk5s
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-p5nsp
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-5lk5s
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-p5nsp
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-5lk5s
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-p5nsp
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-5lk5s
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-p5nsp
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.533: INFO: Received response from host: affinity-clusterip-transition-5lk5s
    Feb 27 11:48:10.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-1077 exec execpod-affinity2s9s8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.19.52:80/ ; done'
    Feb 27 11:48:10.946: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.19.52:80/\n"
    Feb 27 11:48:10.946: INFO: stdout: "\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c\naffinity-clusterip-transition-9hx9c"
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Received response from host: affinity-clusterip-transition-9hx9c
    Feb 27 11:48:10.946: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1077, will wait for the garbage collector to delete the pods 02/27/23 11:48:10.98
    Feb 27 11:48:11.064: INFO: Deleting ReplicationController affinity-clusterip-transition took: 10.967625ms
    Feb 27 11:48:11.166: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 102.082451ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 11:48:13.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1077" for this suite. 02/27/23 11:48:13.514
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:48:13.533
Feb 27 11:48:13.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 11:48:13.535
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:48:13.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:48:13.58
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 02/27/23 11:48:13.602
Feb 27 11:48:13.603: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb 27 11:48:13.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 create -f -'
Feb 27 11:48:13.994: INFO: stderr: ""
Feb 27 11:48:13.994: INFO: stdout: "service/agnhost-replica created\n"
Feb 27 11:48:13.994: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb 27 11:48:13.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 create -f -'
Feb 27 11:48:14.325: INFO: stderr: ""
Feb 27 11:48:14.325: INFO: stdout: "service/agnhost-primary created\n"
Feb 27 11:48:14.326: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 27 11:48:14.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 create -f -'
Feb 27 11:48:14.679: INFO: stderr: ""
Feb 27 11:48:14.679: INFO: stdout: "service/frontend created\n"
Feb 27 11:48:14.687: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 27 11:48:14.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 create -f -'
Feb 27 11:48:15.659: INFO: stderr: ""
Feb 27 11:48:15.659: INFO: stdout: "deployment.apps/frontend created\n"
Feb 27 11:48:15.659: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 27 11:48:15.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 create -f -'
Feb 27 11:48:16.010: INFO: stderr: ""
Feb 27 11:48:16.010: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb 27 11:48:16.010: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 27 11:48:16.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 create -f -'
Feb 27 11:48:16.574: INFO: stderr: ""
Feb 27 11:48:16.574: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 02/27/23 11:48:16.574
Feb 27 11:48:16.574: INFO: Waiting for all frontend pods to be Running.
Feb 27 11:48:21.625: INFO: Waiting for frontend to serve content.
Feb 27 11:48:21.663: INFO: Trying to add a new entry to the guestbook.
Feb 27 11:48:21.686: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 02/27/23 11:48:21.716
Feb 27 11:48:21.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 delete --grace-period=0 --force -f -'
Feb 27 11:48:21.908: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 11:48:21.910: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 11:48:21.91
Feb 27 11:48:21.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 delete --grace-period=0 --force -f -'
Feb 27 11:48:22.099: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 11:48:22.099: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 11:48:22.099
Feb 27 11:48:22.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 delete --grace-period=0 --force -f -'
Feb 27 11:48:22.242: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 11:48:22.242: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 11:48:22.242
Feb 27 11:48:22.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 delete --grace-period=0 --force -f -'
Feb 27 11:48:22.374: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 11:48:22.374: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 11:48:22.374
Feb 27 11:48:22.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 delete --grace-period=0 --force -f -'
Feb 27 11:48:22.572: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 11:48:22.572: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 11:48:22.572
Feb 27 11:48:22.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 delete --grace-period=0 --force -f -'
Feb 27 11:48:22.910: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 11:48:22.910: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 11:48:22.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7249" for this suite. 02/27/23 11:48:22.936
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":122,"skipped":2475,"failed":0}
------------------------------
• [SLOW TEST] [9.438 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:48:13.533
    Feb 27 11:48:13.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 11:48:13.535
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:48:13.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:48:13.58
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 02/27/23 11:48:13.602
    Feb 27 11:48:13.603: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Feb 27 11:48:13.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 create -f -'
    Feb 27 11:48:13.994: INFO: stderr: ""
    Feb 27 11:48:13.994: INFO: stdout: "service/agnhost-replica created\n"
    Feb 27 11:48:13.994: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Feb 27 11:48:13.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 create -f -'
    Feb 27 11:48:14.325: INFO: stderr: ""
    Feb 27 11:48:14.325: INFO: stdout: "service/agnhost-primary created\n"
    Feb 27 11:48:14.326: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Feb 27 11:48:14.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 create -f -'
    Feb 27 11:48:14.679: INFO: stderr: ""
    Feb 27 11:48:14.679: INFO: stdout: "service/frontend created\n"
    Feb 27 11:48:14.687: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Feb 27 11:48:14.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 create -f -'
    Feb 27 11:48:15.659: INFO: stderr: ""
    Feb 27 11:48:15.659: INFO: stdout: "deployment.apps/frontend created\n"
    Feb 27 11:48:15.659: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 27 11:48:15.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 create -f -'
    Feb 27 11:48:16.010: INFO: stderr: ""
    Feb 27 11:48:16.010: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Feb 27 11:48:16.010: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 27 11:48:16.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 create -f -'
    Feb 27 11:48:16.574: INFO: stderr: ""
    Feb 27 11:48:16.574: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 02/27/23 11:48:16.574
    Feb 27 11:48:16.574: INFO: Waiting for all frontend pods to be Running.
    Feb 27 11:48:21.625: INFO: Waiting for frontend to serve content.
    Feb 27 11:48:21.663: INFO: Trying to add a new entry to the guestbook.
    Feb 27 11:48:21.686: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 02/27/23 11:48:21.716
    Feb 27 11:48:21.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 delete --grace-period=0 --force -f -'
    Feb 27 11:48:21.908: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 11:48:21.910: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 11:48:21.91
    Feb 27 11:48:21.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 delete --grace-period=0 --force -f -'
    Feb 27 11:48:22.099: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 11:48:22.099: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 11:48:22.099
    Feb 27 11:48:22.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 delete --grace-period=0 --force -f -'
    Feb 27 11:48:22.242: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 11:48:22.242: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 11:48:22.242
    Feb 27 11:48:22.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 delete --grace-period=0 --force -f -'
    Feb 27 11:48:22.374: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 11:48:22.374: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 11:48:22.374
    Feb 27 11:48:22.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 delete --grace-period=0 --force -f -'
    Feb 27 11:48:22.572: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 11:48:22.572: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 11:48:22.572
    Feb 27 11:48:22.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-7249 delete --grace-period=0 --force -f -'
    Feb 27 11:48:22.910: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 11:48:22.910: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 11:48:22.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7249" for this suite. 02/27/23 11:48:22.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:48:23.046
Feb 27 11:48:23.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 11:48:23.054
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:48:23.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:48:23.121
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Feb 27 11:48:23.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:48:24.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9102" for this suite. 02/27/23 11:48:24.216
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":123,"skipped":2519,"failed":0}
------------------------------
• [1.182 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:48:23.046
    Feb 27 11:48:23.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 11:48:23.054
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:48:23.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:48:23.121
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Feb 27 11:48:23.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:48:24.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9102" for this suite. 02/27/23 11:48:24.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:48:24.23
Feb 27 11:48:24.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 11:48:24.232
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:48:24.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:48:24.298
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 02/27/23 11:48:24.312
Feb 27 11:48:24.330: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486" in namespace "projected-989" to be "Succeeded or Failed"
Feb 27 11:48:24.343: INFO: Pod "downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486": Phase="Pending", Reason="", readiness=false. Elapsed: 13.276816ms
Feb 27 11:48:26.354: INFO: Pod "downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024139105s
Feb 27 11:48:28.358: INFO: Pod "downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028175577s
Feb 27 11:48:30.354: INFO: Pod "downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024510527s
STEP: Saw pod success 02/27/23 11:48:30.355
Feb 27 11:48:30.355: INFO: Pod "downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486" satisfied condition "Succeeded or Failed"
Feb 27 11:48:30.368: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486 container client-container: <nil>
STEP: delete the pod 02/27/23 11:48:30.387
Feb 27 11:48:30.409: INFO: Waiting for pod downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486 to disappear
Feb 27 11:48:30.416: INFO: Pod downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 27 11:48:30.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-989" for this suite. 02/27/23 11:48:30.426
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":124,"skipped":2559,"failed":0}
------------------------------
• [SLOW TEST] [6.227 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:48:24.23
    Feb 27 11:48:24.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 11:48:24.232
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:48:24.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:48:24.298
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 02/27/23 11:48:24.312
    Feb 27 11:48:24.330: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486" in namespace "projected-989" to be "Succeeded or Failed"
    Feb 27 11:48:24.343: INFO: Pod "downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486": Phase="Pending", Reason="", readiness=false. Elapsed: 13.276816ms
    Feb 27 11:48:26.354: INFO: Pod "downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024139105s
    Feb 27 11:48:28.358: INFO: Pod "downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028175577s
    Feb 27 11:48:30.354: INFO: Pod "downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024510527s
    STEP: Saw pod success 02/27/23 11:48:30.355
    Feb 27 11:48:30.355: INFO: Pod "downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486" satisfied condition "Succeeded or Failed"
    Feb 27 11:48:30.368: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486 container client-container: <nil>
    STEP: delete the pod 02/27/23 11:48:30.387
    Feb 27 11:48:30.409: INFO: Waiting for pod downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486 to disappear
    Feb 27 11:48:30.416: INFO: Pod downwardapi-volume-4f2323a0-21a9-4cf8-b196-36ebd65a2486 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 27 11:48:30.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-989" for this suite. 02/27/23 11:48:30.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:48:30.462
Feb 27 11:48:30.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename statefulset 02/27/23 11:48:30.464
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:48:30.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:48:30.513
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5935 02/27/23 11:48:30.526
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Feb 27 11:48:30.576: INFO: Found 0 stateful pods, waiting for 1
Feb 27 11:48:40.587: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 02/27/23 11:48:40.602
W0227 11:48:40.618749      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 27 11:48:40.635: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 11:48:40.635: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Feb 27 11:48:50.647: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 11:48:50.647: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 02/27/23 11:48:50.67
STEP: Delete all of the StatefulSets 02/27/23 11:48:50.679
STEP: Verify that StatefulSets have been deleted 02/27/23 11:48:50.704
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 27 11:48:50.721: INFO: Deleting all statefulset in ns statefulset-5935
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 27 11:48:50.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5935" for this suite. 02/27/23 11:48:50.8
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":125,"skipped":2580,"failed":0}
------------------------------
• [SLOW TEST] [20.370 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:48:30.462
    Feb 27 11:48:30.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename statefulset 02/27/23 11:48:30.464
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:48:30.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:48:30.513
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5935 02/27/23 11:48:30.526
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Feb 27 11:48:30.576: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 11:48:40.587: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 02/27/23 11:48:40.602
    W0227 11:48:40.618749      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 27 11:48:40.635: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 11:48:40.635: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Feb 27 11:48:50.647: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 11:48:50.647: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 02/27/23 11:48:50.67
    STEP: Delete all of the StatefulSets 02/27/23 11:48:50.679
    STEP: Verify that StatefulSets have been deleted 02/27/23 11:48:50.704
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 27 11:48:50.721: INFO: Deleting all statefulset in ns statefulset-5935
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 27 11:48:50.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5935" for this suite. 02/27/23 11:48:50.8
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:48:50.839
Feb 27 11:48:50.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pod-network-test 02/27/23 11:48:50.84
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:48:50.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:48:50.912
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-4173 02/27/23 11:48:50.925
STEP: creating a selector 02/27/23 11:48:50.925
STEP: Creating the service pods in kubernetes 02/27/23 11:48:50.925
Feb 27 11:48:50.926: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 27 11:48:51.040: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4173" to be "running and ready"
Feb 27 11:48:51.075: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.845297ms
Feb 27 11:48:51.075: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:48:53.084: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.043630862s
Feb 27 11:48:53.084: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 11:48:55.086: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.045673217s
Feb 27 11:48:55.086: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 11:48:57.085: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.044896284s
Feb 27 11:48:57.085: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 11:48:59.085: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.044547189s
Feb 27 11:48:59.085: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 11:49:01.086: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.045046118s
Feb 27 11:49:01.086: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 11:49:03.088: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.047365371s
Feb 27 11:49:03.088: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 27 11:49:03.088: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 27 11:49:03.097: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4173" to be "running and ready"
Feb 27 11:49:03.121: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 23.790799ms
Feb 27 11:49:03.121: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 27 11:49:05.130: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.033247171s
Feb 27 11:49:05.130: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 27 11:49:07.130: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.033575924s
Feb 27 11:49:07.130: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 27 11:49:09.135: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.038576555s
Feb 27 11:49:09.135: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 27 11:49:11.137: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.040410994s
Feb 27 11:49:11.137: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 27 11:49:13.130: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.033077085s
Feb 27 11:49:13.130: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 27 11:49:13.130: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 27 11:49:13.141: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4173" to be "running and ready"
Feb 27 11:49:13.158: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 17.650804ms
Feb 27 11:49:13.158: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 27 11:49:13.158: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/27/23 11:49:13.17
Feb 27 11:49:13.200: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4173" to be "running"
Feb 27 11:49:13.217: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.98882ms
Feb 27 11:49:15.226: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025627876s
Feb 27 11:49:15.226: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 27 11:49:15.235: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4173" to be "running"
Feb 27 11:49:15.242: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.576651ms
Feb 27 11:49:15.242: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 27 11:49:15.250: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 27 11:49:15.250: INFO: Going to poll 172.25.1.84 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 27 11:49:15.263: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.84 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4173 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 11:49:15.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 11:49:15.264: INFO: ExecWithOptions: Clientset creation
Feb 27 11:49:15.264: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-4173/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.1.84+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 11:49:16.581: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 27 11:49:16.581: INFO: Going to poll 172.25.2.96 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 27 11:49:16.591: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.96 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4173 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 11:49:16.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 11:49:16.592: INFO: ExecWithOptions: Clientset creation
Feb 27 11:49:16.592: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-4173/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.2.96+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 11:49:17.700: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 27 11:49:17.700: INFO: Going to poll 172.25.0.22 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 27 11:49:17.711: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.22 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4173 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 11:49:17.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 11:49:17.712: INFO: ExecWithOptions: Clientset creation
Feb 27 11:49:17.712: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-4173/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.0.22+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 11:49:18.845: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 27 11:49:18.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4173" for this suite. 02/27/23 11:49:18.86
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":126,"skipped":2581,"failed":0}
------------------------------
• [SLOW TEST] [28.043 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:48:50.839
    Feb 27 11:48:50.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pod-network-test 02/27/23 11:48:50.84
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:48:50.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:48:50.912
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-4173 02/27/23 11:48:50.925
    STEP: creating a selector 02/27/23 11:48:50.925
    STEP: Creating the service pods in kubernetes 02/27/23 11:48:50.925
    Feb 27 11:48:50.926: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 27 11:48:51.040: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4173" to be "running and ready"
    Feb 27 11:48:51.075: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.845297ms
    Feb 27 11:48:51.075: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:48:53.084: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.043630862s
    Feb 27 11:48:53.084: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 11:48:55.086: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.045673217s
    Feb 27 11:48:55.086: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 11:48:57.085: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.044896284s
    Feb 27 11:48:57.085: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 11:48:59.085: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.044547189s
    Feb 27 11:48:59.085: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 11:49:01.086: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.045046118s
    Feb 27 11:49:01.086: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 11:49:03.088: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.047365371s
    Feb 27 11:49:03.088: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 27 11:49:03.088: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 27 11:49:03.097: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4173" to be "running and ready"
    Feb 27 11:49:03.121: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 23.790799ms
    Feb 27 11:49:03.121: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 27 11:49:05.130: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.033247171s
    Feb 27 11:49:05.130: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 27 11:49:07.130: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.033575924s
    Feb 27 11:49:07.130: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 27 11:49:09.135: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.038576555s
    Feb 27 11:49:09.135: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 27 11:49:11.137: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.040410994s
    Feb 27 11:49:11.137: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 27 11:49:13.130: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.033077085s
    Feb 27 11:49:13.130: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 27 11:49:13.130: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 27 11:49:13.141: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4173" to be "running and ready"
    Feb 27 11:49:13.158: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 17.650804ms
    Feb 27 11:49:13.158: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 27 11:49:13.158: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/27/23 11:49:13.17
    Feb 27 11:49:13.200: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4173" to be "running"
    Feb 27 11:49:13.217: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.98882ms
    Feb 27 11:49:15.226: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025627876s
    Feb 27 11:49:15.226: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 27 11:49:15.235: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4173" to be "running"
    Feb 27 11:49:15.242: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.576651ms
    Feb 27 11:49:15.242: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 27 11:49:15.250: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 27 11:49:15.250: INFO: Going to poll 172.25.1.84 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 27 11:49:15.263: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.84 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4173 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 11:49:15.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 11:49:15.264: INFO: ExecWithOptions: Clientset creation
    Feb 27 11:49:15.264: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-4173/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.1.84+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 11:49:16.581: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 27 11:49:16.581: INFO: Going to poll 172.25.2.96 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 27 11:49:16.591: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.96 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4173 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 11:49:16.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 11:49:16.592: INFO: ExecWithOptions: Clientset creation
    Feb 27 11:49:16.592: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-4173/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.2.96+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 11:49:17.700: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 27 11:49:17.700: INFO: Going to poll 172.25.0.22 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 27 11:49:17.711: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.22 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4173 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 11:49:17.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 11:49:17.712: INFO: ExecWithOptions: Clientset creation
    Feb 27 11:49:17.712: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-4173/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.0.22+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 11:49:18.845: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 27 11:49:18.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4173" for this suite. 02/27/23 11:49:18.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:49:18.889
Feb 27 11:49:18.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename watch 02/27/23 11:49:18.891
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:49:18.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:49:18.943
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 02/27/23 11:49:18.956
STEP: modifying the configmap once 02/27/23 11:49:18.977
STEP: modifying the configmap a second time 02/27/23 11:49:18.996
STEP: deleting the configmap 02/27/23 11:49:19.024
STEP: creating a watch on configmaps from the resource version returned by the first update 02/27/23 11:49:19.064
STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/27/23 11:49:19.07
Feb 27 11:49:19.070: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2108  6b94b727-ef10-46e2-8ccf-9b2ca52eaef0 70266 0 2023-02-27 11:49:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-27 11:49:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 11:49:19.070: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2108  6b94b727-ef10-46e2-8ccf-9b2ca52eaef0 70267 0 2023-02-27 11:49:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-27 11:49:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 27 11:49:19.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2108" for this suite. 02/27/23 11:49:19.08
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":127,"skipped":2642,"failed":0}
------------------------------
• [0.206 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:49:18.889
    Feb 27 11:49:18.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename watch 02/27/23 11:49:18.891
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:49:18.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:49:18.943
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 02/27/23 11:49:18.956
    STEP: modifying the configmap once 02/27/23 11:49:18.977
    STEP: modifying the configmap a second time 02/27/23 11:49:18.996
    STEP: deleting the configmap 02/27/23 11:49:19.024
    STEP: creating a watch on configmaps from the resource version returned by the first update 02/27/23 11:49:19.064
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/27/23 11:49:19.07
    Feb 27 11:49:19.070: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2108  6b94b727-ef10-46e2-8ccf-9b2ca52eaef0 70266 0 2023-02-27 11:49:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-27 11:49:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 11:49:19.070: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2108  6b94b727-ef10-46e2-8ccf-9b2ca52eaef0 70267 0 2023-02-27 11:49:18 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-27 11:49:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 27 11:49:19.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2108" for this suite. 02/27/23 11:49:19.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:49:19.102
Feb 27 11:49:19.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 11:49:19.103
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:49:19.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:49:19.145
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/27/23 11:49:19.156
Feb 27 11:49:19.188: INFO: Waiting up to 5m0s for pod "pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9" in namespace "emptydir-6726" to be "Succeeded or Failed"
Feb 27 11:49:19.202: INFO: Pod "pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.096728ms
Feb 27 11:49:21.211: INFO: Pod "pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.023216667s
Feb 27 11:49:23.213: INFO: Pod "pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9": Phase="Running", Reason="", readiness=false. Elapsed: 4.024771491s
Feb 27 11:49:25.219: INFO: Pod "pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030883174s
STEP: Saw pod success 02/27/23 11:49:25.219
Feb 27 11:49:25.219: INFO: Pod "pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9" satisfied condition "Succeeded or Failed"
Feb 27 11:49:25.235: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9 container test-container: <nil>
STEP: delete the pod 02/27/23 11:49:25.269
Feb 27 11:49:25.321: INFO: Waiting for pod pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9 to disappear
Feb 27 11:49:25.332: INFO: Pod pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 11:49:25.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6726" for this suite. 02/27/23 11:49:25.346
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":128,"skipped":2661,"failed":0}
------------------------------
• [SLOW TEST] [6.268 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:49:19.102
    Feb 27 11:49:19.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 11:49:19.103
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:49:19.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:49:19.145
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/27/23 11:49:19.156
    Feb 27 11:49:19.188: INFO: Waiting up to 5m0s for pod "pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9" in namespace "emptydir-6726" to be "Succeeded or Failed"
    Feb 27 11:49:19.202: INFO: Pod "pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.096728ms
    Feb 27 11:49:21.211: INFO: Pod "pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.023216667s
    Feb 27 11:49:23.213: INFO: Pod "pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9": Phase="Running", Reason="", readiness=false. Elapsed: 4.024771491s
    Feb 27 11:49:25.219: INFO: Pod "pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030883174s
    STEP: Saw pod success 02/27/23 11:49:25.219
    Feb 27 11:49:25.219: INFO: Pod "pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9" satisfied condition "Succeeded or Failed"
    Feb 27 11:49:25.235: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9 container test-container: <nil>
    STEP: delete the pod 02/27/23 11:49:25.269
    Feb 27 11:49:25.321: INFO: Waiting for pod pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9 to disappear
    Feb 27 11:49:25.332: INFO: Pod pod-a142a6d9-02fd-4bd2-a534-9e3b931d92a9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 11:49:25.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6726" for this suite. 02/27/23 11:49:25.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:49:25.379
Feb 27 11:49:25.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubelet-test 02/27/23 11:49:25.382
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:49:25.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:49:25.428
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 27 11:49:25.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5184" for this suite. 02/27/23 11:49:25.567
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":129,"skipped":2684,"failed":0}
------------------------------
• [0.216 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:49:25.379
    Feb 27 11:49:25.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 11:49:25.382
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:49:25.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:49:25.428
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 27 11:49:25.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5184" for this suite. 02/27/23 11:49:25.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:49:25.597
Feb 27 11:49:25.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename svc-latency 02/27/23 11:49:25.599
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:49:25.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:49:25.665
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Feb 27 11:49:25.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8951 02/27/23 11:49:25.684
I0227 11:49:25.706521      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8951, replica count: 1
I0227 11:49:26.759014      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0227 11:49:27.759972      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 11:49:27.882: INFO: Created: latency-svc-q7lcl
Feb 27 11:49:27.898: INFO: Got endpoints: latency-svc-q7lcl [37.444988ms]
Feb 27 11:49:27.932: INFO: Created: latency-svc-j2sr8
Feb 27 11:49:27.946: INFO: Created: latency-svc-cllhh
Feb 27 11:49:27.959: INFO: Got endpoints: latency-svc-cllhh [60.155165ms]
Feb 27 11:49:27.959: INFO: Got endpoints: latency-svc-j2sr8 [60.223806ms]
Feb 27 11:49:27.964: INFO: Created: latency-svc-6czh8
Feb 27 11:49:27.971: INFO: Got endpoints: latency-svc-6czh8 [67.922449ms]
Feb 27 11:49:27.978: INFO: Created: latency-svc-clmk7
Feb 27 11:49:28.012: INFO: Created: latency-svc-z5f9m
Feb 27 11:49:28.023: INFO: Got endpoints: latency-svc-clmk7 [119.163716ms]
Feb 27 11:49:28.043: INFO: Created: latency-svc-qx7b9
Feb 27 11:49:28.056: INFO: Got endpoints: latency-svc-z5f9m [151.207041ms]
Feb 27 11:49:28.064: INFO: Got endpoints: latency-svc-qx7b9 [160.31388ms]
Feb 27 11:49:28.098: INFO: Created: latency-svc-tkhkm
Feb 27 11:49:28.131: INFO: Got endpoints: latency-svc-tkhkm [226.758251ms]
Feb 27 11:49:28.131: INFO: Created: latency-svc-d4gzt
Feb 27 11:49:28.134: INFO: Got endpoints: latency-svc-d4gzt [229.727833ms]
Feb 27 11:49:28.136: INFO: Created: latency-svc-wqksl
Feb 27 11:49:28.144: INFO: Got endpoints: latency-svc-wqksl [243.530762ms]
Feb 27 11:49:28.156: INFO: Created: latency-svc-f8rhk
Feb 27 11:49:28.166: INFO: Created: latency-svc-xrkcp
Feb 27 11:49:28.171: INFO: Got endpoints: latency-svc-f8rhk [266.977701ms]
Feb 27 11:49:28.179: INFO: Got endpoints: latency-svc-xrkcp [273.72389ms]
Feb 27 11:49:28.194: INFO: Created: latency-svc-2st9g
Feb 27 11:49:28.209: INFO: Created: latency-svc-m8r74
Feb 27 11:49:28.210: INFO: Got endpoints: latency-svc-2st9g [304.943522ms]
Feb 27 11:49:28.219: INFO: Created: latency-svc-w6dj5
Feb 27 11:49:28.220: INFO: Got endpoints: latency-svc-m8r74 [315.699917ms]
Feb 27 11:49:28.245: INFO: Got endpoints: latency-svc-w6dj5 [341.210816ms]
Feb 27 11:49:28.245: INFO: Created: latency-svc-nqz66
Feb 27 11:49:28.250: INFO: Got endpoints: latency-svc-nqz66 [345.095441ms]
Feb 27 11:49:28.259: INFO: Created: latency-svc-h68bb
Feb 27 11:49:28.270: INFO: Created: latency-svc-ksgdb
Feb 27 11:49:28.274: INFO: Got endpoints: latency-svc-h68bb [315.362586ms]
Feb 27 11:49:28.288: INFO: Got endpoints: latency-svc-ksgdb [329.289955ms]
Feb 27 11:49:28.302: INFO: Created: latency-svc-7vfld
Feb 27 11:49:28.315: INFO: Got endpoints: latency-svc-7vfld [344.265867ms]
Feb 27 11:49:28.441: INFO: Created: latency-svc-h6hhk
Feb 27 11:49:28.463: INFO: Created: latency-svc-hg646
Feb 27 11:49:28.463: INFO: Created: latency-svc-lrsds
Feb 27 11:49:28.463: INFO: Created: latency-svc-cxmcd
Feb 27 11:49:28.464: INFO: Created: latency-svc-sv96l
Feb 27 11:49:28.464: INFO: Created: latency-svc-wph86
Feb 27 11:49:28.469: INFO: Created: latency-svc-qwxfk
Feb 27 11:49:28.471: INFO: Got endpoints: latency-svc-h6hhk [447.954105ms]
Feb 27 11:49:28.473: INFO: Created: latency-svc-trjq9
Feb 27 11:49:28.496: INFO: Created: latency-svc-r479x
Feb 27 11:49:28.510: INFO: Got endpoints: latency-svc-r479x [264.796959ms]
Feb 27 11:49:28.524: INFO: Created: latency-svc-87cdm
Feb 27 11:49:28.542: INFO: Created: latency-svc-r52vq
Feb 27 11:49:28.543: INFO: Created: latency-svc-8v7xb
Feb 27 11:49:28.554: INFO: Created: latency-svc-jx44l
Feb 27 11:49:28.556: INFO: Created: latency-svc-rpq6c
Feb 27 11:49:28.556: INFO: Created: latency-svc-8n7zx
Feb 27 11:49:28.601: INFO: Created: latency-svc-dpmkb
Feb 27 11:49:28.621: INFO: Got endpoints: latency-svc-rpq6c [400.290514ms]
Feb 27 11:49:28.621: INFO: Got endpoints: latency-svc-87cdm [556.956777ms]
Feb 27 11:49:28.643: INFO: Got endpoints: latency-svc-hg646 [507.259257ms]
Feb 27 11:49:28.643: INFO: Got endpoints: latency-svc-sv96l [511.576955ms]
Feb 27 11:49:28.678: INFO: Created: latency-svc-gcglb
Feb 27 11:49:28.679: INFO: Got endpoints: latency-svc-cxmcd [427.994451ms]
Feb 27 11:49:28.695: INFO: Got endpoints: latency-svc-trjq9 [485.680445ms]
Feb 27 11:49:28.698: INFO: Got endpoints: latency-svc-wph86 [518.922466ms]
Feb 27 11:49:28.703: INFO: Got endpoints: latency-svc-lrsds [428.237282ms]
Feb 27 11:49:28.724: INFO: Got endpoints: latency-svc-qwxfk [408.802821ms]
Feb 27 11:49:28.725: INFO: Got endpoints: latency-svc-8v7xb [669.349583ms]
Feb 27 11:49:28.727: INFO: Created: latency-svc-77cs5
Feb 27 11:49:28.727: INFO: Got endpoints: latency-svc-r52vq [438.149744ms]
Feb 27 11:49:28.740: INFO: Got endpoints: latency-svc-jx44l [568.407415ms]
Feb 27 11:49:28.740: INFO: Got endpoints: latency-svc-8n7zx [595.51591ms]
Feb 27 11:49:28.753: INFO: Got endpoints: latency-svc-dpmkb [281.799682ms]
Feb 27 11:49:28.754: INFO: Got endpoints: latency-svc-gcglb [244.228394ms]
Feb 27 11:49:28.754: INFO: Got endpoints: latency-svc-77cs5 [133.102284ms]
Feb 27 11:49:28.774: INFO: Created: latency-svc-stpc6
Feb 27 11:49:28.780: INFO: Got endpoints: latency-svc-stpc6 [159.078863ms]
Feb 27 11:49:28.808: INFO: Created: latency-svc-xgdqg
Feb 27 11:49:28.812: INFO: Got endpoints: latency-svc-xgdqg [56.545519ms]
Feb 27 11:49:28.827: INFO: Created: latency-svc-sw6g8
Feb 27 11:49:28.836: INFO: Got endpoints: latency-svc-sw6g8 [192.815045ms]
Feb 27 11:49:28.841: INFO: Created: latency-svc-k6z2j
Feb 27 11:49:28.846: INFO: Got endpoints: latency-svc-k6z2j [202.575527ms]
Feb 27 11:49:28.869: INFO: Created: latency-svc-xvqln
Feb 27 11:49:28.869: INFO: Got endpoints: latency-svc-xvqln [190.150745ms]
Feb 27 11:49:28.869: INFO: Created: latency-svc-p86zm
Feb 27 11:49:28.891: INFO: Got endpoints: latency-svc-p86zm [195.039906ms]
Feb 27 11:49:28.891: INFO: Created: latency-svc-6h4ps
Feb 27 11:49:28.908: INFO: Got endpoints: latency-svc-6h4ps [205.875911ms]
Feb 27 11:49:28.909: INFO: Created: latency-svc-sl8mn
Feb 27 11:49:28.915: INFO: Created: latency-svc-79mmc
Feb 27 11:49:28.936: INFO: Got endpoints: latency-svc-sl8mn [237.662216ms]
Feb 27 11:49:28.937: INFO: Got endpoints: latency-svc-79mmc [212.239978ms]
Feb 27 11:49:29.051: INFO: Created: latency-svc-cs8dx
Feb 27 11:49:29.061: INFO: Created: latency-svc-4hqhj
Feb 27 11:49:29.061: INFO: Created: latency-svc-2lgd7
Feb 27 11:49:29.068: INFO: Created: latency-svc-q4nbv
Feb 27 11:49:29.068: INFO: Created: latency-svc-x8w57
Feb 27 11:49:29.068: INFO: Created: latency-svc-2xlzf
Feb 27 11:49:29.068: INFO: Created: latency-svc-xt875
Feb 27 11:49:29.068: INFO: Created: latency-svc-crvwz
Feb 27 11:49:29.068: INFO: Created: latency-svc-bcnl4
Feb 27 11:49:29.068: INFO: Created: latency-svc-q8zc9
Feb 27 11:49:29.081: INFO: Got endpoints: latency-svc-cs8dx [144.841474ms]
Feb 27 11:49:29.090: INFO: Created: latency-svc-8jpgb
Feb 27 11:49:29.091: INFO: Created: latency-svc-j5xmg
Feb 27 11:49:29.096: INFO: Got endpoints: latency-svc-crvwz [355.537536ms]
Feb 27 11:49:29.097: INFO: Created: latency-svc-jw9dl
Feb 27 11:49:29.098: INFO: Got endpoints: latency-svc-x8w57 [370.588239ms]
Feb 27 11:49:29.098: INFO: Got endpoints: latency-svc-4hqhj [343.625035ms]
Feb 27 11:49:29.098: INFO: Got endpoints: latency-svc-2lgd7 [317.692755ms]
Feb 27 11:49:29.098: INFO: Created: latency-svc-9zq5z
Feb 27 11:49:29.098: INFO: Created: latency-svc-2lm4v
Feb 27 11:49:29.116: INFO: Got endpoints: latency-svc-q8zc9 [361.832402ms]
Feb 27 11:49:29.123: INFO: Got endpoints: latency-svc-q4nbv [287.189128ms]
Feb 27 11:49:29.124: INFO: Got endpoints: latency-svc-xt875 [212.870932ms]
Feb 27 11:49:29.131: INFO: Created: latency-svc-6cl7j
Feb 27 11:49:29.165: INFO: Got endpoints: latency-svc-bcnl4 [273.95111ms]
Feb 27 11:49:29.170: INFO: Created: latency-svc-txwgp
Feb 27 11:49:29.195: INFO: Got endpoints: latency-svc-2xlzf [348.544348ms]
Feb 27 11:49:29.246: INFO: Got endpoints: latency-svc-j5xmg [309.171221ms]
Feb 27 11:49:29.264: INFO: Created: latency-svc-vdq58
Feb 27 11:49:29.266: INFO: Created: latency-svc-97fgh
Feb 27 11:49:29.266: INFO: Created: latency-svc-pmqkl
Feb 27 11:49:29.267: INFO: Created: latency-svc-hkcnz
Feb 27 11:49:29.267: INFO: Created: latency-svc-kpnpv
Feb 27 11:49:29.267: INFO: Created: latency-svc-pfzdw
Feb 27 11:49:29.268: INFO: Created: latency-svc-bcnds
Feb 27 11:49:29.268: INFO: Created: latency-svc-27dkj
Feb 27 11:49:29.275: INFO: Created: latency-svc-s4dlm
Feb 27 11:49:29.294: INFO: Got endpoints: latency-svc-jw9dl [568.5927ms]
Feb 27 11:49:29.314: INFO: Created: latency-svc-qw5r8
Feb 27 11:49:29.356: INFO: Got endpoints: latency-svc-8jpgb [615.71765ms]
Feb 27 11:49:29.375: INFO: Created: latency-svc-j5fxm
Feb 27 11:49:29.392: INFO: Got endpoints: latency-svc-9zq5z [580.21911ms]
Feb 27 11:49:29.413: INFO: Created: latency-svc-q9zqc
Feb 27 11:49:29.442: INFO: Got endpoints: latency-svc-2lm4v [572.936739ms]
Feb 27 11:49:29.459: INFO: Created: latency-svc-sgv4k
Feb 27 11:49:29.487: INFO: Got endpoints: latency-svc-6cl7j [405.933981ms]
Feb 27 11:49:29.508: INFO: Created: latency-svc-87mth
Feb 27 11:49:29.541: INFO: Got endpoints: latency-svc-txwgp [445.248527ms]
Feb 27 11:49:29.572: INFO: Created: latency-svc-kdk5j
Feb 27 11:49:29.594: INFO: Got endpoints: latency-svc-hkcnz [395.989517ms]
Feb 27 11:49:29.615: INFO: Created: latency-svc-7s5fk
Feb 27 11:49:29.641: INFO: Got endpoints: latency-svc-pmqkl [516.656688ms]
Feb 27 11:49:29.661: INFO: Created: latency-svc-jp8m2
Feb 27 11:49:29.698: INFO: Got endpoints: latency-svc-kpnpv [597.739731ms]
Feb 27 11:49:29.714: INFO: Created: latency-svc-mvx58
Feb 27 11:49:29.740: INFO: Got endpoints: latency-svc-97fgh [623.37242ms]
Feb 27 11:49:29.763: INFO: Created: latency-svc-wnjw9
Feb 27 11:49:29.790: INFO: Got endpoints: latency-svc-27dkj [689.242079ms]
Feb 27 11:49:29.812: INFO: Created: latency-svc-74q5w
Feb 27 11:49:29.841: INFO: Got endpoints: latency-svc-pfzdw [741.269929ms]
Feb 27 11:49:29.861: INFO: Created: latency-svc-9cmln
Feb 27 11:49:29.900: INFO: Got endpoints: latency-svc-bcnds [776.781189ms]
Feb 27 11:49:29.919: INFO: Created: latency-svc-ntxr2
Feb 27 11:49:29.941: INFO: Got endpoints: latency-svc-vdq58 [775.129732ms]
Feb 27 11:49:29.954: INFO: Created: latency-svc-hjnxw
Feb 27 11:49:29.991: INFO: Got endpoints: latency-svc-s4dlm [745.167834ms]
Feb 27 11:49:30.013: INFO: Created: latency-svc-88hjl
Feb 27 11:49:30.040: INFO: Got endpoints: latency-svc-qw5r8 [746.008107ms]
Feb 27 11:49:30.061: INFO: Created: latency-svc-nz8cx
Feb 27 11:49:30.093: INFO: Got endpoints: latency-svc-j5fxm [737.05262ms]
Feb 27 11:49:30.112: INFO: Created: latency-svc-5bzrp
Feb 27 11:49:30.140: INFO: Got endpoints: latency-svc-q9zqc [748.010326ms]
Feb 27 11:49:30.176: INFO: Created: latency-svc-dj69g
Feb 27 11:49:30.200: INFO: Got endpoints: latency-svc-sgv4k [758.017309ms]
Feb 27 11:49:30.221: INFO: Created: latency-svc-s2dqb
Feb 27 11:49:30.249: INFO: Got endpoints: latency-svc-87mth [761.655314ms]
Feb 27 11:49:30.272: INFO: Created: latency-svc-9nhg2
Feb 27 11:49:30.297: INFO: Got endpoints: latency-svc-kdk5j [755.481409ms]
Feb 27 11:49:30.323: INFO: Created: latency-svc-zlxbm
Feb 27 11:49:30.345: INFO: Got endpoints: latency-svc-7s5fk [750.927479ms]
Feb 27 11:49:30.375: INFO: Created: latency-svc-bcp2g
Feb 27 11:49:30.406: INFO: Got endpoints: latency-svc-jp8m2 [764.394546ms]
Feb 27 11:49:30.426: INFO: Created: latency-svc-6jv7f
Feb 27 11:49:30.455: INFO: Got endpoints: latency-svc-mvx58 [756.873085ms]
Feb 27 11:49:30.477: INFO: Created: latency-svc-jjdfb
Feb 27 11:49:30.496: INFO: Got endpoints: latency-svc-wnjw9 [755.827589ms]
Feb 27 11:49:30.524: INFO: Created: latency-svc-jwg2f
Feb 27 11:49:30.543: INFO: Got endpoints: latency-svc-74q5w [752.807577ms]
Feb 27 11:49:30.560: INFO: Created: latency-svc-h26v9
Feb 27 11:49:30.589: INFO: Got endpoints: latency-svc-9cmln [746.971932ms]
Feb 27 11:49:30.621: INFO: Created: latency-svc-vczqx
Feb 27 11:49:30.649: INFO: Got endpoints: latency-svc-ntxr2 [747.857196ms]
Feb 27 11:49:30.679: INFO: Created: latency-svc-rm4gb
Feb 27 11:49:30.708: INFO: Got endpoints: latency-svc-hjnxw [766.859817ms]
Feb 27 11:49:30.744: INFO: Created: latency-svc-bhtfk
Feb 27 11:49:30.749: INFO: Got endpoints: latency-svc-88hjl [755.379657ms]
Feb 27 11:49:30.774: INFO: Created: latency-svc-l7lzq
Feb 27 11:49:30.794: INFO: Got endpoints: latency-svc-nz8cx [753.50765ms]
Feb 27 11:49:30.844: INFO: Got endpoints: latency-svc-5bzrp [750.371696ms]
Feb 27 11:49:30.844: INFO: Created: latency-svc-wcqkk
Feb 27 11:49:30.875: INFO: Created: latency-svc-fjhlm
Feb 27 11:49:30.895: INFO: Got endpoints: latency-svc-dj69g [755.018366ms]
Feb 27 11:49:30.918: INFO: Created: latency-svc-5bsd7
Feb 27 11:49:30.966: INFO: Got endpoints: latency-svc-s2dqb [765.260619ms]
Feb 27 11:49:31.008: INFO: Created: latency-svc-2m4sm
Feb 27 11:49:31.008: INFO: Got endpoints: latency-svc-9nhg2 [759.003843ms]
Feb 27 11:49:31.047: INFO: Got endpoints: latency-svc-zlxbm [750.598696ms]
Feb 27 11:49:31.047: INFO: Created: latency-svc-n5vbh
Feb 27 11:49:31.069: INFO: Created: latency-svc-rrnr8
Feb 27 11:49:31.088: INFO: Got endpoints: latency-svc-bcp2g [742.966706ms]
Feb 27 11:49:31.120: INFO: Created: latency-svc-zrssg
Feb 27 11:49:31.139: INFO: Got endpoints: latency-svc-6jv7f [733.115713ms]
Feb 27 11:49:31.159: INFO: Created: latency-svc-6pd6l
Feb 27 11:49:31.202: INFO: Got endpoints: latency-svc-jjdfb [746.262429ms]
Feb 27 11:49:31.229: INFO: Created: latency-svc-jqq2p
Feb 27 11:49:31.239: INFO: Got endpoints: latency-svc-jwg2f [742.295433ms]
Feb 27 11:49:31.272: INFO: Created: latency-svc-9wwpl
Feb 27 11:49:31.291: INFO: Got endpoints: latency-svc-h26v9 [747.336135ms]
Feb 27 11:49:31.321: INFO: Created: latency-svc-2bh28
Feb 27 11:49:31.341: INFO: Got endpoints: latency-svc-vczqx [748.55123ms]
Feb 27 11:49:31.360: INFO: Created: latency-svc-nwvr8
Feb 27 11:49:31.391: INFO: Got endpoints: latency-svc-rm4gb [741.972142ms]
Feb 27 11:49:31.428: INFO: Created: latency-svc-nzwdk
Feb 27 11:49:31.442: INFO: Got endpoints: latency-svc-bhtfk [733.613966ms]
Feb 27 11:49:31.466: INFO: Created: latency-svc-x7tjc
Feb 27 11:49:31.495: INFO: Got endpoints: latency-svc-l7lzq [745.521855ms]
Feb 27 11:49:31.517: INFO: Created: latency-svc-6q9cr
Feb 27 11:49:31.539: INFO: Got endpoints: latency-svc-wcqkk [745.193394ms]
Feb 27 11:49:31.560: INFO: Created: latency-svc-h7hwd
Feb 27 11:49:31.591: INFO: Got endpoints: latency-svc-fjhlm [746.537739ms]
Feb 27 11:49:31.614: INFO: Created: latency-svc-zs4dw
Feb 27 11:49:31.639: INFO: Got endpoints: latency-svc-5bsd7 [743.278405ms]
Feb 27 11:49:31.660: INFO: Created: latency-svc-5ddwp
Feb 27 11:49:31.690: INFO: Got endpoints: latency-svc-2m4sm [724.401786ms]
Feb 27 11:49:31.712: INFO: Created: latency-svc-2zpsw
Feb 27 11:49:31.741: INFO: Got endpoints: latency-svc-n5vbh [732.449289ms]
Feb 27 11:49:31.758: INFO: Created: latency-svc-ntgr7
Feb 27 11:49:31.791: INFO: Got endpoints: latency-svc-rrnr8 [743.059755ms]
Feb 27 11:49:31.810: INFO: Created: latency-svc-6278b
Feb 27 11:49:31.839: INFO: Got endpoints: latency-svc-zrssg [750.729387ms]
Feb 27 11:49:31.874: INFO: Created: latency-svc-wj9sk
Feb 27 11:49:31.892: INFO: Got endpoints: latency-svc-6pd6l [752.727565ms]
Feb 27 11:49:31.914: INFO: Created: latency-svc-5z7h6
Feb 27 11:49:31.941: INFO: Got endpoints: latency-svc-jqq2p [735.724783ms]
Feb 27 11:49:31.966: INFO: Created: latency-svc-p7sdk
Feb 27 11:49:31.990: INFO: Got endpoints: latency-svc-9wwpl [751.415239ms]
Feb 27 11:49:32.013: INFO: Created: latency-svc-q29sf
Feb 27 11:49:32.040: INFO: Got endpoints: latency-svc-2bh28 [748.927029ms]
Feb 27 11:49:32.058: INFO: Created: latency-svc-lzs8r
Feb 27 11:49:32.092: INFO: Got endpoints: latency-svc-nwvr8 [750.983278ms]
Feb 27 11:49:32.140: INFO: Created: latency-svc-vgjfb
Feb 27 11:49:32.141: INFO: Got endpoints: latency-svc-nzwdk [749.369321ms]
Feb 27 11:49:32.157: INFO: Created: latency-svc-9jw6x
Feb 27 11:49:32.192: INFO: Got endpoints: latency-svc-x7tjc [750.239774ms]
Feb 27 11:49:32.212: INFO: Created: latency-svc-zkkzn
Feb 27 11:49:32.240: INFO: Got endpoints: latency-svc-6q9cr [745.432695ms]
Feb 27 11:49:32.260: INFO: Created: latency-svc-ppzxz
Feb 27 11:49:32.296: INFO: Got endpoints: latency-svc-h7hwd [756.731732ms]
Feb 27 11:49:32.324: INFO: Created: latency-svc-dd6w8
Feb 27 11:49:32.340: INFO: Got endpoints: latency-svc-zs4dw [744.56181ms]
Feb 27 11:49:32.361: INFO: Created: latency-svc-lllhz
Feb 27 11:49:32.402: INFO: Got endpoints: latency-svc-5ddwp [762.858197ms]
Feb 27 11:49:32.423: INFO: Created: latency-svc-csplh
Feb 27 11:49:32.454: INFO: Got endpoints: latency-svc-2zpsw [760.531496ms]
Feb 27 11:49:32.500: INFO: Got endpoints: latency-svc-ntgr7 [759.318081ms]
Feb 27 11:49:32.513: INFO: Created: latency-svc-svsn4
Feb 27 11:49:32.538: INFO: Created: latency-svc-kt2d2
Feb 27 11:49:32.550: INFO: Got endpoints: latency-svc-6278b [758.579688ms]
Feb 27 11:49:32.585: INFO: Created: latency-svc-khndz
Feb 27 11:49:32.611: INFO: Got endpoints: latency-svc-wj9sk [771.945315ms]
Feb 27 11:49:32.631: INFO: Created: latency-svc-mrzxc
Feb 27 11:49:32.637: INFO: Got endpoints: latency-svc-5z7h6 [744.79745ms]
Feb 27 11:49:32.654: INFO: Created: latency-svc-b4g8h
Feb 27 11:49:32.696: INFO: Got endpoints: latency-svc-p7sdk [755.130145ms]
Feb 27 11:49:32.721: INFO: Created: latency-svc-xqw9f
Feb 27 11:49:32.740: INFO: Got endpoints: latency-svc-q29sf [748.914178ms]
Feb 27 11:49:32.759: INFO: Created: latency-svc-nm7st
Feb 27 11:49:32.805: INFO: Got endpoints: latency-svc-lzs8r [765.404118ms]
Feb 27 11:49:32.824: INFO: Created: latency-svc-w5v6x
Feb 27 11:49:32.846: INFO: Got endpoints: latency-svc-vgjfb [753.717918ms]
Feb 27 11:49:32.865: INFO: Created: latency-svc-pssx9
Feb 27 11:49:32.895: INFO: Got endpoints: latency-svc-9jw6x [753.702188ms]
Feb 27 11:49:32.917: INFO: Created: latency-svc-g4xb9
Feb 27 11:49:32.941: INFO: Got endpoints: latency-svc-zkkzn [748.786907ms]
Feb 27 11:49:32.959: INFO: Created: latency-svc-tgg4d
Feb 27 11:49:32.993: INFO: Got endpoints: latency-svc-ppzxz [752.707635ms]
Feb 27 11:49:33.017: INFO: Created: latency-svc-5x8nt
Feb 27 11:49:33.040: INFO: Got endpoints: latency-svc-dd6w8 [743.652397ms]
Feb 27 11:49:33.070: INFO: Created: latency-svc-zrd4c
Feb 27 11:49:33.093: INFO: Got endpoints: latency-svc-lllhz [753.440859ms]
Feb 27 11:49:33.118: INFO: Created: latency-svc-l6gl6
Feb 27 11:49:33.138: INFO: Got endpoints: latency-svc-csplh [736.283406ms]
Feb 27 11:49:33.157: INFO: Created: latency-svc-gjnc2
Feb 27 11:49:33.205: INFO: Got endpoints: latency-svc-svsn4 [750.986709ms]
Feb 27 11:49:33.224: INFO: Created: latency-svc-4hjjr
Feb 27 11:49:33.244: INFO: Got endpoints: latency-svc-kt2d2 [743.292816ms]
Feb 27 11:49:33.263: INFO: Created: latency-svc-7ttbb
Feb 27 11:49:33.292: INFO: Got endpoints: latency-svc-khndz [742.526373ms]
Feb 27 11:49:33.330: INFO: Created: latency-svc-4qjmf
Feb 27 11:49:33.339: INFO: Got endpoints: latency-svc-mrzxc [728.365183ms]
Feb 27 11:49:33.369: INFO: Created: latency-svc-4pmb8
Feb 27 11:49:33.392: INFO: Got endpoints: latency-svc-b4g8h [755.096727ms]
Feb 27 11:49:33.411: INFO: Created: latency-svc-vtgmv
Feb 27 11:49:33.441: INFO: Got endpoints: latency-svc-xqw9f [744.287522ms]
Feb 27 11:49:33.470: INFO: Created: latency-svc-xn7s6
Feb 27 11:49:33.489: INFO: Got endpoints: latency-svc-nm7st [749.049971ms]
Feb 27 11:49:33.514: INFO: Created: latency-svc-7nzwt
Feb 27 11:49:33.539: INFO: Got endpoints: latency-svc-w5v6x [733.950197ms]
Feb 27 11:49:33.578: INFO: Created: latency-svc-78hx4
Feb 27 11:49:33.590: INFO: Got endpoints: latency-svc-pssx9 [743.603668ms]
Feb 27 11:49:33.616: INFO: Created: latency-svc-wnjmt
Feb 27 11:49:33.641: INFO: Got endpoints: latency-svc-g4xb9 [745.310804ms]
Feb 27 11:49:33.656: INFO: Created: latency-svc-69nrp
Feb 27 11:49:33.692: INFO: Got endpoints: latency-svc-tgg4d [750.745906ms]
Feb 27 11:49:33.714: INFO: Created: latency-svc-psz6d
Feb 27 11:49:33.744: INFO: Got endpoints: latency-svc-5x8nt [750.388035ms]
Feb 27 11:49:33.765: INFO: Created: latency-svc-hzxf4
Feb 27 11:49:33.792: INFO: Got endpoints: latency-svc-zrd4c [752.219122ms]
Feb 27 11:49:33.825: INFO: Created: latency-svc-z2jqz
Feb 27 11:49:33.841: INFO: Got endpoints: latency-svc-l6gl6 [743.568987ms]
Feb 27 11:49:33.859: INFO: Created: latency-svc-kvfv6
Feb 27 11:49:33.906: INFO: Got endpoints: latency-svc-gjnc2 [768.301421ms]
Feb 27 11:49:33.935: INFO: Created: latency-svc-78p55
Feb 27 11:49:33.946: INFO: Got endpoints: latency-svc-4hjjr [740.815045ms]
Feb 27 11:49:33.972: INFO: Created: latency-svc-bpbsx
Feb 27 11:49:33.994: INFO: Got endpoints: latency-svc-7ttbb [750.593967ms]
Feb 27 11:49:34.012: INFO: Created: latency-svc-t2r9g
Feb 27 11:49:34.041: INFO: Got endpoints: latency-svc-4qjmf [743.830317ms]
Feb 27 11:49:34.066: INFO: Created: latency-svc-d8djm
Feb 27 11:49:34.105: INFO: Got endpoints: latency-svc-4pmb8 [765.082706ms]
Feb 27 11:49:34.124: INFO: Created: latency-svc-sxc54
Feb 27 11:49:34.143: INFO: Got endpoints: latency-svc-vtgmv [750.507145ms]
Feb 27 11:49:34.164: INFO: Created: latency-svc-kdq75
Feb 27 11:49:34.194: INFO: Got endpoints: latency-svc-xn7s6 [752.812405ms]
Feb 27 11:49:34.218: INFO: Created: latency-svc-hhlzq
Feb 27 11:49:34.240: INFO: Got endpoints: latency-svc-7nzwt [749.09617ms]
Feb 27 11:49:34.267: INFO: Created: latency-svc-knhc7
Feb 27 11:49:34.302: INFO: Got endpoints: latency-svc-78hx4 [762.332926ms]
Feb 27 11:49:34.337: INFO: Created: latency-svc-chhdg
Feb 27 11:49:34.346: INFO: Got endpoints: latency-svc-wnjmt [755.802449ms]
Feb 27 11:49:34.366: INFO: Created: latency-svc-k8d58
Feb 27 11:49:34.389: INFO: Got endpoints: latency-svc-69nrp [748.191965ms]
Feb 27 11:49:34.439: INFO: Created: latency-svc-424gw
Feb 27 11:49:34.451: INFO: Got endpoints: latency-svc-psz6d [758.68245ms]
Feb 27 11:49:34.476: INFO: Created: latency-svc-vzwzj
Feb 27 11:49:34.504: INFO: Got endpoints: latency-svc-hzxf4 [760.149836ms]
Feb 27 11:49:34.527: INFO: Created: latency-svc-l7xk8
Feb 27 11:49:34.545: INFO: Got endpoints: latency-svc-z2jqz [737.276009ms]
Feb 27 11:49:34.578: INFO: Created: latency-svc-xsx4x
Feb 27 11:49:34.605: INFO: Got endpoints: latency-svc-kvfv6 [763.920921ms]
Feb 27 11:49:34.638: INFO: Created: latency-svc-ffqhm
Feb 27 11:49:34.641: INFO: Got endpoints: latency-svc-78p55 [734.133884ms]
Feb 27 11:49:34.664: INFO: Created: latency-svc-hdhw8
Feb 27 11:49:34.716: INFO: Got endpoints: latency-svc-bpbsx [770.189737ms]
Feb 27 11:49:34.745: INFO: Got endpoints: latency-svc-t2r9g [750.164323ms]
Feb 27 11:49:34.745: INFO: Created: latency-svc-bmv78
Feb 27 11:49:34.778: INFO: Created: latency-svc-2q6v2
Feb 27 11:49:34.798: INFO: Got endpoints: latency-svc-d8djm [757.018632ms]
Feb 27 11:49:34.821: INFO: Created: latency-svc-8hjlh
Feb 27 11:49:34.842: INFO: Got endpoints: latency-svc-sxc54 [737.650762ms]
Feb 27 11:49:34.864: INFO: Created: latency-svc-b78r9
Feb 27 11:49:34.900: INFO: Got endpoints: latency-svc-kdq75 [756.859143ms]
Feb 27 11:49:34.926: INFO: Created: latency-svc-rq5mt
Feb 27 11:49:34.944: INFO: Got endpoints: latency-svc-hhlzq [749.740322ms]
Feb 27 11:49:34.969: INFO: Created: latency-svc-j4bjt
Feb 27 11:49:35.002: INFO: Got endpoints: latency-svc-knhc7 [761.615102ms]
Feb 27 11:49:35.025: INFO: Created: latency-svc-g5w7s
Feb 27 11:49:35.053: INFO: Got endpoints: latency-svc-chhdg [750.933887ms]
Feb 27 11:49:35.080: INFO: Created: latency-svc-jdgm2
Feb 27 11:49:35.093: INFO: Got endpoints: latency-svc-k8d58 [747.506603ms]
Feb 27 11:49:35.129: INFO: Created: latency-svc-wvpfl
Feb 27 11:49:35.139: INFO: Got endpoints: latency-svc-424gw [749.927044ms]
Feb 27 11:49:35.168: INFO: Created: latency-svc-gr8v2
Feb 27 11:49:35.205: INFO: Got endpoints: latency-svc-vzwzj [754.440453ms]
Feb 27 11:49:35.235: INFO: Created: latency-svc-w922c
Feb 27 11:49:35.241: INFO: Got endpoints: latency-svc-l7xk8 [736.456348ms]
Feb 27 11:49:35.271: INFO: Created: latency-svc-vq6mp
Feb 27 11:49:35.298: INFO: Got endpoints: latency-svc-xsx4x [752.849957ms]
Feb 27 11:49:35.323: INFO: Created: latency-svc-dkpzb
Feb 27 11:49:35.346: INFO: Got endpoints: latency-svc-ffqhm [733.553096ms]
Feb 27 11:49:35.377: INFO: Created: latency-svc-6fz52
Feb 27 11:49:35.390: INFO: Got endpoints: latency-svc-hdhw8 [749.315712ms]
Feb 27 11:49:35.441: INFO: Got endpoints: latency-svc-bmv78 [723.872965ms]
Feb 27 11:49:35.450: INFO: Created: latency-svc-kqg2j
Feb 27 11:49:35.487: INFO: Created: latency-svc-74jrs
Feb 27 11:49:35.500: INFO: Got endpoints: latency-svc-2q6v2 [755.128079ms]
Feb 27 11:49:35.533: INFO: Created: latency-svc-xdjtz
Feb 27 11:49:35.540: INFO: Got endpoints: latency-svc-8hjlh [742.106323ms]
Feb 27 11:49:35.598: INFO: Created: latency-svc-2rjhz
Feb 27 11:49:35.601: INFO: Got endpoints: latency-svc-b78r9 [758.813614ms]
Feb 27 11:49:35.622: INFO: Created: latency-svc-7jvpl
Feb 27 11:49:35.645: INFO: Got endpoints: latency-svc-rq5mt [744.924304ms]
Feb 27 11:49:35.671: INFO: Created: latency-svc-4hjh7
Feb 27 11:49:35.694: INFO: Got endpoints: latency-svc-j4bjt [749.912056ms]
Feb 27 11:49:35.728: INFO: Created: latency-svc-9hj2l
Feb 27 11:49:35.740: INFO: Got endpoints: latency-svc-g5w7s [737.645944ms]
Feb 27 11:49:35.808: INFO: Got endpoints: latency-svc-jdgm2 [755.5911ms]
Feb 27 11:49:35.854: INFO: Got endpoints: latency-svc-wvpfl [758.284391ms]
Feb 27 11:49:35.893: INFO: Got endpoints: latency-svc-gr8v2 [753.937953ms]
Feb 27 11:49:35.943: INFO: Got endpoints: latency-svc-w922c [737.870785ms]
Feb 27 11:49:35.998: INFO: Got endpoints: latency-svc-vq6mp [756.901196ms]
Feb 27 11:49:36.040: INFO: Got endpoints: latency-svc-dkpzb [742.183753ms]
Feb 27 11:49:36.093: INFO: Got endpoints: latency-svc-6fz52 [746.988735ms]
Feb 27 11:49:36.141: INFO: Got endpoints: latency-svc-kqg2j [750.924401ms]
Feb 27 11:49:36.191: INFO: Got endpoints: latency-svc-74jrs [749.864765ms]
Feb 27 11:49:36.248: INFO: Got endpoints: latency-svc-xdjtz [747.044793ms]
Feb 27 11:49:36.299: INFO: Got endpoints: latency-svc-2rjhz [758.172921ms]
Feb 27 11:49:36.342: INFO: Got endpoints: latency-svc-7jvpl [737.991525ms]
Feb 27 11:49:36.393: INFO: Got endpoints: latency-svc-4hjh7 [747.659505ms]
Feb 27 11:49:36.446: INFO: Got endpoints: latency-svc-9hj2l [751.840252ms]
Feb 27 11:49:36.446: INFO: Latencies: [56.545519ms 60.155165ms 60.223806ms 67.922449ms 119.163716ms 133.102284ms 144.841474ms 151.207041ms 159.078863ms 160.31388ms 190.150745ms 192.815045ms 195.039906ms 202.575527ms 205.875911ms 212.239978ms 212.870932ms 226.758251ms 229.727833ms 237.662216ms 243.530762ms 244.228394ms 264.796959ms 266.977701ms 273.72389ms 273.95111ms 281.799682ms 287.189128ms 304.943522ms 309.171221ms 315.362586ms 315.699917ms 317.692755ms 329.289955ms 341.210816ms 343.625035ms 344.265867ms 345.095441ms 348.544348ms 355.537536ms 361.832402ms 370.588239ms 395.989517ms 400.290514ms 405.933981ms 408.802821ms 427.994451ms 428.237282ms 438.149744ms 445.248527ms 447.954105ms 485.680445ms 507.259257ms 511.576955ms 516.656688ms 518.922466ms 556.956777ms 568.407415ms 568.5927ms 572.936739ms 580.21911ms 595.51591ms 597.739731ms 615.71765ms 623.37242ms 669.349583ms 689.242079ms 723.872965ms 724.401786ms 728.365183ms 732.449289ms 733.115713ms 733.553096ms 733.613966ms 733.950197ms 734.133884ms 735.724783ms 736.283406ms 736.456348ms 737.05262ms 737.276009ms 737.645944ms 737.650762ms 737.870785ms 737.991525ms 740.815045ms 741.269929ms 741.972142ms 742.106323ms 742.183753ms 742.295433ms 742.526373ms 742.966706ms 743.059755ms 743.278405ms 743.292816ms 743.568987ms 743.603668ms 743.652397ms 743.830317ms 744.287522ms 744.56181ms 744.79745ms 744.924304ms 745.167834ms 745.193394ms 745.310804ms 745.432695ms 745.521855ms 746.008107ms 746.262429ms 746.537739ms 746.971932ms 746.988735ms 747.044793ms 747.336135ms 747.506603ms 747.659505ms 747.857196ms 748.010326ms 748.191965ms 748.55123ms 748.786907ms 748.914178ms 748.927029ms 749.049971ms 749.09617ms 749.315712ms 749.369321ms 749.740322ms 749.864765ms 749.912056ms 749.927044ms 750.164323ms 750.239774ms 750.371696ms 750.388035ms 750.507145ms 750.593967ms 750.598696ms 750.729387ms 750.745906ms 750.924401ms 750.927479ms 750.933887ms 750.983278ms 750.986709ms 751.415239ms 751.840252ms 752.219122ms 752.707635ms 752.727565ms 752.807577ms 752.812405ms 752.849957ms 753.440859ms 753.50765ms 753.702188ms 753.717918ms 753.937953ms 754.440453ms 755.018366ms 755.096727ms 755.128079ms 755.130145ms 755.379657ms 755.481409ms 755.5911ms 755.802449ms 755.827589ms 756.731732ms 756.859143ms 756.873085ms 756.901196ms 757.018632ms 758.017309ms 758.172921ms 758.284391ms 758.579688ms 758.68245ms 758.813614ms 759.003843ms 759.318081ms 760.149836ms 760.531496ms 761.615102ms 761.655314ms 762.332926ms 762.858197ms 763.920921ms 764.394546ms 765.082706ms 765.260619ms 765.404118ms 766.859817ms 768.301421ms 770.189737ms 771.945315ms 775.129732ms 776.781189ms]
Feb 27 11:49:36.447: INFO: 50 %ile: 744.287522ms
Feb 27 11:49:36.447: INFO: 90 %ile: 758.813614ms
Feb 27 11:49:36.447: INFO: 99 %ile: 775.129732ms
Feb 27 11:49:36.447: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Feb 27 11:49:36.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8951" for this suite. 02/27/23 11:49:36.463
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":130,"skipped":2722,"failed":0}
------------------------------
• [SLOW TEST] [11.297 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:49:25.597
    Feb 27 11:49:25.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename svc-latency 02/27/23 11:49:25.599
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:49:25.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:49:25.665
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Feb 27 11:49:25.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-8951 02/27/23 11:49:25.684
    I0227 11:49:25.706521      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8951, replica count: 1
    I0227 11:49:26.759014      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0227 11:49:27.759972      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 11:49:27.882: INFO: Created: latency-svc-q7lcl
    Feb 27 11:49:27.898: INFO: Got endpoints: latency-svc-q7lcl [37.444988ms]
    Feb 27 11:49:27.932: INFO: Created: latency-svc-j2sr8
    Feb 27 11:49:27.946: INFO: Created: latency-svc-cllhh
    Feb 27 11:49:27.959: INFO: Got endpoints: latency-svc-cllhh [60.155165ms]
    Feb 27 11:49:27.959: INFO: Got endpoints: latency-svc-j2sr8 [60.223806ms]
    Feb 27 11:49:27.964: INFO: Created: latency-svc-6czh8
    Feb 27 11:49:27.971: INFO: Got endpoints: latency-svc-6czh8 [67.922449ms]
    Feb 27 11:49:27.978: INFO: Created: latency-svc-clmk7
    Feb 27 11:49:28.012: INFO: Created: latency-svc-z5f9m
    Feb 27 11:49:28.023: INFO: Got endpoints: latency-svc-clmk7 [119.163716ms]
    Feb 27 11:49:28.043: INFO: Created: latency-svc-qx7b9
    Feb 27 11:49:28.056: INFO: Got endpoints: latency-svc-z5f9m [151.207041ms]
    Feb 27 11:49:28.064: INFO: Got endpoints: latency-svc-qx7b9 [160.31388ms]
    Feb 27 11:49:28.098: INFO: Created: latency-svc-tkhkm
    Feb 27 11:49:28.131: INFO: Got endpoints: latency-svc-tkhkm [226.758251ms]
    Feb 27 11:49:28.131: INFO: Created: latency-svc-d4gzt
    Feb 27 11:49:28.134: INFO: Got endpoints: latency-svc-d4gzt [229.727833ms]
    Feb 27 11:49:28.136: INFO: Created: latency-svc-wqksl
    Feb 27 11:49:28.144: INFO: Got endpoints: latency-svc-wqksl [243.530762ms]
    Feb 27 11:49:28.156: INFO: Created: latency-svc-f8rhk
    Feb 27 11:49:28.166: INFO: Created: latency-svc-xrkcp
    Feb 27 11:49:28.171: INFO: Got endpoints: latency-svc-f8rhk [266.977701ms]
    Feb 27 11:49:28.179: INFO: Got endpoints: latency-svc-xrkcp [273.72389ms]
    Feb 27 11:49:28.194: INFO: Created: latency-svc-2st9g
    Feb 27 11:49:28.209: INFO: Created: latency-svc-m8r74
    Feb 27 11:49:28.210: INFO: Got endpoints: latency-svc-2st9g [304.943522ms]
    Feb 27 11:49:28.219: INFO: Created: latency-svc-w6dj5
    Feb 27 11:49:28.220: INFO: Got endpoints: latency-svc-m8r74 [315.699917ms]
    Feb 27 11:49:28.245: INFO: Got endpoints: latency-svc-w6dj5 [341.210816ms]
    Feb 27 11:49:28.245: INFO: Created: latency-svc-nqz66
    Feb 27 11:49:28.250: INFO: Got endpoints: latency-svc-nqz66 [345.095441ms]
    Feb 27 11:49:28.259: INFO: Created: latency-svc-h68bb
    Feb 27 11:49:28.270: INFO: Created: latency-svc-ksgdb
    Feb 27 11:49:28.274: INFO: Got endpoints: latency-svc-h68bb [315.362586ms]
    Feb 27 11:49:28.288: INFO: Got endpoints: latency-svc-ksgdb [329.289955ms]
    Feb 27 11:49:28.302: INFO: Created: latency-svc-7vfld
    Feb 27 11:49:28.315: INFO: Got endpoints: latency-svc-7vfld [344.265867ms]
    Feb 27 11:49:28.441: INFO: Created: latency-svc-h6hhk
    Feb 27 11:49:28.463: INFO: Created: latency-svc-hg646
    Feb 27 11:49:28.463: INFO: Created: latency-svc-lrsds
    Feb 27 11:49:28.463: INFO: Created: latency-svc-cxmcd
    Feb 27 11:49:28.464: INFO: Created: latency-svc-sv96l
    Feb 27 11:49:28.464: INFO: Created: latency-svc-wph86
    Feb 27 11:49:28.469: INFO: Created: latency-svc-qwxfk
    Feb 27 11:49:28.471: INFO: Got endpoints: latency-svc-h6hhk [447.954105ms]
    Feb 27 11:49:28.473: INFO: Created: latency-svc-trjq9
    Feb 27 11:49:28.496: INFO: Created: latency-svc-r479x
    Feb 27 11:49:28.510: INFO: Got endpoints: latency-svc-r479x [264.796959ms]
    Feb 27 11:49:28.524: INFO: Created: latency-svc-87cdm
    Feb 27 11:49:28.542: INFO: Created: latency-svc-r52vq
    Feb 27 11:49:28.543: INFO: Created: latency-svc-8v7xb
    Feb 27 11:49:28.554: INFO: Created: latency-svc-jx44l
    Feb 27 11:49:28.556: INFO: Created: latency-svc-rpq6c
    Feb 27 11:49:28.556: INFO: Created: latency-svc-8n7zx
    Feb 27 11:49:28.601: INFO: Created: latency-svc-dpmkb
    Feb 27 11:49:28.621: INFO: Got endpoints: latency-svc-rpq6c [400.290514ms]
    Feb 27 11:49:28.621: INFO: Got endpoints: latency-svc-87cdm [556.956777ms]
    Feb 27 11:49:28.643: INFO: Got endpoints: latency-svc-hg646 [507.259257ms]
    Feb 27 11:49:28.643: INFO: Got endpoints: latency-svc-sv96l [511.576955ms]
    Feb 27 11:49:28.678: INFO: Created: latency-svc-gcglb
    Feb 27 11:49:28.679: INFO: Got endpoints: latency-svc-cxmcd [427.994451ms]
    Feb 27 11:49:28.695: INFO: Got endpoints: latency-svc-trjq9 [485.680445ms]
    Feb 27 11:49:28.698: INFO: Got endpoints: latency-svc-wph86 [518.922466ms]
    Feb 27 11:49:28.703: INFO: Got endpoints: latency-svc-lrsds [428.237282ms]
    Feb 27 11:49:28.724: INFO: Got endpoints: latency-svc-qwxfk [408.802821ms]
    Feb 27 11:49:28.725: INFO: Got endpoints: latency-svc-8v7xb [669.349583ms]
    Feb 27 11:49:28.727: INFO: Created: latency-svc-77cs5
    Feb 27 11:49:28.727: INFO: Got endpoints: latency-svc-r52vq [438.149744ms]
    Feb 27 11:49:28.740: INFO: Got endpoints: latency-svc-jx44l [568.407415ms]
    Feb 27 11:49:28.740: INFO: Got endpoints: latency-svc-8n7zx [595.51591ms]
    Feb 27 11:49:28.753: INFO: Got endpoints: latency-svc-dpmkb [281.799682ms]
    Feb 27 11:49:28.754: INFO: Got endpoints: latency-svc-gcglb [244.228394ms]
    Feb 27 11:49:28.754: INFO: Got endpoints: latency-svc-77cs5 [133.102284ms]
    Feb 27 11:49:28.774: INFO: Created: latency-svc-stpc6
    Feb 27 11:49:28.780: INFO: Got endpoints: latency-svc-stpc6 [159.078863ms]
    Feb 27 11:49:28.808: INFO: Created: latency-svc-xgdqg
    Feb 27 11:49:28.812: INFO: Got endpoints: latency-svc-xgdqg [56.545519ms]
    Feb 27 11:49:28.827: INFO: Created: latency-svc-sw6g8
    Feb 27 11:49:28.836: INFO: Got endpoints: latency-svc-sw6g8 [192.815045ms]
    Feb 27 11:49:28.841: INFO: Created: latency-svc-k6z2j
    Feb 27 11:49:28.846: INFO: Got endpoints: latency-svc-k6z2j [202.575527ms]
    Feb 27 11:49:28.869: INFO: Created: latency-svc-xvqln
    Feb 27 11:49:28.869: INFO: Got endpoints: latency-svc-xvqln [190.150745ms]
    Feb 27 11:49:28.869: INFO: Created: latency-svc-p86zm
    Feb 27 11:49:28.891: INFO: Got endpoints: latency-svc-p86zm [195.039906ms]
    Feb 27 11:49:28.891: INFO: Created: latency-svc-6h4ps
    Feb 27 11:49:28.908: INFO: Got endpoints: latency-svc-6h4ps [205.875911ms]
    Feb 27 11:49:28.909: INFO: Created: latency-svc-sl8mn
    Feb 27 11:49:28.915: INFO: Created: latency-svc-79mmc
    Feb 27 11:49:28.936: INFO: Got endpoints: latency-svc-sl8mn [237.662216ms]
    Feb 27 11:49:28.937: INFO: Got endpoints: latency-svc-79mmc [212.239978ms]
    Feb 27 11:49:29.051: INFO: Created: latency-svc-cs8dx
    Feb 27 11:49:29.061: INFO: Created: latency-svc-4hqhj
    Feb 27 11:49:29.061: INFO: Created: latency-svc-2lgd7
    Feb 27 11:49:29.068: INFO: Created: latency-svc-q4nbv
    Feb 27 11:49:29.068: INFO: Created: latency-svc-x8w57
    Feb 27 11:49:29.068: INFO: Created: latency-svc-2xlzf
    Feb 27 11:49:29.068: INFO: Created: latency-svc-xt875
    Feb 27 11:49:29.068: INFO: Created: latency-svc-crvwz
    Feb 27 11:49:29.068: INFO: Created: latency-svc-bcnl4
    Feb 27 11:49:29.068: INFO: Created: latency-svc-q8zc9
    Feb 27 11:49:29.081: INFO: Got endpoints: latency-svc-cs8dx [144.841474ms]
    Feb 27 11:49:29.090: INFO: Created: latency-svc-8jpgb
    Feb 27 11:49:29.091: INFO: Created: latency-svc-j5xmg
    Feb 27 11:49:29.096: INFO: Got endpoints: latency-svc-crvwz [355.537536ms]
    Feb 27 11:49:29.097: INFO: Created: latency-svc-jw9dl
    Feb 27 11:49:29.098: INFO: Got endpoints: latency-svc-x8w57 [370.588239ms]
    Feb 27 11:49:29.098: INFO: Got endpoints: latency-svc-4hqhj [343.625035ms]
    Feb 27 11:49:29.098: INFO: Got endpoints: latency-svc-2lgd7 [317.692755ms]
    Feb 27 11:49:29.098: INFO: Created: latency-svc-9zq5z
    Feb 27 11:49:29.098: INFO: Created: latency-svc-2lm4v
    Feb 27 11:49:29.116: INFO: Got endpoints: latency-svc-q8zc9 [361.832402ms]
    Feb 27 11:49:29.123: INFO: Got endpoints: latency-svc-q4nbv [287.189128ms]
    Feb 27 11:49:29.124: INFO: Got endpoints: latency-svc-xt875 [212.870932ms]
    Feb 27 11:49:29.131: INFO: Created: latency-svc-6cl7j
    Feb 27 11:49:29.165: INFO: Got endpoints: latency-svc-bcnl4 [273.95111ms]
    Feb 27 11:49:29.170: INFO: Created: latency-svc-txwgp
    Feb 27 11:49:29.195: INFO: Got endpoints: latency-svc-2xlzf [348.544348ms]
    Feb 27 11:49:29.246: INFO: Got endpoints: latency-svc-j5xmg [309.171221ms]
    Feb 27 11:49:29.264: INFO: Created: latency-svc-vdq58
    Feb 27 11:49:29.266: INFO: Created: latency-svc-97fgh
    Feb 27 11:49:29.266: INFO: Created: latency-svc-pmqkl
    Feb 27 11:49:29.267: INFO: Created: latency-svc-hkcnz
    Feb 27 11:49:29.267: INFO: Created: latency-svc-kpnpv
    Feb 27 11:49:29.267: INFO: Created: latency-svc-pfzdw
    Feb 27 11:49:29.268: INFO: Created: latency-svc-bcnds
    Feb 27 11:49:29.268: INFO: Created: latency-svc-27dkj
    Feb 27 11:49:29.275: INFO: Created: latency-svc-s4dlm
    Feb 27 11:49:29.294: INFO: Got endpoints: latency-svc-jw9dl [568.5927ms]
    Feb 27 11:49:29.314: INFO: Created: latency-svc-qw5r8
    Feb 27 11:49:29.356: INFO: Got endpoints: latency-svc-8jpgb [615.71765ms]
    Feb 27 11:49:29.375: INFO: Created: latency-svc-j5fxm
    Feb 27 11:49:29.392: INFO: Got endpoints: latency-svc-9zq5z [580.21911ms]
    Feb 27 11:49:29.413: INFO: Created: latency-svc-q9zqc
    Feb 27 11:49:29.442: INFO: Got endpoints: latency-svc-2lm4v [572.936739ms]
    Feb 27 11:49:29.459: INFO: Created: latency-svc-sgv4k
    Feb 27 11:49:29.487: INFO: Got endpoints: latency-svc-6cl7j [405.933981ms]
    Feb 27 11:49:29.508: INFO: Created: latency-svc-87mth
    Feb 27 11:49:29.541: INFO: Got endpoints: latency-svc-txwgp [445.248527ms]
    Feb 27 11:49:29.572: INFO: Created: latency-svc-kdk5j
    Feb 27 11:49:29.594: INFO: Got endpoints: latency-svc-hkcnz [395.989517ms]
    Feb 27 11:49:29.615: INFO: Created: latency-svc-7s5fk
    Feb 27 11:49:29.641: INFO: Got endpoints: latency-svc-pmqkl [516.656688ms]
    Feb 27 11:49:29.661: INFO: Created: latency-svc-jp8m2
    Feb 27 11:49:29.698: INFO: Got endpoints: latency-svc-kpnpv [597.739731ms]
    Feb 27 11:49:29.714: INFO: Created: latency-svc-mvx58
    Feb 27 11:49:29.740: INFO: Got endpoints: latency-svc-97fgh [623.37242ms]
    Feb 27 11:49:29.763: INFO: Created: latency-svc-wnjw9
    Feb 27 11:49:29.790: INFO: Got endpoints: latency-svc-27dkj [689.242079ms]
    Feb 27 11:49:29.812: INFO: Created: latency-svc-74q5w
    Feb 27 11:49:29.841: INFO: Got endpoints: latency-svc-pfzdw [741.269929ms]
    Feb 27 11:49:29.861: INFO: Created: latency-svc-9cmln
    Feb 27 11:49:29.900: INFO: Got endpoints: latency-svc-bcnds [776.781189ms]
    Feb 27 11:49:29.919: INFO: Created: latency-svc-ntxr2
    Feb 27 11:49:29.941: INFO: Got endpoints: latency-svc-vdq58 [775.129732ms]
    Feb 27 11:49:29.954: INFO: Created: latency-svc-hjnxw
    Feb 27 11:49:29.991: INFO: Got endpoints: latency-svc-s4dlm [745.167834ms]
    Feb 27 11:49:30.013: INFO: Created: latency-svc-88hjl
    Feb 27 11:49:30.040: INFO: Got endpoints: latency-svc-qw5r8 [746.008107ms]
    Feb 27 11:49:30.061: INFO: Created: latency-svc-nz8cx
    Feb 27 11:49:30.093: INFO: Got endpoints: latency-svc-j5fxm [737.05262ms]
    Feb 27 11:49:30.112: INFO: Created: latency-svc-5bzrp
    Feb 27 11:49:30.140: INFO: Got endpoints: latency-svc-q9zqc [748.010326ms]
    Feb 27 11:49:30.176: INFO: Created: latency-svc-dj69g
    Feb 27 11:49:30.200: INFO: Got endpoints: latency-svc-sgv4k [758.017309ms]
    Feb 27 11:49:30.221: INFO: Created: latency-svc-s2dqb
    Feb 27 11:49:30.249: INFO: Got endpoints: latency-svc-87mth [761.655314ms]
    Feb 27 11:49:30.272: INFO: Created: latency-svc-9nhg2
    Feb 27 11:49:30.297: INFO: Got endpoints: latency-svc-kdk5j [755.481409ms]
    Feb 27 11:49:30.323: INFO: Created: latency-svc-zlxbm
    Feb 27 11:49:30.345: INFO: Got endpoints: latency-svc-7s5fk [750.927479ms]
    Feb 27 11:49:30.375: INFO: Created: latency-svc-bcp2g
    Feb 27 11:49:30.406: INFO: Got endpoints: latency-svc-jp8m2 [764.394546ms]
    Feb 27 11:49:30.426: INFO: Created: latency-svc-6jv7f
    Feb 27 11:49:30.455: INFO: Got endpoints: latency-svc-mvx58 [756.873085ms]
    Feb 27 11:49:30.477: INFO: Created: latency-svc-jjdfb
    Feb 27 11:49:30.496: INFO: Got endpoints: latency-svc-wnjw9 [755.827589ms]
    Feb 27 11:49:30.524: INFO: Created: latency-svc-jwg2f
    Feb 27 11:49:30.543: INFO: Got endpoints: latency-svc-74q5w [752.807577ms]
    Feb 27 11:49:30.560: INFO: Created: latency-svc-h26v9
    Feb 27 11:49:30.589: INFO: Got endpoints: latency-svc-9cmln [746.971932ms]
    Feb 27 11:49:30.621: INFO: Created: latency-svc-vczqx
    Feb 27 11:49:30.649: INFO: Got endpoints: latency-svc-ntxr2 [747.857196ms]
    Feb 27 11:49:30.679: INFO: Created: latency-svc-rm4gb
    Feb 27 11:49:30.708: INFO: Got endpoints: latency-svc-hjnxw [766.859817ms]
    Feb 27 11:49:30.744: INFO: Created: latency-svc-bhtfk
    Feb 27 11:49:30.749: INFO: Got endpoints: latency-svc-88hjl [755.379657ms]
    Feb 27 11:49:30.774: INFO: Created: latency-svc-l7lzq
    Feb 27 11:49:30.794: INFO: Got endpoints: latency-svc-nz8cx [753.50765ms]
    Feb 27 11:49:30.844: INFO: Got endpoints: latency-svc-5bzrp [750.371696ms]
    Feb 27 11:49:30.844: INFO: Created: latency-svc-wcqkk
    Feb 27 11:49:30.875: INFO: Created: latency-svc-fjhlm
    Feb 27 11:49:30.895: INFO: Got endpoints: latency-svc-dj69g [755.018366ms]
    Feb 27 11:49:30.918: INFO: Created: latency-svc-5bsd7
    Feb 27 11:49:30.966: INFO: Got endpoints: latency-svc-s2dqb [765.260619ms]
    Feb 27 11:49:31.008: INFO: Created: latency-svc-2m4sm
    Feb 27 11:49:31.008: INFO: Got endpoints: latency-svc-9nhg2 [759.003843ms]
    Feb 27 11:49:31.047: INFO: Got endpoints: latency-svc-zlxbm [750.598696ms]
    Feb 27 11:49:31.047: INFO: Created: latency-svc-n5vbh
    Feb 27 11:49:31.069: INFO: Created: latency-svc-rrnr8
    Feb 27 11:49:31.088: INFO: Got endpoints: latency-svc-bcp2g [742.966706ms]
    Feb 27 11:49:31.120: INFO: Created: latency-svc-zrssg
    Feb 27 11:49:31.139: INFO: Got endpoints: latency-svc-6jv7f [733.115713ms]
    Feb 27 11:49:31.159: INFO: Created: latency-svc-6pd6l
    Feb 27 11:49:31.202: INFO: Got endpoints: latency-svc-jjdfb [746.262429ms]
    Feb 27 11:49:31.229: INFO: Created: latency-svc-jqq2p
    Feb 27 11:49:31.239: INFO: Got endpoints: latency-svc-jwg2f [742.295433ms]
    Feb 27 11:49:31.272: INFO: Created: latency-svc-9wwpl
    Feb 27 11:49:31.291: INFO: Got endpoints: latency-svc-h26v9 [747.336135ms]
    Feb 27 11:49:31.321: INFO: Created: latency-svc-2bh28
    Feb 27 11:49:31.341: INFO: Got endpoints: latency-svc-vczqx [748.55123ms]
    Feb 27 11:49:31.360: INFO: Created: latency-svc-nwvr8
    Feb 27 11:49:31.391: INFO: Got endpoints: latency-svc-rm4gb [741.972142ms]
    Feb 27 11:49:31.428: INFO: Created: latency-svc-nzwdk
    Feb 27 11:49:31.442: INFO: Got endpoints: latency-svc-bhtfk [733.613966ms]
    Feb 27 11:49:31.466: INFO: Created: latency-svc-x7tjc
    Feb 27 11:49:31.495: INFO: Got endpoints: latency-svc-l7lzq [745.521855ms]
    Feb 27 11:49:31.517: INFO: Created: latency-svc-6q9cr
    Feb 27 11:49:31.539: INFO: Got endpoints: latency-svc-wcqkk [745.193394ms]
    Feb 27 11:49:31.560: INFO: Created: latency-svc-h7hwd
    Feb 27 11:49:31.591: INFO: Got endpoints: latency-svc-fjhlm [746.537739ms]
    Feb 27 11:49:31.614: INFO: Created: latency-svc-zs4dw
    Feb 27 11:49:31.639: INFO: Got endpoints: latency-svc-5bsd7 [743.278405ms]
    Feb 27 11:49:31.660: INFO: Created: latency-svc-5ddwp
    Feb 27 11:49:31.690: INFO: Got endpoints: latency-svc-2m4sm [724.401786ms]
    Feb 27 11:49:31.712: INFO: Created: latency-svc-2zpsw
    Feb 27 11:49:31.741: INFO: Got endpoints: latency-svc-n5vbh [732.449289ms]
    Feb 27 11:49:31.758: INFO: Created: latency-svc-ntgr7
    Feb 27 11:49:31.791: INFO: Got endpoints: latency-svc-rrnr8 [743.059755ms]
    Feb 27 11:49:31.810: INFO: Created: latency-svc-6278b
    Feb 27 11:49:31.839: INFO: Got endpoints: latency-svc-zrssg [750.729387ms]
    Feb 27 11:49:31.874: INFO: Created: latency-svc-wj9sk
    Feb 27 11:49:31.892: INFO: Got endpoints: latency-svc-6pd6l [752.727565ms]
    Feb 27 11:49:31.914: INFO: Created: latency-svc-5z7h6
    Feb 27 11:49:31.941: INFO: Got endpoints: latency-svc-jqq2p [735.724783ms]
    Feb 27 11:49:31.966: INFO: Created: latency-svc-p7sdk
    Feb 27 11:49:31.990: INFO: Got endpoints: latency-svc-9wwpl [751.415239ms]
    Feb 27 11:49:32.013: INFO: Created: latency-svc-q29sf
    Feb 27 11:49:32.040: INFO: Got endpoints: latency-svc-2bh28 [748.927029ms]
    Feb 27 11:49:32.058: INFO: Created: latency-svc-lzs8r
    Feb 27 11:49:32.092: INFO: Got endpoints: latency-svc-nwvr8 [750.983278ms]
    Feb 27 11:49:32.140: INFO: Created: latency-svc-vgjfb
    Feb 27 11:49:32.141: INFO: Got endpoints: latency-svc-nzwdk [749.369321ms]
    Feb 27 11:49:32.157: INFO: Created: latency-svc-9jw6x
    Feb 27 11:49:32.192: INFO: Got endpoints: latency-svc-x7tjc [750.239774ms]
    Feb 27 11:49:32.212: INFO: Created: latency-svc-zkkzn
    Feb 27 11:49:32.240: INFO: Got endpoints: latency-svc-6q9cr [745.432695ms]
    Feb 27 11:49:32.260: INFO: Created: latency-svc-ppzxz
    Feb 27 11:49:32.296: INFO: Got endpoints: latency-svc-h7hwd [756.731732ms]
    Feb 27 11:49:32.324: INFO: Created: latency-svc-dd6w8
    Feb 27 11:49:32.340: INFO: Got endpoints: latency-svc-zs4dw [744.56181ms]
    Feb 27 11:49:32.361: INFO: Created: latency-svc-lllhz
    Feb 27 11:49:32.402: INFO: Got endpoints: latency-svc-5ddwp [762.858197ms]
    Feb 27 11:49:32.423: INFO: Created: latency-svc-csplh
    Feb 27 11:49:32.454: INFO: Got endpoints: latency-svc-2zpsw [760.531496ms]
    Feb 27 11:49:32.500: INFO: Got endpoints: latency-svc-ntgr7 [759.318081ms]
    Feb 27 11:49:32.513: INFO: Created: latency-svc-svsn4
    Feb 27 11:49:32.538: INFO: Created: latency-svc-kt2d2
    Feb 27 11:49:32.550: INFO: Got endpoints: latency-svc-6278b [758.579688ms]
    Feb 27 11:49:32.585: INFO: Created: latency-svc-khndz
    Feb 27 11:49:32.611: INFO: Got endpoints: latency-svc-wj9sk [771.945315ms]
    Feb 27 11:49:32.631: INFO: Created: latency-svc-mrzxc
    Feb 27 11:49:32.637: INFO: Got endpoints: latency-svc-5z7h6 [744.79745ms]
    Feb 27 11:49:32.654: INFO: Created: latency-svc-b4g8h
    Feb 27 11:49:32.696: INFO: Got endpoints: latency-svc-p7sdk [755.130145ms]
    Feb 27 11:49:32.721: INFO: Created: latency-svc-xqw9f
    Feb 27 11:49:32.740: INFO: Got endpoints: latency-svc-q29sf [748.914178ms]
    Feb 27 11:49:32.759: INFO: Created: latency-svc-nm7st
    Feb 27 11:49:32.805: INFO: Got endpoints: latency-svc-lzs8r [765.404118ms]
    Feb 27 11:49:32.824: INFO: Created: latency-svc-w5v6x
    Feb 27 11:49:32.846: INFO: Got endpoints: latency-svc-vgjfb [753.717918ms]
    Feb 27 11:49:32.865: INFO: Created: latency-svc-pssx9
    Feb 27 11:49:32.895: INFO: Got endpoints: latency-svc-9jw6x [753.702188ms]
    Feb 27 11:49:32.917: INFO: Created: latency-svc-g4xb9
    Feb 27 11:49:32.941: INFO: Got endpoints: latency-svc-zkkzn [748.786907ms]
    Feb 27 11:49:32.959: INFO: Created: latency-svc-tgg4d
    Feb 27 11:49:32.993: INFO: Got endpoints: latency-svc-ppzxz [752.707635ms]
    Feb 27 11:49:33.017: INFO: Created: latency-svc-5x8nt
    Feb 27 11:49:33.040: INFO: Got endpoints: latency-svc-dd6w8 [743.652397ms]
    Feb 27 11:49:33.070: INFO: Created: latency-svc-zrd4c
    Feb 27 11:49:33.093: INFO: Got endpoints: latency-svc-lllhz [753.440859ms]
    Feb 27 11:49:33.118: INFO: Created: latency-svc-l6gl6
    Feb 27 11:49:33.138: INFO: Got endpoints: latency-svc-csplh [736.283406ms]
    Feb 27 11:49:33.157: INFO: Created: latency-svc-gjnc2
    Feb 27 11:49:33.205: INFO: Got endpoints: latency-svc-svsn4 [750.986709ms]
    Feb 27 11:49:33.224: INFO: Created: latency-svc-4hjjr
    Feb 27 11:49:33.244: INFO: Got endpoints: latency-svc-kt2d2 [743.292816ms]
    Feb 27 11:49:33.263: INFO: Created: latency-svc-7ttbb
    Feb 27 11:49:33.292: INFO: Got endpoints: latency-svc-khndz [742.526373ms]
    Feb 27 11:49:33.330: INFO: Created: latency-svc-4qjmf
    Feb 27 11:49:33.339: INFO: Got endpoints: latency-svc-mrzxc [728.365183ms]
    Feb 27 11:49:33.369: INFO: Created: latency-svc-4pmb8
    Feb 27 11:49:33.392: INFO: Got endpoints: latency-svc-b4g8h [755.096727ms]
    Feb 27 11:49:33.411: INFO: Created: latency-svc-vtgmv
    Feb 27 11:49:33.441: INFO: Got endpoints: latency-svc-xqw9f [744.287522ms]
    Feb 27 11:49:33.470: INFO: Created: latency-svc-xn7s6
    Feb 27 11:49:33.489: INFO: Got endpoints: latency-svc-nm7st [749.049971ms]
    Feb 27 11:49:33.514: INFO: Created: latency-svc-7nzwt
    Feb 27 11:49:33.539: INFO: Got endpoints: latency-svc-w5v6x [733.950197ms]
    Feb 27 11:49:33.578: INFO: Created: latency-svc-78hx4
    Feb 27 11:49:33.590: INFO: Got endpoints: latency-svc-pssx9 [743.603668ms]
    Feb 27 11:49:33.616: INFO: Created: latency-svc-wnjmt
    Feb 27 11:49:33.641: INFO: Got endpoints: latency-svc-g4xb9 [745.310804ms]
    Feb 27 11:49:33.656: INFO: Created: latency-svc-69nrp
    Feb 27 11:49:33.692: INFO: Got endpoints: latency-svc-tgg4d [750.745906ms]
    Feb 27 11:49:33.714: INFO: Created: latency-svc-psz6d
    Feb 27 11:49:33.744: INFO: Got endpoints: latency-svc-5x8nt [750.388035ms]
    Feb 27 11:49:33.765: INFO: Created: latency-svc-hzxf4
    Feb 27 11:49:33.792: INFO: Got endpoints: latency-svc-zrd4c [752.219122ms]
    Feb 27 11:49:33.825: INFO: Created: latency-svc-z2jqz
    Feb 27 11:49:33.841: INFO: Got endpoints: latency-svc-l6gl6 [743.568987ms]
    Feb 27 11:49:33.859: INFO: Created: latency-svc-kvfv6
    Feb 27 11:49:33.906: INFO: Got endpoints: latency-svc-gjnc2 [768.301421ms]
    Feb 27 11:49:33.935: INFO: Created: latency-svc-78p55
    Feb 27 11:49:33.946: INFO: Got endpoints: latency-svc-4hjjr [740.815045ms]
    Feb 27 11:49:33.972: INFO: Created: latency-svc-bpbsx
    Feb 27 11:49:33.994: INFO: Got endpoints: latency-svc-7ttbb [750.593967ms]
    Feb 27 11:49:34.012: INFO: Created: latency-svc-t2r9g
    Feb 27 11:49:34.041: INFO: Got endpoints: latency-svc-4qjmf [743.830317ms]
    Feb 27 11:49:34.066: INFO: Created: latency-svc-d8djm
    Feb 27 11:49:34.105: INFO: Got endpoints: latency-svc-4pmb8 [765.082706ms]
    Feb 27 11:49:34.124: INFO: Created: latency-svc-sxc54
    Feb 27 11:49:34.143: INFO: Got endpoints: latency-svc-vtgmv [750.507145ms]
    Feb 27 11:49:34.164: INFO: Created: latency-svc-kdq75
    Feb 27 11:49:34.194: INFO: Got endpoints: latency-svc-xn7s6 [752.812405ms]
    Feb 27 11:49:34.218: INFO: Created: latency-svc-hhlzq
    Feb 27 11:49:34.240: INFO: Got endpoints: latency-svc-7nzwt [749.09617ms]
    Feb 27 11:49:34.267: INFO: Created: latency-svc-knhc7
    Feb 27 11:49:34.302: INFO: Got endpoints: latency-svc-78hx4 [762.332926ms]
    Feb 27 11:49:34.337: INFO: Created: latency-svc-chhdg
    Feb 27 11:49:34.346: INFO: Got endpoints: latency-svc-wnjmt [755.802449ms]
    Feb 27 11:49:34.366: INFO: Created: latency-svc-k8d58
    Feb 27 11:49:34.389: INFO: Got endpoints: latency-svc-69nrp [748.191965ms]
    Feb 27 11:49:34.439: INFO: Created: latency-svc-424gw
    Feb 27 11:49:34.451: INFO: Got endpoints: latency-svc-psz6d [758.68245ms]
    Feb 27 11:49:34.476: INFO: Created: latency-svc-vzwzj
    Feb 27 11:49:34.504: INFO: Got endpoints: latency-svc-hzxf4 [760.149836ms]
    Feb 27 11:49:34.527: INFO: Created: latency-svc-l7xk8
    Feb 27 11:49:34.545: INFO: Got endpoints: latency-svc-z2jqz [737.276009ms]
    Feb 27 11:49:34.578: INFO: Created: latency-svc-xsx4x
    Feb 27 11:49:34.605: INFO: Got endpoints: latency-svc-kvfv6 [763.920921ms]
    Feb 27 11:49:34.638: INFO: Created: latency-svc-ffqhm
    Feb 27 11:49:34.641: INFO: Got endpoints: latency-svc-78p55 [734.133884ms]
    Feb 27 11:49:34.664: INFO: Created: latency-svc-hdhw8
    Feb 27 11:49:34.716: INFO: Got endpoints: latency-svc-bpbsx [770.189737ms]
    Feb 27 11:49:34.745: INFO: Got endpoints: latency-svc-t2r9g [750.164323ms]
    Feb 27 11:49:34.745: INFO: Created: latency-svc-bmv78
    Feb 27 11:49:34.778: INFO: Created: latency-svc-2q6v2
    Feb 27 11:49:34.798: INFO: Got endpoints: latency-svc-d8djm [757.018632ms]
    Feb 27 11:49:34.821: INFO: Created: latency-svc-8hjlh
    Feb 27 11:49:34.842: INFO: Got endpoints: latency-svc-sxc54 [737.650762ms]
    Feb 27 11:49:34.864: INFO: Created: latency-svc-b78r9
    Feb 27 11:49:34.900: INFO: Got endpoints: latency-svc-kdq75 [756.859143ms]
    Feb 27 11:49:34.926: INFO: Created: latency-svc-rq5mt
    Feb 27 11:49:34.944: INFO: Got endpoints: latency-svc-hhlzq [749.740322ms]
    Feb 27 11:49:34.969: INFO: Created: latency-svc-j4bjt
    Feb 27 11:49:35.002: INFO: Got endpoints: latency-svc-knhc7 [761.615102ms]
    Feb 27 11:49:35.025: INFO: Created: latency-svc-g5w7s
    Feb 27 11:49:35.053: INFO: Got endpoints: latency-svc-chhdg [750.933887ms]
    Feb 27 11:49:35.080: INFO: Created: latency-svc-jdgm2
    Feb 27 11:49:35.093: INFO: Got endpoints: latency-svc-k8d58 [747.506603ms]
    Feb 27 11:49:35.129: INFO: Created: latency-svc-wvpfl
    Feb 27 11:49:35.139: INFO: Got endpoints: latency-svc-424gw [749.927044ms]
    Feb 27 11:49:35.168: INFO: Created: latency-svc-gr8v2
    Feb 27 11:49:35.205: INFO: Got endpoints: latency-svc-vzwzj [754.440453ms]
    Feb 27 11:49:35.235: INFO: Created: latency-svc-w922c
    Feb 27 11:49:35.241: INFO: Got endpoints: latency-svc-l7xk8 [736.456348ms]
    Feb 27 11:49:35.271: INFO: Created: latency-svc-vq6mp
    Feb 27 11:49:35.298: INFO: Got endpoints: latency-svc-xsx4x [752.849957ms]
    Feb 27 11:49:35.323: INFO: Created: latency-svc-dkpzb
    Feb 27 11:49:35.346: INFO: Got endpoints: latency-svc-ffqhm [733.553096ms]
    Feb 27 11:49:35.377: INFO: Created: latency-svc-6fz52
    Feb 27 11:49:35.390: INFO: Got endpoints: latency-svc-hdhw8 [749.315712ms]
    Feb 27 11:49:35.441: INFO: Got endpoints: latency-svc-bmv78 [723.872965ms]
    Feb 27 11:49:35.450: INFO: Created: latency-svc-kqg2j
    Feb 27 11:49:35.487: INFO: Created: latency-svc-74jrs
    Feb 27 11:49:35.500: INFO: Got endpoints: latency-svc-2q6v2 [755.128079ms]
    Feb 27 11:49:35.533: INFO: Created: latency-svc-xdjtz
    Feb 27 11:49:35.540: INFO: Got endpoints: latency-svc-8hjlh [742.106323ms]
    Feb 27 11:49:35.598: INFO: Created: latency-svc-2rjhz
    Feb 27 11:49:35.601: INFO: Got endpoints: latency-svc-b78r9 [758.813614ms]
    Feb 27 11:49:35.622: INFO: Created: latency-svc-7jvpl
    Feb 27 11:49:35.645: INFO: Got endpoints: latency-svc-rq5mt [744.924304ms]
    Feb 27 11:49:35.671: INFO: Created: latency-svc-4hjh7
    Feb 27 11:49:35.694: INFO: Got endpoints: latency-svc-j4bjt [749.912056ms]
    Feb 27 11:49:35.728: INFO: Created: latency-svc-9hj2l
    Feb 27 11:49:35.740: INFO: Got endpoints: latency-svc-g5w7s [737.645944ms]
    Feb 27 11:49:35.808: INFO: Got endpoints: latency-svc-jdgm2 [755.5911ms]
    Feb 27 11:49:35.854: INFO: Got endpoints: latency-svc-wvpfl [758.284391ms]
    Feb 27 11:49:35.893: INFO: Got endpoints: latency-svc-gr8v2 [753.937953ms]
    Feb 27 11:49:35.943: INFO: Got endpoints: latency-svc-w922c [737.870785ms]
    Feb 27 11:49:35.998: INFO: Got endpoints: latency-svc-vq6mp [756.901196ms]
    Feb 27 11:49:36.040: INFO: Got endpoints: latency-svc-dkpzb [742.183753ms]
    Feb 27 11:49:36.093: INFO: Got endpoints: latency-svc-6fz52 [746.988735ms]
    Feb 27 11:49:36.141: INFO: Got endpoints: latency-svc-kqg2j [750.924401ms]
    Feb 27 11:49:36.191: INFO: Got endpoints: latency-svc-74jrs [749.864765ms]
    Feb 27 11:49:36.248: INFO: Got endpoints: latency-svc-xdjtz [747.044793ms]
    Feb 27 11:49:36.299: INFO: Got endpoints: latency-svc-2rjhz [758.172921ms]
    Feb 27 11:49:36.342: INFO: Got endpoints: latency-svc-7jvpl [737.991525ms]
    Feb 27 11:49:36.393: INFO: Got endpoints: latency-svc-4hjh7 [747.659505ms]
    Feb 27 11:49:36.446: INFO: Got endpoints: latency-svc-9hj2l [751.840252ms]
    Feb 27 11:49:36.446: INFO: Latencies: [56.545519ms 60.155165ms 60.223806ms 67.922449ms 119.163716ms 133.102284ms 144.841474ms 151.207041ms 159.078863ms 160.31388ms 190.150745ms 192.815045ms 195.039906ms 202.575527ms 205.875911ms 212.239978ms 212.870932ms 226.758251ms 229.727833ms 237.662216ms 243.530762ms 244.228394ms 264.796959ms 266.977701ms 273.72389ms 273.95111ms 281.799682ms 287.189128ms 304.943522ms 309.171221ms 315.362586ms 315.699917ms 317.692755ms 329.289955ms 341.210816ms 343.625035ms 344.265867ms 345.095441ms 348.544348ms 355.537536ms 361.832402ms 370.588239ms 395.989517ms 400.290514ms 405.933981ms 408.802821ms 427.994451ms 428.237282ms 438.149744ms 445.248527ms 447.954105ms 485.680445ms 507.259257ms 511.576955ms 516.656688ms 518.922466ms 556.956777ms 568.407415ms 568.5927ms 572.936739ms 580.21911ms 595.51591ms 597.739731ms 615.71765ms 623.37242ms 669.349583ms 689.242079ms 723.872965ms 724.401786ms 728.365183ms 732.449289ms 733.115713ms 733.553096ms 733.613966ms 733.950197ms 734.133884ms 735.724783ms 736.283406ms 736.456348ms 737.05262ms 737.276009ms 737.645944ms 737.650762ms 737.870785ms 737.991525ms 740.815045ms 741.269929ms 741.972142ms 742.106323ms 742.183753ms 742.295433ms 742.526373ms 742.966706ms 743.059755ms 743.278405ms 743.292816ms 743.568987ms 743.603668ms 743.652397ms 743.830317ms 744.287522ms 744.56181ms 744.79745ms 744.924304ms 745.167834ms 745.193394ms 745.310804ms 745.432695ms 745.521855ms 746.008107ms 746.262429ms 746.537739ms 746.971932ms 746.988735ms 747.044793ms 747.336135ms 747.506603ms 747.659505ms 747.857196ms 748.010326ms 748.191965ms 748.55123ms 748.786907ms 748.914178ms 748.927029ms 749.049971ms 749.09617ms 749.315712ms 749.369321ms 749.740322ms 749.864765ms 749.912056ms 749.927044ms 750.164323ms 750.239774ms 750.371696ms 750.388035ms 750.507145ms 750.593967ms 750.598696ms 750.729387ms 750.745906ms 750.924401ms 750.927479ms 750.933887ms 750.983278ms 750.986709ms 751.415239ms 751.840252ms 752.219122ms 752.707635ms 752.727565ms 752.807577ms 752.812405ms 752.849957ms 753.440859ms 753.50765ms 753.702188ms 753.717918ms 753.937953ms 754.440453ms 755.018366ms 755.096727ms 755.128079ms 755.130145ms 755.379657ms 755.481409ms 755.5911ms 755.802449ms 755.827589ms 756.731732ms 756.859143ms 756.873085ms 756.901196ms 757.018632ms 758.017309ms 758.172921ms 758.284391ms 758.579688ms 758.68245ms 758.813614ms 759.003843ms 759.318081ms 760.149836ms 760.531496ms 761.615102ms 761.655314ms 762.332926ms 762.858197ms 763.920921ms 764.394546ms 765.082706ms 765.260619ms 765.404118ms 766.859817ms 768.301421ms 770.189737ms 771.945315ms 775.129732ms 776.781189ms]
    Feb 27 11:49:36.447: INFO: 50 %ile: 744.287522ms
    Feb 27 11:49:36.447: INFO: 90 %ile: 758.813614ms
    Feb 27 11:49:36.447: INFO: 99 %ile: 775.129732ms
    Feb 27 11:49:36.447: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Feb 27 11:49:36.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-8951" for this suite. 02/27/23 11:49:36.463
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:49:36.895
Feb 27 11:49:36.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 11:49:36.896
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:49:36.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:49:36.989
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 11:49:37.061
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:49:38.005
STEP: Deploying the webhook pod 02/27/23 11:49:38.019
STEP: Wait for the deployment to be ready 02/27/23 11:49:38.043
Feb 27 11:49:38.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 11:49:40.084
STEP: Verifying the service has paired with the endpoint 02/27/23 11:49:40.108
Feb 27 11:49:41.108: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/27/23 11:49:41.116
STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 11:49:41.116
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/27/23 11:49:41.166
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/27/23 11:49:42.191
STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 11:49:42.192
STEP: Having no error when timeout is longer than webhook latency 02/27/23 11:49:43.346
STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 11:49:43.352
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/27/23 11:49:48.495
STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 11:49:48.496
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:49:53.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1619" for this suite. 02/27/23 11:49:53.59
STEP: Destroying namespace "webhook-1619-markers" for this suite. 02/27/23 11:49:53.602
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":131,"skipped":2722,"failed":0}
------------------------------
• [SLOW TEST] [16.814 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:49:36.895
    Feb 27 11:49:36.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 11:49:36.896
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:49:36.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:49:36.989
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 11:49:37.061
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:49:38.005
    STEP: Deploying the webhook pod 02/27/23 11:49:38.019
    STEP: Wait for the deployment to be ready 02/27/23 11:49:38.043
    Feb 27 11:49:38.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 11:49:40.084
    STEP: Verifying the service has paired with the endpoint 02/27/23 11:49:40.108
    Feb 27 11:49:41.108: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/27/23 11:49:41.116
    STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 11:49:41.116
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/27/23 11:49:41.166
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/27/23 11:49:42.191
    STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 11:49:42.192
    STEP: Having no error when timeout is longer than webhook latency 02/27/23 11:49:43.346
    STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 11:49:43.352
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/27/23 11:49:48.495
    STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 11:49:48.496
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:49:53.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1619" for this suite. 02/27/23 11:49:53.59
    STEP: Destroying namespace "webhook-1619-markers" for this suite. 02/27/23 11:49:53.602
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:49:53.804
Feb 27 11:49:53.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename gc 02/27/23 11:49:53.806
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:49:53.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:49:53.924
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 02/27/23 11:49:53.956
STEP: delete the rc 02/27/23 11:49:58.985
STEP: wait for the rc to be deleted 02/27/23 11:49:59.011
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/27/23 11:50:04.084
STEP: Gathering metrics 02/27/23 11:50:34.126
W0227 11:50:34.150758      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 27 11:50:34.150: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 27 11:50:34.150: INFO: Deleting pod "simpletest.rc-228zb" in namespace "gc-2088"
Feb 27 11:50:34.357: INFO: Deleting pod "simpletest.rc-26hkz" in namespace "gc-2088"
Feb 27 11:50:34.397: INFO: Deleting pod "simpletest.rc-2cbxp" in namespace "gc-2088"
Feb 27 11:50:34.424: INFO: Deleting pod "simpletest.rc-2hrtb" in namespace "gc-2088"
Feb 27 11:50:34.472: INFO: Deleting pod "simpletest.rc-2qzrt" in namespace "gc-2088"
Feb 27 11:50:34.500: INFO: Deleting pod "simpletest.rc-2wvgm" in namespace "gc-2088"
Feb 27 11:50:34.543: INFO: Deleting pod "simpletest.rc-4ckzk" in namespace "gc-2088"
Feb 27 11:50:34.585: INFO: Deleting pod "simpletest.rc-4kdnf" in namespace "gc-2088"
Feb 27 11:50:34.630: INFO: Deleting pod "simpletest.rc-558h7" in namespace "gc-2088"
Feb 27 11:50:34.660: INFO: Deleting pod "simpletest.rc-56grn" in namespace "gc-2088"
Feb 27 11:50:34.691: INFO: Deleting pod "simpletest.rc-5g5ng" in namespace "gc-2088"
Feb 27 11:50:34.718: INFO: Deleting pod "simpletest.rc-5nchs" in namespace "gc-2088"
Feb 27 11:50:34.763: INFO: Deleting pod "simpletest.rc-5r6xb" in namespace "gc-2088"
Feb 27 11:50:34.830: INFO: Deleting pod "simpletest.rc-5v69v" in namespace "gc-2088"
Feb 27 11:50:34.893: INFO: Deleting pod "simpletest.rc-62q49" in namespace "gc-2088"
Feb 27 11:50:34.914: INFO: Deleting pod "simpletest.rc-6lqj2" in namespace "gc-2088"
Feb 27 11:50:34.945: INFO: Deleting pod "simpletest.rc-6rnhr" in namespace "gc-2088"
Feb 27 11:50:34.970: INFO: Deleting pod "simpletest.rc-6xmxb" in namespace "gc-2088"
Feb 27 11:50:34.994: INFO: Deleting pod "simpletest.rc-7fgb7" in namespace "gc-2088"
Feb 27 11:50:35.047: INFO: Deleting pod "simpletest.rc-7ftz7" in namespace "gc-2088"
Feb 27 11:50:35.108: INFO: Deleting pod "simpletest.rc-7g2mf" in namespace "gc-2088"
Feb 27 11:50:35.187: INFO: Deleting pod "simpletest.rc-7qsvt" in namespace "gc-2088"
Feb 27 11:50:35.273: INFO: Deleting pod "simpletest.rc-82cvd" in namespace "gc-2088"
Feb 27 11:50:35.319: INFO: Deleting pod "simpletest.rc-8cfmk" in namespace "gc-2088"
Feb 27 11:50:35.404: INFO: Deleting pod "simpletest.rc-8gnb5" in namespace "gc-2088"
Feb 27 11:50:35.427: INFO: Deleting pod "simpletest.rc-8v4vt" in namespace "gc-2088"
Feb 27 11:50:35.473: INFO: Deleting pod "simpletest.rc-9872m" in namespace "gc-2088"
Feb 27 11:50:35.494: INFO: Deleting pod "simpletest.rc-9hjn9" in namespace "gc-2088"
Feb 27 11:50:35.539: INFO: Deleting pod "simpletest.rc-9xjzx" in namespace "gc-2088"
Feb 27 11:50:35.585: INFO: Deleting pod "simpletest.rc-b6qts" in namespace "gc-2088"
Feb 27 11:50:35.621: INFO: Deleting pod "simpletest.rc-bbrsv" in namespace "gc-2088"
Feb 27 11:50:35.709: INFO: Deleting pod "simpletest.rc-bc2bc" in namespace "gc-2088"
Feb 27 11:50:35.734: INFO: Deleting pod "simpletest.rc-cg29q" in namespace "gc-2088"
Feb 27 11:50:35.761: INFO: Deleting pod "simpletest.rc-czftw" in namespace "gc-2088"
Feb 27 11:50:35.807: INFO: Deleting pod "simpletest.rc-djllk" in namespace "gc-2088"
Feb 27 11:50:35.866: INFO: Deleting pod "simpletest.rc-dzgft" in namespace "gc-2088"
Feb 27 11:50:35.911: INFO: Deleting pod "simpletest.rc-f7rmk" in namespace "gc-2088"
Feb 27 11:50:35.997: INFO: Deleting pod "simpletest.rc-fdqq4" in namespace "gc-2088"
Feb 27 11:50:36.077: INFO: Deleting pod "simpletest.rc-fpb5x" in namespace "gc-2088"
Feb 27 11:50:36.221: INFO: Deleting pod "simpletest.rc-fvpm8" in namespace "gc-2088"
Feb 27 11:50:36.269: INFO: Deleting pod "simpletest.rc-g456n" in namespace "gc-2088"
Feb 27 11:50:36.318: INFO: Deleting pod "simpletest.rc-g4pjh" in namespace "gc-2088"
Feb 27 11:50:36.367: INFO: Deleting pod "simpletest.rc-g4xjj" in namespace "gc-2088"
Feb 27 11:50:36.419: INFO: Deleting pod "simpletest.rc-gq78k" in namespace "gc-2088"
Feb 27 11:50:36.478: INFO: Deleting pod "simpletest.rc-gr4x6" in namespace "gc-2088"
Feb 27 11:50:36.529: INFO: Deleting pod "simpletest.rc-gtg5z" in namespace "gc-2088"
Feb 27 11:50:36.580: INFO: Deleting pod "simpletest.rc-gtg6v" in namespace "gc-2088"
Feb 27 11:50:36.629: INFO: Deleting pod "simpletest.rc-h2mwg" in namespace "gc-2088"
Feb 27 11:50:36.659: INFO: Deleting pod "simpletest.rc-h88qn" in namespace "gc-2088"
Feb 27 11:50:36.678: INFO: Deleting pod "simpletest.rc-hg8qz" in namespace "gc-2088"
Feb 27 11:50:36.708: INFO: Deleting pod "simpletest.rc-j9vtb" in namespace "gc-2088"
Feb 27 11:50:36.741: INFO: Deleting pod "simpletest.rc-jbj5m" in namespace "gc-2088"
Feb 27 11:50:36.775: INFO: Deleting pod "simpletest.rc-jlhfs" in namespace "gc-2088"
Feb 27 11:50:36.801: INFO: Deleting pod "simpletest.rc-jn8ln" in namespace "gc-2088"
Feb 27 11:50:36.852: INFO: Deleting pod "simpletest.rc-k7pf7" in namespace "gc-2088"
Feb 27 11:50:36.873: INFO: Deleting pod "simpletest.rc-kcjq7" in namespace "gc-2088"
Feb 27 11:50:36.922: INFO: Deleting pod "simpletest.rc-klj29" in namespace "gc-2088"
Feb 27 11:50:36.941: INFO: Deleting pod "simpletest.rc-kw599" in namespace "gc-2088"
Feb 27 11:50:36.992: INFO: Deleting pod "simpletest.rc-kz4jb" in namespace "gc-2088"
Feb 27 11:50:37.012: INFO: Deleting pod "simpletest.rc-l5pn5" in namespace "gc-2088"
Feb 27 11:50:37.031: INFO: Deleting pod "simpletest.rc-ltqfx" in namespace "gc-2088"
Feb 27 11:50:37.070: INFO: Deleting pod "simpletest.rc-lwngv" in namespace "gc-2088"
Feb 27 11:50:37.091: INFO: Deleting pod "simpletest.rc-mlzst" in namespace "gc-2088"
Feb 27 11:50:37.139: INFO: Deleting pod "simpletest.rc-mmpdq" in namespace "gc-2088"
Feb 27 11:50:37.208: INFO: Deleting pod "simpletest.rc-mrlvq" in namespace "gc-2088"
Feb 27 11:50:37.225: INFO: Deleting pod "simpletest.rc-n42qd" in namespace "gc-2088"
Feb 27 11:50:37.252: INFO: Deleting pod "simpletest.rc-n9cwq" in namespace "gc-2088"
Feb 27 11:50:37.275: INFO: Deleting pod "simpletest.rc-nrpch" in namespace "gc-2088"
Feb 27 11:50:37.295: INFO: Deleting pod "simpletest.rc-p54p6" in namespace "gc-2088"
Feb 27 11:50:37.327: INFO: Deleting pod "simpletest.rc-p5d8g" in namespace "gc-2088"
Feb 27 11:50:37.377: INFO: Deleting pod "simpletest.rc-pk6xw" in namespace "gc-2088"
Feb 27 11:50:37.402: INFO: Deleting pod "simpletest.rc-plgtb" in namespace "gc-2088"
Feb 27 11:50:37.438: INFO: Deleting pod "simpletest.rc-q8fb6" in namespace "gc-2088"
Feb 27 11:50:37.476: INFO: Deleting pod "simpletest.rc-qcq6q" in namespace "gc-2088"
Feb 27 11:50:37.564: INFO: Deleting pod "simpletest.rc-qx9tj" in namespace "gc-2088"
Feb 27 11:50:37.581: INFO: Deleting pod "simpletest.rc-rflkv" in namespace "gc-2088"
Feb 27 11:50:37.618: INFO: Deleting pod "simpletest.rc-rfrl5" in namespace "gc-2088"
Feb 27 11:50:38.136: INFO: Deleting pod "simpletest.rc-rl4f8" in namespace "gc-2088"
Feb 27 11:50:38.166: INFO: Deleting pod "simpletest.rc-rqr66" in namespace "gc-2088"
Feb 27 11:50:38.223: INFO: Deleting pod "simpletest.rc-srbsh" in namespace "gc-2088"
Feb 27 11:50:38.307: INFO: Deleting pod "simpletest.rc-t42kn" in namespace "gc-2088"
Feb 27 11:50:38.358: INFO: Deleting pod "simpletest.rc-tnk42" in namespace "gc-2088"
Feb 27 11:50:38.387: INFO: Deleting pod "simpletest.rc-tx7tk" in namespace "gc-2088"
Feb 27 11:50:38.409: INFO: Deleting pod "simpletest.rc-v85g5" in namespace "gc-2088"
Feb 27 11:50:38.440: INFO: Deleting pod "simpletest.rc-v9t64" in namespace "gc-2088"
Feb 27 11:50:38.461: INFO: Deleting pod "simpletest.rc-vsrh4" in namespace "gc-2088"
Feb 27 11:50:38.483: INFO: Deleting pod "simpletest.rc-w4cpw" in namespace "gc-2088"
Feb 27 11:50:38.555: INFO: Deleting pod "simpletest.rc-wczgc" in namespace "gc-2088"
Feb 27 11:50:38.581: INFO: Deleting pod "simpletest.rc-wfxbg" in namespace "gc-2088"
Feb 27 11:50:38.602: INFO: Deleting pod "simpletest.rc-wmpdp" in namespace "gc-2088"
Feb 27 11:50:38.647: INFO: Deleting pod "simpletest.rc-x4wv9" in namespace "gc-2088"
Feb 27 11:50:38.673: INFO: Deleting pod "simpletest.rc-xc7zq" in namespace "gc-2088"
Feb 27 11:50:38.731: INFO: Deleting pod "simpletest.rc-xhb66" in namespace "gc-2088"
Feb 27 11:50:38.789: INFO: Deleting pod "simpletest.rc-xmvv7" in namespace "gc-2088"
Feb 27 11:50:38.831: INFO: Deleting pod "simpletest.rc-z8vtp" in namespace "gc-2088"
Feb 27 11:50:38.876: INFO: Deleting pod "simpletest.rc-z9c6r" in namespace "gc-2088"
Feb 27 11:50:38.975: INFO: Deleting pod "simpletest.rc-zdlqr" in namespace "gc-2088"
Feb 27 11:50:38.998: INFO: Deleting pod "simpletest.rc-zn76z" in namespace "gc-2088"
Feb 27 11:50:39.066: INFO: Deleting pod "simpletest.rc-zsdvp" in namespace "gc-2088"
Feb 27 11:50:39.116: INFO: Deleting pod "simpletest.rc-ztttm" in namespace "gc-2088"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 27 11:50:39.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2088" for this suite. 02/27/23 11:50:39.22
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":132,"skipped":2757,"failed":0}
------------------------------
• [SLOW TEST] [45.440 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:49:53.804
    Feb 27 11:49:53.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename gc 02/27/23 11:49:53.806
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:49:53.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:49:53.924
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 02/27/23 11:49:53.956
    STEP: delete the rc 02/27/23 11:49:58.985
    STEP: wait for the rc to be deleted 02/27/23 11:49:59.011
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/27/23 11:50:04.084
    STEP: Gathering metrics 02/27/23 11:50:34.126
    W0227 11:50:34.150758      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 27 11:50:34.150: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 27 11:50:34.150: INFO: Deleting pod "simpletest.rc-228zb" in namespace "gc-2088"
    Feb 27 11:50:34.357: INFO: Deleting pod "simpletest.rc-26hkz" in namespace "gc-2088"
    Feb 27 11:50:34.397: INFO: Deleting pod "simpletest.rc-2cbxp" in namespace "gc-2088"
    Feb 27 11:50:34.424: INFO: Deleting pod "simpletest.rc-2hrtb" in namespace "gc-2088"
    Feb 27 11:50:34.472: INFO: Deleting pod "simpletest.rc-2qzrt" in namespace "gc-2088"
    Feb 27 11:50:34.500: INFO: Deleting pod "simpletest.rc-2wvgm" in namespace "gc-2088"
    Feb 27 11:50:34.543: INFO: Deleting pod "simpletest.rc-4ckzk" in namespace "gc-2088"
    Feb 27 11:50:34.585: INFO: Deleting pod "simpletest.rc-4kdnf" in namespace "gc-2088"
    Feb 27 11:50:34.630: INFO: Deleting pod "simpletest.rc-558h7" in namespace "gc-2088"
    Feb 27 11:50:34.660: INFO: Deleting pod "simpletest.rc-56grn" in namespace "gc-2088"
    Feb 27 11:50:34.691: INFO: Deleting pod "simpletest.rc-5g5ng" in namespace "gc-2088"
    Feb 27 11:50:34.718: INFO: Deleting pod "simpletest.rc-5nchs" in namespace "gc-2088"
    Feb 27 11:50:34.763: INFO: Deleting pod "simpletest.rc-5r6xb" in namespace "gc-2088"
    Feb 27 11:50:34.830: INFO: Deleting pod "simpletest.rc-5v69v" in namespace "gc-2088"
    Feb 27 11:50:34.893: INFO: Deleting pod "simpletest.rc-62q49" in namespace "gc-2088"
    Feb 27 11:50:34.914: INFO: Deleting pod "simpletest.rc-6lqj2" in namespace "gc-2088"
    Feb 27 11:50:34.945: INFO: Deleting pod "simpletest.rc-6rnhr" in namespace "gc-2088"
    Feb 27 11:50:34.970: INFO: Deleting pod "simpletest.rc-6xmxb" in namespace "gc-2088"
    Feb 27 11:50:34.994: INFO: Deleting pod "simpletest.rc-7fgb7" in namespace "gc-2088"
    Feb 27 11:50:35.047: INFO: Deleting pod "simpletest.rc-7ftz7" in namespace "gc-2088"
    Feb 27 11:50:35.108: INFO: Deleting pod "simpletest.rc-7g2mf" in namespace "gc-2088"
    Feb 27 11:50:35.187: INFO: Deleting pod "simpletest.rc-7qsvt" in namespace "gc-2088"
    Feb 27 11:50:35.273: INFO: Deleting pod "simpletest.rc-82cvd" in namespace "gc-2088"
    Feb 27 11:50:35.319: INFO: Deleting pod "simpletest.rc-8cfmk" in namespace "gc-2088"
    Feb 27 11:50:35.404: INFO: Deleting pod "simpletest.rc-8gnb5" in namespace "gc-2088"
    Feb 27 11:50:35.427: INFO: Deleting pod "simpletest.rc-8v4vt" in namespace "gc-2088"
    Feb 27 11:50:35.473: INFO: Deleting pod "simpletest.rc-9872m" in namespace "gc-2088"
    Feb 27 11:50:35.494: INFO: Deleting pod "simpletest.rc-9hjn9" in namespace "gc-2088"
    Feb 27 11:50:35.539: INFO: Deleting pod "simpletest.rc-9xjzx" in namespace "gc-2088"
    Feb 27 11:50:35.585: INFO: Deleting pod "simpletest.rc-b6qts" in namespace "gc-2088"
    Feb 27 11:50:35.621: INFO: Deleting pod "simpletest.rc-bbrsv" in namespace "gc-2088"
    Feb 27 11:50:35.709: INFO: Deleting pod "simpletest.rc-bc2bc" in namespace "gc-2088"
    Feb 27 11:50:35.734: INFO: Deleting pod "simpletest.rc-cg29q" in namespace "gc-2088"
    Feb 27 11:50:35.761: INFO: Deleting pod "simpletest.rc-czftw" in namespace "gc-2088"
    Feb 27 11:50:35.807: INFO: Deleting pod "simpletest.rc-djllk" in namespace "gc-2088"
    Feb 27 11:50:35.866: INFO: Deleting pod "simpletest.rc-dzgft" in namespace "gc-2088"
    Feb 27 11:50:35.911: INFO: Deleting pod "simpletest.rc-f7rmk" in namespace "gc-2088"
    Feb 27 11:50:35.997: INFO: Deleting pod "simpletest.rc-fdqq4" in namespace "gc-2088"
    Feb 27 11:50:36.077: INFO: Deleting pod "simpletest.rc-fpb5x" in namespace "gc-2088"
    Feb 27 11:50:36.221: INFO: Deleting pod "simpletest.rc-fvpm8" in namespace "gc-2088"
    Feb 27 11:50:36.269: INFO: Deleting pod "simpletest.rc-g456n" in namespace "gc-2088"
    Feb 27 11:50:36.318: INFO: Deleting pod "simpletest.rc-g4pjh" in namespace "gc-2088"
    Feb 27 11:50:36.367: INFO: Deleting pod "simpletest.rc-g4xjj" in namespace "gc-2088"
    Feb 27 11:50:36.419: INFO: Deleting pod "simpletest.rc-gq78k" in namespace "gc-2088"
    Feb 27 11:50:36.478: INFO: Deleting pod "simpletest.rc-gr4x6" in namespace "gc-2088"
    Feb 27 11:50:36.529: INFO: Deleting pod "simpletest.rc-gtg5z" in namespace "gc-2088"
    Feb 27 11:50:36.580: INFO: Deleting pod "simpletest.rc-gtg6v" in namespace "gc-2088"
    Feb 27 11:50:36.629: INFO: Deleting pod "simpletest.rc-h2mwg" in namespace "gc-2088"
    Feb 27 11:50:36.659: INFO: Deleting pod "simpletest.rc-h88qn" in namespace "gc-2088"
    Feb 27 11:50:36.678: INFO: Deleting pod "simpletest.rc-hg8qz" in namespace "gc-2088"
    Feb 27 11:50:36.708: INFO: Deleting pod "simpletest.rc-j9vtb" in namespace "gc-2088"
    Feb 27 11:50:36.741: INFO: Deleting pod "simpletest.rc-jbj5m" in namespace "gc-2088"
    Feb 27 11:50:36.775: INFO: Deleting pod "simpletest.rc-jlhfs" in namespace "gc-2088"
    Feb 27 11:50:36.801: INFO: Deleting pod "simpletest.rc-jn8ln" in namespace "gc-2088"
    Feb 27 11:50:36.852: INFO: Deleting pod "simpletest.rc-k7pf7" in namespace "gc-2088"
    Feb 27 11:50:36.873: INFO: Deleting pod "simpletest.rc-kcjq7" in namespace "gc-2088"
    Feb 27 11:50:36.922: INFO: Deleting pod "simpletest.rc-klj29" in namespace "gc-2088"
    Feb 27 11:50:36.941: INFO: Deleting pod "simpletest.rc-kw599" in namespace "gc-2088"
    Feb 27 11:50:36.992: INFO: Deleting pod "simpletest.rc-kz4jb" in namespace "gc-2088"
    Feb 27 11:50:37.012: INFO: Deleting pod "simpletest.rc-l5pn5" in namespace "gc-2088"
    Feb 27 11:50:37.031: INFO: Deleting pod "simpletest.rc-ltqfx" in namespace "gc-2088"
    Feb 27 11:50:37.070: INFO: Deleting pod "simpletest.rc-lwngv" in namespace "gc-2088"
    Feb 27 11:50:37.091: INFO: Deleting pod "simpletest.rc-mlzst" in namespace "gc-2088"
    Feb 27 11:50:37.139: INFO: Deleting pod "simpletest.rc-mmpdq" in namespace "gc-2088"
    Feb 27 11:50:37.208: INFO: Deleting pod "simpletest.rc-mrlvq" in namespace "gc-2088"
    Feb 27 11:50:37.225: INFO: Deleting pod "simpletest.rc-n42qd" in namespace "gc-2088"
    Feb 27 11:50:37.252: INFO: Deleting pod "simpletest.rc-n9cwq" in namespace "gc-2088"
    Feb 27 11:50:37.275: INFO: Deleting pod "simpletest.rc-nrpch" in namespace "gc-2088"
    Feb 27 11:50:37.295: INFO: Deleting pod "simpletest.rc-p54p6" in namespace "gc-2088"
    Feb 27 11:50:37.327: INFO: Deleting pod "simpletest.rc-p5d8g" in namespace "gc-2088"
    Feb 27 11:50:37.377: INFO: Deleting pod "simpletest.rc-pk6xw" in namespace "gc-2088"
    Feb 27 11:50:37.402: INFO: Deleting pod "simpletest.rc-plgtb" in namespace "gc-2088"
    Feb 27 11:50:37.438: INFO: Deleting pod "simpletest.rc-q8fb6" in namespace "gc-2088"
    Feb 27 11:50:37.476: INFO: Deleting pod "simpletest.rc-qcq6q" in namespace "gc-2088"
    Feb 27 11:50:37.564: INFO: Deleting pod "simpletest.rc-qx9tj" in namespace "gc-2088"
    Feb 27 11:50:37.581: INFO: Deleting pod "simpletest.rc-rflkv" in namespace "gc-2088"
    Feb 27 11:50:37.618: INFO: Deleting pod "simpletest.rc-rfrl5" in namespace "gc-2088"
    Feb 27 11:50:38.136: INFO: Deleting pod "simpletest.rc-rl4f8" in namespace "gc-2088"
    Feb 27 11:50:38.166: INFO: Deleting pod "simpletest.rc-rqr66" in namespace "gc-2088"
    Feb 27 11:50:38.223: INFO: Deleting pod "simpletest.rc-srbsh" in namespace "gc-2088"
    Feb 27 11:50:38.307: INFO: Deleting pod "simpletest.rc-t42kn" in namespace "gc-2088"
    Feb 27 11:50:38.358: INFO: Deleting pod "simpletest.rc-tnk42" in namespace "gc-2088"
    Feb 27 11:50:38.387: INFO: Deleting pod "simpletest.rc-tx7tk" in namespace "gc-2088"
    Feb 27 11:50:38.409: INFO: Deleting pod "simpletest.rc-v85g5" in namespace "gc-2088"
    Feb 27 11:50:38.440: INFO: Deleting pod "simpletest.rc-v9t64" in namespace "gc-2088"
    Feb 27 11:50:38.461: INFO: Deleting pod "simpletest.rc-vsrh4" in namespace "gc-2088"
    Feb 27 11:50:38.483: INFO: Deleting pod "simpletest.rc-w4cpw" in namespace "gc-2088"
    Feb 27 11:50:38.555: INFO: Deleting pod "simpletest.rc-wczgc" in namespace "gc-2088"
    Feb 27 11:50:38.581: INFO: Deleting pod "simpletest.rc-wfxbg" in namespace "gc-2088"
    Feb 27 11:50:38.602: INFO: Deleting pod "simpletest.rc-wmpdp" in namespace "gc-2088"
    Feb 27 11:50:38.647: INFO: Deleting pod "simpletest.rc-x4wv9" in namespace "gc-2088"
    Feb 27 11:50:38.673: INFO: Deleting pod "simpletest.rc-xc7zq" in namespace "gc-2088"
    Feb 27 11:50:38.731: INFO: Deleting pod "simpletest.rc-xhb66" in namespace "gc-2088"
    Feb 27 11:50:38.789: INFO: Deleting pod "simpletest.rc-xmvv7" in namespace "gc-2088"
    Feb 27 11:50:38.831: INFO: Deleting pod "simpletest.rc-z8vtp" in namespace "gc-2088"
    Feb 27 11:50:38.876: INFO: Deleting pod "simpletest.rc-z9c6r" in namespace "gc-2088"
    Feb 27 11:50:38.975: INFO: Deleting pod "simpletest.rc-zdlqr" in namespace "gc-2088"
    Feb 27 11:50:38.998: INFO: Deleting pod "simpletest.rc-zn76z" in namespace "gc-2088"
    Feb 27 11:50:39.066: INFO: Deleting pod "simpletest.rc-zsdvp" in namespace "gc-2088"
    Feb 27 11:50:39.116: INFO: Deleting pod "simpletest.rc-ztttm" in namespace "gc-2088"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 27 11:50:39.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2088" for this suite. 02/27/23 11:50:39.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:50:39.246
Feb 27 11:50:39.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename daemonsets 02/27/23 11:50:39.25
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:50:39.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:50:39.299
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Feb 27 11:50:39.388: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 02/27/23 11:50:39.397
Feb 27 11:50:39.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:39.403: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 02/27/23 11:50:39.403
Feb 27 11:50:39.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:39.491: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:50:41.135: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:41.135: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:50:41.502: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:41.502: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:50:42.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:42.500: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:50:43.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:43.500: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:50:44.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 11:50:44.508: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 02/27/23 11:50:44.522
Feb 27 11:50:44.583: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 11:50:44.583: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Feb 27 11:50:45.595: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:45.595: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/27/23 11:50:45.595
Feb 27 11:50:45.624: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:45.624: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:50:46.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:46.634: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:50:47.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:47.634: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:50:48.631: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:48.631: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:50:49.633: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:49.633: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:50:50.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 11:50:50.637: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/27/23 11:50:50.653
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5958, will wait for the garbage collector to delete the pods 02/27/23 11:50:50.653
Feb 27 11:50:50.724: INFO: Deleting DaemonSet.extensions daemon-set took: 12.570772ms
Feb 27 11:50:50.825: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.940643ms
Feb 27 11:50:53.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:50:53.034: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 11:50:53.041: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74612"},"items":null}

Feb 27 11:50:53.048: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74612"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 27 11:50:53.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5958" for this suite. 02/27/23 11:50:53.12
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":133,"skipped":2763,"failed":0}
------------------------------
• [SLOW TEST] [13.888 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:50:39.246
    Feb 27 11:50:39.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename daemonsets 02/27/23 11:50:39.25
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:50:39.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:50:39.299
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Feb 27 11:50:39.388: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 02/27/23 11:50:39.397
    Feb 27 11:50:39.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:39.403: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 02/27/23 11:50:39.403
    Feb 27 11:50:39.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:39.491: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:50:41.135: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:41.135: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:50:41.502: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:41.502: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:50:42.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:42.500: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:50:43.500: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:43.500: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:50:44.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 11:50:44.508: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 02/27/23 11:50:44.522
    Feb 27 11:50:44.583: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 11:50:44.583: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Feb 27 11:50:45.595: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:45.595: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/27/23 11:50:45.595
    Feb 27 11:50:45.624: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:45.624: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:50:46.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:46.634: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:50:47.634: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:47.634: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:50:48.631: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:48.631: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:50:49.633: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:49.633: INFO: Node ip-172-31-15-17.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:50:50.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 11:50:50.637: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 11:50:50.653
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5958, will wait for the garbage collector to delete the pods 02/27/23 11:50:50.653
    Feb 27 11:50:50.724: INFO: Deleting DaemonSet.extensions daemon-set took: 12.570772ms
    Feb 27 11:50:50.825: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.940643ms
    Feb 27 11:50:53.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:50:53.034: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 11:50:53.041: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74612"},"items":null}

    Feb 27 11:50:53.048: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74612"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 11:50:53.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5958" for this suite. 02/27/23 11:50:53.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:50:53.138
Feb 27 11:50:53.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename podtemplate 02/27/23 11:50:53.14
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:50:53.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:50:53.175
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 02/27/23 11:50:53.185
Feb 27 11:50:53.195: INFO: created test-podtemplate-1
Feb 27 11:50:53.203: INFO: created test-podtemplate-2
Feb 27 11:50:53.212: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 02/27/23 11:50:53.212
STEP: delete collection of pod templates 02/27/23 11:50:53.235
Feb 27 11:50:53.236: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 02/27/23 11:50:53.277
Feb 27 11:50:53.277: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb 27 11:50:53.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5313" for this suite. 02/27/23 11:50:53.291
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":134,"skipped":2789,"failed":0}
------------------------------
• [0.162 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:50:53.138
    Feb 27 11:50:53.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename podtemplate 02/27/23 11:50:53.14
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:50:53.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:50:53.175
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 02/27/23 11:50:53.185
    Feb 27 11:50:53.195: INFO: created test-podtemplate-1
    Feb 27 11:50:53.203: INFO: created test-podtemplate-2
    Feb 27 11:50:53.212: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 02/27/23 11:50:53.212
    STEP: delete collection of pod templates 02/27/23 11:50:53.235
    Feb 27 11:50:53.236: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 02/27/23 11:50:53.277
    Feb 27 11:50:53.277: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb 27 11:50:53.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-5313" for this suite. 02/27/23 11:50:53.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:50:53.303
Feb 27 11:50:53.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 11:50:53.304
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:50:53.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:50:53.348
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 11:50:53.381
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:50:54.297
STEP: Deploying the webhook pod 02/27/23 11:50:54.308
STEP: Wait for the deployment to be ready 02/27/23 11:50:54.332
Feb 27 11:50:54.354: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 11:50:56.378
STEP: Verifying the service has paired with the endpoint 02/27/23 11:50:56.394
Feb 27 11:50:57.395: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Feb 27 11:50:57.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/27/23 11:50:57.925
STEP: Creating a custom resource that should be denied by the webhook 02/27/23 11:50:57.988
STEP: Creating a custom resource whose deletion would be denied by the webhook 02/27/23 11:51:00.084
STEP: Updating the custom resource with disallowed data should be denied 02/27/23 11:51:00.118
STEP: Deleting the custom resource should be denied 02/27/23 11:51:00.148
STEP: Remove the offending key and value from the custom resource data 02/27/23 11:51:00.169
STEP: Deleting the updated custom resource should be successful 02/27/23 11:51:00.202
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:51:00.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5534" for this suite. 02/27/23 11:51:00.809
STEP: Destroying namespace "webhook-5534-markers" for this suite. 02/27/23 11:51:00.825
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":135,"skipped":2800,"failed":0}
------------------------------
• [SLOW TEST] [7.668 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:50:53.303
    Feb 27 11:50:53.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 11:50:53.304
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:50:53.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:50:53.348
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 11:50:53.381
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:50:54.297
    STEP: Deploying the webhook pod 02/27/23 11:50:54.308
    STEP: Wait for the deployment to be ready 02/27/23 11:50:54.332
    Feb 27 11:50:54.354: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 11:50:56.378
    STEP: Verifying the service has paired with the endpoint 02/27/23 11:50:56.394
    Feb 27 11:50:57.395: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Feb 27 11:50:57.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/27/23 11:50:57.925
    STEP: Creating a custom resource that should be denied by the webhook 02/27/23 11:50:57.988
    STEP: Creating a custom resource whose deletion would be denied by the webhook 02/27/23 11:51:00.084
    STEP: Updating the custom resource with disallowed data should be denied 02/27/23 11:51:00.118
    STEP: Deleting the custom resource should be denied 02/27/23 11:51:00.148
    STEP: Remove the offending key and value from the custom resource data 02/27/23 11:51:00.169
    STEP: Deleting the updated custom resource should be successful 02/27/23 11:51:00.202
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:51:00.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5534" for this suite. 02/27/23 11:51:00.809
    STEP: Destroying namespace "webhook-5534-markers" for this suite. 02/27/23 11:51:00.825
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:51:00.977
Feb 27 11:51:00.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 11:51:00.978
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:51:01.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:51:01.135
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-3251 02/27/23 11:51:01.175
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[] 02/27/23 11:51:01.219
Feb 27 11:51:01.269: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3251 02/27/23 11:51:01.269
Feb 27 11:51:01.289: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3251" to be "running and ready"
Feb 27 11:51:01.304: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.852443ms
Feb 27 11:51:01.304: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:51:03.338: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.048992633s
Feb 27 11:51:03.352: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 27 11:51:03.352: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[pod1:[100]] 02/27/23 11:51:03.367
Feb 27 11:51:03.408: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3251 02/27/23 11:51:03.408
Feb 27 11:51:03.424: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3251" to be "running and ready"
Feb 27 11:51:03.443: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.324081ms
Feb 27 11:51:03.444: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:51:05.451: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.027279851s
Feb 27 11:51:05.451: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 27 11:51:05.451: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[pod1:[100] pod2:[101]] 02/27/23 11:51:05.462
Feb 27 11:51:05.549: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 02/27/23 11:51:05.549
Feb 27 11:51:05.549: INFO: Creating new exec pod
Feb 27 11:51:05.575: INFO: Waiting up to 5m0s for pod "execpodvh9w2" in namespace "services-3251" to be "running"
Feb 27 11:51:05.593: INFO: Pod "execpodvh9w2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.292396ms
Feb 27 11:51:07.604: INFO: Pod "execpodvh9w2": Phase="Running", Reason="", readiness=true. Elapsed: 2.028826348s
Feb 27 11:51:07.604: INFO: Pod "execpodvh9w2" satisfied condition "running"
Feb 27 11:51:08.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-3251 exec execpodvh9w2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Feb 27 11:51:09.266: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Feb 27 11:51:09.267: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:51:09.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-3251 exec execpodvh9w2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.101 80'
Feb 27 11:51:09.693: INFO: stderr: "+ nc -v -t -w 2 10.240.19.101 80\n+ echo hostName\nConnection to 10.240.19.101 80 port [tcp/http] succeeded!\n"
Feb 27 11:51:09.693: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:51:09.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-3251 exec execpodvh9w2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Feb 27 11:51:10.025: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Feb 27 11:51:10.025: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:51:10.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-3251 exec execpodvh9w2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.101 81'
Feb 27 11:51:10.333: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.19.101 81\nConnection to 10.240.19.101 81 port [tcp/*] succeeded!\n"
Feb 27 11:51:10.333: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3251 02/27/23 11:51:10.333
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[pod2:[101]] 02/27/23 11:51:10.372
Feb 27 11:51:10.451: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3251 02/27/23 11:51:10.451
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[] 02/27/23 11:51:10.49
Feb 27 11:51:10.569: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 11:51:10.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3251" for this suite. 02/27/23 11:51:10.66
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":136,"skipped":2824,"failed":0}
------------------------------
• [SLOW TEST] [9.696 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:51:00.977
    Feb 27 11:51:00.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 11:51:00.978
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:51:01.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:51:01.135
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-3251 02/27/23 11:51:01.175
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[] 02/27/23 11:51:01.219
    Feb 27 11:51:01.269: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3251 02/27/23 11:51:01.269
    Feb 27 11:51:01.289: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3251" to be "running and ready"
    Feb 27 11:51:01.304: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.852443ms
    Feb 27 11:51:01.304: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:51:03.338: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.048992633s
    Feb 27 11:51:03.352: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 27 11:51:03.352: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[pod1:[100]] 02/27/23 11:51:03.367
    Feb 27 11:51:03.408: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-3251 02/27/23 11:51:03.408
    Feb 27 11:51:03.424: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3251" to be "running and ready"
    Feb 27 11:51:03.443: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.324081ms
    Feb 27 11:51:03.444: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:51:05.451: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.027279851s
    Feb 27 11:51:05.451: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 27 11:51:05.451: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[pod1:[100] pod2:[101]] 02/27/23 11:51:05.462
    Feb 27 11:51:05.549: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 02/27/23 11:51:05.549
    Feb 27 11:51:05.549: INFO: Creating new exec pod
    Feb 27 11:51:05.575: INFO: Waiting up to 5m0s for pod "execpodvh9w2" in namespace "services-3251" to be "running"
    Feb 27 11:51:05.593: INFO: Pod "execpodvh9w2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.292396ms
    Feb 27 11:51:07.604: INFO: Pod "execpodvh9w2": Phase="Running", Reason="", readiness=true. Elapsed: 2.028826348s
    Feb 27 11:51:07.604: INFO: Pod "execpodvh9w2" satisfied condition "running"
    Feb 27 11:51:08.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-3251 exec execpodvh9w2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Feb 27 11:51:09.266: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Feb 27 11:51:09.267: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:51:09.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-3251 exec execpodvh9w2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.101 80'
    Feb 27 11:51:09.693: INFO: stderr: "+ nc -v -t -w 2 10.240.19.101 80\n+ echo hostName\nConnection to 10.240.19.101 80 port [tcp/http] succeeded!\n"
    Feb 27 11:51:09.693: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:51:09.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-3251 exec execpodvh9w2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Feb 27 11:51:10.025: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Feb 27 11:51:10.025: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:51:10.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-3251 exec execpodvh9w2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.101 81'
    Feb 27 11:51:10.333: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.19.101 81\nConnection to 10.240.19.101 81 port [tcp/*] succeeded!\n"
    Feb 27 11:51:10.333: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-3251 02/27/23 11:51:10.333
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[pod2:[101]] 02/27/23 11:51:10.372
    Feb 27 11:51:10.451: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-3251 02/27/23 11:51:10.451
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3251 to expose endpoints map[] 02/27/23 11:51:10.49
    Feb 27 11:51:10.569: INFO: successfully validated that service multi-endpoint-test in namespace services-3251 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 11:51:10.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3251" for this suite. 02/27/23 11:51:10.66
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:51:10.679
Feb 27 11:51:10.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename sched-pred 02/27/23 11:51:10.689
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:51:10.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:51:10.757
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 27 11:51:10.796: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 27 11:51:10.845: INFO: Waiting for terminating namespaces to be deleted...
Feb 27 11:51:10.865: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-11-159.eu-central-1.compute.internal before test
Feb 27 11:51:10.895: INFO: canal-4q9m8 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
Feb 27 11:51:10.896: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 11:51:10.899: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 27 11:51:10.900: INFO: coredns-bf8668b4f-nbc9z from kube-system started at 2023-02-27 11:28:02 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.900: INFO: 	Container coredns ready: true, restart count 0
Feb 27 11:51:10.901: INFO: ebs-csi-node-b6z5h from kube-system started at 2023-02-27 09:22:11 +0000 UTC (3 container statuses recorded)
Feb 27 11:51:10.903: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 11:51:10.904: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 11:51:10.905: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 11:51:10.905: INFO: envoy-agent-2wwht from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
Feb 27 11:51:10.905: INFO: 	Container assign-address ready: true, restart count 0
Feb 27 11:51:10.905: INFO: 	Container envoy-agent ready: true, restart count 0
Feb 27 11:51:10.905: INFO: konnectivity-agent-76c848fdd6-fr8fm from kube-system started at 2023-02-27 11:28:02 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.905: INFO: 	Container konnectivity-agent ready: true, restart count 0
Feb 27 11:51:10.905: INFO: kube-proxy-cz9zt from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.905: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 27 11:51:10.905: INFO: metrics-server-5f7c5d4b9-qh6s5 from kube-system started at 2023-02-27 11:28:02 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.906: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 11:51:10.906: INFO: node-local-dns-r6r4k from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.906: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 11:51:10.906: INFO: user-ssh-keys-agent-rk5t9 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.906: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 27 11:51:10.906: INFO: bin-false6bda9908-2f95-48ed-afb3-16dcf0b54b54 from kubelet-test-5184 started at 2023-02-27 11:49:25 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.906: INFO: 	Container bin-false6bda9908-2f95-48ed-afb3-16dcf0b54b54 ready: false, restart count 0
Feb 27 11:51:10.906: INFO: dashboard-metrics-scraper-85f6dd84d5-c7wsq from kubernetes-dashboard started at 2023-02-27 11:08:24 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.906: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 27 11:51:10.906: INFO: execpodvh9w2 from services-3251 started at 2023-02-27 11:51:05 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.907: INFO: 	Container agnhost-container ready: true, restart count 0
Feb 27 11:51:10.907: INFO: sonobuoy from sonobuoy started at 2023-02-27 11:19:26 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.907: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 27 11:51:10.907: INFO: sonobuoy-e2e-job-18131847dfcd49d5 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 11:51:10.907: INFO: 	Container e2e ready: true, restart count 0
Feb 27 11:51:10.908: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 11:51:10.908: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-rlhml from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 11:51:10.908: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 11:51:10.908: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 11:51:10.908: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-15-17.eu-central-1.compute.internal before test
Feb 27 11:51:10.928: INFO: calico-kube-controllers-55d99d998f-f5ngs from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.928: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 27 11:51:10.928: INFO: canal-bxq4m from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
Feb 27 11:51:10.928: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 11:51:10.928: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 27 11:51:10.928: INFO: coredns-bf8668b4f-5h5v9 from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.928: INFO: 	Container coredns ready: true, restart count 0
Feb 27 11:51:10.928: INFO: ebs-csi-node-z9l5x from kube-system started at 2023-02-27 09:22:52 +0000 UTC (3 container statuses recorded)
Feb 27 11:51:10.928: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 11:51:10.928: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 11:51:10.928: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 11:51:10.928: INFO: envoy-agent-7xhs5 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
Feb 27 11:51:10.929: INFO: 	Container assign-address ready: true, restart count 0
Feb 27 11:51:10.929: INFO: 	Container envoy-agent ready: true, restart count 0
Feb 27 11:51:10.929: INFO: konnectivity-agent-76c848fdd6-56hgq from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.929: INFO: 	Container konnectivity-agent ready: true, restart count 0
Feb 27 11:51:10.929: INFO: kube-proxy-hxmbw from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.929: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 27 11:51:10.929: INFO: metrics-server-5f7c5d4b9-xlr2c from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.929: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 11:51:10.929: INFO: node-local-dns-8z787 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.929: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 11:51:10.929: INFO: user-ssh-keys-agent-fvhdn from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.929: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 27 11:51:10.929: INFO: dashboard-metrics-scraper-85f6dd84d5-2qtjs from kubernetes-dashboard started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.929: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 27 11:51:10.929: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-94vkz from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 11:51:10.929: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 11:51:10.929: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 11:51:10.929: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-7-167.eu-central-1.compute.internal before test
Feb 27 11:51:10.945: INFO: canal-mbg4r from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
Feb 27 11:51:10.945: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 27 11:51:10.945: INFO: ebs-csi-controller-54c5c66b84-mkj54 from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
Feb 27 11:51:10.945: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container csi-resizer ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 11:51:10.945: INFO: ebs-csi-controller-54c5c66b84-rxhdh from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
Feb 27 11:51:10.945: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container csi-resizer ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 11:51:10.945: INFO: ebs-csi-node-7dvrp from kube-system started at 2023-02-27 09:22:08 +0000 UTC (3 container statuses recorded)
Feb 27 11:51:10.945: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 11:51:10.945: INFO: envoy-agent-scd88 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
Feb 27 11:51:10.945: INFO: 	Container assign-address ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container envoy-agent ready: true, restart count 0
Feb 27 11:51:10.945: INFO: kube-proxy-ghd44 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.945: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 27 11:51:10.945: INFO: node-local-dns-kfv2k from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.945: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 11:51:10.945: INFO: user-ssh-keys-agent-gjs99 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
Feb 27 11:51:10.945: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 27 11:51:10.945: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-k5l74 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 11:51:10.945: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 11:51:10.945: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 11:51:10.945
Feb 27 11:51:10.964: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5546" to be "running"
Feb 27 11:51:10.973: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.780027ms
Feb 27 11:51:12.981: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.017225031s
Feb 27 11:51:12.981: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 11:51:12.988
STEP: Trying to apply a random label on the found node. 02/27/23 11:51:13.055
STEP: verifying the node has the label kubernetes.io/e2e-3839a85f-5e9f-4d50-960c-581c73e89bc6 95 02/27/23 11:51:13.108
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/27/23 11:51:13.118
Feb 27 11:51:13.154: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5546" to be "not pending"
Feb 27 11:51:13.182: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.089688ms
Feb 27 11:51:15.191: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.037702536s
Feb 27 11:51:15.191: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.11.159 on the node which pod4 resides and expect not scheduled 02/27/23 11:51:15.191
Feb 27 11:51:15.203: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5546" to be "not pending"
Feb 27 11:51:15.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.685634ms
Feb 27 11:51:17.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026164748s
Feb 27 11:51:19.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029926401s
Feb 27 11:51:21.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025883633s
Feb 27 11:51:23.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032498519s
Feb 27 11:51:25.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0260268s
Feb 27 11:51:27.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.025299186s
Feb 27 11:51:29.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026913122s
Feb 27 11:51:31.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.035259405s
Feb 27 11:51:33.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.026103188s
Feb 27 11:51:35.227: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.024748674s
Feb 27 11:51:37.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.028455638s
Feb 27 11:51:39.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.029688033s
Feb 27 11:51:41.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.029109539s
Feb 27 11:51:43.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.028092994s
Feb 27 11:51:45.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.027553108s
Feb 27 11:51:47.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.027315038s
Feb 27 11:51:49.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.025614929s
Feb 27 11:51:51.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.033630861s
Feb 27 11:51:53.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.02662362s
Feb 27 11:51:55.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025936785s
Feb 27 11:51:57.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.026413454s
Feb 27 11:51:59.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025511515s
Feb 27 11:52:01.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.028919377s
Feb 27 11:52:03.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.029868588s
Feb 27 11:52:05.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.026149176s
Feb 27 11:52:07.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.027501851s
Feb 27 11:52:09.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.028127323s
Feb 27 11:52:11.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.031927671s
Feb 27 11:52:13.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.027792883s
Feb 27 11:52:15.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.028166047s
Feb 27 11:52:17.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.025837746s
Feb 27 11:52:19.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.027679896s
Feb 27 11:52:21.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.026048522s
Feb 27 11:52:23.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.028678197s
Feb 27 11:52:25.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.026077218s
Feb 27 11:52:27.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.026633492s
Feb 27 11:52:29.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.026361394s
Feb 27 11:52:31.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.02985162s
Feb 27 11:52:33.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.025877405s
Feb 27 11:52:35.242: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.039261774s
Feb 27 11:52:37.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.026056391s
Feb 27 11:52:39.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.02812712s
Feb 27 11:52:41.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.026079344s
Feb 27 11:52:43.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.033487404s
Feb 27 11:52:45.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.029674622s
Feb 27 11:52:47.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.025892748s
Feb 27 11:52:49.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.025786549s
Feb 27 11:52:51.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.025315561s
Feb 27 11:52:53.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.027068965s
Feb 27 11:52:55.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.027664417s
Feb 27 11:52:57.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.029509305s
Feb 27 11:52:59.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.026852235s
Feb 27 11:53:01.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.032778538s
Feb 27 11:53:03.243: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.04027392s
Feb 27 11:53:05.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.026120931s
Feb 27 11:53:07.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.026714381s
Feb 27 11:53:09.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.026340219s
Feb 27 11:53:11.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.030199028s
Feb 27 11:53:13.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.028042422s
Feb 27 11:53:15.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.028364467s
Feb 27 11:53:17.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.029409746s
Feb 27 11:53:19.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.02764591s
Feb 27 11:53:21.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.025433703s
Feb 27 11:53:23.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.027581793s
Feb 27 11:53:25.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.026092298s
Feb 27 11:53:27.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.027554276s
Feb 27 11:53:29.242: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.039037354s
Feb 27 11:53:31.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.037655729s
Feb 27 11:53:33.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.025288611s
Feb 27 11:53:35.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.028565116s
Feb 27 11:53:37.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.027240573s
Feb 27 11:53:39.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.026640422s
Feb 27 11:53:41.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.045405153s
Feb 27 11:53:43.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.029679561s
Feb 27 11:53:45.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.027747978s
Feb 27 11:53:47.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.027108419s
Feb 27 11:53:49.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.026481213s
Feb 27 11:53:51.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.035464873s
Feb 27 11:53:53.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.031920176s
Feb 27 11:53:55.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.025699697s
Feb 27 11:53:57.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.02757232s
Feb 27 11:53:59.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.028478094s
Feb 27 11:54:01.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.035650535s
Feb 27 11:54:03.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.032513513s
Feb 27 11:54:05.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.025827139s
Feb 27 11:54:07.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.027011715s
Feb 27 11:54:09.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.037536047s
Feb 27 11:54:11.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.028803792s
Feb 27 11:54:13.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.026861394s
Feb 27 11:54:15.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.027706023s
Feb 27 11:54:17.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.026816325s
Feb 27 11:54:19.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.027620384s
Feb 27 11:54:21.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.025480072s
Feb 27 11:54:23.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.027935947s
Feb 27 11:54:25.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.028257794s
Feb 27 11:54:27.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.029891955s
Feb 27 11:54:29.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.02773864s
Feb 27 11:54:31.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.02750583s
Feb 27 11:54:33.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.032789383s
Feb 27 11:54:35.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.026586738s
Feb 27 11:54:37.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.027751005s
Feb 27 11:54:39.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.032009909s
Feb 27 11:54:41.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.028295454s
Feb 27 11:54:43.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.027786435s
Feb 27 11:54:45.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.030833168s
Feb 27 11:54:47.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.028473188s
Feb 27 11:54:49.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.02757219s
Feb 27 11:54:51.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.03567617s
Feb 27 11:54:53.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.029124383s
Feb 27 11:54:55.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.026720501s
Feb 27 11:54:57.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.029802715s
Feb 27 11:54:59.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.025803415s
Feb 27 11:55:01.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.028528637s
Feb 27 11:55:03.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.032273695s
Feb 27 11:55:05.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.030281068s
Feb 27 11:55:07.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.028796604s
Feb 27 11:55:09.323: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.120311254s
Feb 27 11:55:11.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.027385105s
Feb 27 11:55:13.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.032493847s
Feb 27 11:55:15.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.029491918s
Feb 27 11:55:17.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.026798206s
Feb 27 11:55:19.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.028110268s
Feb 27 11:55:21.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.027287223s
Feb 27 11:55:23.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.03126417s
Feb 27 11:55:25.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.030806036s
Feb 27 11:55:27.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.027145449s
Feb 27 11:55:29.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.037074485s
Feb 27 11:55:31.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.026982508s
Feb 27 11:55:33.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.031767743s
Feb 27 11:55:35.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.026708688s
Feb 27 11:55:37.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.027923089s
Feb 27 11:55:39.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.028258727s
Feb 27 11:55:41.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.026344709s
Feb 27 11:55:43.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.037551533s
Feb 27 11:55:45.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.026331901s
Feb 27 11:55:47.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.029641711s
Feb 27 11:55:49.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.028092699s
Feb 27 11:55:51.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.025378895s
Feb 27 11:55:53.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.029119886s
Feb 27 11:55:55.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.025948297s
Feb 27 11:55:57.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.027502563s
Feb 27 11:55:59.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.032761194s
Feb 27 11:56:01.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.028547924s
Feb 27 11:56:03.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.028682094s
Feb 27 11:56:05.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.025692558s
Feb 27 11:56:07.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.045852044s
Feb 27 11:56:09.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.029704084s
Feb 27 11:56:11.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.034210747s
Feb 27 11:56:13.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.02852253s
Feb 27 11:56:15.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.026075816s
Feb 27 11:56:15.241: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.038028764s
STEP: removing the label kubernetes.io/e2e-3839a85f-5e9f-4d50-960c-581c73e89bc6 off the node ip-172-31-11-159.eu-central-1.compute.internal 02/27/23 11:56:15.241
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3839a85f-5e9f-4d50-960c-581c73e89bc6 02/27/23 11:56:15.268
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 27 11:56:15.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5546" for this suite. 02/27/23 11:56:15.3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":137,"skipped":2828,"failed":0}
------------------------------
• [SLOW TEST] [304.643 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:51:10.679
    Feb 27 11:51:10.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename sched-pred 02/27/23 11:51:10.689
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:51:10.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:51:10.757
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 27 11:51:10.796: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 27 11:51:10.845: INFO: Waiting for terminating namespaces to be deleted...
    Feb 27 11:51:10.865: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-11-159.eu-central-1.compute.internal before test
    Feb 27 11:51:10.895: INFO: canal-4q9m8 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
    Feb 27 11:51:10.896: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 11:51:10.899: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 27 11:51:10.900: INFO: coredns-bf8668b4f-nbc9z from kube-system started at 2023-02-27 11:28:02 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.900: INFO: 	Container coredns ready: true, restart count 0
    Feb 27 11:51:10.901: INFO: ebs-csi-node-b6z5h from kube-system started at 2023-02-27 09:22:11 +0000 UTC (3 container statuses recorded)
    Feb 27 11:51:10.903: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 11:51:10.904: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 11:51:10.905: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 11:51:10.905: INFO: envoy-agent-2wwht from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
    Feb 27 11:51:10.905: INFO: 	Container assign-address ready: true, restart count 0
    Feb 27 11:51:10.905: INFO: 	Container envoy-agent ready: true, restart count 0
    Feb 27 11:51:10.905: INFO: konnectivity-agent-76c848fdd6-fr8fm from kube-system started at 2023-02-27 11:28:02 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.905: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Feb 27 11:51:10.905: INFO: kube-proxy-cz9zt from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.905: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 27 11:51:10.905: INFO: metrics-server-5f7c5d4b9-qh6s5 from kube-system started at 2023-02-27 11:28:02 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.906: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 11:51:10.906: INFO: node-local-dns-r6r4k from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.906: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 11:51:10.906: INFO: user-ssh-keys-agent-rk5t9 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.906: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Feb 27 11:51:10.906: INFO: bin-false6bda9908-2f95-48ed-afb3-16dcf0b54b54 from kubelet-test-5184 started at 2023-02-27 11:49:25 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.906: INFO: 	Container bin-false6bda9908-2f95-48ed-afb3-16dcf0b54b54 ready: false, restart count 0
    Feb 27 11:51:10.906: INFO: dashboard-metrics-scraper-85f6dd84d5-c7wsq from kubernetes-dashboard started at 2023-02-27 11:08:24 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.906: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Feb 27 11:51:10.906: INFO: execpodvh9w2 from services-3251 started at 2023-02-27 11:51:05 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.907: INFO: 	Container agnhost-container ready: true, restart count 0
    Feb 27 11:51:10.907: INFO: sonobuoy from sonobuoy started at 2023-02-27 11:19:26 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.907: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 27 11:51:10.907: INFO: sonobuoy-e2e-job-18131847dfcd49d5 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 11:51:10.907: INFO: 	Container e2e ready: true, restart count 0
    Feb 27 11:51:10.908: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 11:51:10.908: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-rlhml from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 11:51:10.908: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 11:51:10.908: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 11:51:10.908: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-15-17.eu-central-1.compute.internal before test
    Feb 27 11:51:10.928: INFO: calico-kube-controllers-55d99d998f-f5ngs from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.928: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 27 11:51:10.928: INFO: canal-bxq4m from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
    Feb 27 11:51:10.928: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 11:51:10.928: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 27 11:51:10.928: INFO: coredns-bf8668b4f-5h5v9 from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.928: INFO: 	Container coredns ready: true, restart count 0
    Feb 27 11:51:10.928: INFO: ebs-csi-node-z9l5x from kube-system started at 2023-02-27 09:22:52 +0000 UTC (3 container statuses recorded)
    Feb 27 11:51:10.928: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 11:51:10.928: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 11:51:10.928: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 11:51:10.928: INFO: envoy-agent-7xhs5 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
    Feb 27 11:51:10.929: INFO: 	Container assign-address ready: true, restart count 0
    Feb 27 11:51:10.929: INFO: 	Container envoy-agent ready: true, restart count 0
    Feb 27 11:51:10.929: INFO: konnectivity-agent-76c848fdd6-56hgq from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.929: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Feb 27 11:51:10.929: INFO: kube-proxy-hxmbw from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.929: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 27 11:51:10.929: INFO: metrics-server-5f7c5d4b9-xlr2c from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.929: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 11:51:10.929: INFO: node-local-dns-8z787 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.929: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 11:51:10.929: INFO: user-ssh-keys-agent-fvhdn from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.929: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Feb 27 11:51:10.929: INFO: dashboard-metrics-scraper-85f6dd84d5-2qtjs from kubernetes-dashboard started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.929: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Feb 27 11:51:10.929: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-94vkz from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 11:51:10.929: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 11:51:10.929: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 11:51:10.929: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-7-167.eu-central-1.compute.internal before test
    Feb 27 11:51:10.945: INFO: canal-mbg4r from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
    Feb 27 11:51:10.945: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: ebs-csi-controller-54c5c66b84-mkj54 from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
    Feb 27 11:51:10.945: INFO: 	Container csi-attacher ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container csi-provisioner ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container csi-resizer ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: ebs-csi-controller-54c5c66b84-rxhdh from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
    Feb 27 11:51:10.945: INFO: 	Container csi-attacher ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container csi-provisioner ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container csi-resizer ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: ebs-csi-node-7dvrp from kube-system started at 2023-02-27 09:22:08 +0000 UTC (3 container statuses recorded)
    Feb 27 11:51:10.945: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: envoy-agent-scd88 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
    Feb 27 11:51:10.945: INFO: 	Container assign-address ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container envoy-agent ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: kube-proxy-ghd44 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.945: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: node-local-dns-kfv2k from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.945: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: user-ssh-keys-agent-gjs99 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
    Feb 27 11:51:10.945: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-k5l74 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 11:51:10.945: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 11:51:10.945: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 11:51:10.945
    Feb 27 11:51:10.964: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5546" to be "running"
    Feb 27 11:51:10.973: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.780027ms
    Feb 27 11:51:12.981: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.017225031s
    Feb 27 11:51:12.981: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 11:51:12.988
    STEP: Trying to apply a random label on the found node. 02/27/23 11:51:13.055
    STEP: verifying the node has the label kubernetes.io/e2e-3839a85f-5e9f-4d50-960c-581c73e89bc6 95 02/27/23 11:51:13.108
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/27/23 11:51:13.118
    Feb 27 11:51:13.154: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5546" to be "not pending"
    Feb 27 11:51:13.182: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.089688ms
    Feb 27 11:51:15.191: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.037702536s
    Feb 27 11:51:15.191: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.11.159 on the node which pod4 resides and expect not scheduled 02/27/23 11:51:15.191
    Feb 27 11:51:15.203: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5546" to be "not pending"
    Feb 27 11:51:15.220: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.685634ms
    Feb 27 11:51:17.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026164748s
    Feb 27 11:51:19.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029926401s
    Feb 27 11:51:21.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025883633s
    Feb 27 11:51:23.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032498519s
    Feb 27 11:51:25.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0260268s
    Feb 27 11:51:27.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.025299186s
    Feb 27 11:51:29.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026913122s
    Feb 27 11:51:31.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.035259405s
    Feb 27 11:51:33.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.026103188s
    Feb 27 11:51:35.227: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.024748674s
    Feb 27 11:51:37.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.028455638s
    Feb 27 11:51:39.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.029688033s
    Feb 27 11:51:41.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.029109539s
    Feb 27 11:51:43.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.028092994s
    Feb 27 11:51:45.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.027553108s
    Feb 27 11:51:47.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.027315038s
    Feb 27 11:51:49.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.025614929s
    Feb 27 11:51:51.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.033630861s
    Feb 27 11:51:53.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.02662362s
    Feb 27 11:51:55.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025936785s
    Feb 27 11:51:57.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.026413454s
    Feb 27 11:51:59.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025511515s
    Feb 27 11:52:01.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.028919377s
    Feb 27 11:52:03.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.029868588s
    Feb 27 11:52:05.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.026149176s
    Feb 27 11:52:07.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.027501851s
    Feb 27 11:52:09.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.028127323s
    Feb 27 11:52:11.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.031927671s
    Feb 27 11:52:13.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.027792883s
    Feb 27 11:52:15.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.028166047s
    Feb 27 11:52:17.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.025837746s
    Feb 27 11:52:19.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.027679896s
    Feb 27 11:52:21.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.026048522s
    Feb 27 11:52:23.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.028678197s
    Feb 27 11:52:25.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.026077218s
    Feb 27 11:52:27.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.026633492s
    Feb 27 11:52:29.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.026361394s
    Feb 27 11:52:31.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.02985162s
    Feb 27 11:52:33.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.025877405s
    Feb 27 11:52:35.242: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.039261774s
    Feb 27 11:52:37.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.026056391s
    Feb 27 11:52:39.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.02812712s
    Feb 27 11:52:41.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.026079344s
    Feb 27 11:52:43.236: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.033487404s
    Feb 27 11:52:45.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.029674622s
    Feb 27 11:52:47.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.025892748s
    Feb 27 11:52:49.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.025786549s
    Feb 27 11:52:51.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.025315561s
    Feb 27 11:52:53.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.027068965s
    Feb 27 11:52:55.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.027664417s
    Feb 27 11:52:57.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.029509305s
    Feb 27 11:52:59.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.026852235s
    Feb 27 11:53:01.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.032778538s
    Feb 27 11:53:03.243: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.04027392s
    Feb 27 11:53:05.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.026120931s
    Feb 27 11:53:07.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.026714381s
    Feb 27 11:53:09.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.026340219s
    Feb 27 11:53:11.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.030199028s
    Feb 27 11:53:13.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.028042422s
    Feb 27 11:53:15.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.028364467s
    Feb 27 11:53:17.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.029409746s
    Feb 27 11:53:19.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.02764591s
    Feb 27 11:53:21.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.025433703s
    Feb 27 11:53:23.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.027581793s
    Feb 27 11:53:25.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.026092298s
    Feb 27 11:53:27.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.027554276s
    Feb 27 11:53:29.242: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.039037354s
    Feb 27 11:53:31.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.037655729s
    Feb 27 11:53:33.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.025288611s
    Feb 27 11:53:35.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.028565116s
    Feb 27 11:53:37.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.027240573s
    Feb 27 11:53:39.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.026640422s
    Feb 27 11:53:41.248: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.045405153s
    Feb 27 11:53:43.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.029679561s
    Feb 27 11:53:45.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.027747978s
    Feb 27 11:53:47.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.027108419s
    Feb 27 11:53:49.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.026481213s
    Feb 27 11:53:51.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.035464873s
    Feb 27 11:53:53.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.031920176s
    Feb 27 11:53:55.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.025699697s
    Feb 27 11:53:57.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.02757232s
    Feb 27 11:53:59.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.028478094s
    Feb 27 11:54:01.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.035650535s
    Feb 27 11:54:03.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.032513513s
    Feb 27 11:54:05.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.025827139s
    Feb 27 11:54:07.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.027011715s
    Feb 27 11:54:09.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.037536047s
    Feb 27 11:54:11.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.028803792s
    Feb 27 11:54:13.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.026861394s
    Feb 27 11:54:15.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.027706023s
    Feb 27 11:54:17.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.026816325s
    Feb 27 11:54:19.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.027620384s
    Feb 27 11:54:21.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.025480072s
    Feb 27 11:54:23.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.027935947s
    Feb 27 11:54:25.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.028257794s
    Feb 27 11:54:27.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.029891955s
    Feb 27 11:54:29.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.02773864s
    Feb 27 11:54:31.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.02750583s
    Feb 27 11:54:33.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.032789383s
    Feb 27 11:54:35.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.026586738s
    Feb 27 11:54:37.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.027751005s
    Feb 27 11:54:39.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.032009909s
    Feb 27 11:54:41.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.028295454s
    Feb 27 11:54:43.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.027786435s
    Feb 27 11:54:45.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.030833168s
    Feb 27 11:54:47.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.028473188s
    Feb 27 11:54:49.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.02757219s
    Feb 27 11:54:51.238: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.03567617s
    Feb 27 11:54:53.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.029124383s
    Feb 27 11:54:55.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.026720501s
    Feb 27 11:54:57.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.029802715s
    Feb 27 11:54:59.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.025803415s
    Feb 27 11:55:01.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.028528637s
    Feb 27 11:55:03.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.032273695s
    Feb 27 11:55:05.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.030281068s
    Feb 27 11:55:07.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.028796604s
    Feb 27 11:55:09.323: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.120311254s
    Feb 27 11:55:11.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.027385105s
    Feb 27 11:55:13.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.032493847s
    Feb 27 11:55:15.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.029491918s
    Feb 27 11:55:17.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.026798206s
    Feb 27 11:55:19.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.028110268s
    Feb 27 11:55:21.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.027287223s
    Feb 27 11:55:23.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.03126417s
    Feb 27 11:55:25.233: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.030806036s
    Feb 27 11:55:27.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.027145449s
    Feb 27 11:55:29.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.037074485s
    Feb 27 11:55:31.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.026982508s
    Feb 27 11:55:33.234: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.031767743s
    Feb 27 11:55:35.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.026708688s
    Feb 27 11:55:37.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.027923089s
    Feb 27 11:55:39.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.028258727s
    Feb 27 11:55:41.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.026344709s
    Feb 27 11:55:43.240: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.037551533s
    Feb 27 11:55:45.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.026331901s
    Feb 27 11:55:47.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.029641711s
    Feb 27 11:55:49.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.028092699s
    Feb 27 11:55:51.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.025378895s
    Feb 27 11:55:53.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.029119886s
    Feb 27 11:55:55.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.025948297s
    Feb 27 11:55:57.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.027502563s
    Feb 27 11:55:59.235: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.032761194s
    Feb 27 11:56:01.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.028547924s
    Feb 27 11:56:03.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.028682094s
    Feb 27 11:56:05.228: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.025692558s
    Feb 27 11:56:07.249: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.045852044s
    Feb 27 11:56:09.232: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.029704084s
    Feb 27 11:56:11.237: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.034210747s
    Feb 27 11:56:13.231: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.02852253s
    Feb 27 11:56:15.229: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.026075816s
    Feb 27 11:56:15.241: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.038028764s
    STEP: removing the label kubernetes.io/e2e-3839a85f-5e9f-4d50-960c-581c73e89bc6 off the node ip-172-31-11-159.eu-central-1.compute.internal 02/27/23 11:56:15.241
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-3839a85f-5e9f-4d50-960c-581c73e89bc6 02/27/23 11:56:15.268
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 11:56:15.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5546" for this suite. 02/27/23 11:56:15.3
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:56:15.332
Feb 27 11:56:15.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 11:56:15.333
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:56:15.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:56:15.421
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-27c8f4f4-82c5-4b5c-9dc1-ad98af20f872 02/27/23 11:56:15.435
STEP: Creating a pod to test consume secrets 02/27/23 11:56:15.445
Feb 27 11:56:15.461: INFO: Waiting up to 5m0s for pod "pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999" in namespace "secrets-2638" to be "Succeeded or Failed"
Feb 27 11:56:15.472: INFO: Pod "pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999": Phase="Pending", Reason="", readiness=false. Elapsed: 11.738907ms
Feb 27 11:56:17.482: INFO: Pod "pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999": Phase="Running", Reason="", readiness=true. Elapsed: 2.021630163s
Feb 27 11:56:19.481: INFO: Pod "pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999": Phase="Running", Reason="", readiness=false. Elapsed: 4.020689534s
Feb 27 11:56:21.481: INFO: Pod "pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020473128s
STEP: Saw pod success 02/27/23 11:56:21.481
Feb 27 11:56:21.481: INFO: Pod "pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999" satisfied condition "Succeeded or Failed"
Feb 27 11:56:21.487: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 11:56:21.504
Feb 27 11:56:21.522: INFO: Waiting for pod pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999 to disappear
Feb 27 11:56:21.530: INFO: Pod pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 27 11:56:21.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2638" for this suite. 02/27/23 11:56:21.539
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":138,"skipped":2874,"failed":0}
------------------------------
• [SLOW TEST] [6.243 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:56:15.332
    Feb 27 11:56:15.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 11:56:15.333
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:56:15.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:56:15.421
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-27c8f4f4-82c5-4b5c-9dc1-ad98af20f872 02/27/23 11:56:15.435
    STEP: Creating a pod to test consume secrets 02/27/23 11:56:15.445
    Feb 27 11:56:15.461: INFO: Waiting up to 5m0s for pod "pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999" in namespace "secrets-2638" to be "Succeeded or Failed"
    Feb 27 11:56:15.472: INFO: Pod "pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999": Phase="Pending", Reason="", readiness=false. Elapsed: 11.738907ms
    Feb 27 11:56:17.482: INFO: Pod "pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999": Phase="Running", Reason="", readiness=true. Elapsed: 2.021630163s
    Feb 27 11:56:19.481: INFO: Pod "pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999": Phase="Running", Reason="", readiness=false. Elapsed: 4.020689534s
    Feb 27 11:56:21.481: INFO: Pod "pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020473128s
    STEP: Saw pod success 02/27/23 11:56:21.481
    Feb 27 11:56:21.481: INFO: Pod "pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999" satisfied condition "Succeeded or Failed"
    Feb 27 11:56:21.487: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 11:56:21.504
    Feb 27 11:56:21.522: INFO: Waiting for pod pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999 to disappear
    Feb 27 11:56:21.530: INFO: Pod pod-secrets-fb0411e3-864c-49af-adca-8c5ddee5a999 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 11:56:21.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2638" for this suite. 02/27/23 11:56:21.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:56:21.58
Feb 27 11:56:21.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 11:56:21.581
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:56:21.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:56:21.617
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-734 02/27/23 11:56:21.627
Feb 27 11:56:21.652: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-734" to be "running and ready"
Feb 27 11:56:21.666: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 13.848115ms
Feb 27 11:56:21.666: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 27 11:56:23.676: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.023845531s
Feb 27 11:56:23.676: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 27 11:56:23.676: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Feb 27 11:56:23.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 27 11:56:25.017: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 27 11:56:25.017: INFO: stdout: "ipvs"
Feb 27 11:56:25.017: INFO: proxyMode: ipvs
Feb 27 11:56:25.060: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 27 11:56:25.068: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-734 02/27/23 11:56:25.068
STEP: creating replication controller affinity-nodeport-timeout in namespace services-734 02/27/23 11:56:25.17
I0227 11:56:25.226430      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-734, replica count: 3
I0227 11:56:28.277167      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 11:56:28.300: INFO: Creating new exec pod
Feb 27 11:56:28.314: INFO: Waiting up to 5m0s for pod "execpod-affinitycpbqz" in namespace "services-734" to be "running"
Feb 27 11:56:28.320: INFO: Pod "execpod-affinitycpbqz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.991734ms
Feb 27 11:56:30.329: INFO: Pod "execpod-affinitycpbqz": Phase="Running", Reason="", readiness=true. Elapsed: 2.015404957s
Feb 27 11:56:30.329: INFO: Pod "execpod-affinitycpbqz" satisfied condition "running"
Feb 27 11:56:31.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Feb 27 11:56:31.809: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Feb 27 11:56:31.809: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:56:31.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.218 80'
Feb 27 11:56:32.029: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.20.218 80\nConnection to 10.240.20.218 80 port [tcp/http] succeeded!\n"
Feb 27 11:56:32.029: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:56:32.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.17 31487'
Feb 27 11:56:32.226: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.17 31487\nConnection to 172.31.15.17 31487 port [tcp/*] succeeded!\n"
Feb 27 11:56:32.226: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:56:32.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.167 31487'
Feb 27 11:56:32.471: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.7.167 31487\nConnection to 172.31.7.167 31487 port [tcp/*] succeeded!\n"
Feb 27 11:56:32.471: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 11:56:32.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.159:31487/ ; done'
Feb 27 11:56:33.428: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n"
Feb 27 11:56:33.428: INFO: stdout: "\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5"
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
Feb 27 11:56:33.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.11.159:31487/'
Feb 27 11:56:33.651: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n"
Feb 27 11:56:33.651: INFO: stdout: "affinity-nodeport-timeout-vhgt5"
Feb 27 11:58:43.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.11.159:31487/'
Feb 27 11:58:43.903: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n"
Feb 27 11:58:43.903: INFO: stdout: "affinity-nodeport-timeout-srk5n"
Feb 27 11:58:43.903: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-734, will wait for the garbage collector to delete the pods 02/27/23 11:58:43.926
Feb 27 11:58:44.001: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 12.11424ms
Feb 27 11:58:44.101: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.530913ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 11:58:46.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-734" for this suite. 02/27/23 11:58:46.441
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":139,"skipped":2886,"failed":0}
------------------------------
• [SLOW TEST] [144.877 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:56:21.58
    Feb 27 11:56:21.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 11:56:21.581
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:56:21.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:56:21.617
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-734 02/27/23 11:56:21.627
    Feb 27 11:56:21.652: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-734" to be "running and ready"
    Feb 27 11:56:21.666: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 13.848115ms
    Feb 27 11:56:21.666: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 11:56:23.676: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.023845531s
    Feb 27 11:56:23.676: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Feb 27 11:56:23.676: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Feb 27 11:56:23.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Feb 27 11:56:25.017: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Feb 27 11:56:25.017: INFO: stdout: "ipvs"
    Feb 27 11:56:25.017: INFO: proxyMode: ipvs
    Feb 27 11:56:25.060: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Feb 27 11:56:25.068: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-734 02/27/23 11:56:25.068
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-734 02/27/23 11:56:25.17
    I0227 11:56:25.226430      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-734, replica count: 3
    I0227 11:56:28.277167      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 11:56:28.300: INFO: Creating new exec pod
    Feb 27 11:56:28.314: INFO: Waiting up to 5m0s for pod "execpod-affinitycpbqz" in namespace "services-734" to be "running"
    Feb 27 11:56:28.320: INFO: Pod "execpod-affinitycpbqz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.991734ms
    Feb 27 11:56:30.329: INFO: Pod "execpod-affinitycpbqz": Phase="Running", Reason="", readiness=true. Elapsed: 2.015404957s
    Feb 27 11:56:30.329: INFO: Pod "execpod-affinitycpbqz" satisfied condition "running"
    Feb 27 11:56:31.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Feb 27 11:56:31.809: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Feb 27 11:56:31.809: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:56:31.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.20.218 80'
    Feb 27 11:56:32.029: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.20.218 80\nConnection to 10.240.20.218 80 port [tcp/http] succeeded!\n"
    Feb 27 11:56:32.029: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:56:32.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.17 31487'
    Feb 27 11:56:32.226: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.17 31487\nConnection to 172.31.15.17 31487 port [tcp/*] succeeded!\n"
    Feb 27 11:56:32.226: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:56:32.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.167 31487'
    Feb 27 11:56:32.471: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.7.167 31487\nConnection to 172.31.7.167 31487 port [tcp/*] succeeded!\n"
    Feb 27 11:56:32.471: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 11:56:32.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.159:31487/ ; done'
    Feb 27 11:56:33.428: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n"
    Feb 27 11:56:33.428: INFO: stdout: "\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5\naffinity-nodeport-timeout-vhgt5"
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Received response from host: affinity-nodeport-timeout-vhgt5
    Feb 27 11:56:33.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.11.159:31487/'
    Feb 27 11:56:33.651: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n"
    Feb 27 11:56:33.651: INFO: stdout: "affinity-nodeport-timeout-vhgt5"
    Feb 27 11:58:43.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-734 exec execpod-affinitycpbqz -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.11.159:31487/'
    Feb 27 11:58:43.903: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.11.159:31487/\n"
    Feb 27 11:58:43.903: INFO: stdout: "affinity-nodeport-timeout-srk5n"
    Feb 27 11:58:43.903: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-734, will wait for the garbage collector to delete the pods 02/27/23 11:58:43.926
    Feb 27 11:58:44.001: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 12.11424ms
    Feb 27 11:58:44.101: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.530913ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 11:58:46.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-734" for this suite. 02/27/23 11:58:46.441
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:58:46.464
Feb 27 11:58:46.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 11:58:46.465
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:58:46.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:58:46.512
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 11:58:46.556
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:58:46.962
STEP: Deploying the webhook pod 02/27/23 11:58:46.975
STEP: Wait for the deployment to be ready 02/27/23 11:58:46.998
Feb 27 11:58:47.022: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 11:58:49.065
STEP: Verifying the service has paired with the endpoint 02/27/23 11:58:49.082
Feb 27 11:58:50.083: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 02/27/23 11:58:50.094
STEP: create a pod that should be denied by the webhook 02/27/23 11:58:50.177
STEP: create a pod that causes the webhook to hang 02/27/23 11:58:50.207
STEP: create a configmap that should be denied by the webhook 02/27/23 11:59:00.234
STEP: create a configmap that should be admitted by the webhook 02/27/23 11:59:00.289
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/27/23 11:59:00.326
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/27/23 11:59:00.348
STEP: create a namespace that bypass the webhook 02/27/23 11:59:00.365
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/27/23 11:59:00.384
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 11:59:00.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9399" for this suite. 02/27/23 11:59:00.449
STEP: Destroying namespace "webhook-9399-markers" for this suite. 02/27/23 11:59:00.465
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":140,"skipped":2926,"failed":0}
------------------------------
• [SLOW TEST] [14.141 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:58:46.464
    Feb 27 11:58:46.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 11:58:46.465
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:58:46.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:58:46.512
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 11:58:46.556
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 11:58:46.962
    STEP: Deploying the webhook pod 02/27/23 11:58:46.975
    STEP: Wait for the deployment to be ready 02/27/23 11:58:46.998
    Feb 27 11:58:47.022: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 11:58:49.065
    STEP: Verifying the service has paired with the endpoint 02/27/23 11:58:49.082
    Feb 27 11:58:50.083: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 02/27/23 11:58:50.094
    STEP: create a pod that should be denied by the webhook 02/27/23 11:58:50.177
    STEP: create a pod that causes the webhook to hang 02/27/23 11:58:50.207
    STEP: create a configmap that should be denied by the webhook 02/27/23 11:59:00.234
    STEP: create a configmap that should be admitted by the webhook 02/27/23 11:59:00.289
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/27/23 11:59:00.326
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/27/23 11:59:00.348
    STEP: create a namespace that bypass the webhook 02/27/23 11:59:00.365
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/27/23 11:59:00.384
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 11:59:00.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9399" for this suite. 02/27/23 11:59:00.449
    STEP: Destroying namespace "webhook-9399-markers" for this suite. 02/27/23 11:59:00.465
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:59:00.643
Feb 27 11:59:00.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 11:59:00.644
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:59:00.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:59:00.709
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/27/23 11:59:00.725
Feb 27 11:59:00.748: INFO: Waiting up to 5m0s for pod "pod-de875782-c5b2-4972-8b63-3d43cde27b47" in namespace "emptydir-6336" to be "Succeeded or Failed"
Feb 27 11:59:00.770: INFO: Pod "pod-de875782-c5b2-4972-8b63-3d43cde27b47": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00997ms
Feb 27 11:59:02.783: INFO: Pod "pod-de875782-c5b2-4972-8b63-3d43cde27b47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03521868s
Feb 27 11:59:04.786: INFO: Pod "pod-de875782-c5b2-4972-8b63-3d43cde27b47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038341233s
STEP: Saw pod success 02/27/23 11:59:04.787
Feb 27 11:59:04.787: INFO: Pod "pod-de875782-c5b2-4972-8b63-3d43cde27b47" satisfied condition "Succeeded or Failed"
Feb 27 11:59:04.800: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-de875782-c5b2-4972-8b63-3d43cde27b47 container test-container: <nil>
STEP: delete the pod 02/27/23 11:59:04.819
Feb 27 11:59:04.854: INFO: Waiting for pod pod-de875782-c5b2-4972-8b63-3d43cde27b47 to disappear
Feb 27 11:59:04.865: INFO: Pod pod-de875782-c5b2-4972-8b63-3d43cde27b47 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 11:59:04.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6336" for this suite. 02/27/23 11:59:04.876
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":141,"skipped":2985,"failed":0}
------------------------------
• [4.247 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:59:00.643
    Feb 27 11:59:00.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 11:59:00.644
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:59:00.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:59:00.709
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/27/23 11:59:00.725
    Feb 27 11:59:00.748: INFO: Waiting up to 5m0s for pod "pod-de875782-c5b2-4972-8b63-3d43cde27b47" in namespace "emptydir-6336" to be "Succeeded or Failed"
    Feb 27 11:59:00.770: INFO: Pod "pod-de875782-c5b2-4972-8b63-3d43cde27b47": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00997ms
    Feb 27 11:59:02.783: INFO: Pod "pod-de875782-c5b2-4972-8b63-3d43cde27b47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03521868s
    Feb 27 11:59:04.786: INFO: Pod "pod-de875782-c5b2-4972-8b63-3d43cde27b47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038341233s
    STEP: Saw pod success 02/27/23 11:59:04.787
    Feb 27 11:59:04.787: INFO: Pod "pod-de875782-c5b2-4972-8b63-3d43cde27b47" satisfied condition "Succeeded or Failed"
    Feb 27 11:59:04.800: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-de875782-c5b2-4972-8b63-3d43cde27b47 container test-container: <nil>
    STEP: delete the pod 02/27/23 11:59:04.819
    Feb 27 11:59:04.854: INFO: Waiting for pod pod-de875782-c5b2-4972-8b63-3d43cde27b47 to disappear
    Feb 27 11:59:04.865: INFO: Pod pod-de875782-c5b2-4972-8b63-3d43cde27b47 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 11:59:04.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6336" for this suite. 02/27/23 11:59:04.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:59:04.891
Feb 27 11:59:04.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename namespaces 02/27/23 11:59:04.893
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:59:04.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:59:04.944
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 02/27/23 11:59:04.952
STEP: patching the Namespace 02/27/23 11:59:04.986
STEP: get the Namespace and ensuring it has the label 02/27/23 11:59:05.007
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 27 11:59:05.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9385" for this suite. 02/27/23 11:59:05.037
STEP: Destroying namespace "nspatchtest-d48ffd22-d780-4f5d-9adc-8f53003c8165-270" for this suite. 02/27/23 11:59:05.065
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":142,"skipped":2995,"failed":0}
------------------------------
• [0.190 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:59:04.891
    Feb 27 11:59:04.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename namespaces 02/27/23 11:59:04.893
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:59:04.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:59:04.944
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 02/27/23 11:59:04.952
    STEP: patching the Namespace 02/27/23 11:59:04.986
    STEP: get the Namespace and ensuring it has the label 02/27/23 11:59:05.007
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 11:59:05.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9385" for this suite. 02/27/23 11:59:05.037
    STEP: Destroying namespace "nspatchtest-d48ffd22-d780-4f5d-9adc-8f53003c8165-270" for this suite. 02/27/23 11:59:05.065
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:59:05.085
Feb 27 11:59:05.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename daemonsets 02/27/23 11:59:05.087
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:59:05.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:59:05.129
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Feb 27 11:59:05.210: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 11:59:05.22
Feb 27 11:59:05.529: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:59:05.529: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:59:06.560: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:59:06.561: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:59:07.552: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 11:59:07.552: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 02/27/23 11:59:07.592
STEP: Check that daemon pods images are updated. 02/27/23 11:59:07.627
Feb 27 11:59:07.639: INFO: Wrong image for pod: daemon-set-5ws4c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 27 11:59:07.639: INFO: Wrong image for pod: daemon-set-mrhhw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 27 11:59:07.639: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 27 11:59:08.677: INFO: Wrong image for pod: daemon-set-mrhhw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 27 11:59:08.677: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 27 11:59:09.677: INFO: Wrong image for pod: daemon-set-mrhhw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 27 11:59:09.677: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 27 11:59:10.679: INFO: Pod daemon-set-9g8zw is not available
Feb 27 11:59:10.679: INFO: Wrong image for pod: daemon-set-mrhhw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 27 11:59:10.679: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 27 11:59:11.687: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 27 11:59:12.675: INFO: Pod daemon-set-rn74j is not available
Feb 27 11:59:12.675: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 27 11:59:13.680: INFO: Pod daemon-set-rn74j is not available
Feb 27 11:59:13.680: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 27 11:59:16.675: INFO: Pod daemon-set-4xbj8 is not available
STEP: Check that daemon pods are still running on every node of the cluster. 02/27/23 11:59:16.687
Feb 27 11:59:16.710: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 11:59:16.710: INFO: Node ip-172-31-7-167.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 11:59:17.732: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 11:59:17.733: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/27/23 11:59:17.771
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1554, will wait for the garbage collector to delete the pods 02/27/23 11:59:17.771
Feb 27 11:59:17.843: INFO: Deleting DaemonSet.extensions daemon-set took: 12.14382ms
Feb 27 11:59:17.944: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.299057ms
Feb 27 11:59:19.851: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 11:59:19.851: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 11:59:19.859: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"77218"},"items":null}

Feb 27 11:59:19.866: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"77218"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 27 11:59:19.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1554" for this suite. 02/27/23 11:59:19.927
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":143,"skipped":2995,"failed":0}
------------------------------
• [SLOW TEST] [14.861 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:59:05.085
    Feb 27 11:59:05.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename daemonsets 02/27/23 11:59:05.087
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:59:05.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:59:05.129
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Feb 27 11:59:05.210: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 11:59:05.22
    Feb 27 11:59:05.529: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:59:05.529: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:59:06.560: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:59:06.561: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:59:07.552: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 11:59:07.552: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 02/27/23 11:59:07.592
    STEP: Check that daemon pods images are updated. 02/27/23 11:59:07.627
    Feb 27 11:59:07.639: INFO: Wrong image for pod: daemon-set-5ws4c. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 27 11:59:07.639: INFO: Wrong image for pod: daemon-set-mrhhw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 27 11:59:07.639: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 27 11:59:08.677: INFO: Wrong image for pod: daemon-set-mrhhw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 27 11:59:08.677: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 27 11:59:09.677: INFO: Wrong image for pod: daemon-set-mrhhw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 27 11:59:09.677: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 27 11:59:10.679: INFO: Pod daemon-set-9g8zw is not available
    Feb 27 11:59:10.679: INFO: Wrong image for pod: daemon-set-mrhhw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 27 11:59:10.679: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 27 11:59:11.687: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 27 11:59:12.675: INFO: Pod daemon-set-rn74j is not available
    Feb 27 11:59:12.675: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 27 11:59:13.680: INFO: Pod daemon-set-rn74j is not available
    Feb 27 11:59:13.680: INFO: Wrong image for pod: daemon-set-zznz4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 27 11:59:16.675: INFO: Pod daemon-set-4xbj8 is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 02/27/23 11:59:16.687
    Feb 27 11:59:16.710: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 11:59:16.710: INFO: Node ip-172-31-7-167.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 11:59:17.732: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 11:59:17.733: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 11:59:17.771
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1554, will wait for the garbage collector to delete the pods 02/27/23 11:59:17.771
    Feb 27 11:59:17.843: INFO: Deleting DaemonSet.extensions daemon-set took: 12.14382ms
    Feb 27 11:59:17.944: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.299057ms
    Feb 27 11:59:19.851: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 11:59:19.851: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 11:59:19.859: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"77218"},"items":null}

    Feb 27 11:59:19.866: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"77218"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 11:59:19.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1554" for this suite. 02/27/23 11:59:19.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 11:59:19.949
Feb 27 11:59:19.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-probe 02/27/23 11:59:19.951
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:59:19.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:59:19.984
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 27 12:00:20.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3651" for this suite. 02/27/23 12:00:20.058
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":144,"skipped":3012,"failed":0}
------------------------------
• [SLOW TEST] [60.124 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 11:59:19.949
    Feb 27 11:59:19.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-probe 02/27/23 11:59:19.951
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 11:59:19.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 11:59:19.984
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 27 12:00:20.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3651" for this suite. 02/27/23 12:00:20.058
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:00:20.077
Feb 27 12:00:20.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:00:20.079
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:00:20.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:00:20.169
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 02/27/23 12:00:20.198
Feb 27 12:00:20.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b" in namespace "projected-7667" to be "Succeeded or Failed"
Feb 27 12:00:20.250: INFO: Pod "downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.358044ms
Feb 27 12:00:22.260: INFO: Pod "downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02999075s
Feb 27 12:00:24.259: INFO: Pod "downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029563039s
STEP: Saw pod success 02/27/23 12:00:24.259
Feb 27 12:00:24.260: INFO: Pod "downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b" satisfied condition "Succeeded or Failed"
Feb 27 12:00:24.271: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b container client-container: <nil>
STEP: delete the pod 02/27/23 12:00:24.291
Feb 27 12:00:24.314: INFO: Waiting for pod downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b to disappear
Feb 27 12:00:24.323: INFO: Pod downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 27 12:00:24.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7667" for this suite. 02/27/23 12:00:24.334
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":145,"skipped":3013,"failed":0}
------------------------------
• [4.270 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:00:20.077
    Feb 27 12:00:20.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:00:20.079
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:00:20.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:00:20.169
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 02/27/23 12:00:20.198
    Feb 27 12:00:20.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b" in namespace "projected-7667" to be "Succeeded or Failed"
    Feb 27 12:00:20.250: INFO: Pod "downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.358044ms
    Feb 27 12:00:22.260: INFO: Pod "downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02999075s
    Feb 27 12:00:24.259: INFO: Pod "downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029563039s
    STEP: Saw pod success 02/27/23 12:00:24.259
    Feb 27 12:00:24.260: INFO: Pod "downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b" satisfied condition "Succeeded or Failed"
    Feb 27 12:00:24.271: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b container client-container: <nil>
    STEP: delete the pod 02/27/23 12:00:24.291
    Feb 27 12:00:24.314: INFO: Waiting for pod downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b to disappear
    Feb 27 12:00:24.323: INFO: Pod downwardapi-volume-74dcd7e1-68fd-40b0-a14d-1f6a6e767b0b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 27 12:00:24.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7667" for this suite. 02/27/23 12:00:24.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:00:24.351
Feb 27 12:00:24.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 12:00:24.352
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:00:24.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:00:24.395
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 02/27/23 12:00:24.405
Feb 27 12:00:24.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-5825 create -f -'
Feb 27 12:00:24.844: INFO: stderr: ""
Feb 27 12:00:24.844: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/27/23 12:00:24.844
Feb 27 12:00:25.855: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 12:00:25.855: INFO: Found 1 / 1
Feb 27 12:00:25.855: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 02/27/23 12:00:25.855
Feb 27 12:00:25.864: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 12:00:25.864: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 27 12:00:25.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-5825 patch pod agnhost-primary-c8xd6 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 27 12:00:25.996: INFO: stderr: ""
Feb 27 12:00:25.996: INFO: stdout: "pod/agnhost-primary-c8xd6 patched\n"
STEP: checking annotations 02/27/23 12:00:25.996
Feb 27 12:00:26.009: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 12:00:26.009: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 12:00:26.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5825" for this suite. 02/27/23 12:00:26.024
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":146,"skipped":3027,"failed":0}
------------------------------
• [1.697 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:00:24.351
    Feb 27 12:00:24.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 12:00:24.352
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:00:24.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:00:24.395
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 02/27/23 12:00:24.405
    Feb 27 12:00:24.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-5825 create -f -'
    Feb 27 12:00:24.844: INFO: stderr: ""
    Feb 27 12:00:24.844: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/27/23 12:00:24.844
    Feb 27 12:00:25.855: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 12:00:25.855: INFO: Found 1 / 1
    Feb 27 12:00:25.855: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 02/27/23 12:00:25.855
    Feb 27 12:00:25.864: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 12:00:25.864: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 27 12:00:25.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-5825 patch pod agnhost-primary-c8xd6 -p {"metadata":{"annotations":{"x":"y"}}}'
    Feb 27 12:00:25.996: INFO: stderr: ""
    Feb 27 12:00:25.996: INFO: stdout: "pod/agnhost-primary-c8xd6 patched\n"
    STEP: checking annotations 02/27/23 12:00:25.996
    Feb 27 12:00:26.009: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 12:00:26.009: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 12:00:26.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5825" for this suite. 02/27/23 12:00:26.024
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:00:26.049
Feb 27 12:00:26.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 12:00:26.05
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:00:26.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:00:26.09
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-2262 02/27/23 12:00:26.1
STEP: creating service affinity-nodeport in namespace services-2262 02/27/23 12:00:26.1
STEP: creating replication controller affinity-nodeport in namespace services-2262 02/27/23 12:00:26.136
I0227 12:00:26.166455      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2262, replica count: 3
I0227 12:00:29.216931      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 12:00:29.245: INFO: Creating new exec pod
Feb 27 12:00:29.261: INFO: Waiting up to 5m0s for pod "execpod-affinity66xsb" in namespace "services-2262" to be "running"
Feb 27 12:00:29.271: INFO: Pod "execpod-affinity66xsb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.331353ms
Feb 27 12:00:31.286: INFO: Pod "execpod-affinity66xsb": Phase="Running", Reason="", readiness=true. Elapsed: 2.024828977s
Feb 27 12:00:31.286: INFO: Pod "execpod-affinity66xsb" satisfied condition "running"
Feb 27 12:00:32.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2262 exec execpod-affinity66xsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Feb 27 12:00:32.651: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb 27 12:00:32.651: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 12:00:32.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2262 exec execpod-affinity66xsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.81 80'
Feb 27 12:00:32.891: INFO: stderr: "+ nc -v -t -w 2 10.240.26.81 80\n+ echo hostName\nConnection to 10.240.26.81 80 port [tcp/http] succeeded!\n"
Feb 27 12:00:32.891: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 12:00:32.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2262 exec execpod-affinity66xsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.159 30420'
Feb 27 12:00:33.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.11.159 30420\nConnection to 172.31.11.159 30420 port [tcp/*] succeeded!\n"
Feb 27 12:00:33.236: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 12:00:33.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2262 exec execpod-affinity66xsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.167 30420'
Feb 27 12:00:33.531: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.7.167 30420\nConnection to 172.31.7.167 30420 port [tcp/*] succeeded!\n"
Feb 27 12:00:33.531: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 12:00:33.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2262 exec execpod-affinity66xsb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.159:30420/ ; done'
Feb 27 12:00:33.869: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n"
Feb 27 12:00:33.870: INFO: stdout: "\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq"
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
Feb 27 12:00:33.870: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2262, will wait for the garbage collector to delete the pods 02/27/23 12:00:34.339
Feb 27 12:00:34.411: INFO: Deleting ReplicationController affinity-nodeport took: 12.392991ms
Feb 27 12:00:34.511: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.399806ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 12:00:36.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2262" for this suite. 02/27/23 12:00:36.975
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":147,"skipped":3030,"failed":0}
------------------------------
• [SLOW TEST] [10.953 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:00:26.049
    Feb 27 12:00:26.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 12:00:26.05
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:00:26.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:00:26.09
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-2262 02/27/23 12:00:26.1
    STEP: creating service affinity-nodeport in namespace services-2262 02/27/23 12:00:26.1
    STEP: creating replication controller affinity-nodeport in namespace services-2262 02/27/23 12:00:26.136
    I0227 12:00:26.166455      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2262, replica count: 3
    I0227 12:00:29.216931      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 12:00:29.245: INFO: Creating new exec pod
    Feb 27 12:00:29.261: INFO: Waiting up to 5m0s for pod "execpod-affinity66xsb" in namespace "services-2262" to be "running"
    Feb 27 12:00:29.271: INFO: Pod "execpod-affinity66xsb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.331353ms
    Feb 27 12:00:31.286: INFO: Pod "execpod-affinity66xsb": Phase="Running", Reason="", readiness=true. Elapsed: 2.024828977s
    Feb 27 12:00:31.286: INFO: Pod "execpod-affinity66xsb" satisfied condition "running"
    Feb 27 12:00:32.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2262 exec execpod-affinity66xsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Feb 27 12:00:32.651: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Feb 27 12:00:32.651: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 12:00:32.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2262 exec execpod-affinity66xsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.26.81 80'
    Feb 27 12:00:32.891: INFO: stderr: "+ nc -v -t -w 2 10.240.26.81 80\n+ echo hostName\nConnection to 10.240.26.81 80 port [tcp/http] succeeded!\n"
    Feb 27 12:00:32.891: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 12:00:32.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2262 exec execpod-affinity66xsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.159 30420'
    Feb 27 12:00:33.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.11.159 30420\nConnection to 172.31.11.159 30420 port [tcp/*] succeeded!\n"
    Feb 27 12:00:33.236: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 12:00:33.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2262 exec execpod-affinity66xsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.167 30420'
    Feb 27 12:00:33.531: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.7.167 30420\nConnection to 172.31.7.167 30420 port [tcp/*] succeeded!\n"
    Feb 27 12:00:33.531: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 12:00:33.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2262 exec execpod-affinity66xsb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.159:30420/ ; done'
    Feb 27 12:00:33.869: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.159:30420/\n"
    Feb 27 12:00:33.870: INFO: stdout: "\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq\naffinity-nodeport-mdfbq"
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Received response from host: affinity-nodeport-mdfbq
    Feb 27 12:00:33.870: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-2262, will wait for the garbage collector to delete the pods 02/27/23 12:00:34.339
    Feb 27 12:00:34.411: INFO: Deleting ReplicationController affinity-nodeport took: 12.392991ms
    Feb 27 12:00:34.511: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.399806ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 12:00:36.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2262" for this suite. 02/27/23 12:00:36.975
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:00:37.003
Feb 27 12:00:37.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pods 02/27/23 12:00:37.004
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:00:37.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:00:37.067
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Feb 27 12:00:37.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: creating the pod 02/27/23 12:00:37.078
STEP: submitting the pod to kubernetes 02/27/23 12:00:37.078
Feb 27 12:00:37.095: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-c5d24100-46d5-4f79-8826-dbf4b69e166f" in namespace "pods-7635" to be "running and ready"
Feb 27 12:00:37.108: INFO: Pod "pod-logs-websocket-c5d24100-46d5-4f79-8826-dbf4b69e166f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.145421ms
Feb 27 12:00:37.108: INFO: The phase of Pod pod-logs-websocket-c5d24100-46d5-4f79-8826-dbf4b69e166f is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:00:39.125: INFO: Pod "pod-logs-websocket-c5d24100-46d5-4f79-8826-dbf4b69e166f": Phase="Running", Reason="", readiness=true. Elapsed: 2.029104824s
Feb 27 12:00:39.125: INFO: The phase of Pod pod-logs-websocket-c5d24100-46d5-4f79-8826-dbf4b69e166f is Running (Ready = true)
Feb 27 12:00:39.125: INFO: Pod "pod-logs-websocket-c5d24100-46d5-4f79-8826-dbf4b69e166f" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 27 12:00:39.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7635" for this suite. 02/27/23 12:00:39.203
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":148,"skipped":3031,"failed":0}
------------------------------
• [2.223 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:00:37.003
    Feb 27 12:00:37.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pods 02/27/23 12:00:37.004
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:00:37.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:00:37.067
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Feb 27 12:00:37.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: creating the pod 02/27/23 12:00:37.078
    STEP: submitting the pod to kubernetes 02/27/23 12:00:37.078
    Feb 27 12:00:37.095: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-c5d24100-46d5-4f79-8826-dbf4b69e166f" in namespace "pods-7635" to be "running and ready"
    Feb 27 12:00:37.108: INFO: Pod "pod-logs-websocket-c5d24100-46d5-4f79-8826-dbf4b69e166f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.145421ms
    Feb 27 12:00:37.108: INFO: The phase of Pod pod-logs-websocket-c5d24100-46d5-4f79-8826-dbf4b69e166f is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:00:39.125: INFO: Pod "pod-logs-websocket-c5d24100-46d5-4f79-8826-dbf4b69e166f": Phase="Running", Reason="", readiness=true. Elapsed: 2.029104824s
    Feb 27 12:00:39.125: INFO: The phase of Pod pod-logs-websocket-c5d24100-46d5-4f79-8826-dbf4b69e166f is Running (Ready = true)
    Feb 27 12:00:39.125: INFO: Pod "pod-logs-websocket-c5d24100-46d5-4f79-8826-dbf4b69e166f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 27 12:00:39.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7635" for this suite. 02/27/23 12:00:39.203
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:00:39.227
Feb 27 12:00:39.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 12:00:39.229
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:00:39.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:00:39.28
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-2816 02/27/23 12:00:39.289
STEP: creating replication controller nodeport-test in namespace services-2816 02/27/23 12:00:39.339
I0227 12:00:39.360502      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2816, replica count: 2
I0227 12:00:42.411947      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 12:00:42.412: INFO: Creating new exec pod
Feb 27 12:00:42.475: INFO: Waiting up to 5m0s for pod "execpodhggnp" in namespace "services-2816" to be "running"
Feb 27 12:00:42.487: INFO: Pod "execpodhggnp": Phase="Pending", Reason="", readiness=false. Elapsed: 12.12673ms
Feb 27 12:00:44.497: INFO: Pod "execpodhggnp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021968372s
Feb 27 12:00:46.502: INFO: Pod "execpodhggnp": Phase="Running", Reason="", readiness=true. Elapsed: 4.027186815s
Feb 27 12:00:46.502: INFO: Pod "execpodhggnp" satisfied condition "running"
Feb 27 12:00:47.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 27 12:00:47.782: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 27 12:00:47.782: INFO: stdout: ""
Feb 27 12:00:48.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 27 12:00:49.113: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 27 12:00:49.113: INFO: stdout: "nodeport-test-b2k8f"
Feb 27 12:00:49.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.130 80'
Feb 27 12:00:49.370: INFO: stderr: "+ nc -v -t -w 2 10.240.23.130 80\n+ echo hostName\nConnection to 10.240.23.130 80 port [tcp/http] succeeded!\n"
Feb 27 12:00:49.371: INFO: stdout: ""
Feb 27 12:00:50.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.130 80'
Feb 27 12:00:50.661: INFO: stderr: "+ nc -v -t -w 2 10.240.23.130 80\n+ echo hostName\nConnection to 10.240.23.130 80 port [tcp/http] succeeded!\n"
Feb 27 12:00:50.661: INFO: stdout: ""
Feb 27 12:00:51.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.130 80'
Feb 27 12:00:51.668: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.23.130 80\nConnection to 10.240.23.130 80 port [tcp/http] succeeded!\n"
Feb 27 12:00:51.668: INFO: stdout: ""
Feb 27 12:00:52.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.130 80'
Feb 27 12:00:52.682: INFO: stderr: "+ + nc -vecho -t hostName -w\n 2 10.240.23.130 80\nConnection to 10.240.23.130 80 port [tcp/http] succeeded!\n"
Feb 27 12:00:52.682: INFO: stdout: "nodeport-test-b2k8f"
Feb 27 12:00:52.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.17 31491'
Feb 27 12:00:52.901: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.17 31491\nConnection to 172.31.15.17 31491 port [tcp/*] succeeded!\n"
Feb 27 12:00:52.901: INFO: stdout: ""
Feb 27 12:00:53.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.17 31491'
Feb 27 12:00:54.129: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.17 31491\nConnection to 172.31.15.17 31491 port [tcp/*] succeeded!\n"
Feb 27 12:00:54.129: INFO: stdout: ""
Feb 27 12:00:54.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.17 31491'
Feb 27 12:00:55.168: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.17 31491\nConnection to 172.31.15.17 31491 port [tcp/*] succeeded!\n"
Feb 27 12:00:55.168: INFO: stdout: ""
Feb 27 12:00:55.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.17 31491'
Feb 27 12:00:56.125: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.17 31491\nConnection to 172.31.15.17 31491 port [tcp/*] succeeded!\n"
Feb 27 12:00:56.125: INFO: stdout: ""
Feb 27 12:00:56.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.17 31491'
Feb 27 12:00:57.145: INFO: stderr: "+ + ncecho hostName\n -v -t -w 2 172.31.15.17 31491\nConnection to 172.31.15.17 31491 port [tcp/*] succeeded!\n"
Feb 27 12:00:57.145: INFO: stdout: "nodeport-test-qkqnz"
Feb 27 12:00:57.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.167 31491'
Feb 27 12:00:57.395: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.7.167 31491\nConnection to 172.31.7.167 31491 port [tcp/*] succeeded!\n"
Feb 27 12:00:57.395: INFO: stdout: "nodeport-test-qkqnz"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 12:00:57.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2816" for this suite. 02/27/23 12:00:57.408
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":149,"skipped":3031,"failed":0}
------------------------------
• [SLOW TEST] [18.195 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:00:39.227
    Feb 27 12:00:39.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 12:00:39.229
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:00:39.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:00:39.28
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-2816 02/27/23 12:00:39.289
    STEP: creating replication controller nodeport-test in namespace services-2816 02/27/23 12:00:39.339
    I0227 12:00:39.360502      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2816, replica count: 2
    I0227 12:00:42.411947      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 12:00:42.412: INFO: Creating new exec pod
    Feb 27 12:00:42.475: INFO: Waiting up to 5m0s for pod "execpodhggnp" in namespace "services-2816" to be "running"
    Feb 27 12:00:42.487: INFO: Pod "execpodhggnp": Phase="Pending", Reason="", readiness=false. Elapsed: 12.12673ms
    Feb 27 12:00:44.497: INFO: Pod "execpodhggnp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021968372s
    Feb 27 12:00:46.502: INFO: Pod "execpodhggnp": Phase="Running", Reason="", readiness=true. Elapsed: 4.027186815s
    Feb 27 12:00:46.502: INFO: Pod "execpodhggnp" satisfied condition "running"
    Feb 27 12:00:47.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Feb 27 12:00:47.782: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb 27 12:00:47.782: INFO: stdout: ""
    Feb 27 12:00:48.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Feb 27 12:00:49.113: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb 27 12:00:49.113: INFO: stdout: "nodeport-test-b2k8f"
    Feb 27 12:00:49.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.130 80'
    Feb 27 12:00:49.370: INFO: stderr: "+ nc -v -t -w 2 10.240.23.130 80\n+ echo hostName\nConnection to 10.240.23.130 80 port [tcp/http] succeeded!\n"
    Feb 27 12:00:49.371: INFO: stdout: ""
    Feb 27 12:00:50.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.130 80'
    Feb 27 12:00:50.661: INFO: stderr: "+ nc -v -t -w 2 10.240.23.130 80\n+ echo hostName\nConnection to 10.240.23.130 80 port [tcp/http] succeeded!\n"
    Feb 27 12:00:50.661: INFO: stdout: ""
    Feb 27 12:00:51.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.130 80'
    Feb 27 12:00:51.668: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.23.130 80\nConnection to 10.240.23.130 80 port [tcp/http] succeeded!\n"
    Feb 27 12:00:51.668: INFO: stdout: ""
    Feb 27 12:00:52.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.23.130 80'
    Feb 27 12:00:52.682: INFO: stderr: "+ + nc -vecho -t hostName -w\n 2 10.240.23.130 80\nConnection to 10.240.23.130 80 port [tcp/http] succeeded!\n"
    Feb 27 12:00:52.682: INFO: stdout: "nodeport-test-b2k8f"
    Feb 27 12:00:52.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.17 31491'
    Feb 27 12:00:52.901: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.17 31491\nConnection to 172.31.15.17 31491 port [tcp/*] succeeded!\n"
    Feb 27 12:00:52.901: INFO: stdout: ""
    Feb 27 12:00:53.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.17 31491'
    Feb 27 12:00:54.129: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.17 31491\nConnection to 172.31.15.17 31491 port [tcp/*] succeeded!\n"
    Feb 27 12:00:54.129: INFO: stdout: ""
    Feb 27 12:00:54.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.17 31491'
    Feb 27 12:00:55.168: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.17 31491\nConnection to 172.31.15.17 31491 port [tcp/*] succeeded!\n"
    Feb 27 12:00:55.168: INFO: stdout: ""
    Feb 27 12:00:55.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.17 31491'
    Feb 27 12:00:56.125: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.15.17 31491\nConnection to 172.31.15.17 31491 port [tcp/*] succeeded!\n"
    Feb 27 12:00:56.125: INFO: stdout: ""
    Feb 27 12:00:56.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.15.17 31491'
    Feb 27 12:00:57.145: INFO: stderr: "+ + ncecho hostName\n -v -t -w 2 172.31.15.17 31491\nConnection to 172.31.15.17 31491 port [tcp/*] succeeded!\n"
    Feb 27 12:00:57.145: INFO: stdout: "nodeport-test-qkqnz"
    Feb 27 12:00:57.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-2816 exec execpodhggnp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.167 31491'
    Feb 27 12:00:57.395: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.7.167 31491\nConnection to 172.31.7.167 31491 port [tcp/*] succeeded!\n"
    Feb 27 12:00:57.395: INFO: stdout: "nodeport-test-qkqnz"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 12:00:57.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2816" for this suite. 02/27/23 12:00:57.408
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:00:57.428
Feb 27 12:00:57.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pod-network-test 02/27/23 12:00:57.429
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:00:57.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:00:57.48
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6696 02/27/23 12:00:57.489
STEP: creating a selector 02/27/23 12:00:57.489
STEP: Creating the service pods in kubernetes 02/27/23 12:00:57.489
Feb 27 12:00:57.489: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 27 12:00:57.553: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6696" to be "running and ready"
Feb 27 12:00:57.568: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.817551ms
Feb 27 12:00:57.568: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:00:59.577: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023252126s
Feb 27 12:00:59.577: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:01:01.579: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025082874s
Feb 27 12:01:01.579: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:01:03.593: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.039478684s
Feb 27 12:01:03.593: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:01:05.632: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.078853546s
Feb 27 12:01:05.632: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:01:07.578: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02474686s
Feb 27 12:01:07.578: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:01:09.579: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.025695524s
Feb 27 12:01:09.579: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:01:11.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.022625462s
Feb 27 12:01:11.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:01:13.582: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.028232321s
Feb 27 12:01:13.582: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:01:15.578: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.024048778s
Feb 27 12:01:15.578: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:01:17.578: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.024871106s
Feb 27 12:01:17.578: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:01:19.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.033678357s
Feb 27 12:01:19.587: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 27 12:01:19.587: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 27 12:01:19.595: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6696" to be "running and ready"
Feb 27 12:01:19.602: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.683143ms
Feb 27 12:01:19.603: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 27 12:01:19.603: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 27 12:01:19.612: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6696" to be "running and ready"
Feb 27 12:01:19.634: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 21.744311ms
Feb 27 12:01:19.634: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 27 12:01:19.634: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/27/23 12:01:19.644
Feb 27 12:01:19.663: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6696" to be "running"
Feb 27 12:01:19.670: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.702962ms
Feb 27 12:01:21.679: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016022782s
Feb 27 12:01:23.679: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016596369s
Feb 27 12:01:23.679: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 27 12:01:23.686: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 27 12:01:23.686: INFO: Breadth first check of 172.25.1.136 on host 172.31.11.159...
Feb 27 12:01:23.703: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.137:9080/dial?request=hostname&protocol=udp&host=172.25.1.136&port=8081&tries=1'] Namespace:pod-network-test-6696 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:01:23.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:01:23.704: INFO: ExecWithOptions: Clientset creation
Feb 27 12:01:23.704: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-6696/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.1.136%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 12:01:23.812: INFO: Waiting for responses: map[]
Feb 27 12:01:23.812: INFO: reached 172.25.1.136 after 0/1 tries
Feb 27 12:01:23.812: INFO: Breadth first check of 172.25.2.145 on host 172.31.15.17...
Feb 27 12:01:23.819: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.137:9080/dial?request=hostname&protocol=udp&host=172.25.2.145&port=8081&tries=1'] Namespace:pod-network-test-6696 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:01:23.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:01:23.820: INFO: ExecWithOptions: Clientset creation
Feb 27 12:01:23.820: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-6696/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.2.145%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 12:01:23.947: INFO: Waiting for responses: map[]
Feb 27 12:01:23.947: INFO: reached 172.25.2.145 after 0/1 tries
Feb 27 12:01:23.947: INFO: Breadth first check of 172.25.0.60 on host 172.31.7.167...
Feb 27 12:01:23.956: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.137:9080/dial?request=hostname&protocol=udp&host=172.25.0.60&port=8081&tries=1'] Namespace:pod-network-test-6696 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:01:23.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:01:23.957: INFO: ExecWithOptions: Clientset creation
Feb 27 12:01:23.957: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-6696/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.0.60%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 12:01:24.102: INFO: Waiting for responses: map[]
Feb 27 12:01:24.102: INFO: reached 172.25.0.60 after 0/1 tries
Feb 27 12:01:24.102: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 27 12:01:24.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6696" for this suite. 02/27/23 12:01:24.118
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":150,"skipped":3044,"failed":0}
------------------------------
• [SLOW TEST] [26.703 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:00:57.428
    Feb 27 12:00:57.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pod-network-test 02/27/23 12:00:57.429
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:00:57.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:00:57.48
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6696 02/27/23 12:00:57.489
    STEP: creating a selector 02/27/23 12:00:57.489
    STEP: Creating the service pods in kubernetes 02/27/23 12:00:57.489
    Feb 27 12:00:57.489: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 27 12:00:57.553: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6696" to be "running and ready"
    Feb 27 12:00:57.568: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.817551ms
    Feb 27 12:00:57.568: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:00:59.577: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023252126s
    Feb 27 12:00:59.577: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:01:01.579: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025082874s
    Feb 27 12:01:01.579: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:01:03.593: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.039478684s
    Feb 27 12:01:03.593: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:01:05.632: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.078853546s
    Feb 27 12:01:05.632: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:01:07.578: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02474686s
    Feb 27 12:01:07.578: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:01:09.579: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.025695524s
    Feb 27 12:01:09.579: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:01:11.576: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.022625462s
    Feb 27 12:01:11.576: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:01:13.582: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.028232321s
    Feb 27 12:01:13.582: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:01:15.578: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.024048778s
    Feb 27 12:01:15.578: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:01:17.578: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.024871106s
    Feb 27 12:01:17.578: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:01:19.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.033678357s
    Feb 27 12:01:19.587: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 27 12:01:19.587: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 27 12:01:19.595: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6696" to be "running and ready"
    Feb 27 12:01:19.602: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.683143ms
    Feb 27 12:01:19.603: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 27 12:01:19.603: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 27 12:01:19.612: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6696" to be "running and ready"
    Feb 27 12:01:19.634: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 21.744311ms
    Feb 27 12:01:19.634: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 27 12:01:19.634: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/27/23 12:01:19.644
    Feb 27 12:01:19.663: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6696" to be "running"
    Feb 27 12:01:19.670: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.702962ms
    Feb 27 12:01:21.679: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016022782s
    Feb 27 12:01:23.679: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016596369s
    Feb 27 12:01:23.679: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 27 12:01:23.686: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 27 12:01:23.686: INFO: Breadth first check of 172.25.1.136 on host 172.31.11.159...
    Feb 27 12:01:23.703: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.137:9080/dial?request=hostname&protocol=udp&host=172.25.1.136&port=8081&tries=1'] Namespace:pod-network-test-6696 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:01:23.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:01:23.704: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:01:23.704: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-6696/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.1.136%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 12:01:23.812: INFO: Waiting for responses: map[]
    Feb 27 12:01:23.812: INFO: reached 172.25.1.136 after 0/1 tries
    Feb 27 12:01:23.812: INFO: Breadth first check of 172.25.2.145 on host 172.31.15.17...
    Feb 27 12:01:23.819: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.137:9080/dial?request=hostname&protocol=udp&host=172.25.2.145&port=8081&tries=1'] Namespace:pod-network-test-6696 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:01:23.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:01:23.820: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:01:23.820: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-6696/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.2.145%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 12:01:23.947: INFO: Waiting for responses: map[]
    Feb 27 12:01:23.947: INFO: reached 172.25.2.145 after 0/1 tries
    Feb 27 12:01:23.947: INFO: Breadth first check of 172.25.0.60 on host 172.31.7.167...
    Feb 27 12:01:23.956: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.137:9080/dial?request=hostname&protocol=udp&host=172.25.0.60&port=8081&tries=1'] Namespace:pod-network-test-6696 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:01:23.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:01:23.957: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:01:23.957: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-6696/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.0.60%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 12:01:24.102: INFO: Waiting for responses: map[]
    Feb 27 12:01:24.102: INFO: reached 172.25.0.60 after 0/1 tries
    Feb 27 12:01:24.102: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 27 12:01:24.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6696" for this suite. 02/27/23 12:01:24.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:01:24.14
Feb 27 12:01:24.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:01:24.141
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:24.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:24.175
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-1bcfdb2e-4982-47da-b795-7ba22610cf84 02/27/23 12:01:24.195
STEP: Creating the pod 02/27/23 12:01:24.211
Feb 27 12:01:24.229: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-149bbbdc-a870-461e-9be3-83362ce1a9e5" in namespace "projected-7392" to be "running and ready"
Feb 27 12:01:24.242: INFO: Pod "pod-projected-configmaps-149bbbdc-a870-461e-9be3-83362ce1a9e5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.029384ms
Feb 27 12:01:24.242: INFO: The phase of Pod pod-projected-configmaps-149bbbdc-a870-461e-9be3-83362ce1a9e5 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:01:26.252: INFO: Pod "pod-projected-configmaps-149bbbdc-a870-461e-9be3-83362ce1a9e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.023653754s
Feb 27 12:01:26.252: INFO: The phase of Pod pod-projected-configmaps-149bbbdc-a870-461e-9be3-83362ce1a9e5 is Running (Ready = true)
Feb 27 12:01:26.252: INFO: Pod "pod-projected-configmaps-149bbbdc-a870-461e-9be3-83362ce1a9e5" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-1bcfdb2e-4982-47da-b795-7ba22610cf84 02/27/23 12:01:26.278
STEP: waiting to observe update in volume 02/27/23 12:01:26.287
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 27 12:01:30.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7392" for this suite. 02/27/23 12:01:30.391
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":151,"skipped":3065,"failed":0}
------------------------------
• [SLOW TEST] [6.263 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:01:24.14
    Feb 27 12:01:24.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:01:24.141
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:24.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:24.175
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-1bcfdb2e-4982-47da-b795-7ba22610cf84 02/27/23 12:01:24.195
    STEP: Creating the pod 02/27/23 12:01:24.211
    Feb 27 12:01:24.229: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-149bbbdc-a870-461e-9be3-83362ce1a9e5" in namespace "projected-7392" to be "running and ready"
    Feb 27 12:01:24.242: INFO: Pod "pod-projected-configmaps-149bbbdc-a870-461e-9be3-83362ce1a9e5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.029384ms
    Feb 27 12:01:24.242: INFO: The phase of Pod pod-projected-configmaps-149bbbdc-a870-461e-9be3-83362ce1a9e5 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:01:26.252: INFO: Pod "pod-projected-configmaps-149bbbdc-a870-461e-9be3-83362ce1a9e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.023653754s
    Feb 27 12:01:26.252: INFO: The phase of Pod pod-projected-configmaps-149bbbdc-a870-461e-9be3-83362ce1a9e5 is Running (Ready = true)
    Feb 27 12:01:26.252: INFO: Pod "pod-projected-configmaps-149bbbdc-a870-461e-9be3-83362ce1a9e5" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-1bcfdb2e-4982-47da-b795-7ba22610cf84 02/27/23 12:01:26.278
    STEP: waiting to observe update in volume 02/27/23 12:01:26.287
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 27 12:01:30.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7392" for this suite. 02/27/23 12:01:30.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:01:30.406
Feb 27 12:01:30.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 12:01:30.408
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:30.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:30.448
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 02/27/23 12:01:30.459
Feb 27 12:01:30.460: INFO: Creating e2e-svc-a-l4fll
Feb 27 12:01:30.479: INFO: Creating e2e-svc-b-kxw9p
Feb 27 12:01:30.499: INFO: Creating e2e-svc-c-f9npj
STEP: deleting service collection 02/27/23 12:01:30.559
Feb 27 12:01:30.615: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 12:01:30.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6730" for this suite. 02/27/23 12:01:30.623
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":152,"skipped":3083,"failed":0}
------------------------------
• [0.229 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:01:30.406
    Feb 27 12:01:30.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 12:01:30.408
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:30.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:30.448
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 02/27/23 12:01:30.459
    Feb 27 12:01:30.460: INFO: Creating e2e-svc-a-l4fll
    Feb 27 12:01:30.479: INFO: Creating e2e-svc-b-kxw9p
    Feb 27 12:01:30.499: INFO: Creating e2e-svc-c-f9npj
    STEP: deleting service collection 02/27/23 12:01:30.559
    Feb 27 12:01:30.615: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 12:01:30.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6730" for this suite. 02/27/23 12:01:30.623
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:01:30.64
Feb 27 12:01:30.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 12:01:30.642
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:30.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:30.678
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 02/27/23 12:01:30.717
Feb 27 12:01:30.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-6742 api-versions'
Feb 27 12:01:30.821: INFO: stderr: ""
Feb 27 12:01:30.821: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps.kubermatic.k8c.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperatingsystemmanager.k8c.io/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 12:01:30.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6742" for this suite. 02/27/23 12:01:30.846
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":153,"skipped":3093,"failed":0}
------------------------------
• [0.253 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:01:30.64
    Feb 27 12:01:30.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 12:01:30.642
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:30.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:30.678
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 02/27/23 12:01:30.717
    Feb 27 12:01:30.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-6742 api-versions'
    Feb 27 12:01:30.821: INFO: stderr: ""
    Feb 27 12:01:30.821: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps.kubermatic.k8c.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperatingsystemmanager.k8c.io/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 12:01:30.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6742" for this suite. 02/27/23 12:01:30.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:01:30.893
Feb 27 12:01:30.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pods 02/27/23 12:01:30.894
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:30.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:30.96
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Feb 27 12:01:30.988: INFO: Waiting up to 5m0s for pod "server-envvars-c4b22d97-927e-4765-8b28-e3c925bfc125" in namespace "pods-4801" to be "running and ready"
Feb 27 12:01:31.015: INFO: Pod "server-envvars-c4b22d97-927e-4765-8b28-e3c925bfc125": Phase="Pending", Reason="", readiness=false. Elapsed: 26.594055ms
Feb 27 12:01:31.015: INFO: The phase of Pod server-envvars-c4b22d97-927e-4765-8b28-e3c925bfc125 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:01:33.024: INFO: Pod "server-envvars-c4b22d97-927e-4765-8b28-e3c925bfc125": Phase="Running", Reason="", readiness=true. Elapsed: 2.035484145s
Feb 27 12:01:33.024: INFO: The phase of Pod server-envvars-c4b22d97-927e-4765-8b28-e3c925bfc125 is Running (Ready = true)
Feb 27 12:01:33.024: INFO: Pod "server-envvars-c4b22d97-927e-4765-8b28-e3c925bfc125" satisfied condition "running and ready"
Feb 27 12:01:33.093: INFO: Waiting up to 5m0s for pod "client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993" in namespace "pods-4801" to be "Succeeded or Failed"
Feb 27 12:01:33.113: INFO: Pod "client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993": Phase="Pending", Reason="", readiness=false. Elapsed: 19.515974ms
Feb 27 12:01:35.126: INFO: Pod "client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032649453s
Feb 27 12:01:37.122: INFO: Pod "client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028721268s
Feb 27 12:01:39.123: INFO: Pod "client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029901222s
STEP: Saw pod success 02/27/23 12:01:39.123
Feb 27 12:01:39.124: INFO: Pod "client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993" satisfied condition "Succeeded or Failed"
Feb 27 12:01:39.133: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993 container env3cont: <nil>
STEP: delete the pod 02/27/23 12:01:39.158
Feb 27 12:01:39.182: INFO: Waiting for pod client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993 to disappear
Feb 27 12:01:39.192: INFO: Pod client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 27 12:01:39.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4801" for this suite. 02/27/23 12:01:39.204
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":154,"skipped":3100,"failed":0}
------------------------------
• [SLOW TEST] [8.334 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:01:30.893
    Feb 27 12:01:30.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pods 02/27/23 12:01:30.894
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:30.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:30.96
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Feb 27 12:01:30.988: INFO: Waiting up to 5m0s for pod "server-envvars-c4b22d97-927e-4765-8b28-e3c925bfc125" in namespace "pods-4801" to be "running and ready"
    Feb 27 12:01:31.015: INFO: Pod "server-envvars-c4b22d97-927e-4765-8b28-e3c925bfc125": Phase="Pending", Reason="", readiness=false. Elapsed: 26.594055ms
    Feb 27 12:01:31.015: INFO: The phase of Pod server-envvars-c4b22d97-927e-4765-8b28-e3c925bfc125 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:01:33.024: INFO: Pod "server-envvars-c4b22d97-927e-4765-8b28-e3c925bfc125": Phase="Running", Reason="", readiness=true. Elapsed: 2.035484145s
    Feb 27 12:01:33.024: INFO: The phase of Pod server-envvars-c4b22d97-927e-4765-8b28-e3c925bfc125 is Running (Ready = true)
    Feb 27 12:01:33.024: INFO: Pod "server-envvars-c4b22d97-927e-4765-8b28-e3c925bfc125" satisfied condition "running and ready"
    Feb 27 12:01:33.093: INFO: Waiting up to 5m0s for pod "client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993" in namespace "pods-4801" to be "Succeeded or Failed"
    Feb 27 12:01:33.113: INFO: Pod "client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993": Phase="Pending", Reason="", readiness=false. Elapsed: 19.515974ms
    Feb 27 12:01:35.126: INFO: Pod "client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032649453s
    Feb 27 12:01:37.122: INFO: Pod "client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028721268s
    Feb 27 12:01:39.123: INFO: Pod "client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029901222s
    STEP: Saw pod success 02/27/23 12:01:39.123
    Feb 27 12:01:39.124: INFO: Pod "client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993" satisfied condition "Succeeded or Failed"
    Feb 27 12:01:39.133: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993 container env3cont: <nil>
    STEP: delete the pod 02/27/23 12:01:39.158
    Feb 27 12:01:39.182: INFO: Waiting for pod client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993 to disappear
    Feb 27 12:01:39.192: INFO: Pod client-envvars-1096f3fd-e269-4a01-a09f-57467a7a5993 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 27 12:01:39.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4801" for this suite. 02/27/23 12:01:39.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:01:39.239
Feb 27 12:01:39.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename resourcequota 02/27/23 12:01:39.24
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:39.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:39.294
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 02/27/23 12:01:39.32
STEP: Creating a ResourceQuota 02/27/23 12:01:44.334
STEP: Ensuring resource quota status is calculated 02/27/23 12:01:44.472
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 27 12:01:46.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6090" for this suite. 02/27/23 12:01:46.495
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":155,"skipped":3133,"failed":0}
------------------------------
• [SLOW TEST] [7.268 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:01:39.239
    Feb 27 12:01:39.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename resourcequota 02/27/23 12:01:39.24
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:39.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:39.294
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 02/27/23 12:01:39.32
    STEP: Creating a ResourceQuota 02/27/23 12:01:44.334
    STEP: Ensuring resource quota status is calculated 02/27/23 12:01:44.472
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 27 12:01:46.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6090" for this suite. 02/27/23 12:01:46.495
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:01:46.51
Feb 27 12:01:46.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename svcaccounts 02/27/23 12:01:46.512
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:46.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:46.561
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Feb 27 12:01:46.603: INFO: Waiting up to 5m0s for pod "pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad" in namespace "svcaccounts-1454" to be "running"
Feb 27 12:01:46.611: INFO: Pod "pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad": Phase="Pending", Reason="", readiness=false. Elapsed: 8.690246ms
Feb 27 12:01:48.621: INFO: Pod "pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.018087538s
Feb 27 12:01:48.621: INFO: Pod "pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad" satisfied condition "running"
STEP: reading a file in the container 02/27/23 12:01:48.621
Feb 27 12:01:48.621: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1454 pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 02/27/23 12:01:48.833
Feb 27 12:01:48.834: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1454 pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 02/27/23 12:01:49.061
Feb 27 12:01:49.061: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1454 pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Feb 27 12:01:49.311: INFO: Got root ca configmap in namespace "svcaccounts-1454"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 27 12:01:49.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1454" for this suite. 02/27/23 12:01:49.336
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":156,"skipped":3134,"failed":0}
------------------------------
• [2.840 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:01:46.51
    Feb 27 12:01:46.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 12:01:46.512
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:46.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:46.561
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Feb 27 12:01:46.603: INFO: Waiting up to 5m0s for pod "pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad" in namespace "svcaccounts-1454" to be "running"
    Feb 27 12:01:46.611: INFO: Pod "pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad": Phase="Pending", Reason="", readiness=false. Elapsed: 8.690246ms
    Feb 27 12:01:48.621: INFO: Pod "pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.018087538s
    Feb 27 12:01:48.621: INFO: Pod "pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad" satisfied condition "running"
    STEP: reading a file in the container 02/27/23 12:01:48.621
    Feb 27 12:01:48.621: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1454 pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 02/27/23 12:01:48.833
    Feb 27 12:01:48.834: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1454 pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 02/27/23 12:01:49.061
    Feb 27 12:01:49.061: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1454 pod-service-account-7590fe91-e3b9-4af7-97ee-fa23443fb5ad -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Feb 27 12:01:49.311: INFO: Got root ca configmap in namespace "svcaccounts-1454"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 27 12:01:49.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1454" for this suite. 02/27/23 12:01:49.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:01:49.356
Feb 27 12:01:49.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename deployment 02/27/23 12:01:49.357
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:49.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:49.396
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Feb 27 12:01:49.409: INFO: Creating simple deployment test-new-deployment
Feb 27 12:01:49.440: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 02/27/23 12:01:51.47
STEP: updating a scale subresource 02/27/23 12:01:51.599
STEP: verifying the deployment Spec.Replicas was modified 02/27/23 12:01:51.615
STEP: Patch a scale subresource 02/27/23 12:01:51.63
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 12:01:51.681: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2392  07e21227-70e7-456f-82b9-0d89ae47e65e 78487 3 2023-02-27 12:01:49 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-27 12:01:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:01:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004268eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-02-27 12:01:51 +0000 UTC,LastTransitionTime:2023-02-27 12:01:49 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 12:01:51 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 12:01:51.693: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-2392  2184346f-582c-47d0-95fb-9d180e29f7cd 78491 3 2023-02-27 12:01:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 07e21227-70e7-456f-82b9-0d89ae47e65e 0xc0042692d7 0xc0042692d8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:01:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e21227-70e7-456f-82b9-0d89ae47e65e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:01:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004269368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 12:01:51.709: INFO: Pod "test-new-deployment-845c8977d9-fs55l" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-fs55l test-new-deployment-845c8977d9- deployment-2392  c3662c93-8289-4eab-80f1-c57d4cf880c1 78492 0 2023-02-27 12:01:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 2184346f-582c-47d0-95fb-9d180e29f7cd 0xc004269757 0xc004269758}] [] [{kube-controller-manager Update v1 2023-02-27 12:01:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2184346f-582c-47d0-95fb-9d180e29f7cd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:01:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9qc46,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9qc46,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:01:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:01:51.709: INFO: Pod "test-new-deployment-845c8977d9-grqlv" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-grqlv test-new-deployment-845c8977d9- deployment-2392  55383c0f-15f8-49a7-8777-358fae35c8d1 78474 0 2023-02-27 12:01:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:82cff13a970ffa34ce2c76fe588a945ecb7c5ca48a5dbafc9f09d89f5af83681 cni.projectcalico.org/podIP:172.25.2.148/32 cni.projectcalico.org/podIPs:172.25.2.148/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 2184346f-582c-47d0-95fb-9d180e29f7cd 0xc004269937 0xc004269938}] [] [{calico Update v1 2023-02-27 12:01:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:01:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2184346f-582c-47d0-95fb-9d180e29f7cd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:01:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdl9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdl9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.148,StartTime:2023-02-27 12:01:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:01:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a3cdee00fc1029e1899f04dadc3e65cb7143bef20131e5e36e13355a1454f340,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 27 12:01:51.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2392" for this suite. 02/27/23 12:01:51.724
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":157,"skipped":3144,"failed":0}
------------------------------
• [2.406 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:01:49.356
    Feb 27 12:01:49.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename deployment 02/27/23 12:01:49.357
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:49.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:49.396
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Feb 27 12:01:49.409: INFO: Creating simple deployment test-new-deployment
    Feb 27 12:01:49.440: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 02/27/23 12:01:51.47
    STEP: updating a scale subresource 02/27/23 12:01:51.599
    STEP: verifying the deployment Spec.Replicas was modified 02/27/23 12:01:51.615
    STEP: Patch a scale subresource 02/27/23 12:01:51.63
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 12:01:51.681: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-2392  07e21227-70e7-456f-82b9-0d89ae47e65e 78487 3 2023-02-27 12:01:49 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-27 12:01:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:01:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004268eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-02-27 12:01:51 +0000 UTC,LastTransitionTime:2023-02-27 12:01:49 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 12:01:51 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 12:01:51.693: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-2392  2184346f-582c-47d0-95fb-9d180e29f7cd 78491 3 2023-02-27 12:01:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 07e21227-70e7-456f-82b9-0d89ae47e65e 0xc0042692d7 0xc0042692d8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:01:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e21227-70e7-456f-82b9-0d89ae47e65e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:01:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004269368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 12:01:51.709: INFO: Pod "test-new-deployment-845c8977d9-fs55l" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-fs55l test-new-deployment-845c8977d9- deployment-2392  c3662c93-8289-4eab-80f1-c57d4cf880c1 78492 0 2023-02-27 12:01:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 2184346f-582c-47d0-95fb-9d180e29f7cd 0xc004269757 0xc004269758}] [] [{kube-controller-manager Update v1 2023-02-27 12:01:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2184346f-582c-47d0-95fb-9d180e29f7cd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:01:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9qc46,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9qc46,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:01:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:01:51.709: INFO: Pod "test-new-deployment-845c8977d9-grqlv" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-grqlv test-new-deployment-845c8977d9- deployment-2392  55383c0f-15f8-49a7-8777-358fae35c8d1 78474 0 2023-02-27 12:01:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:82cff13a970ffa34ce2c76fe588a945ecb7c5ca48a5dbafc9f09d89f5af83681 cni.projectcalico.org/podIP:172.25.2.148/32 cni.projectcalico.org/podIPs:172.25.2.148/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 2184346f-582c-47d0-95fb-9d180e29f7cd 0xc004269937 0xc004269938}] [] [{calico Update v1 2023-02-27 12:01:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:01:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2184346f-582c-47d0-95fb-9d180e29f7cd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:01:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdl9g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdl9g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:01:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.148,StartTime:2023-02-27 12:01:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:01:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a3cdee00fc1029e1899f04dadc3e65cb7143bef20131e5e36e13355a1454f340,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 27 12:01:51.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2392" for this suite. 02/27/23 12:01:51.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:01:51.763
Feb 27 12:01:51.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-probe 02/27/23 12:01:51.765
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:51.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:51.816
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-b9eec221-5596-4e59-9e67-ffb993b57883 in namespace container-probe-2940 02/27/23 12:01:51.824
Feb 27 12:01:51.842: INFO: Waiting up to 5m0s for pod "liveness-b9eec221-5596-4e59-9e67-ffb993b57883" in namespace "container-probe-2940" to be "not pending"
Feb 27 12:01:51.851: INFO: Pod "liveness-b9eec221-5596-4e59-9e67-ffb993b57883": Phase="Pending", Reason="", readiness=false. Elapsed: 9.457619ms
Feb 27 12:01:53.860: INFO: Pod "liveness-b9eec221-5596-4e59-9e67-ffb993b57883": Phase="Running", Reason="", readiness=true. Elapsed: 2.018732789s
Feb 27 12:01:53.860: INFO: Pod "liveness-b9eec221-5596-4e59-9e67-ffb993b57883" satisfied condition "not pending"
Feb 27 12:01:53.861: INFO: Started pod liveness-b9eec221-5596-4e59-9e67-ffb993b57883 in namespace container-probe-2940
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 12:01:53.861
Feb 27 12:01:53.870: INFO: Initial restart count of pod liveness-b9eec221-5596-4e59-9e67-ffb993b57883 is 0
Feb 27 12:02:13.991: INFO: Restart count of pod container-probe-2940/liveness-b9eec221-5596-4e59-9e67-ffb993b57883 is now 1 (20.121290287s elapsed)
Feb 27 12:02:34.084: INFO: Restart count of pod container-probe-2940/liveness-b9eec221-5596-4e59-9e67-ffb993b57883 is now 2 (40.214690969s elapsed)
Feb 27 12:02:54.188: INFO: Restart count of pod container-probe-2940/liveness-b9eec221-5596-4e59-9e67-ffb993b57883 is now 3 (1m0.318810625s elapsed)
Feb 27 12:03:14.300: INFO: Restart count of pod container-probe-2940/liveness-b9eec221-5596-4e59-9e67-ffb993b57883 is now 4 (1m20.430689447s elapsed)
Feb 27 12:04:28.702: INFO: Restart count of pod container-probe-2940/liveness-b9eec221-5596-4e59-9e67-ffb993b57883 is now 5 (2m34.832457s elapsed)
STEP: deleting the pod 02/27/23 12:04:28.702
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 27 12:04:28.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2940" for this suite. 02/27/23 12:04:28.735
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":158,"skipped":3149,"failed":0}
------------------------------
• [SLOW TEST] [156.986 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:01:51.763
    Feb 27 12:01:51.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-probe 02/27/23 12:01:51.765
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:01:51.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:01:51.816
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-b9eec221-5596-4e59-9e67-ffb993b57883 in namespace container-probe-2940 02/27/23 12:01:51.824
    Feb 27 12:01:51.842: INFO: Waiting up to 5m0s for pod "liveness-b9eec221-5596-4e59-9e67-ffb993b57883" in namespace "container-probe-2940" to be "not pending"
    Feb 27 12:01:51.851: INFO: Pod "liveness-b9eec221-5596-4e59-9e67-ffb993b57883": Phase="Pending", Reason="", readiness=false. Elapsed: 9.457619ms
    Feb 27 12:01:53.860: INFO: Pod "liveness-b9eec221-5596-4e59-9e67-ffb993b57883": Phase="Running", Reason="", readiness=true. Elapsed: 2.018732789s
    Feb 27 12:01:53.860: INFO: Pod "liveness-b9eec221-5596-4e59-9e67-ffb993b57883" satisfied condition "not pending"
    Feb 27 12:01:53.861: INFO: Started pod liveness-b9eec221-5596-4e59-9e67-ffb993b57883 in namespace container-probe-2940
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 12:01:53.861
    Feb 27 12:01:53.870: INFO: Initial restart count of pod liveness-b9eec221-5596-4e59-9e67-ffb993b57883 is 0
    Feb 27 12:02:13.991: INFO: Restart count of pod container-probe-2940/liveness-b9eec221-5596-4e59-9e67-ffb993b57883 is now 1 (20.121290287s elapsed)
    Feb 27 12:02:34.084: INFO: Restart count of pod container-probe-2940/liveness-b9eec221-5596-4e59-9e67-ffb993b57883 is now 2 (40.214690969s elapsed)
    Feb 27 12:02:54.188: INFO: Restart count of pod container-probe-2940/liveness-b9eec221-5596-4e59-9e67-ffb993b57883 is now 3 (1m0.318810625s elapsed)
    Feb 27 12:03:14.300: INFO: Restart count of pod container-probe-2940/liveness-b9eec221-5596-4e59-9e67-ffb993b57883 is now 4 (1m20.430689447s elapsed)
    Feb 27 12:04:28.702: INFO: Restart count of pod container-probe-2940/liveness-b9eec221-5596-4e59-9e67-ffb993b57883 is now 5 (2m34.832457s elapsed)
    STEP: deleting the pod 02/27/23 12:04:28.702
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 27 12:04:28.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2940" for this suite. 02/27/23 12:04:28.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:04:28.751
Feb 27 12:04:28.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:04:28.753
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:04:28.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:04:28.797
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-a69256fd-3483-445a-a3ae-ca087c2a3c66 02/27/23 12:04:28.806
STEP: Creating a pod to test consume configMaps 02/27/23 12:04:28.815
Feb 27 12:04:28.830: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86" in namespace "projected-4348" to be "Succeeded or Failed"
Feb 27 12:04:28.841: INFO: Pod "pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86": Phase="Pending", Reason="", readiness=false. Elapsed: 11.202006ms
Feb 27 12:04:30.852: INFO: Pod "pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022045552s
Feb 27 12:04:32.853: INFO: Pod "pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02245344s
STEP: Saw pod success 02/27/23 12:04:32.853
Feb 27 12:04:32.853: INFO: Pod "pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86" satisfied condition "Succeeded or Failed"
Feb 27 12:04:32.869: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 12:04:32.908
Feb 27 12:04:32.930: INFO: Waiting for pod pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86 to disappear
Feb 27 12:04:32.940: INFO: Pod pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 27 12:04:32.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4348" for this suite. 02/27/23 12:04:32.949
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":159,"skipped":3164,"failed":0}
------------------------------
• [4.213 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:04:28.751
    Feb 27 12:04:28.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:04:28.753
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:04:28.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:04:28.797
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-a69256fd-3483-445a-a3ae-ca087c2a3c66 02/27/23 12:04:28.806
    STEP: Creating a pod to test consume configMaps 02/27/23 12:04:28.815
    Feb 27 12:04:28.830: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86" in namespace "projected-4348" to be "Succeeded or Failed"
    Feb 27 12:04:28.841: INFO: Pod "pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86": Phase="Pending", Reason="", readiness=false. Elapsed: 11.202006ms
    Feb 27 12:04:30.852: INFO: Pod "pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022045552s
    Feb 27 12:04:32.853: INFO: Pod "pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02245344s
    STEP: Saw pod success 02/27/23 12:04:32.853
    Feb 27 12:04:32.853: INFO: Pod "pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86" satisfied condition "Succeeded or Failed"
    Feb 27 12:04:32.869: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 12:04:32.908
    Feb 27 12:04:32.930: INFO: Waiting for pod pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86 to disappear
    Feb 27 12:04:32.940: INFO: Pod pod-projected-configmaps-829b62bf-0b18-4a95-9ebb-68656ad56d86 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 27 12:04:32.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4348" for this suite. 02/27/23 12:04:32.949
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:04:32.967
Feb 27 12:04:32.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename resourcequota 02/27/23 12:04:32.968
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:04:33.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:04:33.026
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 02/27/23 12:04:33.037
STEP: Ensuring ResourceQuota status is calculated 02/27/23 12:04:33.061
STEP: Creating a ResourceQuota with not terminating scope 02/27/23 12:04:35.069
STEP: Ensuring ResourceQuota status is calculated 02/27/23 12:04:35.079
STEP: Creating a long running pod 02/27/23 12:04:37.087
STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/27/23 12:04:37.109
STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/27/23 12:04:39.117
STEP: Deleting the pod 02/27/23 12:04:41.125
STEP: Ensuring resource quota status released the pod usage 02/27/23 12:04:41.155
STEP: Creating a terminating pod 02/27/23 12:04:43.163
STEP: Ensuring resource quota with terminating scope captures the pod usage 02/27/23 12:04:43.187
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/27/23 12:04:45.196
STEP: Deleting the pod 02/27/23 12:04:47.206
STEP: Ensuring resource quota status released the pod usage 02/27/23 12:04:47.23
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 27 12:04:49.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8997" for this suite. 02/27/23 12:04:49.252
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":160,"skipped":3167,"failed":0}
------------------------------
• [SLOW TEST] [16.302 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:04:32.967
    Feb 27 12:04:32.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename resourcequota 02/27/23 12:04:32.968
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:04:33.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:04:33.026
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 02/27/23 12:04:33.037
    STEP: Ensuring ResourceQuota status is calculated 02/27/23 12:04:33.061
    STEP: Creating a ResourceQuota with not terminating scope 02/27/23 12:04:35.069
    STEP: Ensuring ResourceQuota status is calculated 02/27/23 12:04:35.079
    STEP: Creating a long running pod 02/27/23 12:04:37.087
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/27/23 12:04:37.109
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/27/23 12:04:39.117
    STEP: Deleting the pod 02/27/23 12:04:41.125
    STEP: Ensuring resource quota status released the pod usage 02/27/23 12:04:41.155
    STEP: Creating a terminating pod 02/27/23 12:04:43.163
    STEP: Ensuring resource quota with terminating scope captures the pod usage 02/27/23 12:04:43.187
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/27/23 12:04:45.196
    STEP: Deleting the pod 02/27/23 12:04:47.206
    STEP: Ensuring resource quota status released the pod usage 02/27/23 12:04:47.23
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 27 12:04:49.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8997" for this suite. 02/27/23 12:04:49.252
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:04:49.271
Feb 27 12:04:49.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 12:04:49.275
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:04:49.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:04:49.317
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 02/27/23 12:04:49.325
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 12:04:49.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3372" for this suite. 02/27/23 12:04:49.345
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":161,"skipped":3170,"failed":0}
------------------------------
• [0.090 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:04:49.271
    Feb 27 12:04:49.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 12:04:49.275
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:04:49.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:04:49.317
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 02/27/23 12:04:49.325
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 12:04:49.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3372" for this suite. 02/27/23 12:04:49.345
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:04:49.364
Feb 27 12:04:49.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename ingress 02/27/23 12:04:49.366
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:04:49.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:04:49.414
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 02/27/23 12:04:49.429
STEP: getting /apis/networking.k8s.io 02/27/23 12:04:49.439
STEP: getting /apis/networking.k8s.iov1 02/27/23 12:04:49.443
STEP: creating 02/27/23 12:04:49.449
STEP: getting 02/27/23 12:04:49.487
STEP: listing 02/27/23 12:04:49.495
STEP: watching 02/27/23 12:04:49.504
Feb 27 12:04:49.504: INFO: starting watch
STEP: cluster-wide listing 02/27/23 12:04:49.509
STEP: cluster-wide watching 02/27/23 12:04:49.518
Feb 27 12:04:49.518: INFO: starting watch
STEP: patching 02/27/23 12:04:49.522
STEP: updating 02/27/23 12:04:49.537
Feb 27 12:04:49.561: INFO: waiting for watch events with expected annotations
Feb 27 12:04:49.561: INFO: saw patched and updated annotations
STEP: patching /status 02/27/23 12:04:49.561
STEP: updating /status 02/27/23 12:04:49.571
STEP: get /status 02/27/23 12:04:49.588
STEP: deleting 02/27/23 12:04:49.595
STEP: deleting a collection 02/27/23 12:04:49.621
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Feb 27 12:04:49.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9941" for this suite. 02/27/23 12:04:49.66
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":162,"skipped":3173,"failed":0}
------------------------------
• [0.314 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:04:49.364
    Feb 27 12:04:49.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename ingress 02/27/23 12:04:49.366
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:04:49.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:04:49.414
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 02/27/23 12:04:49.429
    STEP: getting /apis/networking.k8s.io 02/27/23 12:04:49.439
    STEP: getting /apis/networking.k8s.iov1 02/27/23 12:04:49.443
    STEP: creating 02/27/23 12:04:49.449
    STEP: getting 02/27/23 12:04:49.487
    STEP: listing 02/27/23 12:04:49.495
    STEP: watching 02/27/23 12:04:49.504
    Feb 27 12:04:49.504: INFO: starting watch
    STEP: cluster-wide listing 02/27/23 12:04:49.509
    STEP: cluster-wide watching 02/27/23 12:04:49.518
    Feb 27 12:04:49.518: INFO: starting watch
    STEP: patching 02/27/23 12:04:49.522
    STEP: updating 02/27/23 12:04:49.537
    Feb 27 12:04:49.561: INFO: waiting for watch events with expected annotations
    Feb 27 12:04:49.561: INFO: saw patched and updated annotations
    STEP: patching /status 02/27/23 12:04:49.561
    STEP: updating /status 02/27/23 12:04:49.571
    STEP: get /status 02/27/23 12:04:49.588
    STEP: deleting 02/27/23 12:04:49.595
    STEP: deleting a collection 02/27/23 12:04:49.621
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Feb 27 12:04:49.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-9941" for this suite. 02/27/23 12:04:49.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:04:49.685
Feb 27 12:04:49.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 12:04:49.687
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:04:49.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:04:49.734
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 12:04:49.821
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 12:04:50.769
STEP: Deploying the webhook pod 02/27/23 12:04:50.811
STEP: Wait for the deployment to be ready 02/27/23 12:04:50.881
Feb 27 12:04:50.905: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 12:04:52.934
STEP: Verifying the service has paired with the endpoint 02/27/23 12:04:52.948
Feb 27 12:04:53.949: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/27/23 12:04:53.959
STEP: create a namespace for the webhook 02/27/23 12:04:53.99
STEP: create a configmap should be unconditionally rejected by the webhook 02/27/23 12:04:54.007
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:04:54.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9476" for this suite. 02/27/23 12:04:54.079
STEP: Destroying namespace "webhook-9476-markers" for this suite. 02/27/23 12:04:54.092
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":163,"skipped":3179,"failed":0}
------------------------------
• [4.514 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:04:49.685
    Feb 27 12:04:49.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 12:04:49.687
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:04:49.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:04:49.734
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 12:04:49.821
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 12:04:50.769
    STEP: Deploying the webhook pod 02/27/23 12:04:50.811
    STEP: Wait for the deployment to be ready 02/27/23 12:04:50.881
    Feb 27 12:04:50.905: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 12:04:52.934
    STEP: Verifying the service has paired with the endpoint 02/27/23 12:04:52.948
    Feb 27 12:04:53.949: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/27/23 12:04:53.959
    STEP: create a namespace for the webhook 02/27/23 12:04:53.99
    STEP: create a configmap should be unconditionally rejected by the webhook 02/27/23 12:04:54.007
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:04:54.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9476" for this suite. 02/27/23 12:04:54.079
    STEP: Destroying namespace "webhook-9476-markers" for this suite. 02/27/23 12:04:54.092
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:04:54.207
Feb 27 12:04:54.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 12:04:54.208
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:04:54.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:04:54.249
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Feb 27 12:04:54.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 12:04:59.437
Feb 27 12:04:59.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-8384 --namespace=crd-publish-openapi-8384 create -f -'
Feb 27 12:05:00.745: INFO: stderr: ""
Feb 27 12:05:00.745: INFO: stdout: "e2e-test-crd-publish-openapi-8539-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 27 12:05:00.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-8384 --namespace=crd-publish-openapi-8384 delete e2e-test-crd-publish-openapi-8539-crds test-cr'
Feb 27 12:05:00.954: INFO: stderr: ""
Feb 27 12:05:00.954: INFO: stdout: "e2e-test-crd-publish-openapi-8539-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 27 12:05:00.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-8384 --namespace=crd-publish-openapi-8384 apply -f -'
Feb 27 12:05:02.232: INFO: stderr: ""
Feb 27 12:05:02.232: INFO: stdout: "e2e-test-crd-publish-openapi-8539-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 27 12:05:02.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-8384 --namespace=crd-publish-openapi-8384 delete e2e-test-crd-publish-openapi-8539-crds test-cr'
Feb 27 12:05:02.348: INFO: stderr: ""
Feb 27 12:05:02.348: INFO: stdout: "e2e-test-crd-publish-openapi-8539-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/27/23 12:05:02.348
Feb 27 12:05:02.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-8384 explain e2e-test-crd-publish-openapi-8539-crds'
Feb 27 12:05:02.667: INFO: stderr: ""
Feb 27 12:05:02.667: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8539-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:05:07.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8384" for this suite. 02/27/23 12:05:07.684
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":164,"skipped":3211,"failed":0}
------------------------------
• [SLOW TEST] [13.492 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:04:54.207
    Feb 27 12:04:54.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 12:04:54.208
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:04:54.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:04:54.249
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Feb 27 12:04:54.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 12:04:59.437
    Feb 27 12:04:59.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-8384 --namespace=crd-publish-openapi-8384 create -f -'
    Feb 27 12:05:00.745: INFO: stderr: ""
    Feb 27 12:05:00.745: INFO: stdout: "e2e-test-crd-publish-openapi-8539-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 27 12:05:00.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-8384 --namespace=crd-publish-openapi-8384 delete e2e-test-crd-publish-openapi-8539-crds test-cr'
    Feb 27 12:05:00.954: INFO: stderr: ""
    Feb 27 12:05:00.954: INFO: stdout: "e2e-test-crd-publish-openapi-8539-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Feb 27 12:05:00.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-8384 --namespace=crd-publish-openapi-8384 apply -f -'
    Feb 27 12:05:02.232: INFO: stderr: ""
    Feb 27 12:05:02.232: INFO: stdout: "e2e-test-crd-publish-openapi-8539-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 27 12:05:02.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-8384 --namespace=crd-publish-openapi-8384 delete e2e-test-crd-publish-openapi-8539-crds test-cr'
    Feb 27 12:05:02.348: INFO: stderr: ""
    Feb 27 12:05:02.348: INFO: stdout: "e2e-test-crd-publish-openapi-8539-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/27/23 12:05:02.348
    Feb 27 12:05:02.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-8384 explain e2e-test-crd-publish-openapi-8539-crds'
    Feb 27 12:05:02.667: INFO: stderr: ""
    Feb 27 12:05:02.667: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8539-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:05:07.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8384" for this suite. 02/27/23 12:05:07.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:05:07.702
Feb 27 12:05:07.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 12:05:07.703
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:07.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:07.742
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-933a2461-e8c5-4c93-958d-fdcfa320d7a4 02/27/23 12:05:07.752
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 12:05:07.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7314" for this suite. 02/27/23 12:05:07.777
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":165,"skipped":3233,"failed":0}
------------------------------
• [0.088 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:05:07.702
    Feb 27 12:05:07.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 12:05:07.703
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:07.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:07.742
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-933a2461-e8c5-4c93-958d-fdcfa320d7a4 02/27/23 12:05:07.752
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 12:05:07.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7314" for this suite. 02/27/23 12:05:07.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:05:07.795
Feb 27 12:05:07.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename cronjob 02/27/23 12:05:07.797
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:07.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:07.838
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 02/27/23 12:05:07.846
STEP: creating 02/27/23 12:05:07.846
STEP: getting 02/27/23 12:05:07.858
STEP: listing 02/27/23 12:05:07.865
STEP: watching 02/27/23 12:05:07.876
Feb 27 12:05:07.876: INFO: starting watch
STEP: cluster-wide listing 02/27/23 12:05:07.88
STEP: cluster-wide watching 02/27/23 12:05:07.887
Feb 27 12:05:07.888: INFO: starting watch
STEP: patching 02/27/23 12:05:07.892
STEP: updating 02/27/23 12:05:08.251
Feb 27 12:05:08.276: INFO: waiting for watch events with expected annotations
Feb 27 12:05:08.276: INFO: saw patched and updated annotations
STEP: patching /status 02/27/23 12:05:08.276
STEP: updating /status 02/27/23 12:05:08.286
STEP: get /status 02/27/23 12:05:08.302
STEP: deleting 02/27/23 12:05:08.309
STEP: deleting a collection 02/27/23 12:05:08.341
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 27 12:05:08.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7108" for this suite. 02/27/23 12:05:08.372
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":166,"skipped":3250,"failed":0}
------------------------------
• [0.591 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:05:07.795
    Feb 27 12:05:07.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename cronjob 02/27/23 12:05:07.797
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:07.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:07.838
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 02/27/23 12:05:07.846
    STEP: creating 02/27/23 12:05:07.846
    STEP: getting 02/27/23 12:05:07.858
    STEP: listing 02/27/23 12:05:07.865
    STEP: watching 02/27/23 12:05:07.876
    Feb 27 12:05:07.876: INFO: starting watch
    STEP: cluster-wide listing 02/27/23 12:05:07.88
    STEP: cluster-wide watching 02/27/23 12:05:07.887
    Feb 27 12:05:07.888: INFO: starting watch
    STEP: patching 02/27/23 12:05:07.892
    STEP: updating 02/27/23 12:05:08.251
    Feb 27 12:05:08.276: INFO: waiting for watch events with expected annotations
    Feb 27 12:05:08.276: INFO: saw patched and updated annotations
    STEP: patching /status 02/27/23 12:05:08.276
    STEP: updating /status 02/27/23 12:05:08.286
    STEP: get /status 02/27/23 12:05:08.302
    STEP: deleting 02/27/23 12:05:08.309
    STEP: deleting a collection 02/27/23 12:05:08.341
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 27 12:05:08.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7108" for this suite. 02/27/23 12:05:08.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:05:08.393
Feb 27 12:05:08.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 12:05:08.394
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:08.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:08.434
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-2b5d37c8-5de1-4438-8ce1-5c261754208c 02/27/23 12:05:08.443
STEP: Creating a pod to test consume configMaps 02/27/23 12:05:08.453
Feb 27 12:05:08.471: INFO: Waiting up to 5m0s for pod "pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3" in namespace "configmap-8095" to be "Succeeded or Failed"
Feb 27 12:05:08.479: INFO: Pod "pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.437555ms
Feb 27 12:05:10.487: INFO: Pod "pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016424346s
Feb 27 12:05:12.490: INFO: Pod "pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019646116s
STEP: Saw pod success 02/27/23 12:05:12.49
Feb 27 12:05:12.491: INFO: Pod "pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3" satisfied condition "Succeeded or Failed"
Feb 27 12:05:12.499: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 12:05:12.525
Feb 27 12:05:12.545: INFO: Waiting for pod pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3 to disappear
Feb 27 12:05:12.555: INFO: Pod pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 12:05:12.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8095" for this suite. 02/27/23 12:05:12.572
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":167,"skipped":3277,"failed":0}
------------------------------
• [4.193 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:05:08.393
    Feb 27 12:05:08.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 12:05:08.394
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:08.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:08.434
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-2b5d37c8-5de1-4438-8ce1-5c261754208c 02/27/23 12:05:08.443
    STEP: Creating a pod to test consume configMaps 02/27/23 12:05:08.453
    Feb 27 12:05:08.471: INFO: Waiting up to 5m0s for pod "pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3" in namespace "configmap-8095" to be "Succeeded or Failed"
    Feb 27 12:05:08.479: INFO: Pod "pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.437555ms
    Feb 27 12:05:10.487: INFO: Pod "pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016424346s
    Feb 27 12:05:12.490: INFO: Pod "pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019646116s
    STEP: Saw pod success 02/27/23 12:05:12.49
    Feb 27 12:05:12.491: INFO: Pod "pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3" satisfied condition "Succeeded or Failed"
    Feb 27 12:05:12.499: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 12:05:12.525
    Feb 27 12:05:12.545: INFO: Waiting for pod pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3 to disappear
    Feb 27 12:05:12.555: INFO: Pod pod-configmaps-457e5e59-66c2-4edb-9233-a3371cf487b3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 12:05:12.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8095" for this suite. 02/27/23 12:05:12.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:05:12.592
Feb 27 12:05:12.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename replication-controller 02/27/23 12:05:12.593
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:12.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:12.641
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 02/27/23 12:05:12.672
STEP: waiting for RC to be added 02/27/23 12:05:12.682
STEP: waiting for available Replicas 02/27/23 12:05:12.683
STEP: patching ReplicationController 02/27/23 12:05:13.896
STEP: waiting for RC to be modified 02/27/23 12:05:13.928
STEP: patching ReplicationController status 02/27/23 12:05:13.928
STEP: waiting for RC to be modified 02/27/23 12:05:13.94
STEP: waiting for available Replicas 02/27/23 12:05:13.94
STEP: fetching ReplicationController status 02/27/23 12:05:13.946
STEP: patching ReplicationController scale 02/27/23 12:05:13.955
STEP: waiting for RC to be modified 02/27/23 12:05:13.968
STEP: waiting for ReplicationController's scale to be the max amount 02/27/23 12:05:13.968
STEP: fetching ReplicationController; ensuring that it's patched 02/27/23 12:05:14.943
STEP: updating ReplicationController status 02/27/23 12:05:14.955
STEP: waiting for RC to be modified 02/27/23 12:05:14.967
STEP: listing all ReplicationControllers 02/27/23 12:05:14.967
STEP: checking that ReplicationController has expected values 02/27/23 12:05:14.976
STEP: deleting ReplicationControllers by collection 02/27/23 12:05:14.976
STEP: waiting for ReplicationController to have a DELETED watchEvent 02/27/23 12:05:14.993
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 27 12:05:15.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1853" for this suite. 02/27/23 12:05:15.121
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":168,"skipped":3306,"failed":0}
------------------------------
• [2.547 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:05:12.592
    Feb 27 12:05:12.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename replication-controller 02/27/23 12:05:12.593
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:12.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:12.641
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 02/27/23 12:05:12.672
    STEP: waiting for RC to be added 02/27/23 12:05:12.682
    STEP: waiting for available Replicas 02/27/23 12:05:12.683
    STEP: patching ReplicationController 02/27/23 12:05:13.896
    STEP: waiting for RC to be modified 02/27/23 12:05:13.928
    STEP: patching ReplicationController status 02/27/23 12:05:13.928
    STEP: waiting for RC to be modified 02/27/23 12:05:13.94
    STEP: waiting for available Replicas 02/27/23 12:05:13.94
    STEP: fetching ReplicationController status 02/27/23 12:05:13.946
    STEP: patching ReplicationController scale 02/27/23 12:05:13.955
    STEP: waiting for RC to be modified 02/27/23 12:05:13.968
    STEP: waiting for ReplicationController's scale to be the max amount 02/27/23 12:05:13.968
    STEP: fetching ReplicationController; ensuring that it's patched 02/27/23 12:05:14.943
    STEP: updating ReplicationController status 02/27/23 12:05:14.955
    STEP: waiting for RC to be modified 02/27/23 12:05:14.967
    STEP: listing all ReplicationControllers 02/27/23 12:05:14.967
    STEP: checking that ReplicationController has expected values 02/27/23 12:05:14.976
    STEP: deleting ReplicationControllers by collection 02/27/23 12:05:14.976
    STEP: waiting for ReplicationController to have a DELETED watchEvent 02/27/23 12:05:14.993
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 27 12:05:15.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1853" for this suite. 02/27/23 12:05:15.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:05:15.143
Feb 27 12:05:15.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir-wrapper 02/27/23 12:05:15.149
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:15.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:15.193
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Feb 27 12:05:15.251: INFO: Waiting up to 5m0s for pod "pod-secrets-72557c59-1c21-43eb-8fae-f4b159d73c65" in namespace "emptydir-wrapper-8814" to be "running and ready"
Feb 27 12:05:15.275: INFO: Pod "pod-secrets-72557c59-1c21-43eb-8fae-f4b159d73c65": Phase="Pending", Reason="", readiness=false. Elapsed: 22.967754ms
Feb 27 12:05:15.275: INFO: The phase of Pod pod-secrets-72557c59-1c21-43eb-8fae-f4b159d73c65 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:05:17.285: INFO: Pod "pod-secrets-72557c59-1c21-43eb-8fae-f4b159d73c65": Phase="Running", Reason="", readiness=true. Elapsed: 2.032930067s
Feb 27 12:05:17.285: INFO: The phase of Pod pod-secrets-72557c59-1c21-43eb-8fae-f4b159d73c65 is Running (Ready = true)
Feb 27 12:05:17.285: INFO: Pod "pod-secrets-72557c59-1c21-43eb-8fae-f4b159d73c65" satisfied condition "running and ready"
STEP: Cleaning up the secret 02/27/23 12:05:17.292
STEP: Cleaning up the configmap 02/27/23 12:05:17.305
STEP: Cleaning up the pod 02/27/23 12:05:17.32
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Feb 27 12:05:17.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8814" for this suite. 02/27/23 12:05:17.364
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":169,"skipped":3322,"failed":0}
------------------------------
• [2.237 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:05:15.143
    Feb 27 12:05:15.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir-wrapper 02/27/23 12:05:15.149
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:15.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:15.193
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Feb 27 12:05:15.251: INFO: Waiting up to 5m0s for pod "pod-secrets-72557c59-1c21-43eb-8fae-f4b159d73c65" in namespace "emptydir-wrapper-8814" to be "running and ready"
    Feb 27 12:05:15.275: INFO: Pod "pod-secrets-72557c59-1c21-43eb-8fae-f4b159d73c65": Phase="Pending", Reason="", readiness=false. Elapsed: 22.967754ms
    Feb 27 12:05:15.275: INFO: The phase of Pod pod-secrets-72557c59-1c21-43eb-8fae-f4b159d73c65 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:05:17.285: INFO: Pod "pod-secrets-72557c59-1c21-43eb-8fae-f4b159d73c65": Phase="Running", Reason="", readiness=true. Elapsed: 2.032930067s
    Feb 27 12:05:17.285: INFO: The phase of Pod pod-secrets-72557c59-1c21-43eb-8fae-f4b159d73c65 is Running (Ready = true)
    Feb 27 12:05:17.285: INFO: Pod "pod-secrets-72557c59-1c21-43eb-8fae-f4b159d73c65" satisfied condition "running and ready"
    STEP: Cleaning up the secret 02/27/23 12:05:17.292
    STEP: Cleaning up the configmap 02/27/23 12:05:17.305
    STEP: Cleaning up the pod 02/27/23 12:05:17.32
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Feb 27 12:05:17.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-8814" for this suite. 02/27/23 12:05:17.364
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:05:17.383
Feb 27 12:05:17.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 12:05:17.385
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:17.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:17.424
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-30379c25-770f-4673-b99e-b94f1d798746 02/27/23 12:05:17.442
STEP: Creating a pod to test consume secrets 02/27/23 12:05:17.458
Feb 27 12:05:17.474: INFO: Waiting up to 5m0s for pod "pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084" in namespace "secrets-5980" to be "Succeeded or Failed"
Feb 27 12:05:17.486: INFO: Pod "pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084": Phase="Pending", Reason="", readiness=false. Elapsed: 11.688347ms
Feb 27 12:05:19.496: INFO: Pod "pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022197187s
Feb 27 12:05:21.497: INFO: Pod "pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02294423s
STEP: Saw pod success 02/27/23 12:05:21.497
Feb 27 12:05:21.497: INFO: Pod "pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084" satisfied condition "Succeeded or Failed"
Feb 27 12:05:21.522: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 12:05:21.542
Feb 27 12:05:21.572: INFO: Waiting for pod pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084 to disappear
Feb 27 12:05:21.600: INFO: Pod pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 27 12:05:21.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5980" for this suite. 02/27/23 12:05:21.617
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":170,"skipped":3325,"failed":0}
------------------------------
• [4.252 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:05:17.383
    Feb 27 12:05:17.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 12:05:17.385
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:17.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:17.424
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-30379c25-770f-4673-b99e-b94f1d798746 02/27/23 12:05:17.442
    STEP: Creating a pod to test consume secrets 02/27/23 12:05:17.458
    Feb 27 12:05:17.474: INFO: Waiting up to 5m0s for pod "pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084" in namespace "secrets-5980" to be "Succeeded or Failed"
    Feb 27 12:05:17.486: INFO: Pod "pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084": Phase="Pending", Reason="", readiness=false. Elapsed: 11.688347ms
    Feb 27 12:05:19.496: INFO: Pod "pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022197187s
    Feb 27 12:05:21.497: INFO: Pod "pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02294423s
    STEP: Saw pod success 02/27/23 12:05:21.497
    Feb 27 12:05:21.497: INFO: Pod "pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084" satisfied condition "Succeeded or Failed"
    Feb 27 12:05:21.522: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 12:05:21.542
    Feb 27 12:05:21.572: INFO: Waiting for pod pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084 to disappear
    Feb 27 12:05:21.600: INFO: Pod pod-secrets-5b528d01-6a36-4168-9334-5c47d35dd084 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 12:05:21.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5980" for this suite. 02/27/23 12:05:21.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:05:21.639
Feb 27 12:05:21.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename subpath 02/27/23 12:05:21.641
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:21.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:21.694
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 12:05:21.702
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-6ltq 02/27/23 12:05:21.725
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 12:05:21.725
Feb 27 12:05:21.740: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6ltq" in namespace "subpath-9117" to be "Succeeded or Failed"
Feb 27 12:05:21.749: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.068996ms
Feb 27 12:05:23.759: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 2.019580007s
Feb 27 12:05:25.757: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 4.017132787s
Feb 27 12:05:27.757: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 6.017717508s
Feb 27 12:05:29.757: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 8.017133593s
Feb 27 12:05:31.756: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 10.016612048s
Feb 27 12:05:33.757: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 12.017084994s
Feb 27 12:05:35.760: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 14.01992275s
Feb 27 12:05:37.760: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 16.019914587s
Feb 27 12:05:39.756: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 18.016741818s
Feb 27 12:05:41.758: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 20.017910737s
Feb 27 12:05:43.758: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=false. Elapsed: 22.018503367s
Feb 27 12:05:45.757: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016986994s
STEP: Saw pod success 02/27/23 12:05:45.757
Feb 27 12:05:45.757: INFO: Pod "pod-subpath-test-configmap-6ltq" satisfied condition "Succeeded or Failed"
Feb 27 12:05:45.765: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-subpath-test-configmap-6ltq container test-container-subpath-configmap-6ltq: <nil>
STEP: delete the pod 02/27/23 12:05:45.782
Feb 27 12:05:45.808: INFO: Waiting for pod pod-subpath-test-configmap-6ltq to disappear
Feb 27 12:05:45.819: INFO: Pod pod-subpath-test-configmap-6ltq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6ltq 02/27/23 12:05:45.819
Feb 27 12:05:45.819: INFO: Deleting pod "pod-subpath-test-configmap-6ltq" in namespace "subpath-9117"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 27 12:05:45.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9117" for this suite. 02/27/23 12:05:45.842
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":171,"skipped":3354,"failed":0}
------------------------------
• [SLOW TEST] [24.221 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:05:21.639
    Feb 27 12:05:21.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename subpath 02/27/23 12:05:21.641
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:21.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:21.694
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 12:05:21.702
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-6ltq 02/27/23 12:05:21.725
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 12:05:21.725
    Feb 27 12:05:21.740: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6ltq" in namespace "subpath-9117" to be "Succeeded or Failed"
    Feb 27 12:05:21.749: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Pending", Reason="", readiness=false. Elapsed: 9.068996ms
    Feb 27 12:05:23.759: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 2.019580007s
    Feb 27 12:05:25.757: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 4.017132787s
    Feb 27 12:05:27.757: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 6.017717508s
    Feb 27 12:05:29.757: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 8.017133593s
    Feb 27 12:05:31.756: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 10.016612048s
    Feb 27 12:05:33.757: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 12.017084994s
    Feb 27 12:05:35.760: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 14.01992275s
    Feb 27 12:05:37.760: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 16.019914587s
    Feb 27 12:05:39.756: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 18.016741818s
    Feb 27 12:05:41.758: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=true. Elapsed: 20.017910737s
    Feb 27 12:05:43.758: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Running", Reason="", readiness=false. Elapsed: 22.018503367s
    Feb 27 12:05:45.757: INFO: Pod "pod-subpath-test-configmap-6ltq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016986994s
    STEP: Saw pod success 02/27/23 12:05:45.757
    Feb 27 12:05:45.757: INFO: Pod "pod-subpath-test-configmap-6ltq" satisfied condition "Succeeded or Failed"
    Feb 27 12:05:45.765: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-subpath-test-configmap-6ltq container test-container-subpath-configmap-6ltq: <nil>
    STEP: delete the pod 02/27/23 12:05:45.782
    Feb 27 12:05:45.808: INFO: Waiting for pod pod-subpath-test-configmap-6ltq to disappear
    Feb 27 12:05:45.819: INFO: Pod pod-subpath-test-configmap-6ltq no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-6ltq 02/27/23 12:05:45.819
    Feb 27 12:05:45.819: INFO: Deleting pod "pod-subpath-test-configmap-6ltq" in namespace "subpath-9117"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 27 12:05:45.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9117" for this suite. 02/27/23 12:05:45.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:05:45.866
Feb 27 12:05:45.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename var-expansion 02/27/23 12:05:45.867
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:45.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:45.916
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Feb 27 12:05:45.940: INFO: Waiting up to 2m0s for pod "var-expansion-6b0dfcbd-a5d7-467d-a3c5-cf9b1bb36145" in namespace "var-expansion-4358" to be "container 0 failed with reason CreateContainerConfigError"
Feb 27 12:05:45.951: INFO: Pod "var-expansion-6b0dfcbd-a5d7-467d-a3c5-cf9b1bb36145": Phase="Pending", Reason="", readiness=false. Elapsed: 10.699603ms
Feb 27 12:05:47.967: INFO: Pod "var-expansion-6b0dfcbd-a5d7-467d-a3c5-cf9b1bb36145": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026541545s
Feb 27 12:05:47.967: INFO: Pod "var-expansion-6b0dfcbd-a5d7-467d-a3c5-cf9b1bb36145" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 27 12:05:47.967: INFO: Deleting pod "var-expansion-6b0dfcbd-a5d7-467d-a3c5-cf9b1bb36145" in namespace "var-expansion-4358"
Feb 27 12:05:47.982: INFO: Wait up to 5m0s for pod "var-expansion-6b0dfcbd-a5d7-467d-a3c5-cf9b1bb36145" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 27 12:05:51.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4358" for this suite. 02/27/23 12:05:52.014
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":172,"skipped":3381,"failed":0}
------------------------------
• [SLOW TEST] [6.167 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:05:45.866
    Feb 27 12:05:45.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename var-expansion 02/27/23 12:05:45.867
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:45.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:45.916
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Feb 27 12:05:45.940: INFO: Waiting up to 2m0s for pod "var-expansion-6b0dfcbd-a5d7-467d-a3c5-cf9b1bb36145" in namespace "var-expansion-4358" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 27 12:05:45.951: INFO: Pod "var-expansion-6b0dfcbd-a5d7-467d-a3c5-cf9b1bb36145": Phase="Pending", Reason="", readiness=false. Elapsed: 10.699603ms
    Feb 27 12:05:47.967: INFO: Pod "var-expansion-6b0dfcbd-a5d7-467d-a3c5-cf9b1bb36145": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026541545s
    Feb 27 12:05:47.967: INFO: Pod "var-expansion-6b0dfcbd-a5d7-467d-a3c5-cf9b1bb36145" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 27 12:05:47.967: INFO: Deleting pod "var-expansion-6b0dfcbd-a5d7-467d-a3c5-cf9b1bb36145" in namespace "var-expansion-4358"
    Feb 27 12:05:47.982: INFO: Wait up to 5m0s for pod "var-expansion-6b0dfcbd-a5d7-467d-a3c5-cf9b1bb36145" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 27 12:05:51.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4358" for this suite. 02/27/23 12:05:52.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:05:52.036
Feb 27 12:05:52.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename subpath 02/27/23 12:05:52.037
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:52.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:52.07
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 12:05:52.081
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-np2t 02/27/23 12:05:52.103
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 12:05:52.103
Feb 27 12:05:52.116: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-np2t" in namespace "subpath-2590" to be "Succeeded or Failed"
Feb 27 12:05:52.135: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Pending", Reason="", readiness=false. Elapsed: 18.259046ms
Feb 27 12:05:54.144: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 2.027096498s
Feb 27 12:05:56.148: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 4.03138869s
Feb 27 12:05:58.146: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 6.029346641s
Feb 27 12:06:00.143: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 8.026831355s
Feb 27 12:06:02.144: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 10.027158813s
Feb 27 12:06:04.149: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 12.03248827s
Feb 27 12:06:06.147: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 14.030956953s
Feb 27 12:06:08.148: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 16.031699923s
Feb 27 12:06:10.145: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 18.028695776s
Feb 27 12:06:12.144: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 20.027710195s
Feb 27 12:06:14.152: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=false. Elapsed: 22.035082759s
Feb 27 12:06:16.144: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.02738961s
STEP: Saw pod success 02/27/23 12:06:16.144
Feb 27 12:06:16.144: INFO: Pod "pod-subpath-test-configmap-np2t" satisfied condition "Succeeded or Failed"
Feb 27 12:06:16.151: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-subpath-test-configmap-np2t container test-container-subpath-configmap-np2t: <nil>
STEP: delete the pod 02/27/23 12:06:16.172
Feb 27 12:06:16.201: INFO: Waiting for pod pod-subpath-test-configmap-np2t to disappear
Feb 27 12:06:16.209: INFO: Pod pod-subpath-test-configmap-np2t no longer exists
STEP: Deleting pod pod-subpath-test-configmap-np2t 02/27/23 12:06:16.209
Feb 27 12:06:16.209: INFO: Deleting pod "pod-subpath-test-configmap-np2t" in namespace "subpath-2590"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 27 12:06:16.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2590" for this suite. 02/27/23 12:06:16.232
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":173,"skipped":3387,"failed":0}
------------------------------
• [SLOW TEST] [24.213 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:05:52.036
    Feb 27 12:05:52.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename subpath 02/27/23 12:05:52.037
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:05:52.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:05:52.07
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 12:05:52.081
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-np2t 02/27/23 12:05:52.103
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 12:05:52.103
    Feb 27 12:05:52.116: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-np2t" in namespace "subpath-2590" to be "Succeeded or Failed"
    Feb 27 12:05:52.135: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Pending", Reason="", readiness=false. Elapsed: 18.259046ms
    Feb 27 12:05:54.144: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 2.027096498s
    Feb 27 12:05:56.148: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 4.03138869s
    Feb 27 12:05:58.146: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 6.029346641s
    Feb 27 12:06:00.143: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 8.026831355s
    Feb 27 12:06:02.144: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 10.027158813s
    Feb 27 12:06:04.149: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 12.03248827s
    Feb 27 12:06:06.147: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 14.030956953s
    Feb 27 12:06:08.148: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 16.031699923s
    Feb 27 12:06:10.145: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 18.028695776s
    Feb 27 12:06:12.144: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=true. Elapsed: 20.027710195s
    Feb 27 12:06:14.152: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Running", Reason="", readiness=false. Elapsed: 22.035082759s
    Feb 27 12:06:16.144: INFO: Pod "pod-subpath-test-configmap-np2t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.02738961s
    STEP: Saw pod success 02/27/23 12:06:16.144
    Feb 27 12:06:16.144: INFO: Pod "pod-subpath-test-configmap-np2t" satisfied condition "Succeeded or Failed"
    Feb 27 12:06:16.151: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-subpath-test-configmap-np2t container test-container-subpath-configmap-np2t: <nil>
    STEP: delete the pod 02/27/23 12:06:16.172
    Feb 27 12:06:16.201: INFO: Waiting for pod pod-subpath-test-configmap-np2t to disappear
    Feb 27 12:06:16.209: INFO: Pod pod-subpath-test-configmap-np2t no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-np2t 02/27/23 12:06:16.209
    Feb 27 12:06:16.209: INFO: Deleting pod "pod-subpath-test-configmap-np2t" in namespace "subpath-2590"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 27 12:06:16.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2590" for this suite. 02/27/23 12:06:16.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:06:16.254
Feb 27 12:06:16.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename statefulset 02/27/23 12:06:16.255
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:06:16.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:06:16.293
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6011 02/27/23 12:06:16.302
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-6011 02/27/23 12:06:16.326
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6011 02/27/23 12:06:16.337
Feb 27 12:06:16.352: INFO: Found 0 stateful pods, waiting for 1
Feb 27 12:06:26.361: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/27/23 12:06:26.361
Feb 27 12:06:26.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 12:06:26.593: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 12:06:26.593: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 12:06:26.593: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 12:06:26.602: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 27 12:06:36.615: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 12:06:36.615: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 12:06:36.653: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Feb 27 12:06:36.653: INFO: ss-0  ip-172-31-11-159.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:16 +0000 UTC  }]
Feb 27 12:06:36.653: INFO: ss-1                                                  Pending         []
Feb 27 12:06:36.653: INFO: 
Feb 27 12:06:36.653: INFO: StatefulSet ss has not reached scale 3, at 2
Feb 27 12:06:37.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989240796s
Feb 27 12:06:38.672: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980141499s
Feb 27 12:06:39.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.969074784s
Feb 27 12:06:40.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.958066326s
Feb 27 12:06:41.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.949391939s
Feb 27 12:06:42.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.933577873s
Feb 27 12:06:43.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.913765503s
Feb 27 12:06:44.749: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.903982331s
Feb 27 12:06:45.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 893.390876ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6011 02/27/23 12:06:46.767
Feb 27 12:06:46.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 12:06:46.994: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 12:06:46.994: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 12:06:46.994: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 12:06:46.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 12:06:47.254: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 27 12:06:47.254: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 12:06:47.254: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 12:06:47.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 12:06:47.524: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 27 12:06:47.524: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 12:06:47.524: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 12:06:47.534: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 27 12:06:57.546: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 12:06:57.546: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 12:06:57.546: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 02/27/23 12:06:57.546
Feb 27 12:06:57.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 12:06:57.791: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 12:06:57.791: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 12:06:57.791: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 12:06:57.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 12:06:58.049: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 12:06:58.049: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 12:06:58.049: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 12:06:58.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 12:06:58.296: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 12:06:58.296: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 12:06:58.296: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 12:06:58.296: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 12:06:58.305: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 27 12:07:08.326: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 12:07:08.329: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 12:07:08.329: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 12:07:08.426: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Feb 27 12:07:08.426: INFO: ss-0  ip-172-31-11-159.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:16 +0000 UTC  }]
Feb 27 12:07:08.427: INFO: ss-1  ip-172-31-15-17.eu-central-1.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:36 +0000 UTC  }]
Feb 27 12:07:08.427: INFO: ss-2  ip-172-31-7-167.eu-central-1.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:36 +0000 UTC  }]
Feb 27 12:07:08.427: INFO: 
Feb 27 12:07:08.427: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 27 12:07:09.442: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.986790109s
Feb 27 12:07:10.450: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.972851081s
Feb 27 12:07:11.458: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.965558263s
Feb 27 12:07:12.466: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.957346851s
Feb 27 12:07:13.474: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.948530375s
Feb 27 12:07:14.481: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.941091683s
Feb 27 12:07:15.491: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.933650254s
Feb 27 12:07:16.504: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.924440093s
Feb 27 12:07:17.513: INFO: Verifying statefulset ss doesn't scale past 0 for another 910.871757ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6011 02/27/23 12:07:18.514
Feb 27 12:07:18.522: INFO: Scaling statefulset ss to 0
Feb 27 12:07:18.554: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 27 12:07:18.569: INFO: Deleting all statefulset in ns statefulset-6011
Feb 27 12:07:18.577: INFO: Scaling statefulset ss to 0
Feb 27 12:07:18.607: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 12:07:18.616: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 27 12:07:18.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6011" for this suite. 02/27/23 12:07:18.665
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":174,"skipped":3411,"failed":0}
------------------------------
• [SLOW TEST] [62.428 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:06:16.254
    Feb 27 12:06:16.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename statefulset 02/27/23 12:06:16.255
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:06:16.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:06:16.293
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6011 02/27/23 12:06:16.302
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-6011 02/27/23 12:06:16.326
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6011 02/27/23 12:06:16.337
    Feb 27 12:06:16.352: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 12:06:26.361: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/27/23 12:06:26.361
    Feb 27 12:06:26.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 12:06:26.593: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 12:06:26.593: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 12:06:26.593: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 12:06:26.602: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 27 12:06:36.615: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 12:06:36.615: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 12:06:36.653: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
    Feb 27 12:06:36.653: INFO: ss-0  ip-172-31-11-159.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:16 +0000 UTC  }]
    Feb 27 12:06:36.653: INFO: ss-1                                                  Pending         []
    Feb 27 12:06:36.653: INFO: 
    Feb 27 12:06:36.653: INFO: StatefulSet ss has not reached scale 3, at 2
    Feb 27 12:06:37.662: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989240796s
    Feb 27 12:06:38.672: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980141499s
    Feb 27 12:06:39.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.969074784s
    Feb 27 12:06:40.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.958066326s
    Feb 27 12:06:41.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.949391939s
    Feb 27 12:06:42.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.933577873s
    Feb 27 12:06:43.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.913765503s
    Feb 27 12:06:44.749: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.903982331s
    Feb 27 12:06:45.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 893.390876ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6011 02/27/23 12:06:46.767
    Feb 27 12:06:46.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 12:06:46.994: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 12:06:46.994: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 12:06:46.994: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 12:06:46.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 12:06:47.254: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 27 12:06:47.254: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 12:06:47.254: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 12:06:47.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 12:06:47.524: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 27 12:06:47.524: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 12:06:47.524: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 12:06:47.534: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Feb 27 12:06:57.546: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 12:06:57.546: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 12:06:57.546: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 02/27/23 12:06:57.546
    Feb 27 12:06:57.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 12:06:57.791: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 12:06:57.791: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 12:06:57.791: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 12:06:57.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 12:06:58.049: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 12:06:58.049: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 12:06:58.049: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 12:06:58.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-6011 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 12:06:58.296: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 12:06:58.296: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 12:06:58.296: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 12:06:58.296: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 12:06:58.305: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Feb 27 12:07:08.326: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 12:07:08.329: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 12:07:08.329: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 12:07:08.426: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
    Feb 27 12:07:08.426: INFO: ss-0  ip-172-31-11-159.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:16 +0000 UTC  }]
    Feb 27 12:07:08.427: INFO: ss-1  ip-172-31-15-17.eu-central-1.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:36 +0000 UTC  }]
    Feb 27 12:07:08.427: INFO: ss-2  ip-172-31-7-167.eu-central-1.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:06:36 +0000 UTC  }]
    Feb 27 12:07:08.427: INFO: 
    Feb 27 12:07:08.427: INFO: StatefulSet ss has not reached scale 0, at 3
    Feb 27 12:07:09.442: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.986790109s
    Feb 27 12:07:10.450: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.972851081s
    Feb 27 12:07:11.458: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.965558263s
    Feb 27 12:07:12.466: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.957346851s
    Feb 27 12:07:13.474: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.948530375s
    Feb 27 12:07:14.481: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.941091683s
    Feb 27 12:07:15.491: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.933650254s
    Feb 27 12:07:16.504: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.924440093s
    Feb 27 12:07:17.513: INFO: Verifying statefulset ss doesn't scale past 0 for another 910.871757ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6011 02/27/23 12:07:18.514
    Feb 27 12:07:18.522: INFO: Scaling statefulset ss to 0
    Feb 27 12:07:18.554: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 27 12:07:18.569: INFO: Deleting all statefulset in ns statefulset-6011
    Feb 27 12:07:18.577: INFO: Scaling statefulset ss to 0
    Feb 27 12:07:18.607: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 12:07:18.616: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 27 12:07:18.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6011" for this suite. 02/27/23 12:07:18.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:07:18.688
Feb 27 12:07:18.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename svcaccounts 02/27/23 12:07:18.689
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:18.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:18.73
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  02/27/23 12:07:18.741
Feb 27 12:07:18.754: INFO: Waiting up to 5m0s for pod "test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573" in namespace "svcaccounts-2536" to be "Succeeded or Failed"
Feb 27 12:07:18.769: INFO: Pod "test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573": Phase="Pending", Reason="", readiness=false. Elapsed: 14.325439ms
Feb 27 12:07:20.779: INFO: Pod "test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024635054s
Feb 27 12:07:22.783: INFO: Pod "test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02899031s
STEP: Saw pod success 02/27/23 12:07:22.784
Feb 27 12:07:22.784: INFO: Pod "test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573" satisfied condition "Succeeded or Failed"
Feb 27 12:07:22.792: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 12:07:22.807
Feb 27 12:07:22.839: INFO: Waiting for pod test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573 to disappear
Feb 27 12:07:22.846: INFO: Pod test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 27 12:07:22.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2536" for this suite. 02/27/23 12:07:22.86
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":175,"skipped":3449,"failed":0}
------------------------------
• [4.184 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:07:18.688
    Feb 27 12:07:18.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 12:07:18.689
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:18.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:18.73
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  02/27/23 12:07:18.741
    Feb 27 12:07:18.754: INFO: Waiting up to 5m0s for pod "test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573" in namespace "svcaccounts-2536" to be "Succeeded or Failed"
    Feb 27 12:07:18.769: INFO: Pod "test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573": Phase="Pending", Reason="", readiness=false. Elapsed: 14.325439ms
    Feb 27 12:07:20.779: INFO: Pod "test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024635054s
    Feb 27 12:07:22.783: INFO: Pod "test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02899031s
    STEP: Saw pod success 02/27/23 12:07:22.784
    Feb 27 12:07:22.784: INFO: Pod "test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573" satisfied condition "Succeeded or Failed"
    Feb 27 12:07:22.792: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 12:07:22.807
    Feb 27 12:07:22.839: INFO: Waiting for pod test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573 to disappear
    Feb 27 12:07:22.846: INFO: Pod test-pod-b6ae87b5-862d-4135-89e1-7ec746b3a573 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 27 12:07:22.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2536" for this suite. 02/27/23 12:07:22.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:07:22.879
Feb 27 12:07:22.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename dns 02/27/23 12:07:22.88
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:22.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:22.938
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 02/27/23 12:07:22.956
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-843.svc.cluster.local;sleep 1; done
 02/27/23 12:07:22.97
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-843.svc.cluster.local;sleep 1; done
 02/27/23 12:07:22.97
STEP: creating a pod to probe DNS 02/27/23 12:07:22.97
STEP: submitting the pod to kubernetes 02/27/23 12:07:22.97
Feb 27 12:07:22.993: INFO: Waiting up to 15m0s for pod "dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1" in namespace "dns-843" to be "running"
Feb 27 12:07:23.010: INFO: Pod "dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.371032ms
Feb 27 12:07:25.022: INFO: Pod "dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.02906541s
Feb 27 12:07:25.022: INFO: Pod "dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1" satisfied condition "running"
STEP: retrieving the pod 02/27/23 12:07:25.022
STEP: looking for the results for each expected name from probers 02/27/23 12:07:25.032
Feb 27 12:07:25.051: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
Feb 27 12:07:25.070: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
Feb 27 12:07:25.083: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
Feb 27 12:07:25.103: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
Feb 27 12:07:25.114: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
Feb 27 12:07:25.124: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
Feb 27 12:07:25.136: INFO: Unable to read jessie_udp@dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
Feb 27 12:07:25.154: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
Feb 27 12:07:25.154: INFO: Lookups using dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local wheezy_udp@dns-test-service-2.dns-843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-843.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local jessie_udp@dns-test-service-2.dns-843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-843.svc.cluster.local]

Feb 27 12:07:30.259: INFO: DNS probes using dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1 succeeded

STEP: deleting the pod 02/27/23 12:07:30.259
STEP: deleting the test headless service 02/27/23 12:07:30.302
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 27 12:07:30.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-843" for this suite. 02/27/23 12:07:30.358
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":176,"skipped":3461,"failed":0}
------------------------------
• [SLOW TEST] [7.507 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:07:22.879
    Feb 27 12:07:22.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename dns 02/27/23 12:07:22.88
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:22.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:22.938
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 02/27/23 12:07:22.956
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-843.svc.cluster.local;sleep 1; done
     02/27/23 12:07:22.97
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-843.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-843.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-843.svc.cluster.local;sleep 1; done
     02/27/23 12:07:22.97
    STEP: creating a pod to probe DNS 02/27/23 12:07:22.97
    STEP: submitting the pod to kubernetes 02/27/23 12:07:22.97
    Feb 27 12:07:22.993: INFO: Waiting up to 15m0s for pod "dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1" in namespace "dns-843" to be "running"
    Feb 27 12:07:23.010: INFO: Pod "dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.371032ms
    Feb 27 12:07:25.022: INFO: Pod "dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1": Phase="Running", Reason="", readiness=true. Elapsed: 2.02906541s
    Feb 27 12:07:25.022: INFO: Pod "dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 12:07:25.022
    STEP: looking for the results for each expected name from probers 02/27/23 12:07:25.032
    Feb 27 12:07:25.051: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
    Feb 27 12:07:25.070: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
    Feb 27 12:07:25.083: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
    Feb 27 12:07:25.103: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
    Feb 27 12:07:25.114: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
    Feb 27 12:07:25.124: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
    Feb 27 12:07:25.136: INFO: Unable to read jessie_udp@dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
    Feb 27 12:07:25.154: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-843.svc.cluster.local from pod dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1: the server could not find the requested resource (get pods dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1)
    Feb 27 12:07:25.154: INFO: Lookups using dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local wheezy_udp@dns-test-service-2.dns-843.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-843.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-843.svc.cluster.local jessie_udp@dns-test-service-2.dns-843.svc.cluster.local jessie_tcp@dns-test-service-2.dns-843.svc.cluster.local]

    Feb 27 12:07:30.259: INFO: DNS probes using dns-843/dns-test-e52fe2da-795f-4d1e-9f47-d5660d24c5f1 succeeded

    STEP: deleting the pod 02/27/23 12:07:30.259
    STEP: deleting the test headless service 02/27/23 12:07:30.302
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 27 12:07:30.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-843" for this suite. 02/27/23 12:07:30.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:07:30.402
Feb 27 12:07:30.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 12:07:30.404
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:30.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:30.451
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/27/23 12:07:30.47
Feb 27 12:07:30.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4813 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 27 12:07:30.575: INFO: stderr: ""
Feb 27 12:07:30.575: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 02/27/23 12:07:30.575
STEP: verifying the pod e2e-test-httpd-pod was created 02/27/23 12:07:35.627
Feb 27 12:07:35.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4813 get pod e2e-test-httpd-pod -o json'
Feb 27 12:07:35.783: INFO: stderr: ""
Feb 27 12:07:35.784: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"cf0e2c77ca3398d3b2d6d143c380c560cc119557f5f01f778578629fc381a04b\",\n            \"cni.projectcalico.org/podIP\": \"172.25.2.157/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.25.2.157/32\"\n        },\n        \"creationTimestamp\": \"2023-02-27T12:07:30Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4813\",\n        \"resourceVersion\": \"80709\",\n        \"uid\": \"b03229c2-e5b1-4904-a55c-afe1afd451b8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-bndwm\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-15-17.eu-central-1.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-bndwm\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T12:07:30Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T12:07:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T12:07:32Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T12:07:30Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://424c84a4b6107dd79b8d06b9b8476990b7f245357d832740a6b9396b713cbd48\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-27T12:07:31Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.15.17\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.2.157\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.2.157\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-27T12:07:30Z\"\n    }\n}\n"
STEP: replace the image in the pod 02/27/23 12:07:35.784
Feb 27 12:07:35.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4813 replace -f -'
Feb 27 12:07:36.323: INFO: stderr: ""
Feb 27 12:07:36.323: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 02/27/23 12:07:36.323
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Feb 27 12:07:36.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4813 delete pods e2e-test-httpd-pod'
Feb 27 12:07:38.448: INFO: stderr: ""
Feb 27 12:07:38.448: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 12:07:38.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4813" for this suite. 02/27/23 12:07:38.459
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":177,"skipped":3470,"failed":0}
------------------------------
• [SLOW TEST] [8.070 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:07:30.402
    Feb 27 12:07:30.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 12:07:30.404
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:30.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:30.451
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/27/23 12:07:30.47
    Feb 27 12:07:30.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4813 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 27 12:07:30.575: INFO: stderr: ""
    Feb 27 12:07:30.575: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 02/27/23 12:07:30.575
    STEP: verifying the pod e2e-test-httpd-pod was created 02/27/23 12:07:35.627
    Feb 27 12:07:35.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4813 get pod e2e-test-httpd-pod -o json'
    Feb 27 12:07:35.783: INFO: stderr: ""
    Feb 27 12:07:35.784: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"cf0e2c77ca3398d3b2d6d143c380c560cc119557f5f01f778578629fc381a04b\",\n            \"cni.projectcalico.org/podIP\": \"172.25.2.157/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.25.2.157/32\"\n        },\n        \"creationTimestamp\": \"2023-02-27T12:07:30Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4813\",\n        \"resourceVersion\": \"80709\",\n        \"uid\": \"b03229c2-e5b1-4904-a55c-afe1afd451b8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-bndwm\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-15-17.eu-central-1.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-bndwm\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T12:07:30Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T12:07:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T12:07:32Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T12:07:30Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://424c84a4b6107dd79b8d06b9b8476990b7f245357d832740a6b9396b713cbd48\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-27T12:07:31Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.15.17\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.2.157\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.2.157\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-27T12:07:30Z\"\n    }\n}\n"
    STEP: replace the image in the pod 02/27/23 12:07:35.784
    Feb 27 12:07:35.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4813 replace -f -'
    Feb 27 12:07:36.323: INFO: stderr: ""
    Feb 27 12:07:36.323: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 02/27/23 12:07:36.323
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Feb 27 12:07:36.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4813 delete pods e2e-test-httpd-pod'
    Feb 27 12:07:38.448: INFO: stderr: ""
    Feb 27 12:07:38.448: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 12:07:38.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4813" for this suite. 02/27/23 12:07:38.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:07:38.481
Feb 27 12:07:38.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename watch 02/27/23 12:07:38.482
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:38.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:38.53
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 02/27/23 12:07:38.538
STEP: creating a new configmap 02/27/23 12:07:38.542
STEP: modifying the configmap once 02/27/23 12:07:38.562
STEP: closing the watch once it receives two notifications 02/27/23 12:07:38.579
Feb 27 12:07:38.579: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6507  331e85ee-3a28-4174-a110-cd9ea87073b6 80769 0 2023-02-27 12:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 12:07:38.579: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6507  331e85ee-3a28-4174-a110-cd9ea87073b6 80770 0 2023-02-27 12:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 02/27/23 12:07:38.58
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/27/23 12:07:38.598
STEP: deleting the configmap 02/27/23 12:07:38.607
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/27/23 12:07:38.618
Feb 27 12:07:38.618: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6507  331e85ee-3a28-4174-a110-cd9ea87073b6 80771 0 2023-02-27 12:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 12:07:38.619: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6507  331e85ee-3a28-4174-a110-cd9ea87073b6 80772 0 2023-02-27 12:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 27 12:07:38.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6507" for this suite. 02/27/23 12:07:38.632
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":178,"skipped":3513,"failed":0}
------------------------------
• [0.163 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:07:38.481
    Feb 27 12:07:38.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename watch 02/27/23 12:07:38.482
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:38.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:38.53
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 02/27/23 12:07:38.538
    STEP: creating a new configmap 02/27/23 12:07:38.542
    STEP: modifying the configmap once 02/27/23 12:07:38.562
    STEP: closing the watch once it receives two notifications 02/27/23 12:07:38.579
    Feb 27 12:07:38.579: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6507  331e85ee-3a28-4174-a110-cd9ea87073b6 80769 0 2023-02-27 12:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 12:07:38.579: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6507  331e85ee-3a28-4174-a110-cd9ea87073b6 80770 0 2023-02-27 12:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 02/27/23 12:07:38.58
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/27/23 12:07:38.598
    STEP: deleting the configmap 02/27/23 12:07:38.607
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/27/23 12:07:38.618
    Feb 27 12:07:38.618: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6507  331e85ee-3a28-4174-a110-cd9ea87073b6 80771 0 2023-02-27 12:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 12:07:38.619: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6507  331e85ee-3a28-4174-a110-cd9ea87073b6 80772 0 2023-02-27 12:07:38 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 27 12:07:38.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6507" for this suite. 02/27/23 12:07:38.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:07:38.646
Feb 27 12:07:38.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename containers 02/27/23 12:07:38.648
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:38.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:38.69
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 02/27/23 12:07:38.703
Feb 27 12:07:38.719: INFO: Waiting up to 5m0s for pod "client-containers-a072a3ab-a985-4215-b65e-b24d5db36016" in namespace "containers-5289" to be "Succeeded or Failed"
Feb 27 12:07:38.735: INFO: Pod "client-containers-a072a3ab-a985-4215-b65e-b24d5db36016": Phase="Pending", Reason="", readiness=false. Elapsed: 16.73781ms
Feb 27 12:07:40.751: INFO: Pod "client-containers-a072a3ab-a985-4215-b65e-b24d5db36016": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032003082s
Feb 27 12:07:42.744: INFO: Pod "client-containers-a072a3ab-a985-4215-b65e-b24d5db36016": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025779961s
STEP: Saw pod success 02/27/23 12:07:42.744
Feb 27 12:07:42.745: INFO: Pod "client-containers-a072a3ab-a985-4215-b65e-b24d5db36016" satisfied condition "Succeeded or Failed"
Feb 27 12:07:42.753: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod client-containers-a072a3ab-a985-4215-b65e-b24d5db36016 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 12:07:42.776
Feb 27 12:07:42.796: INFO: Waiting for pod client-containers-a072a3ab-a985-4215-b65e-b24d5db36016 to disappear
Feb 27 12:07:42.804: INFO: Pod client-containers-a072a3ab-a985-4215-b65e-b24d5db36016 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 27 12:07:42.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5289" for this suite. 02/27/23 12:07:42.816
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":179,"skipped":3547,"failed":0}
------------------------------
• [4.183 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:07:38.646
    Feb 27 12:07:38.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename containers 02/27/23 12:07:38.648
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:38.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:38.69
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 02/27/23 12:07:38.703
    Feb 27 12:07:38.719: INFO: Waiting up to 5m0s for pod "client-containers-a072a3ab-a985-4215-b65e-b24d5db36016" in namespace "containers-5289" to be "Succeeded or Failed"
    Feb 27 12:07:38.735: INFO: Pod "client-containers-a072a3ab-a985-4215-b65e-b24d5db36016": Phase="Pending", Reason="", readiness=false. Elapsed: 16.73781ms
    Feb 27 12:07:40.751: INFO: Pod "client-containers-a072a3ab-a985-4215-b65e-b24d5db36016": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032003082s
    Feb 27 12:07:42.744: INFO: Pod "client-containers-a072a3ab-a985-4215-b65e-b24d5db36016": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025779961s
    STEP: Saw pod success 02/27/23 12:07:42.744
    Feb 27 12:07:42.745: INFO: Pod "client-containers-a072a3ab-a985-4215-b65e-b24d5db36016" satisfied condition "Succeeded or Failed"
    Feb 27 12:07:42.753: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod client-containers-a072a3ab-a985-4215-b65e-b24d5db36016 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 12:07:42.776
    Feb 27 12:07:42.796: INFO: Waiting for pod client-containers-a072a3ab-a985-4215-b65e-b24d5db36016 to disappear
    Feb 27 12:07:42.804: INFO: Pod client-containers-a072a3ab-a985-4215-b65e-b24d5db36016 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 27 12:07:42.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5289" for this suite. 02/27/23 12:07:42.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:07:42.836
Feb 27 12:07:42.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename ephemeral-containers-test 02/27/23 12:07:42.838
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:42.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:42.872
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 02/27/23 12:07:42.884
Feb 27 12:07:42.899: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2233" to be "running and ready"
Feb 27 12:07:42.908: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.582869ms
Feb 27 12:07:42.908: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:07:44.918: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019577378s
Feb 27 12:07:44.918: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Feb 27 12:07:44.918: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 02/27/23 12:07:44.927
Feb 27 12:07:44.946: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2233" to be "container debugger running"
Feb 27 12:07:44.960: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.590406ms
Feb 27 12:07:46.975: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028140006s
Feb 27 12:07:48.968: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.021661535s
Feb 27 12:07:48.968: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 02/27/23 12:07:48.968
Feb 27 12:07:48.969: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2233 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:07:48.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:07:48.969: INFO: ExecWithOptions: Clientset creation
Feb 27 12:07:48.969: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/ephemeral-containers-test-2233/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Feb 27 12:07:49.091: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 27 12:07:49.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-2233" for this suite. 02/27/23 12:07:49.605
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":180,"skipped":3572,"failed":0}
------------------------------
• [SLOW TEST] [6.787 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:07:42.836
    Feb 27 12:07:42.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename ephemeral-containers-test 02/27/23 12:07:42.838
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:42.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:42.872
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 02/27/23 12:07:42.884
    Feb 27 12:07:42.899: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2233" to be "running and ready"
    Feb 27 12:07:42.908: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.582869ms
    Feb 27 12:07:42.908: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:07:44.918: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019577378s
    Feb 27 12:07:44.918: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Feb 27 12:07:44.918: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 02/27/23 12:07:44.927
    Feb 27 12:07:44.946: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2233" to be "container debugger running"
    Feb 27 12:07:44.960: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.590406ms
    Feb 27 12:07:46.975: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028140006s
    Feb 27 12:07:48.968: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.021661535s
    Feb 27 12:07:48.968: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 02/27/23 12:07:48.968
    Feb 27 12:07:48.969: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2233 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:07:48.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:07:48.969: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:07:48.969: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/ephemeral-containers-test-2233/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Feb 27 12:07:49.091: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 27 12:07:49.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-2233" for this suite. 02/27/23 12:07:49.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:07:49.628
Feb 27 12:07:49.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename conformance-tests 02/27/23 12:07:49.629
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:49.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:49.666
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 02/27/23 12:07:49.677
Feb 27 12:07:49.677: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Feb 27 12:07:49.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-2403" for this suite. 02/27/23 12:07:49.703
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":181,"skipped":3593,"failed":0}
------------------------------
• [0.090 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:07:49.628
    Feb 27 12:07:49.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename conformance-tests 02/27/23 12:07:49.629
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:49.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:49.666
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 02/27/23 12:07:49.677
    Feb 27 12:07:49.677: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Feb 27 12:07:49.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-2403" for this suite. 02/27/23 12:07:49.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:07:49.718
Feb 27 12:07:49.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename watch 02/27/23 12:07:49.72
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:49.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:49.765
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 02/27/23 12:07:49.774
STEP: creating a watch on configmaps with label B 02/27/23 12:07:49.779
STEP: creating a watch on configmaps with label A or B 02/27/23 12:07:49.783
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/27/23 12:07:49.788
Feb 27 12:07:49.803: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80901 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 12:07:49.803: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80901 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/27/23 12:07:49.803
Feb 27 12:07:49.829: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80902 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 12:07:49.829: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80902 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/27/23 12:07:49.829
Feb 27 12:07:49.849: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80903 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 12:07:49.849: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80903 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/27/23 12:07:49.849
Feb 27 12:07:49.860: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80904 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 12:07:49.860: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80904 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/27/23 12:07:49.86
Feb 27 12:07:49.873: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7927  43523562-ddf0-4e30-a935-f1273a310e86 80905 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 12:07:49.873: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7927  43523562-ddf0-4e30-a935-f1273a310e86 80905 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/27/23 12:07:59.874
Feb 27 12:07:59.889: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7927  43523562-ddf0-4e30-a935-f1273a310e86 80965 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 12:07:59.889: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7927  43523562-ddf0-4e30-a935-f1273a310e86 80965 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 27 12:08:09.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7927" for this suite. 02/27/23 12:08:09.907
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":182,"skipped":3599,"failed":0}
------------------------------
• [SLOW TEST] [20.203 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:07:49.718
    Feb 27 12:07:49.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename watch 02/27/23 12:07:49.72
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:07:49.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:07:49.765
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 02/27/23 12:07:49.774
    STEP: creating a watch on configmaps with label B 02/27/23 12:07:49.779
    STEP: creating a watch on configmaps with label A or B 02/27/23 12:07:49.783
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/27/23 12:07:49.788
    Feb 27 12:07:49.803: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80901 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 12:07:49.803: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80901 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/27/23 12:07:49.803
    Feb 27 12:07:49.829: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80902 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 12:07:49.829: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80902 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/27/23 12:07:49.829
    Feb 27 12:07:49.849: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80903 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 12:07:49.849: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80903 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/27/23 12:07:49.849
    Feb 27 12:07:49.860: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80904 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 12:07:49.860: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7927  65b57863-5ef4-4b1e-9727-e8e412ed6b54 80904 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/27/23 12:07:49.86
    Feb 27 12:07:49.873: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7927  43523562-ddf0-4e30-a935-f1273a310e86 80905 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 12:07:49.873: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7927  43523562-ddf0-4e30-a935-f1273a310e86 80905 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/27/23 12:07:59.874
    Feb 27 12:07:59.889: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7927  43523562-ddf0-4e30-a935-f1273a310e86 80965 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 12:07:59.889: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7927  43523562-ddf0-4e30-a935-f1273a310e86 80965 0 2023-02-27 12:07:49 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 12:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 27 12:08:09.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7927" for this suite. 02/27/23 12:08:09.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:08:09.928
Feb 27 12:08:09.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 12:08:09.93
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:08:09.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:08:09.976
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-89510555-6935-4bb2-8224-b3293c1e7aa4 02/27/23 12:08:09.996
STEP: Creating a pod to test consume configMaps 02/27/23 12:08:10.013
Feb 27 12:08:10.036: INFO: Waiting up to 5m0s for pod "pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22" in namespace "configmap-2885" to be "Succeeded or Failed"
Feb 27 12:08:10.052: INFO: Pod "pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22": Phase="Pending", Reason="", readiness=false. Elapsed: 15.719835ms
Feb 27 12:08:12.061: INFO: Pod "pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024579516s
Feb 27 12:08:14.060: INFO: Pod "pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024088901s
STEP: Saw pod success 02/27/23 12:08:14.06
Feb 27 12:08:14.060: INFO: Pod "pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22" satisfied condition "Succeeded or Failed"
Feb 27 12:08:14.068: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 12:08:14.086
Feb 27 12:08:14.113: INFO: Waiting for pod pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22 to disappear
Feb 27 12:08:14.123: INFO: Pod pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 12:08:14.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2885" for this suite. 02/27/23 12:08:14.135
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":183,"skipped":3612,"failed":0}
------------------------------
• [4.222 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:08:09.928
    Feb 27 12:08:09.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 12:08:09.93
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:08:09.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:08:09.976
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-89510555-6935-4bb2-8224-b3293c1e7aa4 02/27/23 12:08:09.996
    STEP: Creating a pod to test consume configMaps 02/27/23 12:08:10.013
    Feb 27 12:08:10.036: INFO: Waiting up to 5m0s for pod "pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22" in namespace "configmap-2885" to be "Succeeded or Failed"
    Feb 27 12:08:10.052: INFO: Pod "pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22": Phase="Pending", Reason="", readiness=false. Elapsed: 15.719835ms
    Feb 27 12:08:12.061: INFO: Pod "pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024579516s
    Feb 27 12:08:14.060: INFO: Pod "pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024088901s
    STEP: Saw pod success 02/27/23 12:08:14.06
    Feb 27 12:08:14.060: INFO: Pod "pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22" satisfied condition "Succeeded or Failed"
    Feb 27 12:08:14.068: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 12:08:14.086
    Feb 27 12:08:14.113: INFO: Waiting for pod pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22 to disappear
    Feb 27 12:08:14.123: INFO: Pod pod-configmaps-f93b3753-564d-4e8e-a2ea-81d065ab7f22 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 12:08:14.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2885" for this suite. 02/27/23 12:08:14.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:08:14.165
Feb 27 12:08:14.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename replication-controller 02/27/23 12:08:14.166
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:08:14.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:08:14.206
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 02/27/23 12:08:14.216
STEP: When the matched label of one of its pods change 02/27/23 12:08:14.232
Feb 27 12:08:14.253: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 27 12:08:19.261: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 02/27/23 12:08:19.288
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 27 12:08:19.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4392" for this suite. 02/27/23 12:08:19.335
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":184,"skipped":3647,"failed":0}
------------------------------
• [SLOW TEST] [5.189 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:08:14.165
    Feb 27 12:08:14.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename replication-controller 02/27/23 12:08:14.166
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:08:14.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:08:14.206
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 02/27/23 12:08:14.216
    STEP: When the matched label of one of its pods change 02/27/23 12:08:14.232
    Feb 27 12:08:14.253: INFO: Pod name pod-release: Found 0 pods out of 1
    Feb 27 12:08:19.261: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/27/23 12:08:19.288
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 27 12:08:19.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4392" for this suite. 02/27/23 12:08:19.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:08:19.368
Feb 27 12:08:19.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename statefulset 02/27/23 12:08:19.369
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:08:19.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:08:19.416
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-549 02/27/23 12:08:19.43
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 02/27/23 12:08:19.446
Feb 27 12:08:19.469: INFO: Found 0 stateful pods, waiting for 3
Feb 27 12:08:29.479: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 12:08:29.479: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 12:08:29.479: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/27/23 12:08:29.503
Feb 27 12:08:29.532: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/27/23 12:08:29.532
STEP: Not applying an update when the partition is greater than the number of replicas 02/27/23 12:08:39.57
STEP: Performing a canary update 02/27/23 12:08:39.57
Feb 27 12:08:39.603: INFO: Updating stateful set ss2
Feb 27 12:08:39.633: INFO: Waiting for Pod statefulset-549/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 02/27/23 12:08:49.653
Feb 27 12:08:49.741: INFO: Found 2 stateful pods, waiting for 3
Feb 27 12:08:59.752: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 12:08:59.752: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 12:08:59.752: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 02/27/23 12:08:59.769
Feb 27 12:08:59.800: INFO: Updating stateful set ss2
Feb 27 12:08:59.818: INFO: Waiting for Pod statefulset-549/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Feb 27 12:09:09.882: INFO: Updating stateful set ss2
Feb 27 12:09:09.922: INFO: Waiting for StatefulSet statefulset-549/ss2 to complete update
Feb 27 12:09:09.922: INFO: Waiting for Pod statefulset-549/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 27 12:09:19.947: INFO: Deleting all statefulset in ns statefulset-549
Feb 27 12:09:19.955: INFO: Scaling statefulset ss2 to 0
Feb 27 12:09:30.002: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 12:09:30.012: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 27 12:09:30.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-549" for this suite. 02/27/23 12:09:30.061
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":185,"skipped":3703,"failed":0}
------------------------------
• [SLOW TEST] [70.709 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:08:19.368
    Feb 27 12:08:19.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename statefulset 02/27/23 12:08:19.369
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:08:19.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:08:19.416
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-549 02/27/23 12:08:19.43
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 02/27/23 12:08:19.446
    Feb 27 12:08:19.469: INFO: Found 0 stateful pods, waiting for 3
    Feb 27 12:08:29.479: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 12:08:29.479: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 12:08:29.479: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/27/23 12:08:29.503
    Feb 27 12:08:29.532: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/27/23 12:08:29.532
    STEP: Not applying an update when the partition is greater than the number of replicas 02/27/23 12:08:39.57
    STEP: Performing a canary update 02/27/23 12:08:39.57
    Feb 27 12:08:39.603: INFO: Updating stateful set ss2
    Feb 27 12:08:39.633: INFO: Waiting for Pod statefulset-549/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 02/27/23 12:08:49.653
    Feb 27 12:08:49.741: INFO: Found 2 stateful pods, waiting for 3
    Feb 27 12:08:59.752: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 12:08:59.752: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 12:08:59.752: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 02/27/23 12:08:59.769
    Feb 27 12:08:59.800: INFO: Updating stateful set ss2
    Feb 27 12:08:59.818: INFO: Waiting for Pod statefulset-549/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Feb 27 12:09:09.882: INFO: Updating stateful set ss2
    Feb 27 12:09:09.922: INFO: Waiting for StatefulSet statefulset-549/ss2 to complete update
    Feb 27 12:09:09.922: INFO: Waiting for Pod statefulset-549/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 27 12:09:19.947: INFO: Deleting all statefulset in ns statefulset-549
    Feb 27 12:09:19.955: INFO: Scaling statefulset ss2 to 0
    Feb 27 12:09:30.002: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 12:09:30.012: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 27 12:09:30.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-549" for this suite. 02/27/23 12:09:30.061
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:09:30.078
Feb 27 12:09:30.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename statefulset 02/27/23 12:09:30.079
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:09:30.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:09:30.114
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4051 02/27/23 12:09:30.122
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 02/27/23 12:09:30.139
Feb 27 12:09:30.161: INFO: Found 0 stateful pods, waiting for 3
Feb 27 12:09:40.181: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 12:09:40.181: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 12:09:40.181: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 12:09:40.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-4051 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 12:09:40.493: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 12:09:40.493: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 12:09:40.493: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/27/23 12:09:50.533
Feb 27 12:09:50.566: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/27/23 12:09:50.566
STEP: Updating Pods in reverse ordinal order 02/27/23 12:10:00.608
Feb 27 12:10:00.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-4051 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 12:10:01.009: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 12:10:01.009: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 12:10:01.009: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 02/27/23 12:10:11.093
Feb 27 12:10:11.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-4051 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 12:10:11.375: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 12:10:11.375: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 12:10:11.375: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 12:10:21.446: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 02/27/23 12:10:31.49
Feb 27 12:10:31.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-4051 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 12:10:31.752: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 12:10:31.752: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 12:10:31.752: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 27 12:10:41.813: INFO: Deleting all statefulset in ns statefulset-4051
Feb 27 12:10:41.824: INFO: Scaling statefulset ss2 to 0
Feb 27 12:10:51.867: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 12:10:51.874: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 27 12:10:51.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4051" for this suite. 02/27/23 12:10:51.948
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":186,"skipped":3707,"failed":0}
------------------------------
• [SLOW TEST] [81.895 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:09:30.078
    Feb 27 12:09:30.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename statefulset 02/27/23 12:09:30.079
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:09:30.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:09:30.114
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4051 02/27/23 12:09:30.122
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 02/27/23 12:09:30.139
    Feb 27 12:09:30.161: INFO: Found 0 stateful pods, waiting for 3
    Feb 27 12:09:40.181: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 12:09:40.181: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 12:09:40.181: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 12:09:40.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-4051 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 12:09:40.493: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 12:09:40.493: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 12:09:40.493: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/27/23 12:09:50.533
    Feb 27 12:09:50.566: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/27/23 12:09:50.566
    STEP: Updating Pods in reverse ordinal order 02/27/23 12:10:00.608
    Feb 27 12:10:00.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-4051 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 12:10:01.009: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 12:10:01.009: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 12:10:01.009: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 02/27/23 12:10:11.093
    Feb 27 12:10:11.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-4051 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 12:10:11.375: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 12:10:11.375: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 12:10:11.375: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 12:10:21.446: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 02/27/23 12:10:31.49
    Feb 27 12:10:31.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-4051 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 12:10:31.752: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 12:10:31.752: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 12:10:31.752: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 27 12:10:41.813: INFO: Deleting all statefulset in ns statefulset-4051
    Feb 27 12:10:41.824: INFO: Scaling statefulset ss2 to 0
    Feb 27 12:10:51.867: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 12:10:51.874: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 27 12:10:51.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4051" for this suite. 02/27/23 12:10:51.948
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:10:51.977
Feb 27 12:10:51.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 12:10:51.978
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:10:52.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:10:52.024
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 02/27/23 12:10:52.039
Feb 27 12:10:52.058: INFO: Waiting up to 5m0s for pod "downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09" in namespace "downward-api-4792" to be "Succeeded or Failed"
Feb 27 12:10:52.073: INFO: Pod "downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09": Phase="Pending", Reason="", readiness=false. Elapsed: 14.5199ms
Feb 27 12:10:54.081: INFO: Pod "downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023338282s
Feb 27 12:10:56.084: INFO: Pod "downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025881024s
Feb 27 12:10:58.082: INFO: Pod "downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024221805s
STEP: Saw pod success 02/27/23 12:10:58.082
Feb 27 12:10:58.083: INFO: Pod "downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09" satisfied condition "Succeeded or Failed"
Feb 27 12:10:58.090: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09 container dapi-container: <nil>
STEP: delete the pod 02/27/23 12:10:58.113
Feb 27 12:10:58.136: INFO: Waiting for pod downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09 to disappear
Feb 27 12:10:58.146: INFO: Pod downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 27 12:10:58.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4792" for this suite. 02/27/23 12:10:58.156
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":187,"skipped":3707,"failed":0}
------------------------------
• [SLOW TEST] [6.199 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:10:51.977
    Feb 27 12:10:51.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 12:10:51.978
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:10:52.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:10:52.024
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 02/27/23 12:10:52.039
    Feb 27 12:10:52.058: INFO: Waiting up to 5m0s for pod "downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09" in namespace "downward-api-4792" to be "Succeeded or Failed"
    Feb 27 12:10:52.073: INFO: Pod "downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09": Phase="Pending", Reason="", readiness=false. Elapsed: 14.5199ms
    Feb 27 12:10:54.081: INFO: Pod "downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023338282s
    Feb 27 12:10:56.084: INFO: Pod "downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025881024s
    Feb 27 12:10:58.082: INFO: Pod "downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024221805s
    STEP: Saw pod success 02/27/23 12:10:58.082
    Feb 27 12:10:58.083: INFO: Pod "downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09" satisfied condition "Succeeded or Failed"
    Feb 27 12:10:58.090: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 12:10:58.113
    Feb 27 12:10:58.136: INFO: Waiting for pod downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09 to disappear
    Feb 27 12:10:58.146: INFO: Pod downward-api-0ed58e8c-d2ba-4b2e-b562-96d75fb4ee09 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 27 12:10:58.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4792" for this suite. 02/27/23 12:10:58.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:10:58.179
Feb 27 12:10:58.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-probe 02/27/23 12:10:58.18
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:10:58.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:10:58.238
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f in namespace container-probe-4707 02/27/23 12:10:58.248
Feb 27 12:10:58.265: INFO: Waiting up to 5m0s for pod "busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f" in namespace "container-probe-4707" to be "not pending"
Feb 27 12:10:58.280: INFO: Pod "busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.114071ms
Feb 27 12:11:00.291: INFO: Pod "busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f": Phase="Running", Reason="", readiness=true. Elapsed: 2.026145743s
Feb 27 12:11:00.292: INFO: Pod "busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f" satisfied condition "not pending"
Feb 27 12:11:00.292: INFO: Started pod busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f in namespace container-probe-4707
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 12:11:00.292
Feb 27 12:11:00.300: INFO: Initial restart count of pod busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f is 0
Feb 27 12:11:50.594: INFO: Restart count of pod container-probe-4707/busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f is now 1 (50.294367173s elapsed)
STEP: deleting the pod 02/27/23 12:11:50.594
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 27 12:11:50.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4707" for this suite. 02/27/23 12:11:50.631
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":188,"skipped":3714,"failed":0}
------------------------------
• [SLOW TEST] [52.464 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:10:58.179
    Feb 27 12:10:58.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-probe 02/27/23 12:10:58.18
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:10:58.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:10:58.238
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f in namespace container-probe-4707 02/27/23 12:10:58.248
    Feb 27 12:10:58.265: INFO: Waiting up to 5m0s for pod "busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f" in namespace "container-probe-4707" to be "not pending"
    Feb 27 12:10:58.280: INFO: Pod "busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.114071ms
    Feb 27 12:11:00.291: INFO: Pod "busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f": Phase="Running", Reason="", readiness=true. Elapsed: 2.026145743s
    Feb 27 12:11:00.292: INFO: Pod "busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f" satisfied condition "not pending"
    Feb 27 12:11:00.292: INFO: Started pod busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f in namespace container-probe-4707
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 12:11:00.292
    Feb 27 12:11:00.300: INFO: Initial restart count of pod busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f is 0
    Feb 27 12:11:50.594: INFO: Restart count of pod container-probe-4707/busybox-881ed957-9cd7-4bbd-b49c-1a64d6b4dd6f is now 1 (50.294367173s elapsed)
    STEP: deleting the pod 02/27/23 12:11:50.594
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 27 12:11:50.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4707" for this suite. 02/27/23 12:11:50.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:11:50.651
Feb 27 12:11:50.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 12:11:50.652
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:11:50.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:11:50.697
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 12:11:50.739
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 12:11:51.371
STEP: Deploying the webhook pod 02/27/23 12:11:51.385
STEP: Wait for the deployment to be ready 02/27/23 12:11:51.408
Feb 27 12:11:51.439: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 12:11:53.466
STEP: Verifying the service has paired with the endpoint 02/27/23 12:11:53.494
Feb 27 12:11:54.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 02/27/23 12:11:54.618
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 12:11:54.694
STEP: Deleting the collection of validation webhooks 02/27/23 12:11:54.759
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 12:11:54.876
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:11:54.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8587" for this suite. 02/27/23 12:11:54.92
STEP: Destroying namespace "webhook-8587-markers" for this suite. 02/27/23 12:11:54.936
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":189,"skipped":3727,"failed":0}
------------------------------
• [4.498 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:11:50.651
    Feb 27 12:11:50.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 12:11:50.652
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:11:50.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:11:50.697
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 12:11:50.739
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 12:11:51.371
    STEP: Deploying the webhook pod 02/27/23 12:11:51.385
    STEP: Wait for the deployment to be ready 02/27/23 12:11:51.408
    Feb 27 12:11:51.439: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 12:11:53.466
    STEP: Verifying the service has paired with the endpoint 02/27/23 12:11:53.494
    Feb 27 12:11:54.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 02/27/23 12:11:54.618
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 12:11:54.694
    STEP: Deleting the collection of validation webhooks 02/27/23 12:11:54.759
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 12:11:54.876
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:11:54.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8587" for this suite. 02/27/23 12:11:54.92
    STEP: Destroying namespace "webhook-8587-markers" for this suite. 02/27/23 12:11:54.936
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:11:55.151
Feb 27 12:11:55.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename dns 02/27/23 12:11:55.152
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:11:55.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:11:55.226
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 02/27/23 12:11:55.236
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6330 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6330;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6330 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6330;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6330.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6330.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6330.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6330.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6330.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6330.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6330.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6330.svc;check="$$(dig +notcp +noall +answer +search 206.28.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.28.206_udp@PTR;check="$$(dig +tcp +noall +answer +search 206.28.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.28.206_tcp@PTR;sleep 1; done
 02/27/23 12:11:55.289
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6330 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6330;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6330 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6330;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6330.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6330.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6330.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6330.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6330.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6330.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6330.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6330.svc;check="$$(dig +notcp +noall +answer +search 206.28.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.28.206_udp@PTR;check="$$(dig +tcp +noall +answer +search 206.28.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.28.206_tcp@PTR;sleep 1; done
 02/27/23 12:11:55.289
STEP: creating a pod to probe DNS 02/27/23 12:11:55.289
STEP: submitting the pod to kubernetes 02/27/23 12:11:55.29
Feb 27 12:11:55.319: INFO: Waiting up to 15m0s for pod "dns-test-ef72cdfe-9540-469f-af58-57c9d493690d" in namespace "dns-6330" to be "running"
Feb 27 12:11:55.330: INFO: Pod "dns-test-ef72cdfe-9540-469f-af58-57c9d493690d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.149872ms
Feb 27 12:11:57.342: INFO: Pod "dns-test-ef72cdfe-9540-469f-af58-57c9d493690d": Phase="Running", Reason="", readiness=true. Elapsed: 2.022925154s
Feb 27 12:11:57.342: INFO: Pod "dns-test-ef72cdfe-9540-469f-af58-57c9d493690d" satisfied condition "running"
STEP: retrieving the pod 02/27/23 12:11:57.343
STEP: looking for the results for each expected name from probers 02/27/23 12:11:57.353
Feb 27 12:11:57.368: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.380: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.395: INFO: Unable to read wheezy_udp@dns-test-service.dns-6330 from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.408: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6330 from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.428: INFO: Unable to read wheezy_udp@dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.439: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.450: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.460: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.471: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.493: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.532: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.543: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.560: INFO: Unable to read jessie_udp@dns-test-service.dns-6330 from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.571: INFO: Unable to read jessie_tcp@dns-test-service.dns-6330 from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.587: INFO: Unable to read jessie_udp@dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.606: INFO: Unable to read jessie_tcp@dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.622: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.649: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
Feb 27 12:11:57.705: INFO: Lookups using dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6330 wheezy_tcp@dns-test-service.dns-6330 wheezy_udp@dns-test-service.dns-6330.svc wheezy_tcp@dns-test-service.dns-6330.svc wheezy_udp@_http._tcp.dns-test-service.dns-6330.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6330.svc wheezy_udp@_http._tcp.test-service-2.dns-6330.svc wheezy_tcp@_http._tcp.test-service-2.dns-6330.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6330 jessie_tcp@dns-test-service.dns-6330 jessie_udp@dns-test-service.dns-6330.svc jessie_tcp@dns-test-service.dns-6330.svc jessie_udp@_http._tcp.dns-test-service.dns-6330.svc jessie_tcp@_http._tcp.dns-test-service.dns-6330.svc]

Feb 27 12:12:03.110: INFO: DNS probes using dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d succeeded

STEP: deleting the pod 02/27/23 12:12:03.11
STEP: deleting the test service 02/27/23 12:12:03.155
STEP: deleting the test headless service 02/27/23 12:12:03.216
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 27 12:12:03.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6330" for this suite. 02/27/23 12:12:03.309
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":190,"skipped":3742,"failed":0}
------------------------------
• [SLOW TEST] [8.194 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:11:55.151
    Feb 27 12:11:55.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename dns 02/27/23 12:11:55.152
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:11:55.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:11:55.226
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 02/27/23 12:11:55.236
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6330 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6330;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6330 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6330;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6330.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6330.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6330.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6330.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6330.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6330.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6330.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6330.svc;check="$$(dig +notcp +noall +answer +search 206.28.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.28.206_udp@PTR;check="$$(dig +tcp +noall +answer +search 206.28.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.28.206_tcp@PTR;sleep 1; done
     02/27/23 12:11:55.289
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6330 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6330;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6330 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6330;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6330.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6330.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6330.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6330.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6330.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6330.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6330.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6330.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6330.svc;check="$$(dig +notcp +noall +answer +search 206.28.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.28.206_udp@PTR;check="$$(dig +tcp +noall +answer +search 206.28.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.28.206_tcp@PTR;sleep 1; done
     02/27/23 12:11:55.289
    STEP: creating a pod to probe DNS 02/27/23 12:11:55.289
    STEP: submitting the pod to kubernetes 02/27/23 12:11:55.29
    Feb 27 12:11:55.319: INFO: Waiting up to 15m0s for pod "dns-test-ef72cdfe-9540-469f-af58-57c9d493690d" in namespace "dns-6330" to be "running"
    Feb 27 12:11:55.330: INFO: Pod "dns-test-ef72cdfe-9540-469f-af58-57c9d493690d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.149872ms
    Feb 27 12:11:57.342: INFO: Pod "dns-test-ef72cdfe-9540-469f-af58-57c9d493690d": Phase="Running", Reason="", readiness=true. Elapsed: 2.022925154s
    Feb 27 12:11:57.342: INFO: Pod "dns-test-ef72cdfe-9540-469f-af58-57c9d493690d" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 12:11:57.343
    STEP: looking for the results for each expected name from probers 02/27/23 12:11:57.353
    Feb 27 12:11:57.368: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.380: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.395: INFO: Unable to read wheezy_udp@dns-test-service.dns-6330 from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.408: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6330 from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.428: INFO: Unable to read wheezy_udp@dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.439: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.450: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.460: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.471: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.493: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.532: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.543: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.560: INFO: Unable to read jessie_udp@dns-test-service.dns-6330 from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.571: INFO: Unable to read jessie_tcp@dns-test-service.dns-6330 from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.587: INFO: Unable to read jessie_udp@dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.606: INFO: Unable to read jessie_tcp@dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.622: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.649: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6330.svc from pod dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d: the server could not find the requested resource (get pods dns-test-ef72cdfe-9540-469f-af58-57c9d493690d)
    Feb 27 12:11:57.705: INFO: Lookups using dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6330 wheezy_tcp@dns-test-service.dns-6330 wheezy_udp@dns-test-service.dns-6330.svc wheezy_tcp@dns-test-service.dns-6330.svc wheezy_udp@_http._tcp.dns-test-service.dns-6330.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6330.svc wheezy_udp@_http._tcp.test-service-2.dns-6330.svc wheezy_tcp@_http._tcp.test-service-2.dns-6330.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6330 jessie_tcp@dns-test-service.dns-6330 jessie_udp@dns-test-service.dns-6330.svc jessie_tcp@dns-test-service.dns-6330.svc jessie_udp@_http._tcp.dns-test-service.dns-6330.svc jessie_tcp@_http._tcp.dns-test-service.dns-6330.svc]

    Feb 27 12:12:03.110: INFO: DNS probes using dns-6330/dns-test-ef72cdfe-9540-469f-af58-57c9d493690d succeeded

    STEP: deleting the pod 02/27/23 12:12:03.11
    STEP: deleting the test service 02/27/23 12:12:03.155
    STEP: deleting the test headless service 02/27/23 12:12:03.216
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 27 12:12:03.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6330" for this suite. 02/27/23 12:12:03.309
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:12:03.355
Feb 27 12:12:03.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename dns 02/27/23 12:12:03.357
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:03.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:03.412
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/27/23 12:12:03.427
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/27/23 12:12:03.427
STEP: creating a pod to probe DNS 02/27/23 12:12:03.428
STEP: submitting the pod to kubernetes 02/27/23 12:12:03.428
Feb 27 12:12:03.450: INFO: Waiting up to 15m0s for pod "dns-test-1330534e-7f0d-4d5b-98b2-34056c6bdcf1" in namespace "dns-8069" to be "running"
Feb 27 12:12:03.476: INFO: Pod "dns-test-1330534e-7f0d-4d5b-98b2-34056c6bdcf1": Phase="Pending", Reason="", readiness=false. Elapsed: 25.553624ms
Feb 27 12:12:05.494: INFO: Pod "dns-test-1330534e-7f0d-4d5b-98b2-34056c6bdcf1": Phase="Running", Reason="", readiness=true. Elapsed: 2.043185725s
Feb 27 12:12:05.494: INFO: Pod "dns-test-1330534e-7f0d-4d5b-98b2-34056c6bdcf1" satisfied condition "running"
STEP: retrieving the pod 02/27/23 12:12:05.494
STEP: looking for the results for each expected name from probers 02/27/23 12:12:05.512
Feb 27 12:12:05.582: INFO: DNS probes using dns-8069/dns-test-1330534e-7f0d-4d5b-98b2-34056c6bdcf1 succeeded

STEP: deleting the pod 02/27/23 12:12:05.582
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 27 12:12:05.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8069" for this suite. 02/27/23 12:12:05.62
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":191,"skipped":3744,"failed":0}
------------------------------
• [2.284 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:12:03.355
    Feb 27 12:12:03.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename dns 02/27/23 12:12:03.357
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:03.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:03.412
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/27/23 12:12:03.427
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/27/23 12:12:03.427
    STEP: creating a pod to probe DNS 02/27/23 12:12:03.428
    STEP: submitting the pod to kubernetes 02/27/23 12:12:03.428
    Feb 27 12:12:03.450: INFO: Waiting up to 15m0s for pod "dns-test-1330534e-7f0d-4d5b-98b2-34056c6bdcf1" in namespace "dns-8069" to be "running"
    Feb 27 12:12:03.476: INFO: Pod "dns-test-1330534e-7f0d-4d5b-98b2-34056c6bdcf1": Phase="Pending", Reason="", readiness=false. Elapsed: 25.553624ms
    Feb 27 12:12:05.494: INFO: Pod "dns-test-1330534e-7f0d-4d5b-98b2-34056c6bdcf1": Phase="Running", Reason="", readiness=true. Elapsed: 2.043185725s
    Feb 27 12:12:05.494: INFO: Pod "dns-test-1330534e-7f0d-4d5b-98b2-34056c6bdcf1" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 12:12:05.494
    STEP: looking for the results for each expected name from probers 02/27/23 12:12:05.512
    Feb 27 12:12:05.582: INFO: DNS probes using dns-8069/dns-test-1330534e-7f0d-4d5b-98b2-34056c6bdcf1 succeeded

    STEP: deleting the pod 02/27/23 12:12:05.582
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 27 12:12:05.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8069" for this suite. 02/27/23 12:12:05.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:12:05.681
Feb 27 12:12:05.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename svcaccounts 02/27/23 12:12:05.683
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:05.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:05.731
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 02/27/23 12:12:05.744
STEP: watching for the ServiceAccount to be added 02/27/23 12:12:05.763
STEP: patching the ServiceAccount 02/27/23 12:12:05.77
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/27/23 12:12:05.786
STEP: deleting the ServiceAccount 02/27/23 12:12:05.813
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 27 12:12:05.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6024" for this suite. 02/27/23 12:12:05.923
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":192,"skipped":3831,"failed":0}
------------------------------
• [0.281 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:12:05.681
    Feb 27 12:12:05.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 12:12:05.683
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:05.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:05.731
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 02/27/23 12:12:05.744
    STEP: watching for the ServiceAccount to be added 02/27/23 12:12:05.763
    STEP: patching the ServiceAccount 02/27/23 12:12:05.77
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/27/23 12:12:05.786
    STEP: deleting the ServiceAccount 02/27/23 12:12:05.813
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 27 12:12:05.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6024" for this suite. 02/27/23 12:12:05.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:12:05.968
Feb 27 12:12:05.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubelet-test 02/27/23 12:12:05.969
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:06.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:06.028
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Feb 27 12:12:06.059: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsa38b9abc-835b-4737-9365-700ac11c0c00" in namespace "kubelet-test-668" to be "running and ready"
Feb 27 12:12:06.068: INFO: Pod "busybox-readonly-fsa38b9abc-835b-4737-9365-700ac11c0c00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.966617ms
Feb 27 12:12:06.068: INFO: The phase of Pod busybox-readonly-fsa38b9abc-835b-4737-9365-700ac11c0c00 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:12:08.076: INFO: Pod "busybox-readonly-fsa38b9abc-835b-4737-9365-700ac11c0c00": Phase="Running", Reason="", readiness=true. Elapsed: 2.01738681s
Feb 27 12:12:08.077: INFO: The phase of Pod busybox-readonly-fsa38b9abc-835b-4737-9365-700ac11c0c00 is Running (Ready = true)
Feb 27 12:12:08.077: INFO: Pod "busybox-readonly-fsa38b9abc-835b-4737-9365-700ac11c0c00" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 27 12:12:08.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-668" for this suite. 02/27/23 12:12:08.121
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":193,"skipped":3877,"failed":0}
------------------------------
• [2.173 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:12:05.968
    Feb 27 12:12:05.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 12:12:05.969
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:06.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:06.028
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Feb 27 12:12:06.059: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsa38b9abc-835b-4737-9365-700ac11c0c00" in namespace "kubelet-test-668" to be "running and ready"
    Feb 27 12:12:06.068: INFO: Pod "busybox-readonly-fsa38b9abc-835b-4737-9365-700ac11c0c00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.966617ms
    Feb 27 12:12:06.068: INFO: The phase of Pod busybox-readonly-fsa38b9abc-835b-4737-9365-700ac11c0c00 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:12:08.076: INFO: Pod "busybox-readonly-fsa38b9abc-835b-4737-9365-700ac11c0c00": Phase="Running", Reason="", readiness=true. Elapsed: 2.01738681s
    Feb 27 12:12:08.077: INFO: The phase of Pod busybox-readonly-fsa38b9abc-835b-4737-9365-700ac11c0c00 is Running (Ready = true)
    Feb 27 12:12:08.077: INFO: Pod "busybox-readonly-fsa38b9abc-835b-4737-9365-700ac11c0c00" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 27 12:12:08.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-668" for this suite. 02/27/23 12:12:08.121
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:12:08.146
Feb 27 12:12:08.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 12:12:08.147
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:08.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:08.189
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 02/27/23 12:12:08.201
STEP: listing secrets in all namespaces to ensure that there are more than zero 02/27/23 12:12:08.213
STEP: patching the secret 02/27/23 12:12:08.226
STEP: deleting the secret using a LabelSelector 02/27/23 12:12:08.248
STEP: listing secrets in all namespaces, searching for label name and value in patch 02/27/23 12:12:08.266
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 27 12:12:08.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2180" for this suite. 02/27/23 12:12:08.302
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":194,"skipped":3879,"failed":0}
------------------------------
• [0.173 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:12:08.146
    Feb 27 12:12:08.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 12:12:08.147
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:08.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:08.189
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 02/27/23 12:12:08.201
    STEP: listing secrets in all namespaces to ensure that there are more than zero 02/27/23 12:12:08.213
    STEP: patching the secret 02/27/23 12:12:08.226
    STEP: deleting the secret using a LabelSelector 02/27/23 12:12:08.248
    STEP: listing secrets in all namespaces, searching for label name and value in patch 02/27/23 12:12:08.266
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 12:12:08.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2180" for this suite. 02/27/23 12:12:08.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:12:08.328
Feb 27 12:12:08.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename daemonsets 02/27/23 12:12:08.332
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:08.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:08.373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 02/27/23 12:12:08.444
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 12:12:08.456
Feb 27 12:12:08.477: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 12:12:08.477: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 12:12:09.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 12:12:09.507: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 12:12:10.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 12:12:10.504: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 02/27/23 12:12:10.51
Feb 27 12:12:10.519: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 02/27/23 12:12:10.519
Feb 27 12:12:10.546: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 02/27/23 12:12:10.546
Feb 27 12:12:10.553: INFO: Observed &DaemonSet event: ADDED
Feb 27 12:12:10.554: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 12:12:10.554: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 12:12:10.554: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 12:12:10.555: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 12:12:10.555: INFO: Found daemon set daemon-set in namespace daemonsets-1832 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 27 12:12:10.555: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 02/27/23 12:12:10.555
STEP: watching for the daemon set status to be patched 02/27/23 12:12:10.567
Feb 27 12:12:10.573: INFO: Observed &DaemonSet event: ADDED
Feb 27 12:12:10.574: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 12:12:10.574: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 12:12:10.575: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 12:12:10.575: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 12:12:10.575: INFO: Observed daemon set daemon-set in namespace daemonsets-1832 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 27 12:12:10.576: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 12:12:10.576: INFO: Found daemon set daemon-set in namespace daemonsets-1832 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Feb 27 12:12:10.576: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/27/23 12:12:10.583
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1832, will wait for the garbage collector to delete the pods 02/27/23 12:12:10.583
Feb 27 12:12:10.658: INFO: Deleting DaemonSet.extensions daemon-set took: 14.630239ms
Feb 27 12:12:10.758: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.313969ms
Feb 27 12:12:13.568: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 12:12:13.568: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 12:12:13.576: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"83141"},"items":null}

Feb 27 12:12:13.586: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"83141"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 27 12:12:13.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1832" for this suite. 02/27/23 12:12:13.642
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":195,"skipped":3921,"failed":0}
------------------------------
• [SLOW TEST] [5.329 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:12:08.328
    Feb 27 12:12:08.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename daemonsets 02/27/23 12:12:08.332
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:08.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:08.373
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 02/27/23 12:12:08.444
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 12:12:08.456
    Feb 27 12:12:08.477: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 12:12:08.477: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 12:12:09.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 12:12:09.507: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 12:12:10.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 12:12:10.504: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 02/27/23 12:12:10.51
    Feb 27 12:12:10.519: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 02/27/23 12:12:10.519
    Feb 27 12:12:10.546: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 02/27/23 12:12:10.546
    Feb 27 12:12:10.553: INFO: Observed &DaemonSet event: ADDED
    Feb 27 12:12:10.554: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 12:12:10.554: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 12:12:10.554: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 12:12:10.555: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 12:12:10.555: INFO: Found daemon set daemon-set in namespace daemonsets-1832 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 27 12:12:10.555: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 02/27/23 12:12:10.555
    STEP: watching for the daemon set status to be patched 02/27/23 12:12:10.567
    Feb 27 12:12:10.573: INFO: Observed &DaemonSet event: ADDED
    Feb 27 12:12:10.574: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 12:12:10.574: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 12:12:10.575: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 12:12:10.575: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 12:12:10.575: INFO: Observed daemon set daemon-set in namespace daemonsets-1832 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 27 12:12:10.576: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 12:12:10.576: INFO: Found daemon set daemon-set in namespace daemonsets-1832 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Feb 27 12:12:10.576: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 12:12:10.583
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1832, will wait for the garbage collector to delete the pods 02/27/23 12:12:10.583
    Feb 27 12:12:10.658: INFO: Deleting DaemonSet.extensions daemon-set took: 14.630239ms
    Feb 27 12:12:10.758: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.313969ms
    Feb 27 12:12:13.568: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 12:12:13.568: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 12:12:13.576: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"83141"},"items":null}

    Feb 27 12:12:13.586: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"83141"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 12:12:13.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1832" for this suite. 02/27/23 12:12:13.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:12:13.665
Feb 27 12:12:13.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:12:13.668
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:13.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:13.713
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-cbd80c04-05f9-4d55-9e21-b48da10b3bc7 02/27/23 12:12:13.724
STEP: Creating a pod to test consume secrets 02/27/23 12:12:13.734
Feb 27 12:12:13.761: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb" in namespace "projected-245" to be "Succeeded or Failed"
Feb 27 12:12:13.773: INFO: Pod "pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.959119ms
Feb 27 12:12:15.786: INFO: Pod "pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025109743s
Feb 27 12:12:17.780: INFO: Pod "pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01969486s
STEP: Saw pod success 02/27/23 12:12:17.78
Feb 27 12:12:17.781: INFO: Pod "pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb" satisfied condition "Succeeded or Failed"
Feb 27 12:12:17.789: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 12:12:17.809
Feb 27 12:12:17.841: INFO: Waiting for pod pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb to disappear
Feb 27 12:12:17.847: INFO: Pod pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 27 12:12:17.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-245" for this suite. 02/27/23 12:12:17.862
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":196,"skipped":3945,"failed":0}
------------------------------
• [4.222 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:12:13.665
    Feb 27 12:12:13.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:12:13.668
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:13.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:13.713
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-cbd80c04-05f9-4d55-9e21-b48da10b3bc7 02/27/23 12:12:13.724
    STEP: Creating a pod to test consume secrets 02/27/23 12:12:13.734
    Feb 27 12:12:13.761: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb" in namespace "projected-245" to be "Succeeded or Failed"
    Feb 27 12:12:13.773: INFO: Pod "pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.959119ms
    Feb 27 12:12:15.786: INFO: Pod "pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025109743s
    Feb 27 12:12:17.780: INFO: Pod "pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01969486s
    STEP: Saw pod success 02/27/23 12:12:17.78
    Feb 27 12:12:17.781: INFO: Pod "pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb" satisfied condition "Succeeded or Failed"
    Feb 27 12:12:17.789: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 12:12:17.809
    Feb 27 12:12:17.841: INFO: Waiting for pod pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb to disappear
    Feb 27 12:12:17.847: INFO: Pod pod-projected-secrets-586a301c-dd9d-4eb2-a79a-cef0cdd32cdb no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 27 12:12:17.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-245" for this suite. 02/27/23 12:12:17.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:12:17.888
Feb 27 12:12:17.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename replication-controller 02/27/23 12:12:17.889
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:17.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:17.938
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 02/27/23 12:12:17.953
Feb 27 12:12:17.978: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1236" to be "running and ready"
Feb 27 12:12:17.988: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050021ms
Feb 27 12:12:17.988: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:12:19.997: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.019163972s
Feb 27 12:12:19.997: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Feb 27 12:12:19.997: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 02/27/23 12:12:20.004
STEP: Then the orphan pod is adopted 02/27/23 12:12:20.016
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 27 12:12:21.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1236" for this suite. 02/27/23 12:12:21.059
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":197,"skipped":3952,"failed":0}
------------------------------
• [3.184 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:12:17.888
    Feb 27 12:12:17.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename replication-controller 02/27/23 12:12:17.889
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:17.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:17.938
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 02/27/23 12:12:17.953
    Feb 27 12:12:17.978: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1236" to be "running and ready"
    Feb 27 12:12:17.988: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050021ms
    Feb 27 12:12:17.988: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:12:19.997: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.019163972s
    Feb 27 12:12:19.997: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Feb 27 12:12:19.997: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 02/27/23 12:12:20.004
    STEP: Then the orphan pod is adopted 02/27/23 12:12:20.016
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 27 12:12:21.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1236" for this suite. 02/27/23 12:12:21.059
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:12:21.073
Feb 27 12:12:21.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 12:12:21.074
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:21.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:21.128
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-75a5060b-7ca0-4d24-b22f-52c3029d3b6f 02/27/23 12:12:21.139
STEP: Creating a pod to test consume configMaps 02/27/23 12:12:21.158
Feb 27 12:12:21.184: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645" in namespace "configmap-5292" to be "Succeeded or Failed"
Feb 27 12:12:21.195: INFO: Pod "pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645": Phase="Pending", Reason="", readiness=false. Elapsed: 10.901674ms
Feb 27 12:12:23.209: INFO: Pod "pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024193811s
Feb 27 12:12:25.206: INFO: Pod "pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021456774s
STEP: Saw pod success 02/27/23 12:12:25.206
Feb 27 12:12:25.206: INFO: Pod "pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645" satisfied condition "Succeeded or Failed"
Feb 27 12:12:25.215: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645 container configmap-volume-test: <nil>
STEP: delete the pod 02/27/23 12:12:25.23
Feb 27 12:12:25.256: INFO: Waiting for pod pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645 to disappear
Feb 27 12:12:25.264: INFO: Pod pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 12:12:25.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5292" for this suite. 02/27/23 12:12:25.276
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":198,"skipped":3952,"failed":0}
------------------------------
• [4.220 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:12:21.073
    Feb 27 12:12:21.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 12:12:21.074
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:21.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:21.128
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-75a5060b-7ca0-4d24-b22f-52c3029d3b6f 02/27/23 12:12:21.139
    STEP: Creating a pod to test consume configMaps 02/27/23 12:12:21.158
    Feb 27 12:12:21.184: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645" in namespace "configmap-5292" to be "Succeeded or Failed"
    Feb 27 12:12:21.195: INFO: Pod "pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645": Phase="Pending", Reason="", readiness=false. Elapsed: 10.901674ms
    Feb 27 12:12:23.209: INFO: Pod "pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024193811s
    Feb 27 12:12:25.206: INFO: Pod "pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021456774s
    STEP: Saw pod success 02/27/23 12:12:25.206
    Feb 27 12:12:25.206: INFO: Pod "pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645" satisfied condition "Succeeded or Failed"
    Feb 27 12:12:25.215: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645 container configmap-volume-test: <nil>
    STEP: delete the pod 02/27/23 12:12:25.23
    Feb 27 12:12:25.256: INFO: Waiting for pod pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645 to disappear
    Feb 27 12:12:25.264: INFO: Pod pod-configmaps-0ed7f092-9784-4b56-8ce6-bd41295f5645 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 12:12:25.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5292" for this suite. 02/27/23 12:12:25.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:12:25.306
Feb 27 12:12:25.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename runtimeclass 02/27/23 12:12:25.309
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:25.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:25.365
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-8616-delete-me 02/27/23 12:12:25.391
STEP: Waiting for the RuntimeClass to disappear 02/27/23 12:12:25.407
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 27 12:12:25.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8616" for this suite. 02/27/23 12:12:25.443
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":199,"skipped":3990,"failed":0}
------------------------------
• [0.154 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:12:25.306
    Feb 27 12:12:25.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 12:12:25.309
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:25.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:25.365
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-8616-delete-me 02/27/23 12:12:25.391
    STEP: Waiting for the RuntimeClass to disappear 02/27/23 12:12:25.407
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 27 12:12:25.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8616" for this suite. 02/27/23 12:12:25.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:12:25.463
Feb 27 12:12:25.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename proxy 02/27/23 12:12:25.466
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:25.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:25.509
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Feb 27 12:12:25.526: INFO: Creating pod...
Feb 27 12:12:25.549: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6923" to be "running"
Feb 27 12:12:25.585: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 35.028632ms
Feb 27 12:12:27.598: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.04864929s
Feb 27 12:12:27.598: INFO: Pod "agnhost" satisfied condition "running"
Feb 27 12:12:27.598: INFO: Creating service...
Feb 27 12:12:27.628: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/DELETE
Feb 27 12:12:27.648: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 27 12:12:27.648: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/GET
Feb 27 12:12:27.662: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 27 12:12:27.662: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/HEAD
Feb 27 12:12:27.674: INFO: http.Client request:HEAD | StatusCode:200
Feb 27 12:12:27.675: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/OPTIONS
Feb 27 12:12:27.688: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 27 12:12:27.690: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/PATCH
Feb 27 12:12:27.703: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 27 12:12:27.703: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/POST
Feb 27 12:12:27.716: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 27 12:12:27.716: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/PUT
Feb 27 12:12:27.727: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 27 12:12:27.727: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/DELETE
Feb 27 12:12:27.738: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 27 12:12:27.738: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/GET
Feb 27 12:12:27.763: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 27 12:12:27.763: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/HEAD
Feb 27 12:12:27.781: INFO: http.Client request:HEAD | StatusCode:200
Feb 27 12:12:27.781: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/OPTIONS
Feb 27 12:12:27.804: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 27 12:12:27.804: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/PATCH
Feb 27 12:12:27.839: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 27 12:12:27.839: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/POST
Feb 27 12:12:27.851: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 27 12:12:27.852: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/PUT
Feb 27 12:12:27.863: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb 27 12:12:27.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6923" for this suite. 02/27/23 12:12:27.88
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":200,"skipped":4005,"failed":0}
------------------------------
• [2.435 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:12:25.463
    Feb 27 12:12:25.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename proxy 02/27/23 12:12:25.466
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:25.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:25.509
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Feb 27 12:12:25.526: INFO: Creating pod...
    Feb 27 12:12:25.549: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6923" to be "running"
    Feb 27 12:12:25.585: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 35.028632ms
    Feb 27 12:12:27.598: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.04864929s
    Feb 27 12:12:27.598: INFO: Pod "agnhost" satisfied condition "running"
    Feb 27 12:12:27.598: INFO: Creating service...
    Feb 27 12:12:27.628: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/DELETE
    Feb 27 12:12:27.648: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 27 12:12:27.648: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/GET
    Feb 27 12:12:27.662: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 27 12:12:27.662: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/HEAD
    Feb 27 12:12:27.674: INFO: http.Client request:HEAD | StatusCode:200
    Feb 27 12:12:27.675: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/OPTIONS
    Feb 27 12:12:27.688: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 27 12:12:27.690: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/PATCH
    Feb 27 12:12:27.703: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 27 12:12:27.703: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/POST
    Feb 27 12:12:27.716: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 27 12:12:27.716: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/pods/agnhost/proxy/some/path/with/PUT
    Feb 27 12:12:27.727: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 27 12:12:27.727: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/DELETE
    Feb 27 12:12:27.738: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 27 12:12:27.738: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/GET
    Feb 27 12:12:27.763: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 27 12:12:27.763: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/HEAD
    Feb 27 12:12:27.781: INFO: http.Client request:HEAD | StatusCode:200
    Feb 27 12:12:27.781: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/OPTIONS
    Feb 27 12:12:27.804: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 27 12:12:27.804: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/PATCH
    Feb 27 12:12:27.839: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 27 12:12:27.839: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/POST
    Feb 27 12:12:27.851: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 27 12:12:27.852: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-6923/services/test-service/proxy/some/path/with/PUT
    Feb 27 12:12:27.863: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb 27 12:12:27.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6923" for this suite. 02/27/23 12:12:27.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:12:27.901
Feb 27 12:12:27.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 12:12:27.902
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:27.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:27.947
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 12:12:27.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5478" for this suite. 02/27/23 12:12:27.995
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":201,"skipped":4022,"failed":0}
------------------------------
• [0.122 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:12:27.901
    Feb 27 12:12:27.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 12:12:27.902
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:27.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:27.947
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 12:12:27.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5478" for this suite. 02/27/23 12:12:27.995
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:12:28.026
Feb 27 12:12:28.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename init-container 02/27/23 12:12:28.028
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:28.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:28.086
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 02/27/23 12:12:28.103
Feb 27 12:12:28.103: INFO: PodSpec: initContainers in spec.initContainers
Feb 27 12:13:07.606: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1cd53842-6a61-4026-b653-9360846dd428", GenerateName:"", Namespace:"init-container-6482", SelfLink:"", UID:"f361ada0-da2c-4cdf-8917-0d7342cdee7e", ResourceVersion:"83590", Generation:0, CreationTimestamp:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"103613448"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"db7e3c283d1bae6cddd7fd14e71bc10250afa0333019130f1a03ed2ff13951c6", "cni.projectcalico.org/podIP":"172.25.2.172/32", "cni.projectcalico.org/podIPs":"172.25.2.172/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012e0930), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012e0960), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 12, 13, 7, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012e0990), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-nxm9b", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc006877cc0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nxm9b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nxm9b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nxm9b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005967808), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-15-17.eu-central-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000561c70), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005967880)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0059678a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0059678a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0059678ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00131d750), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.15.17", PodIP:"172.25.2.172", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.2.172"}}, StartTime:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000561dc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000561e30)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://67c7f3974ed24198f9f81d89da1e4954ad4ddd5e24e3e345d83d47c762abb1be", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006877d40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006877d20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00596792f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 27 12:13:07.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6482" for this suite. 02/27/23 12:13:07.62
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":202,"skipped":4034,"failed":0}
------------------------------
• [SLOW TEST] [39.608 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:12:28.026
    Feb 27 12:12:28.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename init-container 02/27/23 12:12:28.028
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:12:28.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:12:28.086
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 02/27/23 12:12:28.103
    Feb 27 12:12:28.103: INFO: PodSpec: initContainers in spec.initContainers
    Feb 27 12:13:07.606: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1cd53842-6a61-4026-b653-9360846dd428", GenerateName:"", Namespace:"init-container-6482", SelfLink:"", UID:"f361ada0-da2c-4cdf-8917-0d7342cdee7e", ResourceVersion:"83590", Generation:0, CreationTimestamp:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"103613448"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"db7e3c283d1bae6cddd7fd14e71bc10250afa0333019130f1a03ed2ff13951c6", "cni.projectcalico.org/podIP":"172.25.2.172/32", "cni.projectcalico.org/podIPs":"172.25.2.172/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012e0930), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012e0960), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 12, 13, 7, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012e0990), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-nxm9b", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc006877cc0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nxm9b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nxm9b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nxm9b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005967808), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-15-17.eu-central-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000561c70), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005967880)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0059678a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0059678a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0059678ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00131d750), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.15.17", PodIP:"172.25.2.172", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.2.172"}}, StartTime:time.Date(2023, time.February, 27, 12, 12, 28, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000561dc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000561e30)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://67c7f3974ed24198f9f81d89da1e4954ad4ddd5e24e3e345d83d47c762abb1be", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006877d40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006877d20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00596792f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 27 12:13:07.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6482" for this suite. 02/27/23 12:13:07.62
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:13:07.637
Feb 27 12:13:07.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename var-expansion 02/27/23 12:13:07.638
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:13:07.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:13:07.675
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 02/27/23 12:13:07.685
Feb 27 12:13:07.699: INFO: Waiting up to 5m0s for pod "var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8" in namespace "var-expansion-2505" to be "Succeeded or Failed"
Feb 27 12:13:07.711: INFO: Pod "var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.536962ms
Feb 27 12:13:09.721: INFO: Pod "var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022250651s
Feb 27 12:13:11.722: INFO: Pod "var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022697461s
STEP: Saw pod success 02/27/23 12:13:11.722
Feb 27 12:13:11.722: INFO: Pod "var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8" satisfied condition "Succeeded or Failed"
Feb 27 12:13:11.735: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8 container dapi-container: <nil>
STEP: delete the pod 02/27/23 12:13:11.753
Feb 27 12:13:11.774: INFO: Waiting for pod var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8 to disappear
Feb 27 12:13:11.781: INFO: Pod var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 27 12:13:11.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2505" for this suite. 02/27/23 12:13:11.793
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":203,"skipped":4037,"failed":0}
------------------------------
• [4.171 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:13:07.637
    Feb 27 12:13:07.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename var-expansion 02/27/23 12:13:07.638
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:13:07.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:13:07.675
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 02/27/23 12:13:07.685
    Feb 27 12:13:07.699: INFO: Waiting up to 5m0s for pod "var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8" in namespace "var-expansion-2505" to be "Succeeded or Failed"
    Feb 27 12:13:07.711: INFO: Pod "var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.536962ms
    Feb 27 12:13:09.721: INFO: Pod "var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022250651s
    Feb 27 12:13:11.722: INFO: Pod "var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022697461s
    STEP: Saw pod success 02/27/23 12:13:11.722
    Feb 27 12:13:11.722: INFO: Pod "var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8" satisfied condition "Succeeded or Failed"
    Feb 27 12:13:11.735: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 12:13:11.753
    Feb 27 12:13:11.774: INFO: Waiting for pod var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8 to disappear
    Feb 27 12:13:11.781: INFO: Pod var-expansion-36e6f01d-b456-485f-bed2-920aab8d9cc8 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 27 12:13:11.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2505" for this suite. 02/27/23 12:13:11.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:13:11.814
Feb 27 12:13:11.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir-wrapper 02/27/23 12:13:11.815
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:13:11.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:13:11.895
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 02/27/23 12:13:11.905
STEP: Creating RC which spawns configmap-volume pods 02/27/23 12:13:12.568
Feb 27 12:13:12.604: INFO: Pod name wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1: Found 0 pods out of 5
Feb 27 12:13:17.633: INFO: Pod name wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/27/23 12:13:17.633
Feb 27 12:13:17.634: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:13:17.655: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 21.255357ms
Feb 27 12:13:19.668: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034558019s
Feb 27 12:13:21.666: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031845101s
Feb 27 12:13:23.666: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032363225s
Feb 27 12:13:25.668: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033834733s
Feb 27 12:13:27.666: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9": Phase="Running", Reason="", readiness=true. Elapsed: 10.032543599s
Feb 27 12:13:27.667: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9" satisfied condition "running"
Feb 27 12:13:27.667: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-8nzsm" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:13:27.680: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-8nzsm": Phase="Running", Reason="", readiness=true. Elapsed: 13.657665ms
Feb 27 12:13:27.681: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-8nzsm" satisfied condition "running"
Feb 27 12:13:27.681: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-gxqd4" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:13:27.690: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-gxqd4": Phase="Running", Reason="", readiness=true. Elapsed: 9.127247ms
Feb 27 12:13:27.690: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-gxqd4" satisfied condition "running"
Feb 27 12:13:27.690: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-mkjqh" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:13:27.697: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-mkjqh": Phase="Running", Reason="", readiness=true. Elapsed: 7.40231ms
Feb 27 12:13:27.697: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-mkjqh" satisfied condition "running"
Feb 27 12:13:27.697: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-wjhg6" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:13:27.705: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-wjhg6": Phase="Running", Reason="", readiness=true. Elapsed: 7.357109ms
Feb 27 12:13:27.705: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-wjhg6" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1 in namespace emptydir-wrapper-5802, will wait for the garbage collector to delete the pods 02/27/23 12:13:27.705
Feb 27 12:13:27.787: INFO: Deleting ReplicationController wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1 took: 23.679916ms
Feb 27 12:13:27.993: INFO: Terminating ReplicationController wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1 pods took: 205.712192ms
STEP: Creating RC which spawns configmap-volume pods 02/27/23 12:13:31.805
Feb 27 12:13:31.841: INFO: Pod name wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639: Found 0 pods out of 5
Feb 27 12:13:36.861: INFO: Pod name wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/27/23 12:13:36.861
Feb 27 12:13:36.861: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:13:36.869: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.223573ms
Feb 27 12:13:38.928: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067084107s
Feb 27 12:13:40.886: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024664509s
Feb 27 12:13:42.887: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02556493s
Feb 27 12:13:44.884: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022295244s
Feb 27 12:13:46.901: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4": Phase="Running", Reason="", readiness=true. Elapsed: 10.039660183s
Feb 27 12:13:46.901: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4" satisfied condition "running"
Feb 27 12:13:46.901: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-mlkt7" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:13:46.915: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-mlkt7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.242489ms
Feb 27 12:13:48.926: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-mlkt7": Phase="Running", Reason="", readiness=true. Elapsed: 2.024849252s
Feb 27 12:13:48.926: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-mlkt7" satisfied condition "running"
Feb 27 12:13:48.927: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-s67wf" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:13:48.936: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-s67wf": Phase="Running", Reason="", readiness=true. Elapsed: 9.337788ms
Feb 27 12:13:48.936: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-s67wf" satisfied condition "running"
Feb 27 12:13:48.936: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-t9pv9" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:13:48.944: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-t9pv9": Phase="Running", Reason="", readiness=true. Elapsed: 8.163254ms
Feb 27 12:13:48.944: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-t9pv9" satisfied condition "running"
Feb 27 12:13:48.944: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-tjlc5" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:13:48.952: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-tjlc5": Phase="Running", Reason="", readiness=true. Elapsed: 7.974883ms
Feb 27 12:13:48.953: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-tjlc5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639 in namespace emptydir-wrapper-5802, will wait for the garbage collector to delete the pods 02/27/23 12:13:48.953
Feb 27 12:13:49.032: INFO: Deleting ReplicationController wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639 took: 17.311361ms
Feb 27 12:13:49.133: INFO: Terminating ReplicationController wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639 pods took: 100.997185ms
STEP: Creating RC which spawns configmap-volume pods 02/27/23 12:13:52.445
Feb 27 12:13:52.493: INFO: Pod name wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a: Found 0 pods out of 5
Feb 27 12:13:57.512: INFO: Pod name wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/27/23 12:13:57.512
Feb 27 12:13:57.512: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:13:57.522: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.421648ms
Feb 27 12:13:59.536: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022987742s
Feb 27 12:14:01.534: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021500427s
Feb 27 12:14:03.536: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023515349s
Feb 27 12:14:05.539: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026770184s
Feb 27 12:14:07.534: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021132494s
Feb 27 12:14:09.534: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Running", Reason="", readiness=true. Elapsed: 12.021555097s
Feb 27 12:14:09.534: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf" satisfied condition "running"
Feb 27 12:14:09.534: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-cl5c9" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:14:09.544: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-cl5c9": Phase="Running", Reason="", readiness=true. Elapsed: 9.281897ms
Feb 27 12:14:09.544: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-cl5c9" satisfied condition "running"
Feb 27 12:14:09.544: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-dgn2z" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:14:09.553: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-dgn2z": Phase="Running", Reason="", readiness=true. Elapsed: 9.686679ms
Feb 27 12:14:09.553: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-dgn2z" satisfied condition "running"
Feb 27 12:14:09.553: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-j5tcf" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:14:09.567: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-j5tcf": Phase="Running", Reason="", readiness=true. Elapsed: 13.666925ms
Feb 27 12:14:09.567: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-j5tcf" satisfied condition "running"
Feb 27 12:14:09.567: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-pdz8r" in namespace "emptydir-wrapper-5802" to be "running"
Feb 27 12:14:09.584: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-pdz8r": Phase="Running", Reason="", readiness=true. Elapsed: 16.924618ms
Feb 27 12:14:09.584: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-pdz8r" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a in namespace emptydir-wrapper-5802, will wait for the garbage collector to delete the pods 02/27/23 12:14:09.584
Feb 27 12:14:09.666: INFO: Deleting ReplicationController wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a took: 22.10343ms
Feb 27 12:14:09.767: INFO: Terminating ReplicationController wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a pods took: 100.90969ms
STEP: Cleaning up the configMaps 02/27/23 12:14:13.667
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Feb 27 12:14:14.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5802" for this suite. 02/27/23 12:14:14.37
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":204,"skipped":4047,"failed":0}
------------------------------
• [SLOW TEST] [62.570 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:13:11.814
    Feb 27 12:13:11.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir-wrapper 02/27/23 12:13:11.815
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:13:11.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:13:11.895
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 02/27/23 12:13:11.905
    STEP: Creating RC which spawns configmap-volume pods 02/27/23 12:13:12.568
    Feb 27 12:13:12.604: INFO: Pod name wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1: Found 0 pods out of 5
    Feb 27 12:13:17.633: INFO: Pod name wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/27/23 12:13:17.633
    Feb 27 12:13:17.634: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:13:17.655: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 21.255357ms
    Feb 27 12:13:19.668: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034558019s
    Feb 27 12:13:21.666: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031845101s
    Feb 27 12:13:23.666: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032363225s
    Feb 27 12:13:25.668: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033834733s
    Feb 27 12:13:27.666: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9": Phase="Running", Reason="", readiness=true. Elapsed: 10.032543599s
    Feb 27 12:13:27.667: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-7z6v9" satisfied condition "running"
    Feb 27 12:13:27.667: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-8nzsm" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:13:27.680: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-8nzsm": Phase="Running", Reason="", readiness=true. Elapsed: 13.657665ms
    Feb 27 12:13:27.681: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-8nzsm" satisfied condition "running"
    Feb 27 12:13:27.681: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-gxqd4" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:13:27.690: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-gxqd4": Phase="Running", Reason="", readiness=true. Elapsed: 9.127247ms
    Feb 27 12:13:27.690: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-gxqd4" satisfied condition "running"
    Feb 27 12:13:27.690: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-mkjqh" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:13:27.697: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-mkjqh": Phase="Running", Reason="", readiness=true. Elapsed: 7.40231ms
    Feb 27 12:13:27.697: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-mkjqh" satisfied condition "running"
    Feb 27 12:13:27.697: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-wjhg6" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:13:27.705: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-wjhg6": Phase="Running", Reason="", readiness=true. Elapsed: 7.357109ms
    Feb 27 12:13:27.705: INFO: Pod "wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1-wjhg6" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1 in namespace emptydir-wrapper-5802, will wait for the garbage collector to delete the pods 02/27/23 12:13:27.705
    Feb 27 12:13:27.787: INFO: Deleting ReplicationController wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1 took: 23.679916ms
    Feb 27 12:13:27.993: INFO: Terminating ReplicationController wrapped-volume-race-0f5ba99b-dee3-4c3d-89ad-b4dcb81c31d1 pods took: 205.712192ms
    STEP: Creating RC which spawns configmap-volume pods 02/27/23 12:13:31.805
    Feb 27 12:13:31.841: INFO: Pod name wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639: Found 0 pods out of 5
    Feb 27 12:13:36.861: INFO: Pod name wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/27/23 12:13:36.861
    Feb 27 12:13:36.861: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:13:36.869: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.223573ms
    Feb 27 12:13:38.928: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067084107s
    Feb 27 12:13:40.886: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024664509s
    Feb 27 12:13:42.887: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02556493s
    Feb 27 12:13:44.884: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022295244s
    Feb 27 12:13:46.901: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4": Phase="Running", Reason="", readiness=true. Elapsed: 10.039660183s
    Feb 27 12:13:46.901: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-gvtp4" satisfied condition "running"
    Feb 27 12:13:46.901: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-mlkt7" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:13:46.915: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-mlkt7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.242489ms
    Feb 27 12:13:48.926: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-mlkt7": Phase="Running", Reason="", readiness=true. Elapsed: 2.024849252s
    Feb 27 12:13:48.926: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-mlkt7" satisfied condition "running"
    Feb 27 12:13:48.927: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-s67wf" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:13:48.936: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-s67wf": Phase="Running", Reason="", readiness=true. Elapsed: 9.337788ms
    Feb 27 12:13:48.936: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-s67wf" satisfied condition "running"
    Feb 27 12:13:48.936: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-t9pv9" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:13:48.944: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-t9pv9": Phase="Running", Reason="", readiness=true. Elapsed: 8.163254ms
    Feb 27 12:13:48.944: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-t9pv9" satisfied condition "running"
    Feb 27 12:13:48.944: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-tjlc5" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:13:48.952: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-tjlc5": Phase="Running", Reason="", readiness=true. Elapsed: 7.974883ms
    Feb 27 12:13:48.953: INFO: Pod "wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639-tjlc5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639 in namespace emptydir-wrapper-5802, will wait for the garbage collector to delete the pods 02/27/23 12:13:48.953
    Feb 27 12:13:49.032: INFO: Deleting ReplicationController wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639 took: 17.311361ms
    Feb 27 12:13:49.133: INFO: Terminating ReplicationController wrapped-volume-race-b0282dcc-94ea-47b7-a5d9-2ef8a1ed1639 pods took: 100.997185ms
    STEP: Creating RC which spawns configmap-volume pods 02/27/23 12:13:52.445
    Feb 27 12:13:52.493: INFO: Pod name wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a: Found 0 pods out of 5
    Feb 27 12:13:57.512: INFO: Pod name wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/27/23 12:13:57.512
    Feb 27 12:13:57.512: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:13:57.522: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.421648ms
    Feb 27 12:13:59.536: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022987742s
    Feb 27 12:14:01.534: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021500427s
    Feb 27 12:14:03.536: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023515349s
    Feb 27 12:14:05.539: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026770184s
    Feb 27 12:14:07.534: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021132494s
    Feb 27 12:14:09.534: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf": Phase="Running", Reason="", readiness=true. Elapsed: 12.021555097s
    Feb 27 12:14:09.534: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-8jxqf" satisfied condition "running"
    Feb 27 12:14:09.534: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-cl5c9" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:14:09.544: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-cl5c9": Phase="Running", Reason="", readiness=true. Elapsed: 9.281897ms
    Feb 27 12:14:09.544: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-cl5c9" satisfied condition "running"
    Feb 27 12:14:09.544: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-dgn2z" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:14:09.553: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-dgn2z": Phase="Running", Reason="", readiness=true. Elapsed: 9.686679ms
    Feb 27 12:14:09.553: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-dgn2z" satisfied condition "running"
    Feb 27 12:14:09.553: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-j5tcf" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:14:09.567: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-j5tcf": Phase="Running", Reason="", readiness=true. Elapsed: 13.666925ms
    Feb 27 12:14:09.567: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-j5tcf" satisfied condition "running"
    Feb 27 12:14:09.567: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-pdz8r" in namespace "emptydir-wrapper-5802" to be "running"
    Feb 27 12:14:09.584: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-pdz8r": Phase="Running", Reason="", readiness=true. Elapsed: 16.924618ms
    Feb 27 12:14:09.584: INFO: Pod "wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a-pdz8r" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a in namespace emptydir-wrapper-5802, will wait for the garbage collector to delete the pods 02/27/23 12:14:09.584
    Feb 27 12:14:09.666: INFO: Deleting ReplicationController wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a took: 22.10343ms
    Feb 27 12:14:09.767: INFO: Terminating ReplicationController wrapped-volume-race-669be25b-6550-4bb0-bf77-1126ae6a2a5a pods took: 100.90969ms
    STEP: Cleaning up the configMaps 02/27/23 12:14:13.667
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Feb 27 12:14:14.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-5802" for this suite. 02/27/23 12:14:14.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:14:14.387
Feb 27 12:14:14.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 12:14:14.389
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:14:14.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:14:14.424
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 02/27/23 12:14:14.434
Feb 27 12:14:14.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: rename a version 02/27/23 12:14:26.072
STEP: check the new version name is served 02/27/23 12:14:26.105
STEP: check the old version name is removed 02/27/23 12:14:30.268
STEP: check the other version is not changed 02/27/23 12:14:32.209
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:14:41.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1864" for this suite. 02/27/23 12:14:41.714
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":205,"skipped":4067,"failed":0}
------------------------------
• [SLOW TEST] [27.339 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:14:14.387
    Feb 27 12:14:14.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 12:14:14.389
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:14:14.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:14:14.424
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 02/27/23 12:14:14.434
    Feb 27 12:14:14.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: rename a version 02/27/23 12:14:26.072
    STEP: check the new version name is served 02/27/23 12:14:26.105
    STEP: check the old version name is removed 02/27/23 12:14:30.268
    STEP: check the other version is not changed 02/27/23 12:14:32.209
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:14:41.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1864" for this suite. 02/27/23 12:14:41.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:14:41.73
Feb 27 12:14:41.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 12:14:41.731
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:14:41.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:14:41.804
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 02/27/23 12:14:41.821
Feb 27 12:14:41.854: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88" in namespace "downward-api-1902" to be "Succeeded or Failed"
Feb 27 12:14:41.863: INFO: Pod "downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88": Phase="Pending", Reason="", readiness=false. Elapsed: 8.982127ms
Feb 27 12:14:43.873: INFO: Pod "downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01837619s
Feb 27 12:14:45.877: INFO: Pod "downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022903768s
STEP: Saw pod success 02/27/23 12:14:45.877
Feb 27 12:14:45.877: INFO: Pod "downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88" satisfied condition "Succeeded or Failed"
Feb 27 12:14:45.886: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88 container client-container: <nil>
STEP: delete the pod 02/27/23 12:14:45.904
Feb 27 12:14:45.932: INFO: Waiting for pod downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88 to disappear
Feb 27 12:14:45.939: INFO: Pod downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 27 12:14:45.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1902" for this suite. 02/27/23 12:14:45.958
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":206,"skipped":4076,"failed":0}
------------------------------
• [4.244 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:14:41.73
    Feb 27 12:14:41.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 12:14:41.731
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:14:41.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:14:41.804
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 02/27/23 12:14:41.821
    Feb 27 12:14:41.854: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88" in namespace "downward-api-1902" to be "Succeeded or Failed"
    Feb 27 12:14:41.863: INFO: Pod "downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88": Phase="Pending", Reason="", readiness=false. Elapsed: 8.982127ms
    Feb 27 12:14:43.873: INFO: Pod "downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01837619s
    Feb 27 12:14:45.877: INFO: Pod "downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022903768s
    STEP: Saw pod success 02/27/23 12:14:45.877
    Feb 27 12:14:45.877: INFO: Pod "downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88" satisfied condition "Succeeded or Failed"
    Feb 27 12:14:45.886: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88 container client-container: <nil>
    STEP: delete the pod 02/27/23 12:14:45.904
    Feb 27 12:14:45.932: INFO: Waiting for pod downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88 to disappear
    Feb 27 12:14:45.939: INFO: Pod downwardapi-volume-82fbca0d-4b32-4aa8-b768-2bc02c8aab88 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 27 12:14:45.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1902" for this suite. 02/27/23 12:14:45.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:14:45.975
Feb 27 12:14:45.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename subpath 02/27/23 12:14:45.976
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:14:46.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:14:46.025
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 12:14:46.04
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-92m9 02/27/23 12:14:46.072
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 12:14:46.072
Feb 27 12:14:46.093: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-92m9" in namespace "subpath-2315" to be "Succeeded or Failed"
Feb 27 12:14:46.101: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.112304ms
Feb 27 12:14:48.113: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 2.01935861s
Feb 27 12:14:50.113: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 4.019980891s
Feb 27 12:14:52.114: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 6.020615601s
Feb 27 12:14:54.114: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 8.021025961s
Feb 27 12:14:56.113: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 10.019491621s
Feb 27 12:14:58.111: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 12.018231166s
Feb 27 12:15:00.118: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 14.025159254s
Feb 27 12:15:02.113: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 16.019546224s
Feb 27 12:15:04.110: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 18.017194678s
Feb 27 12:15:06.111: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 20.018215788s
Feb 27 12:15:08.118: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=false. Elapsed: 22.024491499s
Feb 27 12:15:10.112: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018360517s
STEP: Saw pod success 02/27/23 12:15:10.112
Feb 27 12:15:10.112: INFO: Pod "pod-subpath-test-projected-92m9" satisfied condition "Succeeded or Failed"
Feb 27 12:15:10.121: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-subpath-test-projected-92m9 container test-container-subpath-projected-92m9: <nil>
STEP: delete the pod 02/27/23 12:15:10.142
Feb 27 12:15:10.206: INFO: Waiting for pod pod-subpath-test-projected-92m9 to disappear
Feb 27 12:15:10.216: INFO: Pod pod-subpath-test-projected-92m9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-92m9 02/27/23 12:15:10.216
Feb 27 12:15:10.217: INFO: Deleting pod "pod-subpath-test-projected-92m9" in namespace "subpath-2315"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 27 12:15:10.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2315" for this suite. 02/27/23 12:15:10.263
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":207,"skipped":4086,"failed":0}
------------------------------
• [SLOW TEST] [24.307 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:14:45.975
    Feb 27 12:14:45.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename subpath 02/27/23 12:14:45.976
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:14:46.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:14:46.025
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 12:14:46.04
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-92m9 02/27/23 12:14:46.072
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 12:14:46.072
    Feb 27 12:14:46.093: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-92m9" in namespace "subpath-2315" to be "Succeeded or Failed"
    Feb 27 12:14:46.101: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.112304ms
    Feb 27 12:14:48.113: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 2.01935861s
    Feb 27 12:14:50.113: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 4.019980891s
    Feb 27 12:14:52.114: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 6.020615601s
    Feb 27 12:14:54.114: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 8.021025961s
    Feb 27 12:14:56.113: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 10.019491621s
    Feb 27 12:14:58.111: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 12.018231166s
    Feb 27 12:15:00.118: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 14.025159254s
    Feb 27 12:15:02.113: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 16.019546224s
    Feb 27 12:15:04.110: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 18.017194678s
    Feb 27 12:15:06.111: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=true. Elapsed: 20.018215788s
    Feb 27 12:15:08.118: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Running", Reason="", readiness=false. Elapsed: 22.024491499s
    Feb 27 12:15:10.112: INFO: Pod "pod-subpath-test-projected-92m9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018360517s
    STEP: Saw pod success 02/27/23 12:15:10.112
    Feb 27 12:15:10.112: INFO: Pod "pod-subpath-test-projected-92m9" satisfied condition "Succeeded or Failed"
    Feb 27 12:15:10.121: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-subpath-test-projected-92m9 container test-container-subpath-projected-92m9: <nil>
    STEP: delete the pod 02/27/23 12:15:10.142
    Feb 27 12:15:10.206: INFO: Waiting for pod pod-subpath-test-projected-92m9 to disappear
    Feb 27 12:15:10.216: INFO: Pod pod-subpath-test-projected-92m9 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-92m9 02/27/23 12:15:10.216
    Feb 27 12:15:10.217: INFO: Deleting pod "pod-subpath-test-projected-92m9" in namespace "subpath-2315"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 27 12:15:10.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2315" for this suite. 02/27/23 12:15:10.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:15:10.293
Feb 27 12:15:10.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 12:15:10.294
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:15:10.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:15:10.348
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-9784e270-086b-4690-987b-3d95ec1791af 02/27/23 12:15:10.368
STEP: Creating a pod to test consume secrets 02/27/23 12:15:10.389
Feb 27 12:15:10.412: INFO: Waiting up to 5m0s for pod "pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316" in namespace "secrets-7334" to be "Succeeded or Failed"
Feb 27 12:15:10.421: INFO: Pod "pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316": Phase="Pending", Reason="", readiness=false. Elapsed: 8.941668ms
Feb 27 12:15:12.436: INFO: Pod "pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023398632s
Feb 27 12:15:14.433: INFO: Pod "pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020365007s
STEP: Saw pod success 02/27/23 12:15:14.433
Feb 27 12:15:14.433: INFO: Pod "pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316" satisfied condition "Succeeded or Failed"
Feb 27 12:15:14.441: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 12:15:14.465
Feb 27 12:15:14.487: INFO: Waiting for pod pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316 to disappear
Feb 27 12:15:14.496: INFO: Pod pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 27 12:15:14.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7334" for this suite. 02/27/23 12:15:14.511
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":208,"skipped":4111,"failed":0}
------------------------------
• [4.240 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:15:10.293
    Feb 27 12:15:10.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 12:15:10.294
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:15:10.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:15:10.348
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-9784e270-086b-4690-987b-3d95ec1791af 02/27/23 12:15:10.368
    STEP: Creating a pod to test consume secrets 02/27/23 12:15:10.389
    Feb 27 12:15:10.412: INFO: Waiting up to 5m0s for pod "pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316" in namespace "secrets-7334" to be "Succeeded or Failed"
    Feb 27 12:15:10.421: INFO: Pod "pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316": Phase="Pending", Reason="", readiness=false. Elapsed: 8.941668ms
    Feb 27 12:15:12.436: INFO: Pod "pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023398632s
    Feb 27 12:15:14.433: INFO: Pod "pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020365007s
    STEP: Saw pod success 02/27/23 12:15:14.433
    Feb 27 12:15:14.433: INFO: Pod "pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316" satisfied condition "Succeeded or Failed"
    Feb 27 12:15:14.441: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 12:15:14.465
    Feb 27 12:15:14.487: INFO: Waiting for pod pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316 to disappear
    Feb 27 12:15:14.496: INFO: Pod pod-secrets-2b7ee348-4040-441b-baf8-ba1088b6f316 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 12:15:14.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7334" for this suite. 02/27/23 12:15:14.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:15:14.534
Feb 27 12:15:14.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename resourcequota 02/27/23 12:15:14.537
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:15:14.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:15:14.592
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 02/27/23 12:15:14.604
STEP: Getting a ResourceQuota 02/27/23 12:15:14.624
STEP: Updating a ResourceQuota 02/27/23 12:15:14.633
STEP: Verifying a ResourceQuota was modified 02/27/23 12:15:14.648
STEP: Deleting a ResourceQuota 02/27/23 12:15:14.657
STEP: Verifying the deleted ResourceQuota 02/27/23 12:15:14.671
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 27 12:15:14.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3991" for this suite. 02/27/23 12:15:14.693
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":209,"skipped":4128,"failed":0}
------------------------------
• [0.175 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:15:14.534
    Feb 27 12:15:14.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename resourcequota 02/27/23 12:15:14.537
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:15:14.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:15:14.592
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 02/27/23 12:15:14.604
    STEP: Getting a ResourceQuota 02/27/23 12:15:14.624
    STEP: Updating a ResourceQuota 02/27/23 12:15:14.633
    STEP: Verifying a ResourceQuota was modified 02/27/23 12:15:14.648
    STEP: Deleting a ResourceQuota 02/27/23 12:15:14.657
    STEP: Verifying the deleted ResourceQuota 02/27/23 12:15:14.671
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 27 12:15:14.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3991" for this suite. 02/27/23 12:15:14.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:15:14.721
Feb 27 12:15:14.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename sched-preemption 02/27/23 12:15:14.727
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:15:14.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:15:14.779
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 27 12:15:14.829: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 12:16:14.973: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 02/27/23 12:16:14.98
Feb 27 12:16:15.026: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 27 12:16:15.041: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 27 12:16:15.098: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 27 12:16:15.116: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 27 12:16:15.169: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 27 12:16:15.187: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/27/23 12:16:15.188
Feb 27 12:16:15.188: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-117" to be "running"
Feb 27 12:16:15.197: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.080508ms
Feb 27 12:16:17.216: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028012286s
Feb 27 12:16:19.207: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019298689s
Feb 27 12:16:21.207: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019086228s
Feb 27 12:16:23.207: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018862135s
Feb 27 12:16:25.206: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01799981s
Feb 27 12:16:27.208: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019995686s
Feb 27 12:16:29.208: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.020449196s
Feb 27 12:16:29.208: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 27 12:16:29.208: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-117" to be "running"
Feb 27 12:16:29.219: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.194362ms
Feb 27 12:16:29.219: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 12:16:29.219: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-117" to be "running"
Feb 27 12:16:29.233: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.621296ms
Feb 27 12:16:29.233: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 12:16:29.233: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-117" to be "running"
Feb 27 12:16:29.242: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.803666ms
Feb 27 12:16:29.242: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 12:16:29.242: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-117" to be "running"
Feb 27 12:16:29.249: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.532651ms
Feb 27 12:16:29.250: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 12:16:29.250: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-117" to be "running"
Feb 27 12:16:29.258: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.915377ms
Feb 27 12:16:29.259: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/27/23 12:16:29.259
Feb 27 12:16:29.281: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-117" to be "running"
Feb 27 12:16:29.304: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.731644ms
Feb 27 12:16:31.356: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075024789s
Feb 27 12:16:33.350: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.069251034s
Feb 27 12:16:33.350: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 27 12:16:33.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-117" for this suite. 02/27/23 12:16:33.438
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":210,"skipped":4166,"failed":0}
------------------------------
• [SLOW TEST] [78.870 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:15:14.721
    Feb 27 12:15:14.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename sched-preemption 02/27/23 12:15:14.727
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:15:14.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:15:14.779
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 27 12:15:14.829: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 12:16:14.973: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 02/27/23 12:16:14.98
    Feb 27 12:16:15.026: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 27 12:16:15.041: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 27 12:16:15.098: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 27 12:16:15.116: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 27 12:16:15.169: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 27 12:16:15.187: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/27/23 12:16:15.188
    Feb 27 12:16:15.188: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-117" to be "running"
    Feb 27 12:16:15.197: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.080508ms
    Feb 27 12:16:17.216: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028012286s
    Feb 27 12:16:19.207: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019298689s
    Feb 27 12:16:21.207: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019086228s
    Feb 27 12:16:23.207: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018862135s
    Feb 27 12:16:25.206: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01799981s
    Feb 27 12:16:27.208: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019995686s
    Feb 27 12:16:29.208: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.020449196s
    Feb 27 12:16:29.208: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 27 12:16:29.208: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-117" to be "running"
    Feb 27 12:16:29.219: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.194362ms
    Feb 27 12:16:29.219: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 12:16:29.219: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-117" to be "running"
    Feb 27 12:16:29.233: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.621296ms
    Feb 27 12:16:29.233: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 12:16:29.233: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-117" to be "running"
    Feb 27 12:16:29.242: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.803666ms
    Feb 27 12:16:29.242: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 12:16:29.242: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-117" to be "running"
    Feb 27 12:16:29.249: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.532651ms
    Feb 27 12:16:29.250: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 12:16:29.250: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-117" to be "running"
    Feb 27 12:16:29.258: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.915377ms
    Feb 27 12:16:29.259: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/27/23 12:16:29.259
    Feb 27 12:16:29.281: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-117" to be "running"
    Feb 27 12:16:29.304: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.731644ms
    Feb 27 12:16:31.356: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075024789s
    Feb 27 12:16:33.350: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.069251034s
    Feb 27 12:16:33.350: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 12:16:33.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-117" for this suite. 02/27/23 12:16:33.438
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:16:33.599
Feb 27 12:16:33.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename replicaset 02/27/23 12:16:33.6
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:33.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:33.653
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/27/23 12:16:33.662
Feb 27 12:16:33.681: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 27 12:16:38.692: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 12:16:38.692
STEP: getting scale subresource 02/27/23 12:16:38.692
STEP: updating a scale subresource 02/27/23 12:16:38.704
STEP: verifying the replicaset Spec.Replicas was modified 02/27/23 12:16:38.714
STEP: Patch a scale subresource 02/27/23 12:16:38.734
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 27 12:16:38.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4301" for this suite. 02/27/23 12:16:38.792
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":211,"skipped":4201,"failed":0}
------------------------------
• [SLOW TEST] [5.210 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:16:33.599
    Feb 27 12:16:33.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename replicaset 02/27/23 12:16:33.6
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:33.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:33.653
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/27/23 12:16:33.662
    Feb 27 12:16:33.681: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 27 12:16:38.692: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 12:16:38.692
    STEP: getting scale subresource 02/27/23 12:16:38.692
    STEP: updating a scale subresource 02/27/23 12:16:38.704
    STEP: verifying the replicaset Spec.Replicas was modified 02/27/23 12:16:38.714
    STEP: Patch a scale subresource 02/27/23 12:16:38.734
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 27 12:16:38.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4301" for this suite. 02/27/23 12:16:38.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:16:38.817
Feb 27 12:16:38.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubelet-test 02/27/23 12:16:38.818
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:38.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:38.885
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 27 12:16:42.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1799" for this suite. 02/27/23 12:16:42.939
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":212,"skipped":4253,"failed":0}
------------------------------
• [4.137 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:16:38.817
    Feb 27 12:16:38.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 12:16:38.818
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:38.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:38.885
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 27 12:16:42.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-1799" for this suite. 02/27/23 12:16:42.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:16:42.958
Feb 27 12:16:42.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 12:16:42.963
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:42.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:43.014
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-be82b45b-e8fc-4f12-9b65-740bbbc2d4e4 02/27/23 12:16:43.062
STEP: Creating configMap with name cm-test-opt-upd-d9589728-e2ed-4b7f-8c8c-80f7814bcbf1 02/27/23 12:16:43.07
STEP: Creating the pod 02/27/23 12:16:43.09
Feb 27 12:16:43.109: INFO: Waiting up to 5m0s for pod "pod-configmaps-1666b262-aa88-4f23-bd88-7105eccbf00b" in namespace "configmap-6677" to be "running and ready"
Feb 27 12:16:43.121: INFO: Pod "pod-configmaps-1666b262-aa88-4f23-bd88-7105eccbf00b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.495467ms
Feb 27 12:16:43.121: INFO: The phase of Pod pod-configmaps-1666b262-aa88-4f23-bd88-7105eccbf00b is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:16:45.131: INFO: Pod "pod-configmaps-1666b262-aa88-4f23-bd88-7105eccbf00b": Phase="Running", Reason="", readiness=true. Elapsed: 2.021473078s
Feb 27 12:16:45.131: INFO: The phase of Pod pod-configmaps-1666b262-aa88-4f23-bd88-7105eccbf00b is Running (Ready = true)
Feb 27 12:16:45.131: INFO: Pod "pod-configmaps-1666b262-aa88-4f23-bd88-7105eccbf00b" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-be82b45b-e8fc-4f12-9b65-740bbbc2d4e4 02/27/23 12:16:45.191
STEP: Updating configmap cm-test-opt-upd-d9589728-e2ed-4b7f-8c8c-80f7814bcbf1 02/27/23 12:16:45.203
STEP: Creating configMap with name cm-test-opt-create-4c126bb2-ac10-475f-8283-1b086efd7975 02/27/23 12:16:45.213
STEP: waiting to observe update in volume 02/27/23 12:16:45.224
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 12:16:47.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6677" for this suite. 02/27/23 12:16:47.314
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":213,"skipped":4262,"failed":0}
------------------------------
• [4.376 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:16:42.958
    Feb 27 12:16:42.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 12:16:42.963
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:42.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:43.014
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-be82b45b-e8fc-4f12-9b65-740bbbc2d4e4 02/27/23 12:16:43.062
    STEP: Creating configMap with name cm-test-opt-upd-d9589728-e2ed-4b7f-8c8c-80f7814bcbf1 02/27/23 12:16:43.07
    STEP: Creating the pod 02/27/23 12:16:43.09
    Feb 27 12:16:43.109: INFO: Waiting up to 5m0s for pod "pod-configmaps-1666b262-aa88-4f23-bd88-7105eccbf00b" in namespace "configmap-6677" to be "running and ready"
    Feb 27 12:16:43.121: INFO: Pod "pod-configmaps-1666b262-aa88-4f23-bd88-7105eccbf00b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.495467ms
    Feb 27 12:16:43.121: INFO: The phase of Pod pod-configmaps-1666b262-aa88-4f23-bd88-7105eccbf00b is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:16:45.131: INFO: Pod "pod-configmaps-1666b262-aa88-4f23-bd88-7105eccbf00b": Phase="Running", Reason="", readiness=true. Elapsed: 2.021473078s
    Feb 27 12:16:45.131: INFO: The phase of Pod pod-configmaps-1666b262-aa88-4f23-bd88-7105eccbf00b is Running (Ready = true)
    Feb 27 12:16:45.131: INFO: Pod "pod-configmaps-1666b262-aa88-4f23-bd88-7105eccbf00b" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-be82b45b-e8fc-4f12-9b65-740bbbc2d4e4 02/27/23 12:16:45.191
    STEP: Updating configmap cm-test-opt-upd-d9589728-e2ed-4b7f-8c8c-80f7814bcbf1 02/27/23 12:16:45.203
    STEP: Creating configMap with name cm-test-opt-create-4c126bb2-ac10-475f-8283-1b086efd7975 02/27/23 12:16:45.213
    STEP: waiting to observe update in volume 02/27/23 12:16:45.224
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 12:16:47.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6677" for this suite. 02/27/23 12:16:47.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:16:47.345
Feb 27 12:16:47.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename replication-controller 02/27/23 12:16:47.354
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:47.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:47.401
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Feb 27 12:16:47.409: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/27/23 12:16:47.436
STEP: Checking rc "condition-test" has the desired failure condition set 02/27/23 12:16:47.45
STEP: Scaling down rc "condition-test" to satisfy pod quota 02/27/23 12:16:48.471
Feb 27 12:16:48.493: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 02/27/23 12:16:48.493
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 27 12:16:49.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2369" for this suite. 02/27/23 12:16:49.524
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":214,"skipped":4271,"failed":0}
------------------------------
• [2.203 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:16:47.345
    Feb 27 12:16:47.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename replication-controller 02/27/23 12:16:47.354
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:47.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:47.401
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Feb 27 12:16:47.409: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/27/23 12:16:47.436
    STEP: Checking rc "condition-test" has the desired failure condition set 02/27/23 12:16:47.45
    STEP: Scaling down rc "condition-test" to satisfy pod quota 02/27/23 12:16:48.471
    Feb 27 12:16:48.493: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 02/27/23 12:16:48.493
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 27 12:16:49.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2369" for this suite. 02/27/23 12:16:49.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:16:49.552
Feb 27 12:16:49.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubelet-test 02/27/23 12:16:49.554
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:49.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:49.588
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 02/27/23 12:16:49.617
Feb 27 12:16:49.617: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases81ee5f72-4ebe-4167-8d4a-021c4e7aec4d" in namespace "kubelet-test-8795" to be "completed"
Feb 27 12:16:49.628: INFO: Pod "agnhost-host-aliases81ee5f72-4ebe-4167-8d4a-021c4e7aec4d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.786844ms
Feb 27 12:16:51.637: INFO: Pod "agnhost-host-aliases81ee5f72-4ebe-4167-8d4a-021c4e7aec4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019709751s
Feb 27 12:16:53.641: INFO: Pod "agnhost-host-aliases81ee5f72-4ebe-4167-8d4a-021c4e7aec4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023936141s
Feb 27 12:16:53.641: INFO: Pod "agnhost-host-aliases81ee5f72-4ebe-4167-8d4a-021c4e7aec4d" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 27 12:16:53.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8795" for this suite. 02/27/23 12:16:53.67
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":215,"skipped":4290,"failed":0}
------------------------------
• [4.129 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:16:49.552
    Feb 27 12:16:49.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 12:16:49.554
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:49.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:49.588
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 02/27/23 12:16:49.617
    Feb 27 12:16:49.617: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases81ee5f72-4ebe-4167-8d4a-021c4e7aec4d" in namespace "kubelet-test-8795" to be "completed"
    Feb 27 12:16:49.628: INFO: Pod "agnhost-host-aliases81ee5f72-4ebe-4167-8d4a-021c4e7aec4d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.786844ms
    Feb 27 12:16:51.637: INFO: Pod "agnhost-host-aliases81ee5f72-4ebe-4167-8d4a-021c4e7aec4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019709751s
    Feb 27 12:16:53.641: INFO: Pod "agnhost-host-aliases81ee5f72-4ebe-4167-8d4a-021c4e7aec4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023936141s
    Feb 27 12:16:53.641: INFO: Pod "agnhost-host-aliases81ee5f72-4ebe-4167-8d4a-021c4e7aec4d" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 27 12:16:53.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8795" for this suite. 02/27/23 12:16:53.67
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:16:53.686
Feb 27 12:16:53.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-runtime 02/27/23 12:16:53.688
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:53.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:53.733
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 02/27/23 12:16:53.742
STEP: wait for the container to reach Succeeded 02/27/23 12:16:53.755
STEP: get the container status 02/27/23 12:16:57.807
STEP: the container should be terminated 02/27/23 12:16:57.818
STEP: the termination message should be set 02/27/23 12:16:57.819
Feb 27 12:16:57.819: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/27/23 12:16:57.819
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 27 12:16:57.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1714" for this suite. 02/27/23 12:16:57.871
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":216,"skipped":4290,"failed":0}
------------------------------
• [4.197 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:16:53.686
    Feb 27 12:16:53.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-runtime 02/27/23 12:16:53.688
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:53.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:53.733
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 02/27/23 12:16:53.742
    STEP: wait for the container to reach Succeeded 02/27/23 12:16:53.755
    STEP: get the container status 02/27/23 12:16:57.807
    STEP: the container should be terminated 02/27/23 12:16:57.818
    STEP: the termination message should be set 02/27/23 12:16:57.819
    Feb 27 12:16:57.819: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/27/23 12:16:57.819
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 27 12:16:57.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1714" for this suite. 02/27/23 12:16:57.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:16:57.884
Feb 27 12:16:57.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 12:16:57.885
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:57.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:57.937
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 02/27/23 12:16:57.95
Feb 27 12:16:57.971: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-ce9901f0-7882-43c9-8f3f-521c1c870f2d" in namespace "emptydir-8209" to be "running"
Feb 27 12:16:57.988: INFO: Pod "pod-sharedvolume-ce9901f0-7882-43c9-8f3f-521c1c870f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.2976ms
Feb 27 12:16:59.999: INFO: Pod "pod-sharedvolume-ce9901f0-7882-43c9-8f3f-521c1c870f2d": Phase="Running", Reason="", readiness=false. Elapsed: 2.027912397s
Feb 27 12:16:59.999: INFO: Pod "pod-sharedvolume-ce9901f0-7882-43c9-8f3f-521c1c870f2d" satisfied condition "running"
STEP: Reading file content from the nginx-container 02/27/23 12:16:59.999
Feb 27 12:17:00.000: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8209 PodName:pod-sharedvolume-ce9901f0-7882-43c9-8f3f-521c1c870f2d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:17:00.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:17:00.001: INFO: ExecWithOptions: Clientset creation
Feb 27 12:17:00.002: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/emptydir-8209/pods/pod-sharedvolume-ce9901f0-7882-43c9-8f3f-521c1c870f2d/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Feb 27 12:17:00.151: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 12:17:00.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8209" for this suite. 02/27/23 12:17:00.168
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":217,"skipped":4309,"failed":0}
------------------------------
• [2.296 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:16:57.884
    Feb 27 12:16:57.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 12:16:57.885
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:16:57.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:16:57.937
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 02/27/23 12:16:57.95
    Feb 27 12:16:57.971: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-ce9901f0-7882-43c9-8f3f-521c1c870f2d" in namespace "emptydir-8209" to be "running"
    Feb 27 12:16:57.988: INFO: Pod "pod-sharedvolume-ce9901f0-7882-43c9-8f3f-521c1c870f2d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.2976ms
    Feb 27 12:16:59.999: INFO: Pod "pod-sharedvolume-ce9901f0-7882-43c9-8f3f-521c1c870f2d": Phase="Running", Reason="", readiness=false. Elapsed: 2.027912397s
    Feb 27 12:16:59.999: INFO: Pod "pod-sharedvolume-ce9901f0-7882-43c9-8f3f-521c1c870f2d" satisfied condition "running"
    STEP: Reading file content from the nginx-container 02/27/23 12:16:59.999
    Feb 27 12:17:00.000: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8209 PodName:pod-sharedvolume-ce9901f0-7882-43c9-8f3f-521c1c870f2d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:17:00.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:17:00.001: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:17:00.002: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/emptydir-8209/pods/pod-sharedvolume-ce9901f0-7882-43c9-8f3f-521c1c870f2d/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Feb 27 12:17:00.151: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 12:17:00.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8209" for this suite. 02/27/23 12:17:00.168
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:17:00.192
Feb 27 12:17:00.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename job 02/27/23 12:17:00.194
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:00.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:00.24
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 02/27/23 12:17:00.263
STEP: Patching the Job 02/27/23 12:17:00.274
STEP: Watching for Job to be patched 02/27/23 12:17:00.297
Feb 27 12:17:00.302: INFO: Event ADDED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb 27 12:17:00.303: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb 27 12:17:00.303: INFO: Event MODIFIED found for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 02/27/23 12:17:00.303
STEP: Watching for Job to be updated 02/27/23 12:17:00.337
Feb 27 12:17:00.345: INFO: Event MODIFIED found for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 12:17:00.345: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 02/27/23 12:17:00.346
Feb 27 12:17:00.355: INFO: Job: e2e-w6mrw as labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched]
STEP: Waiting for job to complete 02/27/23 12:17:00.355
STEP: Delete a job collection with a labelselector 02/27/23 12:17:16.363
STEP: Watching for Job to be deleted 02/27/23 12:17:16.39
Feb 27 12:17:16.397: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 12:17:16.397: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 12:17:16.398: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 12:17:16.398: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 12:17:16.398: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 12:17:16.399: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 12:17:16.399: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 12:17:16.399: INFO: Event DELETED found for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 02/27/23 12:17:16.399
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 27 12:17:16.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5406" for this suite. 02/27/23 12:17:16.45
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":218,"skipped":4310,"failed":0}
------------------------------
• [SLOW TEST] [16.300 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:17:00.192
    Feb 27 12:17:00.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename job 02/27/23 12:17:00.194
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:00.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:00.24
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 02/27/23 12:17:00.263
    STEP: Patching the Job 02/27/23 12:17:00.274
    STEP: Watching for Job to be patched 02/27/23 12:17:00.297
    Feb 27 12:17:00.302: INFO: Event ADDED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb 27 12:17:00.303: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb 27 12:17:00.303: INFO: Event MODIFIED found for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 02/27/23 12:17:00.303
    STEP: Watching for Job to be updated 02/27/23 12:17:00.337
    Feb 27 12:17:00.345: INFO: Event MODIFIED found for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 12:17:00.345: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 02/27/23 12:17:00.346
    Feb 27 12:17:00.355: INFO: Job: e2e-w6mrw as labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched]
    STEP: Waiting for job to complete 02/27/23 12:17:00.355
    STEP: Delete a job collection with a labelselector 02/27/23 12:17:16.363
    STEP: Watching for Job to be deleted 02/27/23 12:17:16.39
    Feb 27 12:17:16.397: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 12:17:16.397: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 12:17:16.398: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 12:17:16.398: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 12:17:16.398: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 12:17:16.399: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 12:17:16.399: INFO: Event MODIFIED observed for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 12:17:16.399: INFO: Event DELETED found for Job e2e-w6mrw in namespace job-5406 with labels: map[e2e-job-label:e2e-w6mrw e2e-w6mrw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 02/27/23 12:17:16.399
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 27 12:17:16.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5406" for this suite. 02/27/23 12:17:16.45
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:17:16.492
Feb 27 12:17:16.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 12:17:16.494
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:16.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:16.56
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 02/27/23 12:17:16.579
Feb 27 12:17:16.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 27 12:17:16.735: INFO: stderr: ""
Feb 27 12:17:16.735: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 02/27/23 12:17:16.735
Feb 27 12:17:16.736: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 27 12:17:16.736: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-338" to be "running and ready, or succeeded"
Feb 27 12:17:16.755: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 19.157728ms
Feb 27 12:17:16.755: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-172-31-11-159.eu-central-1.compute.internal' to be 'Running' but was 'Pending'
Feb 27 12:17:18.772: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036459912s
Feb 27 12:17:18.772: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-172-31-11-159.eu-central-1.compute.internal' to be 'Running' but was 'Pending'
Feb 27 12:17:20.764: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.028187501s
Feb 27 12:17:20.764: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 27 12:17:20.764: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 02/27/23 12:17:20.764
Feb 27 12:17:20.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 logs logs-generator logs-generator'
Feb 27 12:17:20.962: INFO: stderr: ""
Feb 27 12:17:20.962: INFO: stdout: "I0227 12:17:17.851535       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/tkd 403\nI0227 12:17:18.052011       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/ddqq 474\nI0227 12:17:18.252510       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/dngg 390\nI0227 12:17:18.452052       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/fqv 208\nI0227 12:17:18.652657       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/7wf6 522\nI0227 12:17:18.852265       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/wcz 596\nI0227 12:17:19.051569       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/tx8b 568\nI0227 12:17:19.252163       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/x4g 462\nI0227 12:17:19.452649       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/zsz 489\nI0227 12:17:19.652118       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/q9p 580\nI0227 12:17:19.852608       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/xt9t 474\nI0227 12:17:20.052098       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/r79n 521\nI0227 12:17:20.253632       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/dsx 225\nI0227 12:17:20.452259       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/n2c 521\nI0227 12:17:20.652745       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/vb4 444\nI0227 12:17:20.852423       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/dlj 340\n"
STEP: limiting log lines 02/27/23 12:17:20.962
Feb 27 12:17:20.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 logs logs-generator logs-generator --tail=1'
Feb 27 12:17:21.088: INFO: stderr: ""
Feb 27 12:17:21.088: INFO: stdout: "I0227 12:17:21.051552       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/2ll4 386\n"
Feb 27 12:17:21.088: INFO: got output "I0227 12:17:21.051552       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/2ll4 386\n"
STEP: limiting log bytes 02/27/23 12:17:21.088
Feb 27 12:17:21.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 logs logs-generator logs-generator --limit-bytes=1'
Feb 27 12:17:21.209: INFO: stderr: ""
Feb 27 12:17:21.209: INFO: stdout: "I"
Feb 27 12:17:21.209: INFO: got output "I"
STEP: exposing timestamps 02/27/23 12:17:21.209
Feb 27 12:17:21.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 logs logs-generator logs-generator --tail=1 --timestamps'
Feb 27 12:17:21.329: INFO: stderr: ""
Feb 27 12:17:21.329: INFO: stdout: "2023-02-27T12:17:21.251955791Z I0227 12:17:21.251829       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/jqgh 450\n"
Feb 27 12:17:21.329: INFO: got output "2023-02-27T12:17:21.251955791Z I0227 12:17:21.251829       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/jqgh 450\n"
STEP: restricting to a time range 02/27/23 12:17:21.329
Feb 27 12:17:23.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 logs logs-generator logs-generator --since=1s'
Feb 27 12:17:23.946: INFO: stderr: ""
Feb 27 12:17:23.946: INFO: stdout: "I0227 12:17:23.052456       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/9vm 234\nI0227 12:17:23.251789       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/t7k 380\nI0227 12:17:23.452438       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/48z 460\nI0227 12:17:23.651840       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/vnj 597\nI0227 12:17:23.852338       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/9g7m 436\n"
Feb 27 12:17:23.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 logs logs-generator logs-generator --since=24h'
Feb 27 12:17:24.075: INFO: stderr: ""
Feb 27 12:17:24.075: INFO: stdout: "I0227 12:17:17.851535       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/tkd 403\nI0227 12:17:18.052011       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/ddqq 474\nI0227 12:17:18.252510       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/dngg 390\nI0227 12:17:18.452052       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/fqv 208\nI0227 12:17:18.652657       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/7wf6 522\nI0227 12:17:18.852265       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/wcz 596\nI0227 12:17:19.051569       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/tx8b 568\nI0227 12:17:19.252163       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/x4g 462\nI0227 12:17:19.452649       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/zsz 489\nI0227 12:17:19.652118       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/q9p 580\nI0227 12:17:19.852608       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/xt9t 474\nI0227 12:17:20.052098       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/r79n 521\nI0227 12:17:20.253632       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/dsx 225\nI0227 12:17:20.452259       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/n2c 521\nI0227 12:17:20.652745       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/vb4 444\nI0227 12:17:20.852423       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/dlj 340\nI0227 12:17:21.051552       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/2ll4 386\nI0227 12:17:21.251829       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/jqgh 450\nI0227 12:17:21.452618       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/brf 542\nI0227 12:17:21.652208       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/9d6d 583\nI0227 12:17:21.851527       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/5wkn 294\nI0227 12:17:22.052006       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/zvh7 588\nI0227 12:17:22.252424       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/pb2x 433\nI0227 12:17:22.451889       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/8p6 437\nI0227 12:17:22.652428       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/kj9d 278\nI0227 12:17:22.851966       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/zs6 414\nI0227 12:17:23.052456       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/9vm 234\nI0227 12:17:23.251789       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/t7k 380\nI0227 12:17:23.452438       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/48z 460\nI0227 12:17:23.651840       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/vnj 597\nI0227 12:17:23.852338       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/9g7m 436\nI0227 12:17:24.051565       1 logs_generator.go:76] 31 POST /api/v1/namespaces/default/pods/sxfb 282\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Feb 27 12:17:24.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 delete pod logs-generator'
Feb 27 12:17:24.855: INFO: stderr: ""
Feb 27 12:17:24.855: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 12:17:24.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-338" for this suite. 02/27/23 12:17:24.865
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":219,"skipped":4313,"failed":0}
------------------------------
• [SLOW TEST] [8.385 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:17:16.492
    Feb 27 12:17:16.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 12:17:16.494
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:16.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:16.56
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 02/27/23 12:17:16.579
    Feb 27 12:17:16.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Feb 27 12:17:16.735: INFO: stderr: ""
    Feb 27 12:17:16.735: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 02/27/23 12:17:16.735
    Feb 27 12:17:16.736: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Feb 27 12:17:16.736: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-338" to be "running and ready, or succeeded"
    Feb 27 12:17:16.755: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 19.157728ms
    Feb 27 12:17:16.755: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-172-31-11-159.eu-central-1.compute.internal' to be 'Running' but was 'Pending'
    Feb 27 12:17:18.772: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036459912s
    Feb 27 12:17:18.772: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-172-31-11-159.eu-central-1.compute.internal' to be 'Running' but was 'Pending'
    Feb 27 12:17:20.764: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.028187501s
    Feb 27 12:17:20.764: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Feb 27 12:17:20.764: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 02/27/23 12:17:20.764
    Feb 27 12:17:20.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 logs logs-generator logs-generator'
    Feb 27 12:17:20.962: INFO: stderr: ""
    Feb 27 12:17:20.962: INFO: stdout: "I0227 12:17:17.851535       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/tkd 403\nI0227 12:17:18.052011       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/ddqq 474\nI0227 12:17:18.252510       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/dngg 390\nI0227 12:17:18.452052       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/fqv 208\nI0227 12:17:18.652657       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/7wf6 522\nI0227 12:17:18.852265       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/wcz 596\nI0227 12:17:19.051569       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/tx8b 568\nI0227 12:17:19.252163       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/x4g 462\nI0227 12:17:19.452649       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/zsz 489\nI0227 12:17:19.652118       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/q9p 580\nI0227 12:17:19.852608       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/xt9t 474\nI0227 12:17:20.052098       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/r79n 521\nI0227 12:17:20.253632       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/dsx 225\nI0227 12:17:20.452259       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/n2c 521\nI0227 12:17:20.652745       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/vb4 444\nI0227 12:17:20.852423       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/dlj 340\n"
    STEP: limiting log lines 02/27/23 12:17:20.962
    Feb 27 12:17:20.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 logs logs-generator logs-generator --tail=1'
    Feb 27 12:17:21.088: INFO: stderr: ""
    Feb 27 12:17:21.088: INFO: stdout: "I0227 12:17:21.051552       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/2ll4 386\n"
    Feb 27 12:17:21.088: INFO: got output "I0227 12:17:21.051552       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/2ll4 386\n"
    STEP: limiting log bytes 02/27/23 12:17:21.088
    Feb 27 12:17:21.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 logs logs-generator logs-generator --limit-bytes=1'
    Feb 27 12:17:21.209: INFO: stderr: ""
    Feb 27 12:17:21.209: INFO: stdout: "I"
    Feb 27 12:17:21.209: INFO: got output "I"
    STEP: exposing timestamps 02/27/23 12:17:21.209
    Feb 27 12:17:21.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 logs logs-generator logs-generator --tail=1 --timestamps'
    Feb 27 12:17:21.329: INFO: stderr: ""
    Feb 27 12:17:21.329: INFO: stdout: "2023-02-27T12:17:21.251955791Z I0227 12:17:21.251829       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/jqgh 450\n"
    Feb 27 12:17:21.329: INFO: got output "2023-02-27T12:17:21.251955791Z I0227 12:17:21.251829       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/jqgh 450\n"
    STEP: restricting to a time range 02/27/23 12:17:21.329
    Feb 27 12:17:23.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 logs logs-generator logs-generator --since=1s'
    Feb 27 12:17:23.946: INFO: stderr: ""
    Feb 27 12:17:23.946: INFO: stdout: "I0227 12:17:23.052456       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/9vm 234\nI0227 12:17:23.251789       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/t7k 380\nI0227 12:17:23.452438       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/48z 460\nI0227 12:17:23.651840       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/vnj 597\nI0227 12:17:23.852338       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/9g7m 436\n"
    Feb 27 12:17:23.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 logs logs-generator logs-generator --since=24h'
    Feb 27 12:17:24.075: INFO: stderr: ""
    Feb 27 12:17:24.075: INFO: stdout: "I0227 12:17:17.851535       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/tkd 403\nI0227 12:17:18.052011       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/ddqq 474\nI0227 12:17:18.252510       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/dngg 390\nI0227 12:17:18.452052       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/fqv 208\nI0227 12:17:18.652657       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/7wf6 522\nI0227 12:17:18.852265       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/wcz 596\nI0227 12:17:19.051569       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/tx8b 568\nI0227 12:17:19.252163       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/x4g 462\nI0227 12:17:19.452649       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/zsz 489\nI0227 12:17:19.652118       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/q9p 580\nI0227 12:17:19.852608       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/xt9t 474\nI0227 12:17:20.052098       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/r79n 521\nI0227 12:17:20.253632       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/dsx 225\nI0227 12:17:20.452259       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/n2c 521\nI0227 12:17:20.652745       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/vb4 444\nI0227 12:17:20.852423       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/dlj 340\nI0227 12:17:21.051552       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/2ll4 386\nI0227 12:17:21.251829       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/jqgh 450\nI0227 12:17:21.452618       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/brf 542\nI0227 12:17:21.652208       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/9d6d 583\nI0227 12:17:21.851527       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/5wkn 294\nI0227 12:17:22.052006       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/zvh7 588\nI0227 12:17:22.252424       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/pb2x 433\nI0227 12:17:22.451889       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/8p6 437\nI0227 12:17:22.652428       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/kj9d 278\nI0227 12:17:22.851966       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/zs6 414\nI0227 12:17:23.052456       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/9vm 234\nI0227 12:17:23.251789       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/t7k 380\nI0227 12:17:23.452438       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/48z 460\nI0227 12:17:23.651840       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/vnj 597\nI0227 12:17:23.852338       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/9g7m 436\nI0227 12:17:24.051565       1 logs_generator.go:76] 31 POST /api/v1/namespaces/default/pods/sxfb 282\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Feb 27 12:17:24.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-338 delete pod logs-generator'
    Feb 27 12:17:24.855: INFO: stderr: ""
    Feb 27 12:17:24.855: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 12:17:24.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-338" for this suite. 02/27/23 12:17:24.865
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:17:24.877
Feb 27 12:17:24.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename daemonsets 02/27/23 12:17:24.88
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:24.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:24.926
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 02/27/23 12:17:25.007
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 12:17:25.027
Feb 27 12:17:25.061: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 12:17:25.061: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 12:17:26.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 12:17:26.084: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 12:17:27.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 12:17:27.079: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 02/27/23 12:17:27.088
STEP: DeleteCollection of the DaemonSets 02/27/23 12:17:27.097
STEP: Verify that ReplicaSets have been deleted 02/27/23 12:17:27.114
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Feb 27 12:17:27.161: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"86302"},"items":null}

Feb 27 12:17:27.195: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"86303"},"items":[{"metadata":{"name":"daemon-set-hzdsm","generateName":"daemon-set-","namespace":"daemonsets-1594","uid":"efb61714-5b71-4019-8d42-47e560879c09","resourceVersion":"86303","creationTimestamp":"2023-02-27T12:17:25Z","deletionTimestamp":"2023-02-27T12:17:57Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"41134bff9ccd4e05c32084d454433722471198eb7b71ce23f96db9bbfc47a35e","cni.projectcalico.org/podIP":"172.25.2.194/32","cni.projectcalico.org/podIPs":"172.25.2.194/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ca7d9413-c1a6-4c19-b158-0a1b9c611794","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca7d9413-c1a6-4c19-b158-0a1b9c611794\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-p4l6s","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-p4l6s","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-15-17.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-15-17.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:25Z"}],"hostIP":"172.31.15.17","podIP":"172.25.2.194","podIPs":[{"ip":"172.25.2.194"}],"startTime":"2023-02-27T12:17:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T12:17:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://4c675eabf16296abbceb626a57f6b6de274a1cd36af78ed1cc7117beeb3155fc","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qzj8m","generateName":"daemon-set-","namespace":"daemonsets-1594","uid":"ebb9d487-bac0-4771-9fab-dcc89268842a","resourceVersion":"86300","creationTimestamp":"2023-02-27T12:17:25Z","deletionTimestamp":"2023-02-27T12:17:57Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"65a0bc0807ccbadd0567882a5102e381f4b9e8ea8fb1c9a9062e58ae93be1edb","cni.projectcalico.org/podIP":"172.25.0.76/32","cni.projectcalico.org/podIPs":"172.25.0.76/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ca7d9413-c1a6-4c19-b158-0a1b9c611794","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca7d9413-c1a6-4c19-b158-0a1b9c611794\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wsc4t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wsc4t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-7-167.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-7-167.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:25Z"}],"hostIP":"172.31.7.167","podIP":"172.25.0.76","podIPs":[{"ip":"172.25.0.76"}],"startTime":"2023-02-27T12:17:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T12:17:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://5d5eff25d3e2f082ee7bd6642c15c3e8b45083150f2ffbc1f3ef1577f273ee42","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-v8pn8","generateName":"daemon-set-","namespace":"daemonsets-1594","uid":"6339ee4c-c9db-4276-9001-579a6ed7a90b","resourceVersion":"86301","creationTimestamp":"2023-02-27T12:17:25Z","deletionTimestamp":"2023-02-27T12:17:57Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"0677d764cf7ede841b445c49832f63112c62b2c61ec1abf0332e0a60f3903b79","cni.projectcalico.org/podIP":"172.25.1.171/32","cni.projectcalico.org/podIPs":"172.25.1.171/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ca7d9413-c1a6-4c19-b158-0a1b9c611794","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca7d9413-c1a6-4c19-b158-0a1b9c611794\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vbfrb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vbfrb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-11-159.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-11-159.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:25Z"}],"hostIP":"172.31.11.159","podIP":"172.25.1.171","podIPs":[{"ip":"172.25.1.171"}],"startTime":"2023-02-27T12:17:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T12:17:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f2b29c8025f21cdf3b7255522920c8f902be3dbef6057a36b3b1698e68c4c09d","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 27 12:17:27.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1594" for this suite. 02/27/23 12:17:27.286
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":220,"skipped":4315,"failed":0}
------------------------------
• [2.425 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:17:24.877
    Feb 27 12:17:24.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename daemonsets 02/27/23 12:17:24.88
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:24.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:24.926
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 02/27/23 12:17:25.007
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 12:17:25.027
    Feb 27 12:17:25.061: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 12:17:25.061: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 12:17:26.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 12:17:26.084: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 12:17:27.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 12:17:27.079: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 02/27/23 12:17:27.088
    STEP: DeleteCollection of the DaemonSets 02/27/23 12:17:27.097
    STEP: Verify that ReplicaSets have been deleted 02/27/23 12:17:27.114
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Feb 27 12:17:27.161: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"86302"},"items":null}

    Feb 27 12:17:27.195: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"86303"},"items":[{"metadata":{"name":"daemon-set-hzdsm","generateName":"daemon-set-","namespace":"daemonsets-1594","uid":"efb61714-5b71-4019-8d42-47e560879c09","resourceVersion":"86303","creationTimestamp":"2023-02-27T12:17:25Z","deletionTimestamp":"2023-02-27T12:17:57Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"41134bff9ccd4e05c32084d454433722471198eb7b71ce23f96db9bbfc47a35e","cni.projectcalico.org/podIP":"172.25.2.194/32","cni.projectcalico.org/podIPs":"172.25.2.194/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ca7d9413-c1a6-4c19-b158-0a1b9c611794","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca7d9413-c1a6-4c19-b158-0a1b9c611794\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-p4l6s","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-p4l6s","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-15-17.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-15-17.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:25Z"}],"hostIP":"172.31.15.17","podIP":"172.25.2.194","podIPs":[{"ip":"172.25.2.194"}],"startTime":"2023-02-27T12:17:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T12:17:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://4c675eabf16296abbceb626a57f6b6de274a1cd36af78ed1cc7117beeb3155fc","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qzj8m","generateName":"daemon-set-","namespace":"daemonsets-1594","uid":"ebb9d487-bac0-4771-9fab-dcc89268842a","resourceVersion":"86300","creationTimestamp":"2023-02-27T12:17:25Z","deletionTimestamp":"2023-02-27T12:17:57Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"65a0bc0807ccbadd0567882a5102e381f4b9e8ea8fb1c9a9062e58ae93be1edb","cni.projectcalico.org/podIP":"172.25.0.76/32","cni.projectcalico.org/podIPs":"172.25.0.76/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ca7d9413-c1a6-4c19-b158-0a1b9c611794","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca7d9413-c1a6-4c19-b158-0a1b9c611794\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wsc4t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wsc4t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-7-167.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-7-167.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:25Z"}],"hostIP":"172.31.7.167","podIP":"172.25.0.76","podIPs":[{"ip":"172.25.0.76"}],"startTime":"2023-02-27T12:17:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T12:17:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://5d5eff25d3e2f082ee7bd6642c15c3e8b45083150f2ffbc1f3ef1577f273ee42","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-v8pn8","generateName":"daemon-set-","namespace":"daemonsets-1594","uid":"6339ee4c-c9db-4276-9001-579a6ed7a90b","resourceVersion":"86301","creationTimestamp":"2023-02-27T12:17:25Z","deletionTimestamp":"2023-02-27T12:17:57Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"0677d764cf7ede841b445c49832f63112c62b2c61ec1abf0332e0a60f3903b79","cni.projectcalico.org/podIP":"172.25.1.171/32","cni.projectcalico.org/podIPs":"172.25.1.171/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ca7d9413-c1a6-4c19-b158-0a1b9c611794","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca7d9413-c1a6-4c19-b158-0a1b9c611794\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T12:17:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vbfrb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vbfrb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-11-159.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-11-159.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T12:17:25Z"}],"hostIP":"172.31.11.159","podIP":"172.25.1.171","podIPs":[{"ip":"172.25.1.171"}],"startTime":"2023-02-27T12:17:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T12:17:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f2b29c8025f21cdf3b7255522920c8f902be3dbef6057a36b3b1698e68c4c09d","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 12:17:27.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1594" for this suite. 02/27/23 12:17:27.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:17:27.307
Feb 27 12:17:27.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename gc 02/27/23 12:17:27.308
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:27.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:27.386
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 02/27/23 12:17:27.397
STEP: delete the rc 02/27/23 12:17:32.42
STEP: wait for all pods to be garbage collected 02/27/23 12:17:32.44
STEP: Gathering metrics 02/27/23 12:17:37.463
W0227 12:17:37.480871      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 27 12:17:37.480: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 27 12:17:37.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6197" for this suite. 02/27/23 12:17:37.492
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":221,"skipped":4321,"failed":0}
------------------------------
• [SLOW TEST] [10.204 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:17:27.307
    Feb 27 12:17:27.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename gc 02/27/23 12:17:27.308
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:27.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:27.386
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 02/27/23 12:17:27.397
    STEP: delete the rc 02/27/23 12:17:32.42
    STEP: wait for all pods to be garbage collected 02/27/23 12:17:32.44
    STEP: Gathering metrics 02/27/23 12:17:37.463
    W0227 12:17:37.480871      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 27 12:17:37.480: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 27 12:17:37.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6197" for this suite. 02/27/23 12:17:37.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:17:37.514
Feb 27 12:17:37.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:17:37.516
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:37.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:37.562
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-abfc2779-eb74-4854-9d66-4ab5052868be 02/27/23 12:17:37.573
STEP: Creating a pod to test consume secrets 02/27/23 12:17:37.582
Feb 27 12:17:37.603: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491" in namespace "projected-8654" to be "Succeeded or Failed"
Feb 27 12:17:37.616: INFO: Pod "pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491": Phase="Pending", Reason="", readiness=false. Elapsed: 12.16121ms
Feb 27 12:17:39.624: INFO: Pod "pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020067855s
Feb 27 12:17:41.643: INFO: Pod "pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039097445s
STEP: Saw pod success 02/27/23 12:17:41.643
Feb 27 12:17:41.643: INFO: Pod "pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491" satisfied condition "Succeeded or Failed"
Feb 27 12:17:41.652: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 12:17:41.673
Feb 27 12:17:41.706: INFO: Waiting for pod pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491 to disappear
Feb 27 12:17:41.716: INFO: Pod pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 27 12:17:41.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8654" for this suite. 02/27/23 12:17:41.745
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":222,"skipped":4349,"failed":0}
------------------------------
• [4.264 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:17:37.514
    Feb 27 12:17:37.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:17:37.516
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:37.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:37.562
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-abfc2779-eb74-4854-9d66-4ab5052868be 02/27/23 12:17:37.573
    STEP: Creating a pod to test consume secrets 02/27/23 12:17:37.582
    Feb 27 12:17:37.603: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491" in namespace "projected-8654" to be "Succeeded or Failed"
    Feb 27 12:17:37.616: INFO: Pod "pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491": Phase="Pending", Reason="", readiness=false. Elapsed: 12.16121ms
    Feb 27 12:17:39.624: INFO: Pod "pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020067855s
    Feb 27 12:17:41.643: INFO: Pod "pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039097445s
    STEP: Saw pod success 02/27/23 12:17:41.643
    Feb 27 12:17:41.643: INFO: Pod "pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491" satisfied condition "Succeeded or Failed"
    Feb 27 12:17:41.652: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 12:17:41.673
    Feb 27 12:17:41.706: INFO: Waiting for pod pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491 to disappear
    Feb 27 12:17:41.716: INFO: Pod pod-projected-secrets-d6c93cb6-3938-4a05-bbbf-94f68582a491 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 27 12:17:41.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8654" for this suite. 02/27/23 12:17:41.745
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:17:41.786
Feb 27 12:17:41.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename hostport 02/27/23 12:17:41.789
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:41.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:41.837
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/27/23 12:17:41.863
Feb 27 12:17:41.880: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1264" to be "running and ready"
Feb 27 12:17:41.892: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.103635ms
Feb 27 12:17:41.892: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:17:43.901: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.020207161s
Feb 27 12:17:43.901: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 27 12:17:43.901: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.11.159 on the node which pod1 resides and expect scheduled 02/27/23 12:17:43.901
Feb 27 12:17:43.912: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1264" to be "running and ready"
Feb 27 12:17:43.936: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 23.871779ms
Feb 27 12:17:43.936: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:17:45.943: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.031123357s
Feb 27 12:17:45.943: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 27 12:17:45.943: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.11.159 but use UDP protocol on the node which pod2 resides 02/27/23 12:17:45.943
Feb 27 12:17:45.957: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1264" to be "running and ready"
Feb 27 12:17:45.965: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.946978ms
Feb 27 12:17:45.965: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:17:47.978: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.020198511s
Feb 27 12:17:47.980: INFO: The phase of Pod pod3 is Running (Ready = true)
Feb 27 12:17:47.980: INFO: Pod "pod3" satisfied condition "running and ready"
Feb 27 12:17:47.997: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1264" to be "running and ready"
Feb 27 12:17:48.005: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.980942ms
Feb 27 12:17:48.005: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:17:50.014: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.016644206s
Feb 27 12:17:50.014: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Feb 27 12:17:50.014: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/27/23 12:17:50.021
Feb 27 12:17:50.021: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.11.159 http://127.0.0.1:54323/hostname] Namespace:hostport-1264 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:17:50.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:17:50.022: INFO: ExecWithOptions: Clientset creation
Feb 27 12:17:50.022: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-1264/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.11.159+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.11.159, port: 54323 02/27/23 12:17:50.162
Feb 27 12:17:50.163: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.11.159:54323/hostname] Namespace:hostport-1264 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:17:50.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:17:50.164: INFO: ExecWithOptions: Clientset creation
Feb 27 12:17:50.164: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-1264/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.11.159%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.11.159, port: 54323 UDP 02/27/23 12:17:50.292
Feb 27 12:17:50.293: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.11.159 54323] Namespace:hostport-1264 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:17:50.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:17:50.294: INFO: ExecWithOptions: Clientset creation
Feb 27 12:17:50.305: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-1264/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.11.159+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Feb 27 12:17:55.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-1264" for this suite. 02/27/23 12:17:55.464
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":223,"skipped":4350,"failed":0}
------------------------------
• [SLOW TEST] [13.694 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:17:41.786
    Feb 27 12:17:41.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename hostport 02/27/23 12:17:41.789
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:41.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:41.837
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/27/23 12:17:41.863
    Feb 27 12:17:41.880: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1264" to be "running and ready"
    Feb 27 12:17:41.892: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.103635ms
    Feb 27 12:17:41.892: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:17:43.901: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.020207161s
    Feb 27 12:17:43.901: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 27 12:17:43.901: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.11.159 on the node which pod1 resides and expect scheduled 02/27/23 12:17:43.901
    Feb 27 12:17:43.912: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1264" to be "running and ready"
    Feb 27 12:17:43.936: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 23.871779ms
    Feb 27 12:17:43.936: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:17:45.943: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.031123357s
    Feb 27 12:17:45.943: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 27 12:17:45.943: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.11.159 but use UDP protocol on the node which pod2 resides 02/27/23 12:17:45.943
    Feb 27 12:17:45.957: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1264" to be "running and ready"
    Feb 27 12:17:45.965: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.946978ms
    Feb 27 12:17:45.965: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:17:47.978: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.020198511s
    Feb 27 12:17:47.980: INFO: The phase of Pod pod3 is Running (Ready = true)
    Feb 27 12:17:47.980: INFO: Pod "pod3" satisfied condition "running and ready"
    Feb 27 12:17:47.997: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1264" to be "running and ready"
    Feb 27 12:17:48.005: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.980942ms
    Feb 27 12:17:48.005: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:17:50.014: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.016644206s
    Feb 27 12:17:50.014: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Feb 27 12:17:50.014: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/27/23 12:17:50.021
    Feb 27 12:17:50.021: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.11.159 http://127.0.0.1:54323/hostname] Namespace:hostport-1264 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:17:50.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:17:50.022: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:17:50.022: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-1264/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.11.159+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.11.159, port: 54323 02/27/23 12:17:50.162
    Feb 27 12:17:50.163: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.11.159:54323/hostname] Namespace:hostport-1264 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:17:50.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:17:50.164: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:17:50.164: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-1264/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.11.159%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.11.159, port: 54323 UDP 02/27/23 12:17:50.292
    Feb 27 12:17:50.293: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.11.159 54323] Namespace:hostport-1264 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:17:50.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:17:50.294: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:17:50.305: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-1264/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.11.159+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Feb 27 12:17:55.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-1264" for this suite. 02/27/23 12:17:55.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:17:55.498
Feb 27 12:17:55.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 12:17:55.499
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:55.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:55.544
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 02/27/23 12:17:55.561
Feb 27 12:17:55.579: INFO: Waiting up to 5m0s for pod "annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec" in namespace "downward-api-6896" to be "running and ready"
Feb 27 12:17:55.588: INFO: Pod "annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.115227ms
Feb 27 12:17:55.588: INFO: The phase of Pod annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:17:57.599: INFO: Pod "annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec": Phase="Running", Reason="", readiness=true. Elapsed: 2.020699326s
Feb 27 12:17:57.599: INFO: The phase of Pod annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec is Running (Ready = true)
Feb 27 12:17:57.599: INFO: Pod "annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec" satisfied condition "running and ready"
Feb 27 12:17:58.189: INFO: Successfully updated pod "annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 27 12:18:02.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6896" for this suite. 02/27/23 12:18:02.279
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":224,"skipped":4433,"failed":0}
------------------------------
• [SLOW TEST] [6.820 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:17:55.498
    Feb 27 12:17:55.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 12:17:55.499
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:17:55.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:17:55.544
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 02/27/23 12:17:55.561
    Feb 27 12:17:55.579: INFO: Waiting up to 5m0s for pod "annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec" in namespace "downward-api-6896" to be "running and ready"
    Feb 27 12:17:55.588: INFO: Pod "annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec": Phase="Pending", Reason="", readiness=false. Elapsed: 9.115227ms
    Feb 27 12:17:55.588: INFO: The phase of Pod annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:17:57.599: INFO: Pod "annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec": Phase="Running", Reason="", readiness=true. Elapsed: 2.020699326s
    Feb 27 12:17:57.599: INFO: The phase of Pod annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec is Running (Ready = true)
    Feb 27 12:17:57.599: INFO: Pod "annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec" satisfied condition "running and ready"
    Feb 27 12:17:58.189: INFO: Successfully updated pod "annotationupdatea2b9b679-0392-4605-a12c-e7f2afe2aaec"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 27 12:18:02.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6896" for this suite. 02/27/23 12:18:02.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:18:02.34
Feb 27 12:18:02.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename cronjob 02/27/23 12:18:02.342
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:18:02.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:18:02.396
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 02/27/23 12:18:02.408
STEP: Ensuring more than one job is running at a time 02/27/23 12:18:02.424
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/27/23 12:20:00.432
STEP: Removing cronjob 02/27/23 12:20:00.44
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 27 12:20:00.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6944" for this suite. 02/27/23 12:20:00.468
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":225,"skipped":4449,"failed":0}
------------------------------
• [SLOW TEST] [118.144 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:18:02.34
    Feb 27 12:18:02.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename cronjob 02/27/23 12:18:02.342
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:18:02.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:18:02.396
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 02/27/23 12:18:02.408
    STEP: Ensuring more than one job is running at a time 02/27/23 12:18:02.424
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/27/23 12:20:00.432
    STEP: Removing cronjob 02/27/23 12:20:00.44
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 27 12:20:00.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6944" for this suite. 02/27/23 12:20:00.468
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:20:00.484
Feb 27 12:20:00.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 12:20:00.49
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:00.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:00.582
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 02/27/23 12:20:00.596
Feb 27 12:20:00.617: INFO: Waiting up to 5m0s for pod "pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711" in namespace "emptydir-4276" to be "Succeeded or Failed"
Feb 27 12:20:00.634: INFO: Pod "pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711": Phase="Pending", Reason="", readiness=false. Elapsed: 17.410551ms
Feb 27 12:20:02.649: INFO: Pod "pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032069835s
Feb 27 12:20:04.643: INFO: Pod "pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026646317s
STEP: Saw pod success 02/27/23 12:20:04.643
Feb 27 12:20:04.644: INFO: Pod "pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711" satisfied condition "Succeeded or Failed"
Feb 27 12:20:04.652: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711 container test-container: <nil>
STEP: delete the pod 02/27/23 12:20:04.67
Feb 27 12:20:04.692: INFO: Waiting for pod pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711 to disappear
Feb 27 12:20:04.705: INFO: Pod pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 12:20:04.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4276" for this suite. 02/27/23 12:20:04.723
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":226,"skipped":4451,"failed":0}
------------------------------
• [4.267 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:20:00.484
    Feb 27 12:20:00.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 12:20:00.49
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:00.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:00.582
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/27/23 12:20:00.596
    Feb 27 12:20:00.617: INFO: Waiting up to 5m0s for pod "pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711" in namespace "emptydir-4276" to be "Succeeded or Failed"
    Feb 27 12:20:00.634: INFO: Pod "pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711": Phase="Pending", Reason="", readiness=false. Elapsed: 17.410551ms
    Feb 27 12:20:02.649: INFO: Pod "pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032069835s
    Feb 27 12:20:04.643: INFO: Pod "pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026646317s
    STEP: Saw pod success 02/27/23 12:20:04.643
    Feb 27 12:20:04.644: INFO: Pod "pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711" satisfied condition "Succeeded or Failed"
    Feb 27 12:20:04.652: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711 container test-container: <nil>
    STEP: delete the pod 02/27/23 12:20:04.67
    Feb 27 12:20:04.692: INFO: Waiting for pod pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711 to disappear
    Feb 27 12:20:04.705: INFO: Pod pod-1cdaa7a4-d558-4fdb-b36f-8314e6b67711 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 12:20:04.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4276" for this suite. 02/27/23 12:20:04.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:20:04.756
Feb 27 12:20:04.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-runtime 02/27/23 12:20:04.757
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:04.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:04.812
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 02/27/23 12:20:04.821
STEP: wait for the container to reach Succeeded 02/27/23 12:20:04.84
STEP: get the container status 02/27/23 12:20:08.888
STEP: the container should be terminated 02/27/23 12:20:08.895
STEP: the termination message should be set 02/27/23 12:20:08.896
Feb 27 12:20:08.896: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 02/27/23 12:20:08.896
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 27 12:20:08.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-878" for this suite. 02/27/23 12:20:08.94
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":227,"skipped":4518,"failed":0}
------------------------------
• [4.204 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:20:04.756
    Feb 27 12:20:04.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-runtime 02/27/23 12:20:04.757
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:04.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:04.812
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 02/27/23 12:20:04.821
    STEP: wait for the container to reach Succeeded 02/27/23 12:20:04.84
    STEP: get the container status 02/27/23 12:20:08.888
    STEP: the container should be terminated 02/27/23 12:20:08.895
    STEP: the termination message should be set 02/27/23 12:20:08.896
    Feb 27 12:20:08.896: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 02/27/23 12:20:08.896
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 27 12:20:08.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-878" for this suite. 02/27/23 12:20:08.94
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:20:08.961
Feb 27 12:20:08.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename disruption 02/27/23 12:20:08.962
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:08.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:09.012
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:20:09.023
Feb 27 12:20:09.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename disruption-2 02/27/23 12:20:09.026
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:09.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:09.073
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 02/27/23 12:20:09.099
STEP: Waiting for the pdb to be processed 02/27/23 12:20:11.164
STEP: Waiting for the pdb to be processed 02/27/23 12:20:11.206
STEP: listing a collection of PDBs across all namespaces 02/27/23 12:20:11.217
STEP: listing a collection of PDBs in namespace disruption-1030 02/27/23 12:20:11.227
STEP: deleting a collection of PDBs 02/27/23 12:20:11.238
STEP: Waiting for the PDB collection to be deleted 02/27/23 12:20:11.262
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Feb 27 12:20:11.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-1717" for this suite. 02/27/23 12:20:11.287
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 27 12:20:11.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1030" for this suite. 02/27/23 12:20:11.342
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":228,"skipped":4522,"failed":0}
------------------------------
• [2.411 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:20:08.961
    Feb 27 12:20:08.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename disruption 02/27/23 12:20:08.962
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:08.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:09.012
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:20:09.023
    Feb 27 12:20:09.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename disruption-2 02/27/23 12:20:09.026
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:09.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:09.073
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 02/27/23 12:20:09.099
    STEP: Waiting for the pdb to be processed 02/27/23 12:20:11.164
    STEP: Waiting for the pdb to be processed 02/27/23 12:20:11.206
    STEP: listing a collection of PDBs across all namespaces 02/27/23 12:20:11.217
    STEP: listing a collection of PDBs in namespace disruption-1030 02/27/23 12:20:11.227
    STEP: deleting a collection of PDBs 02/27/23 12:20:11.238
    STEP: Waiting for the PDB collection to be deleted 02/27/23 12:20:11.262
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Feb 27 12:20:11.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-1717" for this suite. 02/27/23 12:20:11.287
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 27 12:20:11.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1030" for this suite. 02/27/23 12:20:11.342
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:20:11.374
Feb 27 12:20:11.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename dns 02/27/23 12:20:11.376
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:11.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:11.455
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/27/23 12:20:11.473
Feb 27 12:20:11.494: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8036  e4944b52-2c2d-4bf8-81f6-c02399898af9 87304 0 2023-02-27 12:20:11 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-27 12:20:11 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q4t56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q4t56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:20:11.494: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8036" to be "running and ready"
Feb 27 12:20:11.510: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 15.710925ms
Feb 27 12:20:11.510: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:20:13.525: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.031057778s
Feb 27 12:20:13.525: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Feb 27 12:20:13.525: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 02/27/23 12:20:13.526
Feb 27 12:20:13.526: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8036 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:20:13.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:20:13.527: INFO: ExecWithOptions: Clientset creation
Feb 27 12:20:13.527: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-8036/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 02/27/23 12:20:13.738
Feb 27 12:20:13.739: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8036 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:20:13.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:20:13.740: INFO: ExecWithOptions: Clientset creation
Feb 27 12:20:13.740: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-8036/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 12:20:13.908: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 27 12:20:13.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8036" for this suite. 02/27/23 12:20:13.971
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":229,"skipped":4522,"failed":0}
------------------------------
• [2.622 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:20:11.374
    Feb 27 12:20:11.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename dns 02/27/23 12:20:11.376
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:11.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:11.455
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/27/23 12:20:11.473
    Feb 27 12:20:11.494: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8036  e4944b52-2c2d-4bf8-81f6-c02399898af9 87304 0 2023-02-27 12:20:11 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-27 12:20:11 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q4t56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q4t56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:20:11.494: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8036" to be "running and ready"
    Feb 27 12:20:11.510: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 15.710925ms
    Feb 27 12:20:11.510: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:20:13.525: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.031057778s
    Feb 27 12:20:13.525: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Feb 27 12:20:13.525: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 02/27/23 12:20:13.526
    Feb 27 12:20:13.526: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8036 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:20:13.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:20:13.527: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:20:13.527: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-8036/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 02/27/23 12:20:13.738
    Feb 27 12:20:13.739: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8036 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:20:13.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:20:13.740: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:20:13.740: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-8036/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 12:20:13.908: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 27 12:20:13.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8036" for this suite. 02/27/23 12:20:13.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:20:14.003
Feb 27 12:20:14.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 12:20:14.004
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:14.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:14.089
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 02/27/23 12:20:14.11
Feb 27 12:20:14.146: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e" in namespace "downward-api-1876" to be "Succeeded or Failed"
Feb 27 12:20:14.197: INFO: Pod "downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e": Phase="Pending", Reason="", readiness=false. Elapsed: 50.88296ms
Feb 27 12:20:16.207: INFO: Pod "downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06042675s
Feb 27 12:20:18.207: INFO: Pod "downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06078991s
STEP: Saw pod success 02/27/23 12:20:18.207
Feb 27 12:20:18.208: INFO: Pod "downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e" satisfied condition "Succeeded or Failed"
Feb 27 12:20:18.215: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e container client-container: <nil>
STEP: delete the pod 02/27/23 12:20:18.236
Feb 27 12:20:18.259: INFO: Waiting for pod downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e to disappear
Feb 27 12:20:18.266: INFO: Pod downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 27 12:20:18.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1876" for this suite. 02/27/23 12:20:18.277
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":230,"skipped":4531,"failed":0}
------------------------------
• [4.294 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:20:14.003
    Feb 27 12:20:14.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 12:20:14.004
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:14.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:14.089
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 02/27/23 12:20:14.11
    Feb 27 12:20:14.146: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e" in namespace "downward-api-1876" to be "Succeeded or Failed"
    Feb 27 12:20:14.197: INFO: Pod "downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e": Phase="Pending", Reason="", readiness=false. Elapsed: 50.88296ms
    Feb 27 12:20:16.207: INFO: Pod "downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06042675s
    Feb 27 12:20:18.207: INFO: Pod "downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06078991s
    STEP: Saw pod success 02/27/23 12:20:18.207
    Feb 27 12:20:18.208: INFO: Pod "downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e" satisfied condition "Succeeded or Failed"
    Feb 27 12:20:18.215: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e container client-container: <nil>
    STEP: delete the pod 02/27/23 12:20:18.236
    Feb 27 12:20:18.259: INFO: Waiting for pod downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e to disappear
    Feb 27 12:20:18.266: INFO: Pod downwardapi-volume-66ea87a6-979e-4d8b-92e6-d8b98cbeb62e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 27 12:20:18.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1876" for this suite. 02/27/23 12:20:18.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:20:18.307
Feb 27 12:20:18.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename job 02/27/23 12:20:18.309
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:18.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:18.353
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 02/27/23 12:20:18.363
STEP: Ensuring active pods == parallelism 02/27/23 12:20:18.386
STEP: delete a job 02/27/23 12:20:20.397
STEP: deleting Job.batch foo in namespace job-7448, will wait for the garbage collector to delete the pods 02/27/23 12:20:20.399
Feb 27 12:20:20.493: INFO: Deleting Job.batch foo took: 35.032795ms
Feb 27 12:20:20.593: INFO: Terminating Job.batch foo pods took: 100.511565ms
STEP: Ensuring job was deleted 02/27/23 12:20:53.593
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 27 12:20:53.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7448" for this suite. 02/27/23 12:20:53.612
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":231,"skipped":4547,"failed":0}
------------------------------
• [SLOW TEST] [35.317 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:20:18.307
    Feb 27 12:20:18.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename job 02/27/23 12:20:18.309
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:18.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:18.353
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 02/27/23 12:20:18.363
    STEP: Ensuring active pods == parallelism 02/27/23 12:20:18.386
    STEP: delete a job 02/27/23 12:20:20.397
    STEP: deleting Job.batch foo in namespace job-7448, will wait for the garbage collector to delete the pods 02/27/23 12:20:20.399
    Feb 27 12:20:20.493: INFO: Deleting Job.batch foo took: 35.032795ms
    Feb 27 12:20:20.593: INFO: Terminating Job.batch foo pods took: 100.511565ms
    STEP: Ensuring job was deleted 02/27/23 12:20:53.593
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 27 12:20:53.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7448" for this suite. 02/27/23 12:20:53.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:20:53.632
Feb 27 12:20:53.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 12:20:53.634
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:53.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:53.677
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 02/27/23 12:20:53.689
Feb 27 12:20:53.689: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-2854 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 02/27/23 12:20:53.747
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 12:20:53.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2854" for this suite. 02/27/23 12:20:53.779
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":232,"skipped":4568,"failed":0}
------------------------------
• [0.161 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:20:53.632
    Feb 27 12:20:53.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 12:20:53.634
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:53.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:53.677
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 02/27/23 12:20:53.689
    Feb 27 12:20:53.689: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-2854 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 02/27/23 12:20:53.747
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 12:20:53.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2854" for this suite. 02/27/23 12:20:53.779
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:20:53.793
Feb 27 12:20:53.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 12:20:53.794
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:53.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:53.83
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-9297f9f5-6270-48cd-8dc0-8452e30ec6c6 02/27/23 12:20:53.886
STEP: Creating a pod to test consume secrets 02/27/23 12:20:53.897
Feb 27 12:20:53.922: INFO: Waiting up to 5m0s for pod "pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3" in namespace "secrets-7143" to be "Succeeded or Failed"
Feb 27 12:20:53.936: INFO: Pod "pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.547625ms
Feb 27 12:20:56.096: INFO: Pod "pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.173496703s
Feb 27 12:20:57.944: INFO: Pod "pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02114044s
STEP: Saw pod success 02/27/23 12:20:57.944
Feb 27 12:20:57.944: INFO: Pod "pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3" satisfied condition "Succeeded or Failed"
Feb 27 12:20:57.952: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 12:20:57.965
Feb 27 12:20:57.988: INFO: Waiting for pod pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3 to disappear
Feb 27 12:20:58.000: INFO: Pod pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 27 12:20:58.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7143" for this suite. 02/27/23 12:20:58.012
STEP: Destroying namespace "secret-namespace-5959" for this suite. 02/27/23 12:20:58.026
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":233,"skipped":4568,"failed":0}
------------------------------
• [4.269 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:20:53.793
    Feb 27 12:20:53.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 12:20:53.794
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:53.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:53.83
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-9297f9f5-6270-48cd-8dc0-8452e30ec6c6 02/27/23 12:20:53.886
    STEP: Creating a pod to test consume secrets 02/27/23 12:20:53.897
    Feb 27 12:20:53.922: INFO: Waiting up to 5m0s for pod "pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3" in namespace "secrets-7143" to be "Succeeded or Failed"
    Feb 27 12:20:53.936: INFO: Pod "pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.547625ms
    Feb 27 12:20:56.096: INFO: Pod "pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.173496703s
    Feb 27 12:20:57.944: INFO: Pod "pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02114044s
    STEP: Saw pod success 02/27/23 12:20:57.944
    Feb 27 12:20:57.944: INFO: Pod "pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3" satisfied condition "Succeeded or Failed"
    Feb 27 12:20:57.952: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 12:20:57.965
    Feb 27 12:20:57.988: INFO: Waiting for pod pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3 to disappear
    Feb 27 12:20:58.000: INFO: Pod pod-secrets-3e4bea83-73de-4e3d-abf4-72fc8943deb3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 12:20:58.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7143" for this suite. 02/27/23 12:20:58.012
    STEP: Destroying namespace "secret-namespace-5959" for this suite. 02/27/23 12:20:58.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:20:58.067
Feb 27 12:20:58.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename runtimeclass 02/27/23 12:20:58.068
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:58.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:58.114
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 02/27/23 12:20:58.125
STEP: getting /apis/node.k8s.io 02/27/23 12:20:58.134
STEP: getting /apis/node.k8s.io/v1 02/27/23 12:20:58.138
STEP: creating 02/27/23 12:20:58.142
STEP: watching 02/27/23 12:20:58.178
Feb 27 12:20:58.178: INFO: starting watch
STEP: getting 02/27/23 12:20:58.192
STEP: listing 02/27/23 12:20:58.201
STEP: patching 02/27/23 12:20:58.212
STEP: updating 02/27/23 12:20:58.221
Feb 27 12:20:58.236: INFO: waiting for watch events with expected annotations
STEP: deleting 02/27/23 12:20:58.237
STEP: deleting a collection 02/27/23 12:20:58.268
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 27 12:20:58.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2522" for this suite. 02/27/23 12:20:58.307
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":234,"skipped":4580,"failed":0}
------------------------------
• [0.253 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:20:58.067
    Feb 27 12:20:58.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 12:20:58.068
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:58.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:58.114
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 02/27/23 12:20:58.125
    STEP: getting /apis/node.k8s.io 02/27/23 12:20:58.134
    STEP: getting /apis/node.k8s.io/v1 02/27/23 12:20:58.138
    STEP: creating 02/27/23 12:20:58.142
    STEP: watching 02/27/23 12:20:58.178
    Feb 27 12:20:58.178: INFO: starting watch
    STEP: getting 02/27/23 12:20:58.192
    STEP: listing 02/27/23 12:20:58.201
    STEP: patching 02/27/23 12:20:58.212
    STEP: updating 02/27/23 12:20:58.221
    Feb 27 12:20:58.236: INFO: waiting for watch events with expected annotations
    STEP: deleting 02/27/23 12:20:58.237
    STEP: deleting a collection 02/27/23 12:20:58.268
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 27 12:20:58.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2522" for this suite. 02/27/23 12:20:58.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:20:58.322
Feb 27 12:20:58.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename gc 02/27/23 12:20:58.323
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:58.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:58.367
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 02/27/23 12:20:58.377
STEP: Wait for the Deployment to create new ReplicaSet 02/27/23 12:20:58.388
STEP: delete the deployment 02/27/23 12:20:58.51
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/27/23 12:20:58.525
STEP: Gathering metrics 02/27/23 12:20:59.097
W0227 12:20:59.121732      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 27 12:20:59.121: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 27 12:20:59.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1323" for this suite. 02/27/23 12:20:59.135
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":235,"skipped":4586,"failed":0}
------------------------------
• [0.830 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:20:58.322
    Feb 27 12:20:58.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename gc 02/27/23 12:20:58.323
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:58.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:58.367
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 02/27/23 12:20:58.377
    STEP: Wait for the Deployment to create new ReplicaSet 02/27/23 12:20:58.388
    STEP: delete the deployment 02/27/23 12:20:58.51
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/27/23 12:20:58.525
    STEP: Gathering metrics 02/27/23 12:20:59.097
    W0227 12:20:59.121732      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 27 12:20:59.121: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 27 12:20:59.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1323" for this suite. 02/27/23 12:20:59.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:20:59.161
Feb 27 12:20:59.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:20:59.163
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:59.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:59.2
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-ef2a56cf-2cb7-4daf-b5bc-af86e3d47506 02/27/23 12:20:59.215
STEP: Creating a pod to test consume configMaps 02/27/23 12:20:59.226
Feb 27 12:20:59.247: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d" in namespace "projected-6465" to be "Succeeded or Failed"
Feb 27 12:20:59.255: INFO: Pod "pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.29324ms
Feb 27 12:21:01.265: INFO: Pod "pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017117946s
Feb 27 12:21:03.284: INFO: Pod "pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036701974s
STEP: Saw pod success 02/27/23 12:21:03.285
Feb 27 12:21:03.285: INFO: Pod "pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d" satisfied condition "Succeeded or Failed"
Feb 27 12:21:03.307: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d container agnhost-container: <nil>
STEP: delete the pod 02/27/23 12:21:03.354
Feb 27 12:21:03.391: INFO: Waiting for pod pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d to disappear
Feb 27 12:21:03.400: INFO: Pod pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 27 12:21:03.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6465" for this suite. 02/27/23 12:21:03.413
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":236,"skipped":4605,"failed":0}
------------------------------
• [4.280 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:20:59.161
    Feb 27 12:20:59.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:20:59.163
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:20:59.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:20:59.2
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-ef2a56cf-2cb7-4daf-b5bc-af86e3d47506 02/27/23 12:20:59.215
    STEP: Creating a pod to test consume configMaps 02/27/23 12:20:59.226
    Feb 27 12:20:59.247: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d" in namespace "projected-6465" to be "Succeeded or Failed"
    Feb 27 12:20:59.255: INFO: Pod "pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.29324ms
    Feb 27 12:21:01.265: INFO: Pod "pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017117946s
    Feb 27 12:21:03.284: INFO: Pod "pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036701974s
    STEP: Saw pod success 02/27/23 12:21:03.285
    Feb 27 12:21:03.285: INFO: Pod "pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d" satisfied condition "Succeeded or Failed"
    Feb 27 12:21:03.307: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 12:21:03.354
    Feb 27 12:21:03.391: INFO: Waiting for pod pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d to disappear
    Feb 27 12:21:03.400: INFO: Pod pod-projected-configmaps-6fb87374-3325-4aeb-a3ba-cabd7391328d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 27 12:21:03.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6465" for this suite. 02/27/23 12:21:03.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:21:03.442
Feb 27 12:21:03.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 12:21:03.445
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:03.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:03.52
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 02/27/23 12:21:03.532
Feb 27 12:21:03.551: INFO: Waiting up to 5m0s for pod "pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf" in namespace "emptydir-9990" to be "Succeeded or Failed"
Feb 27 12:21:03.591: INFO: Pod "pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf": Phase="Pending", Reason="", readiness=false. Elapsed: 40.098003ms
Feb 27 12:21:05.601: INFO: Pod "pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050169631s
Feb 27 12:21:07.602: INFO: Pod "pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050984901s
STEP: Saw pod success 02/27/23 12:21:07.602
Feb 27 12:21:07.603: INFO: Pod "pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf" satisfied condition "Succeeded or Failed"
Feb 27 12:21:07.609: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf container test-container: <nil>
STEP: delete the pod 02/27/23 12:21:07.624
Feb 27 12:21:07.647: INFO: Waiting for pod pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf to disappear
Feb 27 12:21:07.653: INFO: Pod pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 12:21:07.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9990" for this suite. 02/27/23 12:21:07.663
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":237,"skipped":4618,"failed":0}
------------------------------
• [4.232 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:21:03.442
    Feb 27 12:21:03.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 12:21:03.445
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:03.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:03.52
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/27/23 12:21:03.532
    Feb 27 12:21:03.551: INFO: Waiting up to 5m0s for pod "pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf" in namespace "emptydir-9990" to be "Succeeded or Failed"
    Feb 27 12:21:03.591: INFO: Pod "pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf": Phase="Pending", Reason="", readiness=false. Elapsed: 40.098003ms
    Feb 27 12:21:05.601: INFO: Pod "pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050169631s
    Feb 27 12:21:07.602: INFO: Pod "pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050984901s
    STEP: Saw pod success 02/27/23 12:21:07.602
    Feb 27 12:21:07.603: INFO: Pod "pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf" satisfied condition "Succeeded or Failed"
    Feb 27 12:21:07.609: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf container test-container: <nil>
    STEP: delete the pod 02/27/23 12:21:07.624
    Feb 27 12:21:07.647: INFO: Waiting for pod pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf to disappear
    Feb 27 12:21:07.653: INFO: Pod pod-65e6300f-9c72-42cd-b5b2-a2646a5480cf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 12:21:07.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9990" for this suite. 02/27/23 12:21:07.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:21:07.68
Feb 27 12:21:07.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename security-context-test 02/27/23 12:21:07.681
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:07.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:07.717
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Feb 27 12:21:07.816: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8b339c45-f005-4140-8344-5473f9fb5c32" in namespace "security-context-test-8950" to be "Succeeded or Failed"
Feb 27 12:21:07.831: INFO: Pod "alpine-nnp-false-8b339c45-f005-4140-8344-5473f9fb5c32": Phase="Pending", Reason="", readiness=false. Elapsed: 14.173528ms
Feb 27 12:21:09.841: INFO: Pod "alpine-nnp-false-8b339c45-f005-4140-8344-5473f9fb5c32": Phase="Running", Reason="", readiness=false. Elapsed: 2.024188266s
Feb 27 12:21:11.841: INFO: Pod "alpine-nnp-false-8b339c45-f005-4140-8344-5473f9fb5c32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024290918s
Feb 27 12:21:11.841: INFO: Pod "alpine-nnp-false-8b339c45-f005-4140-8344-5473f9fb5c32" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 27 12:21:11.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8950" for this suite. 02/27/23 12:21:11.872
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":238,"skipped":4637,"failed":0}
------------------------------
• [4.211 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:21:07.68
    Feb 27 12:21:07.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename security-context-test 02/27/23 12:21:07.681
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:07.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:07.717
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Feb 27 12:21:07.816: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8b339c45-f005-4140-8344-5473f9fb5c32" in namespace "security-context-test-8950" to be "Succeeded or Failed"
    Feb 27 12:21:07.831: INFO: Pod "alpine-nnp-false-8b339c45-f005-4140-8344-5473f9fb5c32": Phase="Pending", Reason="", readiness=false. Elapsed: 14.173528ms
    Feb 27 12:21:09.841: INFO: Pod "alpine-nnp-false-8b339c45-f005-4140-8344-5473f9fb5c32": Phase="Running", Reason="", readiness=false. Elapsed: 2.024188266s
    Feb 27 12:21:11.841: INFO: Pod "alpine-nnp-false-8b339c45-f005-4140-8344-5473f9fb5c32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024290918s
    Feb 27 12:21:11.841: INFO: Pod "alpine-nnp-false-8b339c45-f005-4140-8344-5473f9fb5c32" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 27 12:21:11.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-8950" for this suite. 02/27/23 12:21:11.872
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:21:11.894
Feb 27 12:21:11.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename limitrange 02/27/23 12:21:11.895
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:11.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:11.954
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 02/27/23 12:21:11.968
STEP: Setting up watch 02/27/23 12:21:11.969
STEP: Submitting a LimitRange 02/27/23 12:21:12.086
STEP: Verifying LimitRange creation was observed 02/27/23 12:21:12.103
STEP: Fetching the LimitRange to ensure it has proper values 02/27/23 12:21:12.103
Feb 27 12:21:12.113: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 27 12:21:12.113: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 02/27/23 12:21:12.113
STEP: Ensuring Pod has resource requirements applied from LimitRange 02/27/23 12:21:12.125
Feb 27 12:21:12.136: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 27 12:21:12.136: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 02/27/23 12:21:12.136
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/27/23 12:21:12.146
Feb 27 12:21:12.158: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb 27 12:21:12.158: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 02/27/23 12:21:12.158
STEP: Failing to create a Pod with more than max resources 02/27/23 12:21:12.163
STEP: Updating a LimitRange 02/27/23 12:21:12.168
STEP: Verifying LimitRange updating is effective 02/27/23 12:21:12.178
STEP: Creating a Pod with less than former min resources 02/27/23 12:21:14.191
STEP: Failing to create a Pod with more than max resources 02/27/23 12:21:14.208
STEP: Deleting a LimitRange 02/27/23 12:21:14.217
STEP: Verifying the LimitRange was deleted 02/27/23 12:21:14.228
Feb 27 12:21:19.237: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 02/27/23 12:21:19.237
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Feb 27 12:21:19.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5063" for this suite. 02/27/23 12:21:19.265
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":239,"skipped":4638,"failed":0}
------------------------------
• [SLOW TEST] [7.382 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:21:11.894
    Feb 27 12:21:11.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename limitrange 02/27/23 12:21:11.895
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:11.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:11.954
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 02/27/23 12:21:11.968
    STEP: Setting up watch 02/27/23 12:21:11.969
    STEP: Submitting a LimitRange 02/27/23 12:21:12.086
    STEP: Verifying LimitRange creation was observed 02/27/23 12:21:12.103
    STEP: Fetching the LimitRange to ensure it has proper values 02/27/23 12:21:12.103
    Feb 27 12:21:12.113: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 27 12:21:12.113: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 02/27/23 12:21:12.113
    STEP: Ensuring Pod has resource requirements applied from LimitRange 02/27/23 12:21:12.125
    Feb 27 12:21:12.136: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 27 12:21:12.136: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 02/27/23 12:21:12.136
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/27/23 12:21:12.146
    Feb 27 12:21:12.158: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Feb 27 12:21:12.158: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 02/27/23 12:21:12.158
    STEP: Failing to create a Pod with more than max resources 02/27/23 12:21:12.163
    STEP: Updating a LimitRange 02/27/23 12:21:12.168
    STEP: Verifying LimitRange updating is effective 02/27/23 12:21:12.178
    STEP: Creating a Pod with less than former min resources 02/27/23 12:21:14.191
    STEP: Failing to create a Pod with more than max resources 02/27/23 12:21:14.208
    STEP: Deleting a LimitRange 02/27/23 12:21:14.217
    STEP: Verifying the LimitRange was deleted 02/27/23 12:21:14.228
    Feb 27 12:21:19.237: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 02/27/23 12:21:19.237
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Feb 27 12:21:19.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-5063" for this suite. 02/27/23 12:21:19.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:21:19.283
Feb 27 12:21:19.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 12:21:19.284
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:19.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:19.337
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 12:21:19.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3764" for this suite. 02/27/23 12:21:19.447
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":240,"skipped":4650,"failed":0}
------------------------------
• [0.178 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:21:19.283
    Feb 27 12:21:19.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 12:21:19.284
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:19.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:19.337
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 12:21:19.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3764" for this suite. 02/27/23 12:21:19.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:21:19.47
Feb 27 12:21:19.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename lease-test 02/27/23 12:21:19.472
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:19.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:19.511
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Feb 27 12:21:19.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8633" for this suite. 02/27/23 12:21:19.684
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":241,"skipped":4667,"failed":0}
------------------------------
• [0.226 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:21:19.47
    Feb 27 12:21:19.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename lease-test 02/27/23 12:21:19.472
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:19.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:19.511
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Feb 27 12:21:19.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-8633" for this suite. 02/27/23 12:21:19.684
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:21:19.701
Feb 27 12:21:19.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename watch 02/27/23 12:21:19.702
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:19.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:19.748
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 02/27/23 12:21:19.757
STEP: creating a new configmap 02/27/23 12:21:19.761
STEP: modifying the configmap once 02/27/23 12:21:19.772
STEP: changing the label value of the configmap 02/27/23 12:21:19.792
STEP: Expecting to observe a delete notification for the watched object 02/27/23 12:21:19.816
Feb 27 12:21:19.816: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5986  30747109-4c8b-4a10-a18a-9101d97b9c63 88004 0 2023-02-27 12:21:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 12:21:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 12:21:19.816: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5986  30747109-4c8b-4a10-a18a-9101d97b9c63 88005 0 2023-02-27 12:21:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 12:21:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 12:21:19.816: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5986  30747109-4c8b-4a10-a18a-9101d97b9c63 88006 0 2023-02-27 12:21:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 12:21:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 02/27/23 12:21:19.817
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/27/23 12:21:19.833
STEP: changing the label value of the configmap back 02/27/23 12:21:29.834
STEP: modifying the configmap a third time 02/27/23 12:21:29.862
STEP: deleting the configmap 02/27/23 12:21:29.884
STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/27/23 12:21:29.898
Feb 27 12:21:29.898: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5986  30747109-4c8b-4a10-a18a-9101d97b9c63 88079 0 2023-02-27 12:21:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 12:21:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 12:21:29.899: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5986  30747109-4c8b-4a10-a18a-9101d97b9c63 88080 0 2023-02-27 12:21:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 12:21:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 12:21:29.899: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5986  30747109-4c8b-4a10-a18a-9101d97b9c63 88081 0 2023-02-27 12:21:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 12:21:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 27 12:21:29.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5986" for this suite. 02/27/23 12:21:29.916
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":242,"skipped":4669,"failed":0}
------------------------------
• [SLOW TEST] [10.227 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:21:19.701
    Feb 27 12:21:19.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename watch 02/27/23 12:21:19.702
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:19.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:19.748
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 02/27/23 12:21:19.757
    STEP: creating a new configmap 02/27/23 12:21:19.761
    STEP: modifying the configmap once 02/27/23 12:21:19.772
    STEP: changing the label value of the configmap 02/27/23 12:21:19.792
    STEP: Expecting to observe a delete notification for the watched object 02/27/23 12:21:19.816
    Feb 27 12:21:19.816: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5986  30747109-4c8b-4a10-a18a-9101d97b9c63 88004 0 2023-02-27 12:21:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 12:21:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 12:21:19.816: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5986  30747109-4c8b-4a10-a18a-9101d97b9c63 88005 0 2023-02-27 12:21:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 12:21:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 12:21:19.816: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5986  30747109-4c8b-4a10-a18a-9101d97b9c63 88006 0 2023-02-27 12:21:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 12:21:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 02/27/23 12:21:19.817
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/27/23 12:21:19.833
    STEP: changing the label value of the configmap back 02/27/23 12:21:29.834
    STEP: modifying the configmap a third time 02/27/23 12:21:29.862
    STEP: deleting the configmap 02/27/23 12:21:29.884
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/27/23 12:21:29.898
    Feb 27 12:21:29.898: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5986  30747109-4c8b-4a10-a18a-9101d97b9c63 88079 0 2023-02-27 12:21:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 12:21:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 12:21:29.899: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5986  30747109-4c8b-4a10-a18a-9101d97b9c63 88080 0 2023-02-27 12:21:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 12:21:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 12:21:29.899: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5986  30747109-4c8b-4a10-a18a-9101d97b9c63 88081 0 2023-02-27 12:21:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 12:21:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 27 12:21:29.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5986" for this suite. 02/27/23 12:21:29.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:21:29.937
Feb 27 12:21:29.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename replicaset 02/27/23 12:21:29.938
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:29.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:29.987
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Feb 27 12:21:30.001: INFO: Creating ReplicaSet my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081
Feb 27 12:21:30.062: INFO: Pod name my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081: Found 1 pods out of 1
Feb 27 12:21:30.062: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081" is running
Feb 27 12:21:30.062: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv" in namespace "replicaset-1097" to be "running"
Feb 27 12:21:30.099: INFO: Pod "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv": Phase="Pending", Reason="", readiness=false. Elapsed: 36.170763ms
Feb 27 12:21:32.112: INFO: Pod "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv": Phase="Running", Reason="", readiness=true. Elapsed: 2.049895564s
Feb 27 12:21:32.112: INFO: Pod "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv" satisfied condition "running"
Feb 27 12:21:32.112: INFO: Pod "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv" is running (conditions: [])
Feb 27 12:21:32.112: INFO: Trying to dial the pod
Feb 27 12:21:37.167: INFO: Controller my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081: Got expected result from replica 1 [my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv]: "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 27 12:21:37.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1097" for this suite. 02/27/23 12:21:37.181
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":243,"skipped":4698,"failed":0}
------------------------------
• [SLOW TEST] [7.261 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:21:29.937
    Feb 27 12:21:29.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename replicaset 02/27/23 12:21:29.938
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:29.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:29.987
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Feb 27 12:21:30.001: INFO: Creating ReplicaSet my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081
    Feb 27 12:21:30.062: INFO: Pod name my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081: Found 1 pods out of 1
    Feb 27 12:21:30.062: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081" is running
    Feb 27 12:21:30.062: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv" in namespace "replicaset-1097" to be "running"
    Feb 27 12:21:30.099: INFO: Pod "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv": Phase="Pending", Reason="", readiness=false. Elapsed: 36.170763ms
    Feb 27 12:21:32.112: INFO: Pod "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv": Phase="Running", Reason="", readiness=true. Elapsed: 2.049895564s
    Feb 27 12:21:32.112: INFO: Pod "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv" satisfied condition "running"
    Feb 27 12:21:32.112: INFO: Pod "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv" is running (conditions: [])
    Feb 27 12:21:32.112: INFO: Trying to dial the pod
    Feb 27 12:21:37.167: INFO: Controller my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081: Got expected result from replica 1 [my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv]: "my-hostname-basic-a4c5a717-b7ab-4fa2-b2e2-3affa2f41081-qzcrv", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 27 12:21:37.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1097" for this suite. 02/27/23 12:21:37.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:21:37.207
Feb 27 12:21:37.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename crd-watch 02/27/23 12:21:37.209
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:37.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:37.256
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Feb 27 12:21:37.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Creating first CR  02/27/23 12:21:39.989
Feb 27 12:21:39.998: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T12:21:39Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T12:21:39Z]] name:name1 resourceVersion:88165 uid:fb797a06-e0f0-4495-939c-78d0961c1ba6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 02/27/23 12:21:49.999
Feb 27 12:21:50.021: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T12:21:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T12:21:50Z]] name:name2 resourceVersion:88218 uid:d24faafd-2885-4738-ad5c-0c628b614f06] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 02/27/23 12:22:00.022
Feb 27 12:22:00.034: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T12:21:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T12:22:00Z]] name:name1 resourceVersion:88256 uid:fb797a06-e0f0-4495-939c-78d0961c1ba6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 02/27/23 12:22:10.035
Feb 27 12:22:10.048: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T12:21:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T12:22:10Z]] name:name2 resourceVersion:88294 uid:d24faafd-2885-4738-ad5c-0c628b614f06] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 02/27/23 12:22:20.049
Feb 27 12:22:20.065: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T12:21:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T12:22:00Z]] name:name1 resourceVersion:88332 uid:fb797a06-e0f0-4495-939c-78d0961c1ba6] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 02/27/23 12:22:30.066
Feb 27 12:22:30.079: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T12:21:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T12:22:10Z]] name:name2 resourceVersion:88370 uid:d24faafd-2885-4738-ad5c-0c628b614f06] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:22:40.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6632" for this suite. 02/27/23 12:22:40.641
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":244,"skipped":4721,"failed":0}
------------------------------
• [SLOW TEST] [63.455 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:21:37.207
    Feb 27 12:21:37.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename crd-watch 02/27/23 12:21:37.209
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:21:37.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:21:37.256
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Feb 27 12:21:37.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Creating first CR  02/27/23 12:21:39.989
    Feb 27 12:21:39.998: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T12:21:39Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T12:21:39Z]] name:name1 resourceVersion:88165 uid:fb797a06-e0f0-4495-939c-78d0961c1ba6] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 02/27/23 12:21:49.999
    Feb 27 12:21:50.021: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T12:21:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T12:21:50Z]] name:name2 resourceVersion:88218 uid:d24faafd-2885-4738-ad5c-0c628b614f06] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 02/27/23 12:22:00.022
    Feb 27 12:22:00.034: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T12:21:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T12:22:00Z]] name:name1 resourceVersion:88256 uid:fb797a06-e0f0-4495-939c-78d0961c1ba6] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 02/27/23 12:22:10.035
    Feb 27 12:22:10.048: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T12:21:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T12:22:10Z]] name:name2 resourceVersion:88294 uid:d24faafd-2885-4738-ad5c-0c628b614f06] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 02/27/23 12:22:20.049
    Feb 27 12:22:20.065: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T12:21:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T12:22:00Z]] name:name1 resourceVersion:88332 uid:fb797a06-e0f0-4495-939c-78d0961c1ba6] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 02/27/23 12:22:30.066
    Feb 27 12:22:30.079: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T12:21:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T12:22:10Z]] name:name2 resourceVersion:88370 uid:d24faafd-2885-4738-ad5c-0c628b614f06] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:22:40.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-6632" for this suite. 02/27/23 12:22:40.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:22:40.666
Feb 27 12:22:40.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 12:22:40.667
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:22:40.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:22:40.724
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 12:22:40.79
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 12:22:41.687
STEP: Deploying the webhook pod 02/27/23 12:22:41.716
STEP: Wait for the deployment to be ready 02/27/23 12:22:41.756
Feb 27 12:22:41.777: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 12:22:43.81
STEP: Verifying the service has paired with the endpoint 02/27/23 12:22:43.833
Feb 27 12:22:44.834: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Feb 27 12:22:44.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1357-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 12:22:45.381
STEP: Creating a custom resource that should be mutated by the webhook 02/27/23 12:22:45.44
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:22:48.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6979" for this suite. 02/27/23 12:22:48.118
STEP: Destroying namespace "webhook-6979-markers" for this suite. 02/27/23 12:22:48.148
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":245,"skipped":4731,"failed":0}
------------------------------
• [SLOW TEST] [7.603 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:22:40.666
    Feb 27 12:22:40.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 12:22:40.667
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:22:40.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:22:40.724
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 12:22:40.79
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 12:22:41.687
    STEP: Deploying the webhook pod 02/27/23 12:22:41.716
    STEP: Wait for the deployment to be ready 02/27/23 12:22:41.756
    Feb 27 12:22:41.777: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 12:22:43.81
    STEP: Verifying the service has paired with the endpoint 02/27/23 12:22:43.833
    Feb 27 12:22:44.834: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Feb 27 12:22:44.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1357-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 12:22:45.381
    STEP: Creating a custom resource that should be mutated by the webhook 02/27/23 12:22:45.44
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:22:48.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6979" for this suite. 02/27/23 12:22:48.118
    STEP: Destroying namespace "webhook-6979-markers" for this suite. 02/27/23 12:22:48.148
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:22:48.274
Feb 27 12:22:48.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename deployment 02/27/23 12:22:48.275
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:22:48.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:22:48.345
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 02/27/23 12:22:48.374
STEP: waiting for Deployment to be created 02/27/23 12:22:48.398
STEP: waiting for all Replicas to be Ready 02/27/23 12:22:48.405
Feb 27 12:22:48.413: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 12:22:48.414: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 12:22:48.447: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 12:22:48.447: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 12:22:48.481: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 12:22:48.481: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 12:22:48.618: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 12:22:48.618: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 12:22:49.872: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 27 12:22:49.872: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 27 12:22:50.094: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 02/27/23 12:22:50.094
W0227 12:22:50.112540      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 27 12:22:50.120: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 02/27/23 12:22:50.121
Feb 27 12:22:50.131: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
Feb 27 12:22:50.133: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
Feb 27 12:22:50.134: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
Feb 27 12:22:50.135: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
Feb 27 12:22:50.135: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
Feb 27 12:22:50.135: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
Feb 27 12:22:50.138: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
Feb 27 12:22:50.138: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
Feb 27 12:22:50.138: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
Feb 27 12:22:50.140: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
Feb 27 12:22:50.140: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:50.140: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:50.140: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:50.140: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:50.141: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:50.141: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:50.180: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:50.181: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:50.222: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
Feb 27 12:22:50.222: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
Feb 27 12:22:50.233: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
Feb 27 12:22:50.233: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
Feb 27 12:22:51.898: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:51.898: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:51.945: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
STEP: listing Deployments 02/27/23 12:22:51.945
Feb 27 12:22:51.957: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 02/27/23 12:22:51.957
Feb 27 12:22:51.981: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 02/27/23 12:22:51.981
Feb 27 12:22:52.007: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 12:22:52.008: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 12:22:52.025: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 12:22:52.074: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 12:22:52.085: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 12:22:52.917: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 12:22:53.016: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 12:22:53.027: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 12:22:54.135: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 02/27/23 12:22:54.171
STEP: fetching the DeploymentStatus 02/27/23 12:22:54.188
Feb 27 12:22:54.209: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
Feb 27 12:22:54.209: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
Feb 27 12:22:54.209: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
Feb 27 12:22:54.209: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
Feb 27 12:22:54.210: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
Feb 27 12:22:54.210: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:54.210: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:54.210: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
Feb 27 12:22:54.210: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 3
STEP: deleting the Deployment 02/27/23 12:22:54.21
Feb 27 12:22:54.231: INFO: observed event type MODIFIED
Feb 27 12:22:54.231: INFO: observed event type MODIFIED
Feb 27 12:22:54.231: INFO: observed event type MODIFIED
Feb 27 12:22:54.232: INFO: observed event type MODIFIED
Feb 27 12:22:54.232: INFO: observed event type MODIFIED
Feb 27 12:22:54.232: INFO: observed event type MODIFIED
Feb 27 12:22:54.232: INFO: observed event type MODIFIED
Feb 27 12:22:54.232: INFO: observed event type MODIFIED
Feb 27 12:22:54.232: INFO: observed event type MODIFIED
Feb 27 12:22:54.232: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 12:22:54.243: INFO: Log out all the ReplicaSets if there is no deployment created
Feb 27 12:22:54.257: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-222  3be6ce7c-4092-48f8-8aa3-0ecb377fbe38 88704 4 2023-02-27 12:22:50 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment cfa58815-6849-4718-8d91-4c1f0627db1d 0xc006eeb167 0xc006eeb168}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfa58815-6849-4718-8d91-4c1f0627db1d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:22:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006eeb1f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Feb 27 12:22:54.271: INFO: pod: "test-deployment-54cc775c4b-6xx5q":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-6xx5q test-deployment-54cc775c4b- deployment-222  d2c87a5d-4d89-4c37-ab8d-596685b29c53 88700 0 2023-02-27 12:22:50 +0000 UTC 2023-02-27 12:22:55 +0000 UTC 0xc006eeb648 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:fbea92888ee29e73ecf7692ca9031f041ac4b7f501c2c31a7e03c4b4a7dde05e cni.projectcalico.org/podIP:172.25.2.206/32 cni.projectcalico.org/podIPs:172.25.2.206/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 3be6ce7c-4092-48f8-8aa3-0ecb377fbe38 0xc006eeb697 0xc006eeb698}] [] [{calico Update v1 2023-02-27 12:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be6ce7c-4092-48f8-8aa3-0ecb377fbe38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:22:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdpr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdpr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.206,StartTime:2023-02-27 12:22:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:22:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://2be3d9030038db992bd23e0eb9ad883e6b6a5f1c315cb7701542effc71aa3477,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 27 12:22:54.272: INFO: pod: "test-deployment-54cc775c4b-q7wv8":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-q7wv8 test-deployment-54cc775c4b- deployment-222  b1b0c141-2654-4722-bac3-47bb514bee3f 88697 0 2023-02-27 12:22:51 +0000 UTC 2023-02-27 12:22:53 +0000 UTC 0xc006eeb870 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:31be4d227ef072b6a0810ee62f7fab493d5b40eee8b1776c69f7cfb1b2b237a2 cni.projectcalico.org/podIP:172.25.1.186/32 cni.projectcalico.org/podIPs:172.25.1.186/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 3be6ce7c-4092-48f8-8aa3-0ecb377fbe38 0xc006eeb8c7 0xc006eeb8c8}] [] [{kube-controller-manager Update v1 2023-02-27 12:22:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be6ce7c-4092-48f8-8aa3-0ecb377fbe38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:22:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-988ng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-988ng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:172.25.1.186,StartTime:2023-02-27 12:22:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:22:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://c2cd294aebf0b1a36614f5d138d22241cb4bfee599e5693103dbd10a92efbcb6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 27 12:22:54.272: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-222  68ca0ca3-0c19-485e-ba03-2e82554295f2 88695 2 2023-02-27 12:22:51 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment cfa58815-6849-4718-8d91-4c1f0627db1d 0xc006eeb257 0xc006eeb258}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfa58815-6849-4718-8d91-4c1f0627db1d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:22:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006eeb2e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Feb 27 12:22:54.286: INFO: pod: "test-deployment-7c7d8d58c8-8h595":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-8h595 test-deployment-7c7d8d58c8- deployment-222  719457b0-3a2c-4f0b-9785-f2da627e414d 88711 0 2023-02-27 12:22:52 +0000 UTC 2023-02-27 12:22:55 +0000 UTC 0xc006fbaca8 map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:7822812424ec11b5403c974b4069c35d1aabff0615784c520c717d0a081a7f07 cni.projectcalico.org/podIP:172.25.1.187/32 cni.projectcalico.org/podIPs:172.25.1.187/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 68ca0ca3-0c19-485e-ba03-2e82554295f2 0xc006fbacf7 0xc006fbacf8}] [] [{kube-controller-manager Update v1 2023-02-27 12:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68ca0ca3-0c19-485e-ba03-2e82554295f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:22:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:22:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpvng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpvng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:172.25.1.187,StartTime:2023-02-27 12:22:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:22:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b096c5e3503b1d6a41bb27b23f401e0ce2b2bf57c1c37e9233fdae8175341466,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 27 12:22:54.287: INFO: pod: "test-deployment-7c7d8d58c8-pm4pb":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-pm4pb test-deployment-7c7d8d58c8- deployment-222  e8b1b461-e79c-4f19-859c-6cc9248e2112 88710 0 2023-02-27 12:22:52 +0000 UTC 2023-02-27 12:22:55 +0000 UTC 0xc006fbaee0 map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:290b9576f1583a0046ea300c90d7dfe17291bcc6342eaa6d0944b85fe05eef91 cni.projectcalico.org/podIP:172.25.2.207/32 cni.projectcalico.org/podIPs:172.25.2.207/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 68ca0ca3-0c19-485e-ba03-2e82554295f2 0xc006fbaf47 0xc006fbaf48}] [] [{calico Update v1 2023-02-27 12:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68ca0ca3-0c19-485e-ba03-2e82554295f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:22:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fg285,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fg285,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.207,StartTime:2023-02-27 12:22:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:22:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://20c7e599bb8348fc5b9a23912782fecc7a9dc3da660ffe42db87a3d632ecadc4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 27 12:22:54.287: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-222  33712aee-4254-4956-b563-004b5ece581d 88598 3 2023-02-27 12:22:48 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment cfa58815-6849-4718-8d91-4c1f0627db1d 0xc006eeb347 0xc006eeb348}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:22:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfa58815-6849-4718-8d91-4c1f0627db1d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:22:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006eeb3d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 27 12:22:54.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-222" for this suite. 02/27/23 12:22:54.328
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":246,"skipped":4769,"failed":0}
------------------------------
• [SLOW TEST] [6.071 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:22:48.274
    Feb 27 12:22:48.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename deployment 02/27/23 12:22:48.275
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:22:48.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:22:48.345
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 02/27/23 12:22:48.374
    STEP: waiting for Deployment to be created 02/27/23 12:22:48.398
    STEP: waiting for all Replicas to be Ready 02/27/23 12:22:48.405
    Feb 27 12:22:48.413: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 12:22:48.414: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 12:22:48.447: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 12:22:48.447: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 12:22:48.481: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 12:22:48.481: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 12:22:48.618: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 12:22:48.618: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 12:22:49.872: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 27 12:22:49.872: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 27 12:22:50.094: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 02/27/23 12:22:50.094
    W0227 12:22:50.112540      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 27 12:22:50.120: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 02/27/23 12:22:50.121
    Feb 27 12:22:50.131: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
    Feb 27 12:22:50.133: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
    Feb 27 12:22:50.134: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
    Feb 27 12:22:50.135: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
    Feb 27 12:22:50.135: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
    Feb 27 12:22:50.135: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
    Feb 27 12:22:50.138: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
    Feb 27 12:22:50.138: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 0
    Feb 27 12:22:50.138: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    Feb 27 12:22:50.140: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    Feb 27 12:22:50.140: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:50.140: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:50.140: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:50.140: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:50.141: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:50.141: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:50.180: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:50.181: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:50.222: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    Feb 27 12:22:50.222: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    Feb 27 12:22:50.233: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    Feb 27 12:22:50.233: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    Feb 27 12:22:51.898: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:51.898: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:51.945: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    STEP: listing Deployments 02/27/23 12:22:51.945
    Feb 27 12:22:51.957: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 02/27/23 12:22:51.957
    Feb 27 12:22:51.981: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 02/27/23 12:22:51.981
    Feb 27 12:22:52.007: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 12:22:52.008: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 12:22:52.025: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 12:22:52.074: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 12:22:52.085: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 12:22:52.917: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 12:22:53.016: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 12:22:53.027: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 12:22:54.135: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 02/27/23 12:22:54.171
    STEP: fetching the DeploymentStatus 02/27/23 12:22:54.188
    Feb 27 12:22:54.209: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    Feb 27 12:22:54.209: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    Feb 27 12:22:54.209: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    Feb 27 12:22:54.209: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    Feb 27 12:22:54.210: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 1
    Feb 27 12:22:54.210: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:54.210: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:54.210: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 2
    Feb 27 12:22:54.210: INFO: observed Deployment test-deployment in namespace deployment-222 with ReadyReplicas 3
    STEP: deleting the Deployment 02/27/23 12:22:54.21
    Feb 27 12:22:54.231: INFO: observed event type MODIFIED
    Feb 27 12:22:54.231: INFO: observed event type MODIFIED
    Feb 27 12:22:54.231: INFO: observed event type MODIFIED
    Feb 27 12:22:54.232: INFO: observed event type MODIFIED
    Feb 27 12:22:54.232: INFO: observed event type MODIFIED
    Feb 27 12:22:54.232: INFO: observed event type MODIFIED
    Feb 27 12:22:54.232: INFO: observed event type MODIFIED
    Feb 27 12:22:54.232: INFO: observed event type MODIFIED
    Feb 27 12:22:54.232: INFO: observed event type MODIFIED
    Feb 27 12:22:54.232: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 12:22:54.243: INFO: Log out all the ReplicaSets if there is no deployment created
    Feb 27 12:22:54.257: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-222  3be6ce7c-4092-48f8-8aa3-0ecb377fbe38 88704 4 2023-02-27 12:22:50 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment cfa58815-6849-4718-8d91-4c1f0627db1d 0xc006eeb167 0xc006eeb168}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:22:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfa58815-6849-4718-8d91-4c1f0627db1d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:22:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006eeb1f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Feb 27 12:22:54.271: INFO: pod: "test-deployment-54cc775c4b-6xx5q":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-6xx5q test-deployment-54cc775c4b- deployment-222  d2c87a5d-4d89-4c37-ab8d-596685b29c53 88700 0 2023-02-27 12:22:50 +0000 UTC 2023-02-27 12:22:55 +0000 UTC 0xc006eeb648 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:fbea92888ee29e73ecf7692ca9031f041ac4b7f501c2c31a7e03c4b4a7dde05e cni.projectcalico.org/podIP:172.25.2.206/32 cni.projectcalico.org/podIPs:172.25.2.206/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 3be6ce7c-4092-48f8-8aa3-0ecb377fbe38 0xc006eeb697 0xc006eeb698}] [] [{calico Update v1 2023-02-27 12:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be6ce7c-4092-48f8-8aa3-0ecb377fbe38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:22:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdpr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdpr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.206,StartTime:2023-02-27 12:22:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:22:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://2be3d9030038db992bd23e0eb9ad883e6b6a5f1c315cb7701542effc71aa3477,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 27 12:22:54.272: INFO: pod: "test-deployment-54cc775c4b-q7wv8":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-q7wv8 test-deployment-54cc775c4b- deployment-222  b1b0c141-2654-4722-bac3-47bb514bee3f 88697 0 2023-02-27 12:22:51 +0000 UTC 2023-02-27 12:22:53 +0000 UTC 0xc006eeb870 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:31be4d227ef072b6a0810ee62f7fab493d5b40eee8b1776c69f7cfb1b2b237a2 cni.projectcalico.org/podIP:172.25.1.186/32 cni.projectcalico.org/podIPs:172.25.1.186/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 3be6ce7c-4092-48f8-8aa3-0ecb377fbe38 0xc006eeb8c7 0xc006eeb8c8}] [] [{kube-controller-manager Update v1 2023-02-27 12:22:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be6ce7c-4092-48f8-8aa3-0ecb377fbe38\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:22:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-988ng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-988ng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:172.25.1.186,StartTime:2023-02-27 12:22:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:22:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://c2cd294aebf0b1a36614f5d138d22241cb4bfee599e5693103dbd10a92efbcb6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 27 12:22:54.272: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-222  68ca0ca3-0c19-485e-ba03-2e82554295f2 88695 2 2023-02-27 12:22:51 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment cfa58815-6849-4718-8d91-4c1f0627db1d 0xc006eeb257 0xc006eeb258}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfa58815-6849-4718-8d91-4c1f0627db1d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:22:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006eeb2e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Feb 27 12:22:54.286: INFO: pod: "test-deployment-7c7d8d58c8-8h595":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-8h595 test-deployment-7c7d8d58c8- deployment-222  719457b0-3a2c-4f0b-9785-f2da627e414d 88711 0 2023-02-27 12:22:52 +0000 UTC 2023-02-27 12:22:55 +0000 UTC 0xc006fbaca8 map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:7822812424ec11b5403c974b4069c35d1aabff0615784c520c717d0a081a7f07 cni.projectcalico.org/podIP:172.25.1.187/32 cni.projectcalico.org/podIPs:172.25.1.187/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 68ca0ca3-0c19-485e-ba03-2e82554295f2 0xc006fbacf7 0xc006fbacf8}] [] [{kube-controller-manager Update v1 2023-02-27 12:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68ca0ca3-0c19-485e-ba03-2e82554295f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:22:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:22:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.187\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpvng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpvng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:172.25.1.187,StartTime:2023-02-27 12:22:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:22:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b096c5e3503b1d6a41bb27b23f401e0ce2b2bf57c1c37e9233fdae8175341466,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 27 12:22:54.287: INFO: pod: "test-deployment-7c7d8d58c8-pm4pb":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-pm4pb test-deployment-7c7d8d58c8- deployment-222  e8b1b461-e79c-4f19-859c-6cc9248e2112 88710 0 2023-02-27 12:22:52 +0000 UTC 2023-02-27 12:22:55 +0000 UTC 0xc006fbaee0 map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:290b9576f1583a0046ea300c90d7dfe17291bcc6342eaa6d0944b85fe05eef91 cni.projectcalico.org/podIP:172.25.2.207/32 cni.projectcalico.org/podIPs:172.25.2.207/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 68ca0ca3-0c19-485e-ba03-2e82554295f2 0xc006fbaf47 0xc006fbaf48}] [] [{calico Update v1 2023-02-27 12:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:22:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68ca0ca3-0c19-485e-ba03-2e82554295f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:22:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fg285,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fg285,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:22:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.207,StartTime:2023-02-27 12:22:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:22:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://20c7e599bb8348fc5b9a23912782fecc7a9dc3da660ffe42db87a3d632ecadc4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 27 12:22:54.287: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-222  33712aee-4254-4956-b563-004b5ece581d 88598 3 2023-02-27 12:22:48 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment cfa58815-6849-4718-8d91-4c1f0627db1d 0xc006eeb347 0xc006eeb348}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:22:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfa58815-6849-4718-8d91-4c1f0627db1d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:22:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006eeb3d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 27 12:22:54.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-222" for this suite. 02/27/23 12:22:54.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:22:54.354
Feb 27 12:22:54.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 12:22:54.363
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:22:54.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:22:54.405
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 02/27/23 12:22:54.415
Feb 27 12:22:54.451: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64" in namespace "downward-api-4820" to be "Succeeded or Failed"
Feb 27 12:22:54.476: INFO: Pod "downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64": Phase="Pending", Reason="", readiness=false. Elapsed: 24.708867ms
Feb 27 12:22:56.485: INFO: Pod "downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033329698s
Feb 27 12:22:58.488: INFO: Pod "downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036471782s
STEP: Saw pod success 02/27/23 12:22:58.488
Feb 27 12:22:58.488: INFO: Pod "downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64" satisfied condition "Succeeded or Failed"
Feb 27 12:22:58.501: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64 container client-container: <nil>
STEP: delete the pod 02/27/23 12:22:58.525
Feb 27 12:22:58.558: INFO: Waiting for pod downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64 to disappear
Feb 27 12:22:58.565: INFO: Pod downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 27 12:22:58.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4820" for this suite. 02/27/23 12:22:58.577
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":247,"skipped":4788,"failed":0}
------------------------------
• [4.239 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:22:54.354
    Feb 27 12:22:54.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 12:22:54.363
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:22:54.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:22:54.405
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 02/27/23 12:22:54.415
    Feb 27 12:22:54.451: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64" in namespace "downward-api-4820" to be "Succeeded or Failed"
    Feb 27 12:22:54.476: INFO: Pod "downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64": Phase="Pending", Reason="", readiness=false. Elapsed: 24.708867ms
    Feb 27 12:22:56.485: INFO: Pod "downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033329698s
    Feb 27 12:22:58.488: INFO: Pod "downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036471782s
    STEP: Saw pod success 02/27/23 12:22:58.488
    Feb 27 12:22:58.488: INFO: Pod "downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64" satisfied condition "Succeeded or Failed"
    Feb 27 12:22:58.501: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64 container client-container: <nil>
    STEP: delete the pod 02/27/23 12:22:58.525
    Feb 27 12:22:58.558: INFO: Waiting for pod downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64 to disappear
    Feb 27 12:22:58.565: INFO: Pod downwardapi-volume-55a7309c-03ee-4f71-b643-36501114dd64 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 27 12:22:58.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4820" for this suite. 02/27/23 12:22:58.577
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:22:58.595
Feb 27 12:22:58.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename replicaset 02/27/23 12:22:58.597
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:22:58.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:22:58.662
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/27/23 12:22:58.676
Feb 27 12:22:58.694: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6677" to be "running and ready"
Feb 27 12:22:58.715: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 20.912823ms
Feb 27 12:22:58.715: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:23:00.723: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.029521795s
Feb 27 12:23:00.723: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Feb 27 12:23:00.723: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 02/27/23 12:23:00.732
STEP: Then the orphan pod is adopted 02/27/23 12:23:00.749
STEP: When the matched label of one of its pods change 02/27/23 12:23:01.768
Feb 27 12:23:01.777: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 02/27/23 12:23:01.807
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 27 12:23:01.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6677" for this suite. 02/27/23 12:23:01.884
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":248,"skipped":4792,"failed":0}
------------------------------
• [3.325 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:22:58.595
    Feb 27 12:22:58.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename replicaset 02/27/23 12:22:58.597
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:22:58.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:22:58.662
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/27/23 12:22:58.676
    Feb 27 12:22:58.694: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6677" to be "running and ready"
    Feb 27 12:22:58.715: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 20.912823ms
    Feb 27 12:22:58.715: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:23:00.723: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.029521795s
    Feb 27 12:23:00.723: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Feb 27 12:23:00.723: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 02/27/23 12:23:00.732
    STEP: Then the orphan pod is adopted 02/27/23 12:23:00.749
    STEP: When the matched label of one of its pods change 02/27/23 12:23:01.768
    Feb 27 12:23:01.777: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/27/23 12:23:01.807
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 27 12:23:01.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6677" for this suite. 02/27/23 12:23:01.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:23:01.922
Feb 27 12:23:01.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename containers 02/27/23 12:23:01.929
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:23:01.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:23:01.982
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 02/27/23 12:23:02.007
Feb 27 12:23:02.025: INFO: Waiting up to 5m0s for pod "client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec" in namespace "containers-7892" to be "Succeeded or Failed"
Feb 27 12:23:02.038: INFO: Pod "client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec": Phase="Pending", Reason="", readiness=false. Elapsed: 12.87241ms
Feb 27 12:23:04.050: INFO: Pod "client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec": Phase="Running", Reason="", readiness=false. Elapsed: 2.024607882s
Feb 27 12:23:06.085: INFO: Pod "client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059707389s
STEP: Saw pod success 02/27/23 12:23:06.085
Feb 27 12:23:06.085: INFO: Pod "client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec" satisfied condition "Succeeded or Failed"
Feb 27 12:23:06.119: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec container agnhost-container: <nil>
STEP: delete the pod 02/27/23 12:23:06.149
Feb 27 12:23:06.206: INFO: Waiting for pod client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec to disappear
Feb 27 12:23:06.219: INFO: Pod client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 27 12:23:06.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7892" for this suite. 02/27/23 12:23:06.233
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":249,"skipped":4802,"failed":0}
------------------------------
• [4.336 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:23:01.922
    Feb 27 12:23:01.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename containers 02/27/23 12:23:01.929
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:23:01.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:23:01.982
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 02/27/23 12:23:02.007
    Feb 27 12:23:02.025: INFO: Waiting up to 5m0s for pod "client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec" in namespace "containers-7892" to be "Succeeded or Failed"
    Feb 27 12:23:02.038: INFO: Pod "client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec": Phase="Pending", Reason="", readiness=false. Elapsed: 12.87241ms
    Feb 27 12:23:04.050: INFO: Pod "client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec": Phase="Running", Reason="", readiness=false. Elapsed: 2.024607882s
    Feb 27 12:23:06.085: INFO: Pod "client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059707389s
    STEP: Saw pod success 02/27/23 12:23:06.085
    Feb 27 12:23:06.085: INFO: Pod "client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec" satisfied condition "Succeeded or Failed"
    Feb 27 12:23:06.119: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 12:23:06.149
    Feb 27 12:23:06.206: INFO: Waiting for pod client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec to disappear
    Feb 27 12:23:06.219: INFO: Pod client-containers-c0569a6d-7d19-48a8-9c7b-19a17da89aec no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 27 12:23:06.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7892" for this suite. 02/27/23 12:23:06.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:23:06.261
Feb 27 12:23:06.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename runtimeclass 02/27/23 12:23:06.262
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:23:06.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:23:06.345
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Feb 27 12:23:06.389: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5034 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 27 12:23:06.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5034" for this suite. 02/27/23 12:23:06.453
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":250,"skipped":4808,"failed":0}
------------------------------
• [0.230 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:23:06.261
    Feb 27 12:23:06.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 12:23:06.262
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:23:06.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:23:06.345
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Feb 27 12:23:06.389: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5034 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 27 12:23:06.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5034" for this suite. 02/27/23 12:23:06.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:23:06.499
Feb 27 12:23:06.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-probe 02/27/23 12:23:06.5
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:23:06.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:23:06.556
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e in namespace container-probe-9842 02/27/23 12:23:06.569
Feb 27 12:23:06.588: INFO: Waiting up to 5m0s for pod "busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e" in namespace "container-probe-9842" to be "not pending"
Feb 27 12:23:06.600: INFO: Pod "busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.403057ms
Feb 27 12:23:08.610: INFO: Pod "busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.021620675s
Feb 27 12:23:08.610: INFO: Pod "busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e" satisfied condition "not pending"
Feb 27 12:23:08.610: INFO: Started pod busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e in namespace container-probe-9842
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 12:23:08.61
Feb 27 12:23:08.617: INFO: Initial restart count of pod busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e is 0
STEP: deleting the pod 02/27/23 12:27:10.481
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 27 12:27:10.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9842" for this suite. 02/27/23 12:27:10.544
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":251,"skipped":4822,"failed":0}
------------------------------
• [SLOW TEST] [244.064 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:23:06.499
    Feb 27 12:23:06.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-probe 02/27/23 12:23:06.5
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:23:06.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:23:06.556
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e in namespace container-probe-9842 02/27/23 12:23:06.569
    Feb 27 12:23:06.588: INFO: Waiting up to 5m0s for pod "busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e" in namespace "container-probe-9842" to be "not pending"
    Feb 27 12:23:06.600: INFO: Pod "busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.403057ms
    Feb 27 12:23:08.610: INFO: Pod "busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.021620675s
    Feb 27 12:23:08.610: INFO: Pod "busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e" satisfied condition "not pending"
    Feb 27 12:23:08.610: INFO: Started pod busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e in namespace container-probe-9842
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 12:23:08.61
    Feb 27 12:23:08.617: INFO: Initial restart count of pod busybox-5b308914-f302-4ed0-abb5-b9e50a5ddc6e is 0
    STEP: deleting the pod 02/27/23 12:27:10.481
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 27 12:27:10.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9842" for this suite. 02/27/23 12:27:10.544
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:27:10.565
Feb 27 12:27:10.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename deployment 02/27/23 12:27:10.567
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:10.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:10.626
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Feb 27 12:27:10.655: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 27 12:27:16.175: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 12:27:16.176
Feb 27 12:27:16.176: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 27 12:27:18.186: INFO: Creating deployment "test-rollover-deployment"
Feb 27 12:27:18.206: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 27 12:27:20.226: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 27 12:27:20.243: INFO: Ensure that both replica sets have 1 created replica
Feb 27 12:27:20.263: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 27 12:27:20.284: INFO: Updating deployment test-rollover-deployment
Feb 27 12:27:20.284: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 27 12:27:22.301: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 27 12:27:22.319: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 27 12:27:22.349: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 12:27:22.349: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 12:27:24.365: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 12:27:24.365: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 12:27:26.535: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 12:27:26.535: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 12:27:28.368: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 12:27:28.368: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 12:27:30.369: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 12:27:30.370: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 12:27:32.380: INFO: 
Feb 27 12:27:32.380: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 12:27:32.403: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1866  1ccb914d-bc7b-45bd-a346-0c9cc633287e 90065 2 2023-02-27 12:27:18 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 12:27:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:27:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059cb5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 12:27:18 +0000 UTC,LastTransitionTime:2023-02-27 12:27:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-02-27 12:27:32 +0000 UTC,LastTransitionTime:2023-02-27 12:27:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 12:27:32.412: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-1866  518d2a20-9885-4301-938a-31016835a1b1 90055 2 2023-02-27 12:27:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1ccb914d-bc7b-45bd-a346-0c9cc633287e 0xc0028670b7 0xc0028670b8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:27:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ccb914d-bc7b-45bd-a346-0c9cc633287e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:27:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002867168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 12:27:32.412: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 27 12:27:32.412: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1866  0785c955-19c5-41b5-a3c5-43df0421aa29 90064 2 2023-02-27 12:27:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1ccb914d-bc7b-45bd-a346-0c9cc633287e 0xc002866e67 0xc002866e68}] [] [{e2e.test Update apps/v1 2023-02-27 12:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:27:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ccb914d-bc7b-45bd-a346-0c9cc633287e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:27:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002866f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 12:27:32.412: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-1866  f61d6fca-0b9d-47d2-87d6-fd2238d2e02d 89999 2 2023-02-27 12:27:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1ccb914d-bc7b-45bd-a346-0c9cc633287e 0xc002866f97 0xc002866f98}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:27:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ccb914d-bc7b-45bd-a346-0c9cc633287e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:27:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002867048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 12:27:32.425: INFO: Pod "test-rollover-deployment-6d45fd857b-r8f75" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-r8f75 test-rollover-deployment-6d45fd857b- deployment-1866  4d3f5f6e-c607-46c2-a65c-bee82ede2ec1 90015 0 2023-02-27 12:27:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:90987b5c2d9fae6b588899c62454fd4ea8a2845ad8900a803ccd542e82c288bf cni.projectcalico.org/podIP:172.25.1.190/32 cni.projectcalico.org/podIPs:172.25.1.190/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 518d2a20-9885-4301-938a-31016835a1b1 0xc0059cb9e7 0xc0059cb9e8}] [] [{calico Update v1 2023-02-27 12:27:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:27:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"518d2a20-9885-4301-938a-31016835a1b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:27:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xh829,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xh829,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:27:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:27:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:27:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:27:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:172.25.1.190,StartTime:2023-02-27 12:27:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:27:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://e817fe85f2932076f60487d5c2e78439458ed1ad5932340beacb875671a7b738,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 27 12:27:32.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1866" for this suite. 02/27/23 12:27:32.436
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":252,"skipped":4822,"failed":0}
------------------------------
• [SLOW TEST] [21.882 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:27:10.565
    Feb 27 12:27:10.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename deployment 02/27/23 12:27:10.567
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:10.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:10.626
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Feb 27 12:27:10.655: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Feb 27 12:27:16.175: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 12:27:16.176
    Feb 27 12:27:16.176: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Feb 27 12:27:18.186: INFO: Creating deployment "test-rollover-deployment"
    Feb 27 12:27:18.206: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Feb 27 12:27:20.226: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Feb 27 12:27:20.243: INFO: Ensure that both replica sets have 1 created replica
    Feb 27 12:27:20.263: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Feb 27 12:27:20.284: INFO: Updating deployment test-rollover-deployment
    Feb 27 12:27:20.284: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Feb 27 12:27:22.301: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Feb 27 12:27:22.319: INFO: Make sure deployment "test-rollover-deployment" is complete
    Feb 27 12:27:22.349: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 12:27:22.349: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 12:27:24.365: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 12:27:24.365: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 12:27:26.535: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 12:27:26.535: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 12:27:28.368: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 12:27:28.368: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 12:27:30.369: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 12:27:30.370: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 27, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 27, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 12:27:32.380: INFO: 
    Feb 27 12:27:32.380: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 12:27:32.403: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-1866  1ccb914d-bc7b-45bd-a346-0c9cc633287e 90065 2 2023-02-27 12:27:18 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 12:27:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:27:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059cb5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 12:27:18 +0000 UTC,LastTransitionTime:2023-02-27 12:27:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-02-27 12:27:32 +0000 UTC,LastTransitionTime:2023-02-27 12:27:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 12:27:32.412: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-1866  518d2a20-9885-4301-938a-31016835a1b1 90055 2 2023-02-27 12:27:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1ccb914d-bc7b-45bd-a346-0c9cc633287e 0xc0028670b7 0xc0028670b8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:27:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ccb914d-bc7b-45bd-a346-0c9cc633287e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:27:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002867168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 12:27:32.412: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Feb 27 12:27:32.412: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1866  0785c955-19c5-41b5-a3c5-43df0421aa29 90064 2 2023-02-27 12:27:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1ccb914d-bc7b-45bd-a346-0c9cc633287e 0xc002866e67 0xc002866e68}] [] [{e2e.test Update apps/v1 2023-02-27 12:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:27:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ccb914d-bc7b-45bd-a346-0c9cc633287e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:27:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002866f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 12:27:32.412: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-1866  f61d6fca-0b9d-47d2-87d6-fd2238d2e02d 89999 2 2023-02-27 12:27:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1ccb914d-bc7b-45bd-a346-0c9cc633287e 0xc002866f97 0xc002866f98}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:27:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ccb914d-bc7b-45bd-a346-0c9cc633287e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:27:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002867048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 12:27:32.425: INFO: Pod "test-rollover-deployment-6d45fd857b-r8f75" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-r8f75 test-rollover-deployment-6d45fd857b- deployment-1866  4d3f5f6e-c607-46c2-a65c-bee82ede2ec1 90015 0 2023-02-27 12:27:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:90987b5c2d9fae6b588899c62454fd4ea8a2845ad8900a803ccd542e82c288bf cni.projectcalico.org/podIP:172.25.1.190/32 cni.projectcalico.org/podIPs:172.25.1.190/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 518d2a20-9885-4301-938a-31016835a1b1 0xc0059cb9e7 0xc0059cb9e8}] [] [{calico Update v1 2023-02-27 12:27:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:27:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"518d2a20-9885-4301-938a-31016835a1b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:27:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xh829,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xh829,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:27:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:27:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:27:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:27:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:172.25.1.190,StartTime:2023-02-27 12:27:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:27:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://e817fe85f2932076f60487d5c2e78439458ed1ad5932340beacb875671a7b738,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 27 12:27:32.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1866" for this suite. 02/27/23 12:27:32.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:27:32.45
Feb 27 12:27:32.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 12:27:32.452
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:32.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:32.518
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/27/23 12:27:32.561
Feb 27 12:27:32.589: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6127" to be "running and ready"
Feb 27 12:27:32.599: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.107291ms
Feb 27 12:27:32.599: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:27:34.617: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.028070992s
Feb 27 12:27:34.617: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 27 12:27:34.617: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 02/27/23 12:27:34.634
Feb 27 12:27:34.667: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6127" to be "running and ready"
Feb 27 12:27:34.700: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 32.684253ms
Feb 27 12:27:34.700: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:27:36.711: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.044032279s
Feb 27 12:27:36.711: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Feb 27 12:27:36.711: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/27/23 12:27:36.721
Feb 27 12:27:36.736: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 27 12:27:36.745: INFO: Pod pod-with-prestop-http-hook still exists
Feb 27 12:27:38.746: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 27 12:27:38.758: INFO: Pod pod-with-prestop-http-hook still exists
Feb 27 12:27:40.746: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 27 12:27:40.763: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 02/27/23 12:27:40.763
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 27 12:27:40.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6127" for this suite. 02/27/23 12:27:40.827
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":253,"skipped":4835,"failed":0}
------------------------------
• [SLOW TEST] [8.413 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:27:32.45
    Feb 27 12:27:32.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 12:27:32.452
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:32.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:32.518
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/27/23 12:27:32.561
    Feb 27 12:27:32.589: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6127" to be "running and ready"
    Feb 27 12:27:32.599: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 10.107291ms
    Feb 27 12:27:32.599: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:27:34.617: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.028070992s
    Feb 27 12:27:34.617: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 27 12:27:34.617: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 02/27/23 12:27:34.634
    Feb 27 12:27:34.667: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6127" to be "running and ready"
    Feb 27 12:27:34.700: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 32.684253ms
    Feb 27 12:27:34.700: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:27:36.711: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.044032279s
    Feb 27 12:27:36.711: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Feb 27 12:27:36.711: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/27/23 12:27:36.721
    Feb 27 12:27:36.736: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 27 12:27:36.745: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 27 12:27:38.746: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 27 12:27:38.758: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 27 12:27:40.746: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 27 12:27:40.763: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 02/27/23 12:27:40.763
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 27 12:27:40.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6127" for this suite. 02/27/23 12:27:40.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:27:40.866
Feb 27 12:27:40.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 12:27:40.868
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:40.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:40.989
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3435 02/27/23 12:27:41.028
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/27/23 12:27:41.074
STEP: creating service externalsvc in namespace services-3435 02/27/23 12:27:41.075
STEP: creating replication controller externalsvc in namespace services-3435 02/27/23 12:27:41.105
I0227 12:27:41.119309      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3435, replica count: 2
I0227 12:27:44.170737      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 02/27/23 12:27:44.18
Feb 27 12:27:44.205: INFO: Creating new exec pod
Feb 27 12:27:44.219: INFO: Waiting up to 5m0s for pod "execpodfjrbj" in namespace "services-3435" to be "running"
Feb 27 12:27:44.229: INFO: Pod "execpodfjrbj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.411953ms
Feb 27 12:27:46.238: INFO: Pod "execpodfjrbj": Phase="Running", Reason="", readiness=true. Elapsed: 2.019091959s
Feb 27 12:27:46.238: INFO: Pod "execpodfjrbj" satisfied condition "running"
Feb 27 12:27:46.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-3435 exec execpodfjrbj -- /bin/sh -x -c nslookup clusterip-service.services-3435.svc.cluster.local'
Feb 27 12:27:46.640: INFO: stderr: "+ nslookup clusterip-service.services-3435.svc.cluster.local\n"
Feb 27 12:27:46.640: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-3435.svc.cluster.local\tcanonical name = externalsvc.services-3435.svc.cluster.local.\nName:\texternalsvc.services-3435.svc.cluster.local\nAddress: 10.240.28.175\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3435, will wait for the garbage collector to delete the pods 02/27/23 12:27:46.64
Feb 27 12:27:46.713: INFO: Deleting ReplicationController externalsvc took: 13.697955ms
Feb 27 12:27:46.814: INFO: Terminating ReplicationController externalsvc pods took: 101.102662ms
Feb 27 12:27:49.241: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 12:27:49.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3435" for this suite. 02/27/23 12:27:49.271
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":254,"skipped":4843,"failed":0}
------------------------------
• [SLOW TEST] [8.422 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:27:40.866
    Feb 27 12:27:40.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 12:27:40.868
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:40.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:40.989
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3435 02/27/23 12:27:41.028
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/27/23 12:27:41.074
    STEP: creating service externalsvc in namespace services-3435 02/27/23 12:27:41.075
    STEP: creating replication controller externalsvc in namespace services-3435 02/27/23 12:27:41.105
    I0227 12:27:41.119309      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3435, replica count: 2
    I0227 12:27:44.170737      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 02/27/23 12:27:44.18
    Feb 27 12:27:44.205: INFO: Creating new exec pod
    Feb 27 12:27:44.219: INFO: Waiting up to 5m0s for pod "execpodfjrbj" in namespace "services-3435" to be "running"
    Feb 27 12:27:44.229: INFO: Pod "execpodfjrbj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.411953ms
    Feb 27 12:27:46.238: INFO: Pod "execpodfjrbj": Phase="Running", Reason="", readiness=true. Elapsed: 2.019091959s
    Feb 27 12:27:46.238: INFO: Pod "execpodfjrbj" satisfied condition "running"
    Feb 27 12:27:46.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-3435 exec execpodfjrbj -- /bin/sh -x -c nslookup clusterip-service.services-3435.svc.cluster.local'
    Feb 27 12:27:46.640: INFO: stderr: "+ nslookup clusterip-service.services-3435.svc.cluster.local\n"
    Feb 27 12:27:46.640: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-3435.svc.cluster.local\tcanonical name = externalsvc.services-3435.svc.cluster.local.\nName:\texternalsvc.services-3435.svc.cluster.local\nAddress: 10.240.28.175\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3435, will wait for the garbage collector to delete the pods 02/27/23 12:27:46.64
    Feb 27 12:27:46.713: INFO: Deleting ReplicationController externalsvc took: 13.697955ms
    Feb 27 12:27:46.814: INFO: Terminating ReplicationController externalsvc pods took: 101.102662ms
    Feb 27 12:27:49.241: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 12:27:49.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3435" for this suite. 02/27/23 12:27:49.271
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:27:49.288
Feb 27 12:27:49.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 12:27:49.291
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:49.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:49.336
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-f98595d3-e4c5-4d2b-9bcc-4b9510c1319f 02/27/23 12:27:49.345
STEP: Creating a pod to test consume configMaps 02/27/23 12:27:49.36
Feb 27 12:27:49.381: INFO: Waiting up to 5m0s for pod "pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214" in namespace "configmap-8811" to be "Succeeded or Failed"
Feb 27 12:27:49.390: INFO: Pod "pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214": Phase="Pending", Reason="", readiness=false. Elapsed: 9.201658ms
Feb 27 12:27:51.403: INFO: Pod "pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021978449s
Feb 27 12:27:53.399: INFO: Pod "pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018499164s
STEP: Saw pod success 02/27/23 12:27:53.399
Feb 27 12:27:53.400: INFO: Pod "pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214" satisfied condition "Succeeded or Failed"
Feb 27 12:27:53.407: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 12:27:53.423
Feb 27 12:27:53.450: INFO: Waiting for pod pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214 to disappear
Feb 27 12:27:53.457: INFO: Pod pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 12:27:53.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8811" for this suite. 02/27/23 12:27:53.468
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":255,"skipped":4845,"failed":0}
------------------------------
• [4.192 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:27:49.288
    Feb 27 12:27:49.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 12:27:49.291
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:49.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:49.336
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-f98595d3-e4c5-4d2b-9bcc-4b9510c1319f 02/27/23 12:27:49.345
    STEP: Creating a pod to test consume configMaps 02/27/23 12:27:49.36
    Feb 27 12:27:49.381: INFO: Waiting up to 5m0s for pod "pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214" in namespace "configmap-8811" to be "Succeeded or Failed"
    Feb 27 12:27:49.390: INFO: Pod "pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214": Phase="Pending", Reason="", readiness=false. Elapsed: 9.201658ms
    Feb 27 12:27:51.403: INFO: Pod "pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021978449s
    Feb 27 12:27:53.399: INFO: Pod "pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018499164s
    STEP: Saw pod success 02/27/23 12:27:53.399
    Feb 27 12:27:53.400: INFO: Pod "pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214" satisfied condition "Succeeded or Failed"
    Feb 27 12:27:53.407: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 12:27:53.423
    Feb 27 12:27:53.450: INFO: Waiting for pod pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214 to disappear
    Feb 27 12:27:53.457: INFO: Pod pod-configmaps-c5c5712f-c008-40fa-807e-8856e5d82214 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 12:27:53.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8811" for this suite. 02/27/23 12:27:53.468
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:27:53.484
Feb 27 12:27:53.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename svcaccounts 02/27/23 12:27:53.485
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:53.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:53.53
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Feb 27 12:27:53.546: INFO: Got root ca configmap in namespace "svcaccounts-5836"
Feb 27 12:27:53.561: INFO: Deleted root ca configmap in namespace "svcaccounts-5836"
STEP: waiting for a new root ca configmap created 02/27/23 12:27:54.062
Feb 27 12:27:54.071: INFO: Recreated root ca configmap in namespace "svcaccounts-5836"
Feb 27 12:27:54.079: INFO: Updated root ca configmap in namespace "svcaccounts-5836"
STEP: waiting for the root ca configmap reconciled 02/27/23 12:27:54.58
Feb 27 12:27:54.588: INFO: Reconciled root ca configmap in namespace "svcaccounts-5836"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 27 12:27:54.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5836" for this suite. 02/27/23 12:27:54.6
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":256,"skipped":4849,"failed":0}
------------------------------
• [1.130 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:27:53.484
    Feb 27 12:27:53.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 12:27:53.485
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:53.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:53.53
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Feb 27 12:27:53.546: INFO: Got root ca configmap in namespace "svcaccounts-5836"
    Feb 27 12:27:53.561: INFO: Deleted root ca configmap in namespace "svcaccounts-5836"
    STEP: waiting for a new root ca configmap created 02/27/23 12:27:54.062
    Feb 27 12:27:54.071: INFO: Recreated root ca configmap in namespace "svcaccounts-5836"
    Feb 27 12:27:54.079: INFO: Updated root ca configmap in namespace "svcaccounts-5836"
    STEP: waiting for the root ca configmap reconciled 02/27/23 12:27:54.58
    Feb 27 12:27:54.588: INFO: Reconciled root ca configmap in namespace "svcaccounts-5836"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 27 12:27:54.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5836" for this suite. 02/27/23 12:27:54.6
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:27:54.62
Feb 27 12:27:54.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 12:27:54.622
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:54.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:54.661
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-46/configmap-test-0fa29cf3-0699-4eba-800a-409f19a2ecad 02/27/23 12:27:54.671
STEP: Creating a pod to test consume configMaps 02/27/23 12:27:54.688
Feb 27 12:27:54.707: INFO: Waiting up to 5m0s for pod "pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1" in namespace "configmap-46" to be "Succeeded or Failed"
Feb 27 12:27:54.720: INFO: Pod "pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.37442ms
Feb 27 12:27:56.748: INFO: Pod "pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040560675s
Feb 27 12:27:58.728: INFO: Pod "pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02131257s
STEP: Saw pod success 02/27/23 12:27:58.729
Feb 27 12:27:58.729: INFO: Pod "pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1" satisfied condition "Succeeded or Failed"
Feb 27 12:27:58.737: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1 container env-test: <nil>
STEP: delete the pod 02/27/23 12:27:58.779
Feb 27 12:27:58.799: INFO: Waiting for pod pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1 to disappear
Feb 27 12:27:58.810: INFO: Pod pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 12:27:58.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-46" for this suite. 02/27/23 12:27:58.829
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":257,"skipped":4849,"failed":0}
------------------------------
• [4.229 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:27:54.62
    Feb 27 12:27:54.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 12:27:54.622
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:54.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:54.661
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-46/configmap-test-0fa29cf3-0699-4eba-800a-409f19a2ecad 02/27/23 12:27:54.671
    STEP: Creating a pod to test consume configMaps 02/27/23 12:27:54.688
    Feb 27 12:27:54.707: INFO: Waiting up to 5m0s for pod "pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1" in namespace "configmap-46" to be "Succeeded or Failed"
    Feb 27 12:27:54.720: INFO: Pod "pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.37442ms
    Feb 27 12:27:56.748: INFO: Pod "pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040560675s
    Feb 27 12:27:58.728: INFO: Pod "pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02131257s
    STEP: Saw pod success 02/27/23 12:27:58.729
    Feb 27 12:27:58.729: INFO: Pod "pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1" satisfied condition "Succeeded or Failed"
    Feb 27 12:27:58.737: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1 container env-test: <nil>
    STEP: delete the pod 02/27/23 12:27:58.779
    Feb 27 12:27:58.799: INFO: Waiting for pod pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1 to disappear
    Feb 27 12:27:58.810: INFO: Pod pod-configmaps-56298253-93e4-473b-8266-ae98b47f07f1 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 12:27:58.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-46" for this suite. 02/27/23 12:27:58.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:27:58.859
Feb 27 12:27:58.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename sched-preemption 02/27/23 12:27:58.86
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:58.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:58.917
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 27 12:27:58.971: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 12:28:59.083: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 02/27/23 12:28:59.092
Feb 27 12:28:59.137: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 27 12:28:59.148: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 27 12:28:59.184: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 27 12:28:59.208: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 27 12:28:59.243: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 27 12:28:59.259: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/27/23 12:28:59.26
Feb 27 12:28:59.260: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7809" to be "running"
Feb 27 12:28:59.270: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.6587ms
Feb 27 12:29:01.279: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.018832323s
Feb 27 12:29:01.279: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 27 12:29:01.279: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7809" to be "running"
Feb 27 12:29:01.286: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.218732ms
Feb 27 12:29:03.316: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.037251314s
Feb 27 12:29:03.316: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 12:29:03.316: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7809" to be "running"
Feb 27 12:29:03.340: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 23.938558ms
Feb 27 12:29:03.351: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 12:29:03.351: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7809" to be "running"
Feb 27 12:29:03.366: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.921822ms
Feb 27 12:29:03.366: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 12:29:03.366: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7809" to be "running"
Feb 27 12:29:03.386: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.231522ms
Feb 27 12:29:03.386: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 12:29:03.386: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7809" to be "running"
Feb 27 12:29:03.395: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.555439ms
Feb 27 12:29:03.395: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 02/27/23 12:29:03.395
Feb 27 12:29:03.413: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Feb 27 12:29:03.448: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 35.29651ms
Feb 27 12:29:05.469: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056363692s
Feb 27 12:29:07.461: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.048197906s
Feb 27 12:29:07.464: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 27 12:29:07.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7809" for this suite. 02/27/23 12:29:07.581
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":258,"skipped":4860,"failed":0}
------------------------------
• [SLOW TEST] [68.879 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:27:58.859
    Feb 27 12:27:58.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename sched-preemption 02/27/23 12:27:58.86
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:27:58.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:27:58.917
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 27 12:27:58.971: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 12:28:59.083: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 02/27/23 12:28:59.092
    Feb 27 12:28:59.137: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 27 12:28:59.148: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 27 12:28:59.184: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 27 12:28:59.208: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 27 12:28:59.243: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 27 12:28:59.259: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/27/23 12:28:59.26
    Feb 27 12:28:59.260: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7809" to be "running"
    Feb 27 12:28:59.270: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.6587ms
    Feb 27 12:29:01.279: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.018832323s
    Feb 27 12:29:01.279: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 27 12:29:01.279: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7809" to be "running"
    Feb 27 12:29:01.286: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.218732ms
    Feb 27 12:29:03.316: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.037251314s
    Feb 27 12:29:03.316: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 12:29:03.316: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7809" to be "running"
    Feb 27 12:29:03.340: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 23.938558ms
    Feb 27 12:29:03.351: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 12:29:03.351: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7809" to be "running"
    Feb 27 12:29:03.366: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.921822ms
    Feb 27 12:29:03.366: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 12:29:03.366: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7809" to be "running"
    Feb 27 12:29:03.386: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.231522ms
    Feb 27 12:29:03.386: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 12:29:03.386: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7809" to be "running"
    Feb 27 12:29:03.395: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.555439ms
    Feb 27 12:29:03.395: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 02/27/23 12:29:03.395
    Feb 27 12:29:03.413: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Feb 27 12:29:03.448: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 35.29651ms
    Feb 27 12:29:05.469: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056363692s
    Feb 27 12:29:07.461: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.048197906s
    Feb 27 12:29:07.464: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 12:29:07.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7809" for this suite. 02/27/23 12:29:07.581
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:29:07.741
Feb 27 12:29:07.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 12:29:07.743
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:07.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:07.807
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-24419720-e4d8-4dd6-9c04-e33609895a71 02/27/23 12:29:07.819
STEP: Creating a pod to test consume secrets 02/27/23 12:29:07.831
Feb 27 12:29:07.846: INFO: Waiting up to 5m0s for pod "pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc" in namespace "secrets-3882" to be "Succeeded or Failed"
Feb 27 12:29:07.853: INFO: Pod "pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.219452ms
Feb 27 12:29:09.869: INFO: Pod "pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02257349s
Feb 27 12:29:11.867: INFO: Pod "pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020572051s
STEP: Saw pod success 02/27/23 12:29:11.867
Feb 27 12:29:11.867: INFO: Pod "pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc" satisfied condition "Succeeded or Failed"
Feb 27 12:29:11.877: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 12:29:11.9
Feb 27 12:29:11.921: INFO: Waiting for pod pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc to disappear
Feb 27 12:29:11.941: INFO: Pod pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 27 12:29:11.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3882" for this suite. 02/27/23 12:29:11.956
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":259,"skipped":4862,"failed":0}
------------------------------
• [4.234 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:29:07.741
    Feb 27 12:29:07.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 12:29:07.743
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:07.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:07.807
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-24419720-e4d8-4dd6-9c04-e33609895a71 02/27/23 12:29:07.819
    STEP: Creating a pod to test consume secrets 02/27/23 12:29:07.831
    Feb 27 12:29:07.846: INFO: Waiting up to 5m0s for pod "pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc" in namespace "secrets-3882" to be "Succeeded or Failed"
    Feb 27 12:29:07.853: INFO: Pod "pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.219452ms
    Feb 27 12:29:09.869: INFO: Pod "pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02257349s
    Feb 27 12:29:11.867: INFO: Pod "pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020572051s
    STEP: Saw pod success 02/27/23 12:29:11.867
    Feb 27 12:29:11.867: INFO: Pod "pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc" satisfied condition "Succeeded or Failed"
    Feb 27 12:29:11.877: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 12:29:11.9
    Feb 27 12:29:11.921: INFO: Waiting for pod pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc to disappear
    Feb 27 12:29:11.941: INFO: Pod pod-secrets-0e5f0477-9afa-4b62-80e4-b49df57588cc no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 12:29:11.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3882" for this suite. 02/27/23 12:29:11.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:29:11.977
Feb 27 12:29:11.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename statefulset 02/27/23 12:29:11.979
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:12.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:12.045
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7251 02/27/23 12:29:12.056
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-7251 02/27/23 12:29:12.069
Feb 27 12:29:12.093: INFO: Found 0 stateful pods, waiting for 1
Feb 27 12:29:22.111: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 02/27/23 12:29:22.129
STEP: updating a scale subresource 02/27/23 12:29:22.137
STEP: verifying the statefulset Spec.Replicas was modified 02/27/23 12:29:22.152
STEP: Patch a scale subresource 02/27/23 12:29:22.167
STEP: verifying the statefulset Spec.Replicas was modified 02/27/23 12:29:22.178
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 27 12:29:22.190: INFO: Deleting all statefulset in ns statefulset-7251
Feb 27 12:29:22.198: INFO: Scaling statefulset ss to 0
Feb 27 12:29:32.285: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 12:29:32.296: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 27 12:29:32.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7251" for this suite. 02/27/23 12:29:32.339
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":260,"skipped":4869,"failed":0}
------------------------------
• [SLOW TEST] [20.376 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:29:11.977
    Feb 27 12:29:11.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename statefulset 02/27/23 12:29:11.979
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:12.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:12.045
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7251 02/27/23 12:29:12.056
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-7251 02/27/23 12:29:12.069
    Feb 27 12:29:12.093: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 12:29:22.111: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 02/27/23 12:29:22.129
    STEP: updating a scale subresource 02/27/23 12:29:22.137
    STEP: verifying the statefulset Spec.Replicas was modified 02/27/23 12:29:22.152
    STEP: Patch a scale subresource 02/27/23 12:29:22.167
    STEP: verifying the statefulset Spec.Replicas was modified 02/27/23 12:29:22.178
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 27 12:29:22.190: INFO: Deleting all statefulset in ns statefulset-7251
    Feb 27 12:29:22.198: INFO: Scaling statefulset ss to 0
    Feb 27 12:29:32.285: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 12:29:32.296: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 27 12:29:32.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7251" for this suite. 02/27/23 12:29:32.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:29:32.362
Feb 27 12:29:32.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 12:29:32.364
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:32.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:32.438
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 02/27/23 12:29:32.449
Feb 27 12:29:32.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311" in namespace "downward-api-2428" to be "Succeeded or Failed"
Feb 27 12:29:32.706: INFO: Pod "downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311": Phase="Pending", Reason="", readiness=false. Elapsed: 12.720572ms
Feb 27 12:29:34.715: INFO: Pod "downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311": Phase="Running", Reason="", readiness=true. Elapsed: 2.022339906s
Feb 27 12:29:36.714: INFO: Pod "downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311": Phase="Running", Reason="", readiness=false. Elapsed: 4.020980576s
Feb 27 12:29:38.716: INFO: Pod "downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023414945s
STEP: Saw pod success 02/27/23 12:29:38.717
Feb 27 12:29:38.718: INFO: Pod "downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311" satisfied condition "Succeeded or Failed"
Feb 27 12:29:38.729: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311 container client-container: <nil>
STEP: delete the pod 02/27/23 12:29:38.744
Feb 27 12:29:38.764: INFO: Waiting for pod downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311 to disappear
Feb 27 12:29:38.771: INFO: Pod downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 27 12:29:38.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2428" for this suite. 02/27/23 12:29:38.782
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":261,"skipped":4917,"failed":0}
------------------------------
• [SLOW TEST] [6.434 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:29:32.362
    Feb 27 12:29:32.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 12:29:32.364
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:32.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:32.438
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 02/27/23 12:29:32.449
    Feb 27 12:29:32.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311" in namespace "downward-api-2428" to be "Succeeded or Failed"
    Feb 27 12:29:32.706: INFO: Pod "downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311": Phase="Pending", Reason="", readiness=false. Elapsed: 12.720572ms
    Feb 27 12:29:34.715: INFO: Pod "downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311": Phase="Running", Reason="", readiness=true. Elapsed: 2.022339906s
    Feb 27 12:29:36.714: INFO: Pod "downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311": Phase="Running", Reason="", readiness=false. Elapsed: 4.020980576s
    Feb 27 12:29:38.716: INFO: Pod "downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023414945s
    STEP: Saw pod success 02/27/23 12:29:38.717
    Feb 27 12:29:38.718: INFO: Pod "downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311" satisfied condition "Succeeded or Failed"
    Feb 27 12:29:38.729: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311 container client-container: <nil>
    STEP: delete the pod 02/27/23 12:29:38.744
    Feb 27 12:29:38.764: INFO: Waiting for pod downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311 to disappear
    Feb 27 12:29:38.771: INFO: Pod downwardapi-volume-6b44e5c8-0dde-488c-b386-3d1031918311 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 27 12:29:38.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2428" for this suite. 02/27/23 12:29:38.782
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:29:38.797
Feb 27 12:29:38.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename tables 02/27/23 12:29:38.798
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:38.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:38.844
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Feb 27 12:29:38.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3068" for this suite. 02/27/23 12:29:38.88
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":262,"skipped":4917,"failed":0}
------------------------------
• [0.099 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:29:38.797
    Feb 27 12:29:38.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename tables 02/27/23 12:29:38.798
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:38.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:38.844
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Feb 27 12:29:38.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-3068" for this suite. 02/27/23 12:29:38.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:29:38.902
Feb 27 12:29:38.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:29:38.903
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:38.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:38.945
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-89387a2f-acf3-4e64-9737-d39ddaf4c833 02/27/23 12:29:38.958
STEP: Creating a pod to test consume configMaps 02/27/23 12:29:38.969
Feb 27 12:29:38.996: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190" in namespace "projected-7266" to be "Succeeded or Failed"
Feb 27 12:29:39.003: INFO: Pod "pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190": Phase="Pending", Reason="", readiness=false. Elapsed: 7.415351ms
Feb 27 12:29:41.018: INFO: Pod "pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190": Phase="Running", Reason="", readiness=true. Elapsed: 2.021872899s
Feb 27 12:29:43.015: INFO: Pod "pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190": Phase="Running", Reason="", readiness=false. Elapsed: 4.018887783s
Feb 27 12:29:45.012: INFO: Pod "pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01612984s
STEP: Saw pod success 02/27/23 12:29:45.012
Feb 27 12:29:45.012: INFO: Pod "pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190" satisfied condition "Succeeded or Failed"
Feb 27 12:29:45.024: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 12:29:45.042
Feb 27 12:29:45.070: INFO: Waiting for pod pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190 to disappear
Feb 27 12:29:45.078: INFO: Pod pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 27 12:29:45.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7266" for this suite. 02/27/23 12:29:45.098
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":263,"skipped":4947,"failed":0}
------------------------------
• [SLOW TEST] [6.215 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:29:38.902
    Feb 27 12:29:38.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:29:38.903
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:38.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:38.945
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-89387a2f-acf3-4e64-9737-d39ddaf4c833 02/27/23 12:29:38.958
    STEP: Creating a pod to test consume configMaps 02/27/23 12:29:38.969
    Feb 27 12:29:38.996: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190" in namespace "projected-7266" to be "Succeeded or Failed"
    Feb 27 12:29:39.003: INFO: Pod "pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190": Phase="Pending", Reason="", readiness=false. Elapsed: 7.415351ms
    Feb 27 12:29:41.018: INFO: Pod "pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190": Phase="Running", Reason="", readiness=true. Elapsed: 2.021872899s
    Feb 27 12:29:43.015: INFO: Pod "pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190": Phase="Running", Reason="", readiness=false. Elapsed: 4.018887783s
    Feb 27 12:29:45.012: INFO: Pod "pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01612984s
    STEP: Saw pod success 02/27/23 12:29:45.012
    Feb 27 12:29:45.012: INFO: Pod "pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190" satisfied condition "Succeeded or Failed"
    Feb 27 12:29:45.024: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 12:29:45.042
    Feb 27 12:29:45.070: INFO: Waiting for pod pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190 to disappear
    Feb 27 12:29:45.078: INFO: Pod pod-projected-configmaps-7b9f90c9-fc78-426b-8d71-8941cf1c8190 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 27 12:29:45.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7266" for this suite. 02/27/23 12:29:45.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:29:45.122
Feb 27 12:29:45.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 12:29:45.124
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:45.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:45.17
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 02/27/23 12:29:45.182
Feb 27 12:29:45.199: INFO: Waiting up to 5m0s for pod "pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba" in namespace "emptydir-4383" to be "Succeeded or Failed"
Feb 27 12:29:45.216: INFO: Pod "pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba": Phase="Pending", Reason="", readiness=false. Elapsed: 16.661889ms
Feb 27 12:29:47.225: INFO: Pod "pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025387682s
Feb 27 12:29:49.224: INFO: Pod "pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024638008s
STEP: Saw pod success 02/27/23 12:29:49.224
Feb 27 12:29:49.224: INFO: Pod "pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba" satisfied condition "Succeeded or Failed"
Feb 27 12:29:49.231: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba container test-container: <nil>
STEP: delete the pod 02/27/23 12:29:49.248
Feb 27 12:29:49.265: INFO: Waiting for pod pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba to disappear
Feb 27 12:29:49.271: INFO: Pod pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 12:29:49.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4383" for this suite. 02/27/23 12:29:49.28
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":264,"skipped":4961,"failed":0}
------------------------------
• [4.170 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:29:45.122
    Feb 27 12:29:45.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 12:29:45.124
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:45.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:45.17
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 02/27/23 12:29:45.182
    Feb 27 12:29:45.199: INFO: Waiting up to 5m0s for pod "pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba" in namespace "emptydir-4383" to be "Succeeded or Failed"
    Feb 27 12:29:45.216: INFO: Pod "pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba": Phase="Pending", Reason="", readiness=false. Elapsed: 16.661889ms
    Feb 27 12:29:47.225: INFO: Pod "pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025387682s
    Feb 27 12:29:49.224: INFO: Pod "pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024638008s
    STEP: Saw pod success 02/27/23 12:29:49.224
    Feb 27 12:29:49.224: INFO: Pod "pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba" satisfied condition "Succeeded or Failed"
    Feb 27 12:29:49.231: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba container test-container: <nil>
    STEP: delete the pod 02/27/23 12:29:49.248
    Feb 27 12:29:49.265: INFO: Waiting for pod pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba to disappear
    Feb 27 12:29:49.271: INFO: Pod pod-d2b20475-a4fc-42e4-99ed-b8ea05d2aaba no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 12:29:49.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4383" for this suite. 02/27/23 12:29:49.28
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:29:49.293
Feb 27 12:29:49.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename replicaset 02/27/23 12:29:49.301
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:49.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:49.348
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 02/27/23 12:29:49.366
STEP: Verify that the required pods have come up. 02/27/23 12:29:49.378
Feb 27 12:29:49.385: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 27 12:29:54.398: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 12:29:54.398
STEP: Getting /status 02/27/23 12:29:54.398
Feb 27 12:29:54.408: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 02/27/23 12:29:54.408
Feb 27 12:29:54.524: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 02/27/23 12:29:54.524
Feb 27 12:29:54.531: INFO: Observed &ReplicaSet event: ADDED
Feb 27 12:29:54.531: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 12:29:54.532: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 12:29:54.533: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 12:29:54.533: INFO: Found replicaset test-rs in namespace replicaset-1791 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 27 12:29:54.533: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 02/27/23 12:29:54.533
Feb 27 12:29:54.533: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 27 12:29:54.548: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 02/27/23 12:29:54.548
Feb 27 12:29:54.553: INFO: Observed &ReplicaSet event: ADDED
Feb 27 12:29:54.553: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 12:29:54.553: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 12:29:54.554: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 12:29:54.554: INFO: Observed replicaset test-rs in namespace replicaset-1791 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 12:29:54.554: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 12:29:54.554: INFO: Found replicaset test-rs in namespace replicaset-1791 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Feb 27 12:29:54.554: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 27 12:29:54.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1791" for this suite. 02/27/23 12:29:54.572
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":265,"skipped":4961,"failed":0}
------------------------------
• [SLOW TEST] [5.291 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:29:49.293
    Feb 27 12:29:49.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename replicaset 02/27/23 12:29:49.301
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:49.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:49.348
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 02/27/23 12:29:49.366
    STEP: Verify that the required pods have come up. 02/27/23 12:29:49.378
    Feb 27 12:29:49.385: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 27 12:29:54.398: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 12:29:54.398
    STEP: Getting /status 02/27/23 12:29:54.398
    Feb 27 12:29:54.408: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 02/27/23 12:29:54.408
    Feb 27 12:29:54.524: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 02/27/23 12:29:54.524
    Feb 27 12:29:54.531: INFO: Observed &ReplicaSet event: ADDED
    Feb 27 12:29:54.531: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 12:29:54.532: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 12:29:54.533: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 12:29:54.533: INFO: Found replicaset test-rs in namespace replicaset-1791 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 27 12:29:54.533: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 02/27/23 12:29:54.533
    Feb 27 12:29:54.533: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 27 12:29:54.548: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 02/27/23 12:29:54.548
    Feb 27 12:29:54.553: INFO: Observed &ReplicaSet event: ADDED
    Feb 27 12:29:54.553: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 12:29:54.553: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 12:29:54.554: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 12:29:54.554: INFO: Observed replicaset test-rs in namespace replicaset-1791 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 12:29:54.554: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 12:29:54.554: INFO: Found replicaset test-rs in namespace replicaset-1791 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Feb 27 12:29:54.554: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 27 12:29:54.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1791" for this suite. 02/27/23 12:29:54.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:29:54.59
Feb 27 12:29:54.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 12:29:54.592
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:54.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:54.651
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-c0295bfa-fe26-4e32-abef-539aae1bedc0 02/27/23 12:29:54.677
STEP: Creating the pod 02/27/23 12:29:54.701
Feb 27 12:29:54.718: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b10ef28-4991-448e-80df-d1d8b81e4ed6" in namespace "configmap-4166" to be "running"
Feb 27 12:29:54.728: INFO: Pod "pod-configmaps-7b10ef28-4991-448e-80df-d1d8b81e4ed6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.47625ms
Feb 27 12:29:56.739: INFO: Pod "pod-configmaps-7b10ef28-4991-448e-80df-d1d8b81e4ed6": Phase="Running", Reason="", readiness=false. Elapsed: 2.021122815s
Feb 27 12:29:56.739: INFO: Pod "pod-configmaps-7b10ef28-4991-448e-80df-d1d8b81e4ed6" satisfied condition "running"
STEP: Waiting for pod with text data 02/27/23 12:29:56.74
STEP: Waiting for pod with binary data 02/27/23 12:29:56.76
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 12:29:56.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4166" for this suite. 02/27/23 12:29:56.809
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":266,"skipped":4970,"failed":0}
------------------------------
• [2.240 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:29:54.59
    Feb 27 12:29:54.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 12:29:54.592
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:54.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:54.651
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-c0295bfa-fe26-4e32-abef-539aae1bedc0 02/27/23 12:29:54.677
    STEP: Creating the pod 02/27/23 12:29:54.701
    Feb 27 12:29:54.718: INFO: Waiting up to 5m0s for pod "pod-configmaps-7b10ef28-4991-448e-80df-d1d8b81e4ed6" in namespace "configmap-4166" to be "running"
    Feb 27 12:29:54.728: INFO: Pod "pod-configmaps-7b10ef28-4991-448e-80df-d1d8b81e4ed6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.47625ms
    Feb 27 12:29:56.739: INFO: Pod "pod-configmaps-7b10ef28-4991-448e-80df-d1d8b81e4ed6": Phase="Running", Reason="", readiness=false. Elapsed: 2.021122815s
    Feb 27 12:29:56.739: INFO: Pod "pod-configmaps-7b10ef28-4991-448e-80df-d1d8b81e4ed6" satisfied condition "running"
    STEP: Waiting for pod with text data 02/27/23 12:29:56.74
    STEP: Waiting for pod with binary data 02/27/23 12:29:56.76
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 12:29:56.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4166" for this suite. 02/27/23 12:29:56.809
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:29:56.836
Feb 27 12:29:56.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 12:29:56.838
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:56.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:56.888
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Feb 27 12:29:56.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 create -f -'
Feb 27 12:29:57.244: INFO: stderr: ""
Feb 27 12:29:57.244: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb 27 12:29:57.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 create -f -'
Feb 27 12:29:57.580: INFO: stderr: ""
Feb 27 12:29:57.580: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/27/23 12:29:57.58
Feb 27 12:29:58.590: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 12:29:58.591: INFO: Found 1 / 1
Feb 27 12:29:58.591: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 27 12:29:58.600: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 12:29:58.600: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 27 12:29:58.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 describe pod agnhost-primary-7hpmt'
Feb 27 12:29:58.782: INFO: stderr: ""
Feb 27 12:29:58.782: INFO: stdout: "Name:             agnhost-primary-7hpmt\nNamespace:        kubectl-4336\nPriority:         0\nService Account:  default\nNode:             ip-172-31-15-17.eu-central-1.compute.internal/172.31.15.17\nStart Time:       Mon, 27 Feb 2023 12:29:57 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: bcac29452cb153f95101ae3026e5cfbd57a3d939071169529438902c740b14c2\n                  cni.projectcalico.org/podIP: 172.25.2.225/32\n                  cni.projectcalico.org/podIPs: 172.25.2.225/32\nStatus:           Running\nIP:               172.25.2.225\nIPs:\n  IP:           172.25.2.225\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://7ad45aa3986151bb29a06885e42e09cfbdf3f3d2b96611a294fe8ebdce67e3a4\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 27 Feb 2023 12:29:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-frb4b (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-frb4b:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-4336/agnhost-primary-7hpmt to ip-172-31-15-17.eu-central-1.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Feb 27 12:29:58.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 describe rc agnhost-primary'
Feb 27 12:29:58.942: INFO: stderr: ""
Feb 27 12:29:58.943: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4336\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-7hpmt\n"
Feb 27 12:29:58.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 describe service agnhost-primary'
Feb 27 12:29:59.115: INFO: stderr: ""
Feb 27 12:29:59.115: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4336\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.240.20.22\nIPs:               10.240.20.22\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.2.225:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 27 12:29:59.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 describe node ip-172-31-11-159.eu-central-1.compute.internal'
Feb 27 12:29:59.329: INFO: stderr: ""
Feb 27 12:29:59.329: INFO: stdout: "Name:               ip-172-31-11-159.eu-central-1.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3a.small\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-central-1\n                    failure-domain.beta.kubernetes.io/zone=eu-central-1c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-11-159.eu-central-1.compute.internal\n                    kubernetes.io/os=linux\n                    machine=md-w7c7vmqk6c-vr2jcdknhz\n                    machine-controller/owned-by=d3a1e589-7cb2-495a-875b-823bc9b26947\n                    node.kubernetes.io/instance-type=t3a.small\n                    system/cluster=w7c7vmqk6c\n                    system/project=fhx5k9f6gw\n                    topology.ebs.csi.aws.com/zone=eu-central-1c\n                    topology.kubernetes.io/region=eu-central-1\n                    topology.kubernetes.io/zone=eu-central-1c\n                    v1.machine-controller.kubermatic.io/operating-system=ubuntu\n                    x-kubernetes.io/distribution=ubuntu\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 172.31.11.159\n                    cluster.k8s.io/machine: kube-system/w7c7vmqk6c-worker-465hbf-84fffc6854-zvx4k\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-03201c789aad7ac56\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"9e:38:b9:9d:86:c0\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.11.159\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.11.159/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 27 Feb 2023 09:22:06 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-11-159.eu-central-1.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 27 Feb 2023 12:29:58 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 27 Feb 2023 09:23:15 +0000   Mon, 27 Feb 2023 09:23:15 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 27 Feb 2023 12:29:11 +0000   Mon, 27 Feb 2023 09:22:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 27 Feb 2023 12:29:11 +0000   Mon, 27 Feb 2023 09:22:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 27 Feb 2023 12:29:11 +0000   Mon, 27 Feb 2023 09:22:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 27 Feb 2023 12:29:11 +0000   Mon, 27 Feb 2023 09:23:07 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.11.159\n  ExternalIP:   3.68.105.41\n  InternalDNS:  ip-172-31-11-159.eu-central-1.compute.internal\n  Hostname:     ip-172-31-11-159.eu-central-1.compute.internal\n  ExternalDNS:  ec2-3-68-105-41.eu-central-1.compute.amazonaws.com\nCapacity:\n  cpu:                  2\n  ephemeral-storage:    25215872Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               1990360Ki\n  pods:                 110\nAllocatable:\n  cpu:                  1600m\n  ephemeral-storage:    21091463949\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               1478360Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 ec205bf5731b1735ab982e709e4aa827\n  System UUID:                ec205bf5-731b-1735-ab98-2e709e4aa827\n  Boot ID:                    9e2eeeb5-d9ce-4f0e-8d8c-1ae6ccbc1e36\n  Kernel Version:             5.15.0-1030-aws\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.18\n  Kubelet Version:            v1.25.6\n  Kube-Proxy Version:         v1.25.6\nPodCIDR:                      172.25.1.0/24\nPodCIDRs:                     172.25.1.0/24\nProviderID:                   aws:///eu-central-1c/i-03201c789aad7ac56\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-4q9m8                                                250m (15%)    0 (0%)      0 (0%)           0 (0%)         3h7m\n  kube-system                 coredns-bf8668b4f-nbc9z                                    50m (3%)      100m (6%)   32Mi (2%)        64Mi (4%)      61m\n  kube-system                 ebs-csi-node-b6z5h                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h7m\n  kube-system                 envoy-agent-2wwht                                          50m (3%)      1 (62%)     32Mi (2%)        64Mi (4%)      3h7m\n  kube-system                 konnectivity-agent-76c848fdd6-fr8fm                        10m (0%)      2 (125%)    10Mi (0%)        100Mi (6%)     61m\n  kube-system                 kube-proxy-cz9zt                                           75m (4%)      250m (15%)  50Mi (3%)        250Mi (17%)    3h7m\n  kube-system                 metrics-server-5f7c5d4b9-qh6s5                             100m (6%)     1 (62%)     200Mi (13%)      512Mi (35%)    61m\n  kube-system                 node-local-dns-r6r4k                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h7m\n  kube-system                 user-ssh-keys-agent-rk5t9                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h7m\n  kubernetes-dashboard        dashboard-metrics-scraper-85f6dd84d5-c7wsq                 50m (3%)      100m (6%)   32Mi (2%)        64Mi (4%)      81m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         70m\n  sonobuoy                    sonobuoy-e2e-job-18131847dfcd49d5                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         70m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-52382428c7df4090-rlhml    0 (0%)        0 (0%)      0 (0%)           0 (0%)         70m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests     Limits\n  --------             --------     ------\n  cpu                  585m (36%)   4450m (278%)\n  memory               356Mi (24%)  1054Mi (73%)\n  ephemeral-storage    0 (0%)       0 (0%)\n  hugepages-1Gi        0 (0%)       0 (0%)\n  hugepages-2Mi        0 (0%)       0 (0%)\n  example.com/fakecpu  0            0\nEvents:                <none>\n"
Feb 27 12:29:59.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 describe namespace kubectl-4336'
Feb 27 12:29:59.454: INFO: stderr: ""
Feb 27 12:29:59.454: INFO: stdout: "Name:         kubectl-4336\nLabels:       e2e-framework=kubectl\n              e2e-run=b7a633fd-337d-4a15-aac2-3c71ea508a0e\n              kubernetes.io/metadata.name=kubectl-4336\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 12:29:59.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4336" for this suite. 02/27/23 12:29:59.464
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":267,"skipped":4971,"failed":0}
------------------------------
• [2.641 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:29:56.836
    Feb 27 12:29:56.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 12:29:56.838
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:56.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:56.888
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Feb 27 12:29:56.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 create -f -'
    Feb 27 12:29:57.244: INFO: stderr: ""
    Feb 27 12:29:57.244: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Feb 27 12:29:57.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 create -f -'
    Feb 27 12:29:57.580: INFO: stderr: ""
    Feb 27 12:29:57.580: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/27/23 12:29:57.58
    Feb 27 12:29:58.590: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 12:29:58.591: INFO: Found 1 / 1
    Feb 27 12:29:58.591: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 27 12:29:58.600: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 12:29:58.600: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 27 12:29:58.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 describe pod agnhost-primary-7hpmt'
    Feb 27 12:29:58.782: INFO: stderr: ""
    Feb 27 12:29:58.782: INFO: stdout: "Name:             agnhost-primary-7hpmt\nNamespace:        kubectl-4336\nPriority:         0\nService Account:  default\nNode:             ip-172-31-15-17.eu-central-1.compute.internal/172.31.15.17\nStart Time:       Mon, 27 Feb 2023 12:29:57 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: bcac29452cb153f95101ae3026e5cfbd57a3d939071169529438902c740b14c2\n                  cni.projectcalico.org/podIP: 172.25.2.225/32\n                  cni.projectcalico.org/podIPs: 172.25.2.225/32\nStatus:           Running\nIP:               172.25.2.225\nIPs:\n  IP:           172.25.2.225\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://7ad45aa3986151bb29a06885e42e09cfbdf3f3d2b96611a294fe8ebdce67e3a4\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 27 Feb 2023 12:29:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-frb4b (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-frb4b:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-4336/agnhost-primary-7hpmt to ip-172-31-15-17.eu-central-1.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
    Feb 27 12:29:58.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 describe rc agnhost-primary'
    Feb 27 12:29:58.942: INFO: stderr: ""
    Feb 27 12:29:58.943: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4336\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-7hpmt\n"
    Feb 27 12:29:58.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 describe service agnhost-primary'
    Feb 27 12:29:59.115: INFO: stderr: ""
    Feb 27 12:29:59.115: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4336\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.240.20.22\nIPs:               10.240.20.22\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.2.225:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Feb 27 12:29:59.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 describe node ip-172-31-11-159.eu-central-1.compute.internal'
    Feb 27 12:29:59.329: INFO: stderr: ""
    Feb 27 12:29:59.329: INFO: stdout: "Name:               ip-172-31-11-159.eu-central-1.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3a.small\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-central-1\n                    failure-domain.beta.kubernetes.io/zone=eu-central-1c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-11-159.eu-central-1.compute.internal\n                    kubernetes.io/os=linux\n                    machine=md-w7c7vmqk6c-vr2jcdknhz\n                    machine-controller/owned-by=d3a1e589-7cb2-495a-875b-823bc9b26947\n                    node.kubernetes.io/instance-type=t3a.small\n                    system/cluster=w7c7vmqk6c\n                    system/project=fhx5k9f6gw\n                    topology.ebs.csi.aws.com/zone=eu-central-1c\n                    topology.kubernetes.io/region=eu-central-1\n                    topology.kubernetes.io/zone=eu-central-1c\n                    v1.machine-controller.kubermatic.io/operating-system=ubuntu\n                    x-kubernetes.io/distribution=ubuntu\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 172.31.11.159\n                    cluster.k8s.io/machine: kube-system/w7c7vmqk6c-worker-465hbf-84fffc6854-zvx4k\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-03201c789aad7ac56\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"9e:38:b9:9d:86:c0\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.11.159\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.11.159/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 27 Feb 2023 09:22:06 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-11-159.eu-central-1.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 27 Feb 2023 12:29:58 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 27 Feb 2023 09:23:15 +0000   Mon, 27 Feb 2023 09:23:15 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 27 Feb 2023 12:29:11 +0000   Mon, 27 Feb 2023 09:22:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 27 Feb 2023 12:29:11 +0000   Mon, 27 Feb 2023 09:22:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 27 Feb 2023 12:29:11 +0000   Mon, 27 Feb 2023 09:22:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 27 Feb 2023 12:29:11 +0000   Mon, 27 Feb 2023 09:23:07 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.11.159\n  ExternalIP:   3.68.105.41\n  InternalDNS:  ip-172-31-11-159.eu-central-1.compute.internal\n  Hostname:     ip-172-31-11-159.eu-central-1.compute.internal\n  ExternalDNS:  ec2-3-68-105-41.eu-central-1.compute.amazonaws.com\nCapacity:\n  cpu:                  2\n  ephemeral-storage:    25215872Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               1990360Ki\n  pods:                 110\nAllocatable:\n  cpu:                  1600m\n  ephemeral-storage:    21091463949\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               1478360Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 ec205bf5731b1735ab982e709e4aa827\n  System UUID:                ec205bf5-731b-1735-ab98-2e709e4aa827\n  Boot ID:                    9e2eeeb5-d9ce-4f0e-8d8c-1ae6ccbc1e36\n  Kernel Version:             5.15.0-1030-aws\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.18\n  Kubelet Version:            v1.25.6\n  Kube-Proxy Version:         v1.25.6\nPodCIDR:                      172.25.1.0/24\nPodCIDRs:                     172.25.1.0/24\nProviderID:                   aws:///eu-central-1c/i-03201c789aad7ac56\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-4q9m8                                                250m (15%)    0 (0%)      0 (0%)           0 (0%)         3h7m\n  kube-system                 coredns-bf8668b4f-nbc9z                                    50m (3%)      100m (6%)   32Mi (2%)        64Mi (4%)      61m\n  kube-system                 ebs-csi-node-b6z5h                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h7m\n  kube-system                 envoy-agent-2wwht                                          50m (3%)      1 (62%)     32Mi (2%)        64Mi (4%)      3h7m\n  kube-system                 konnectivity-agent-76c848fdd6-fr8fm                        10m (0%)      2 (125%)    10Mi (0%)        100Mi (6%)     61m\n  kube-system                 kube-proxy-cz9zt                                           75m (4%)      250m (15%)  50Mi (3%)        250Mi (17%)    3h7m\n  kube-system                 metrics-server-5f7c5d4b9-qh6s5                             100m (6%)     1 (62%)     200Mi (13%)      512Mi (35%)    61m\n  kube-system                 node-local-dns-r6r4k                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h7m\n  kube-system                 user-ssh-keys-agent-rk5t9                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h7m\n  kubernetes-dashboard        dashboard-metrics-scraper-85f6dd84d5-c7wsq                 50m (3%)      100m (6%)   32Mi (2%)        64Mi (4%)      81m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         70m\n  sonobuoy                    sonobuoy-e2e-job-18131847dfcd49d5                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         70m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-52382428c7df4090-rlhml    0 (0%)        0 (0%)      0 (0%)           0 (0%)         70m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests     Limits\n  --------             --------     ------\n  cpu                  585m (36%)   4450m (278%)\n  memory               356Mi (24%)  1054Mi (73%)\n  ephemeral-storage    0 (0%)       0 (0%)\n  hugepages-1Gi        0 (0%)       0 (0%)\n  hugepages-2Mi        0 (0%)       0 (0%)\n  example.com/fakecpu  0            0\nEvents:                <none>\n"
    Feb 27 12:29:59.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4336 describe namespace kubectl-4336'
    Feb 27 12:29:59.454: INFO: stderr: ""
    Feb 27 12:29:59.454: INFO: stdout: "Name:         kubectl-4336\nLabels:       e2e-framework=kubectl\n              e2e-run=b7a633fd-337d-4a15-aac2-3c71ea508a0e\n              kubernetes.io/metadata.name=kubectl-4336\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 12:29:59.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4336" for this suite. 02/27/23 12:29:59.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:29:59.489
Feb 27 12:29:59.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 12:29:59.49
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:59.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:59.533
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 02/27/23 12:29:59.541
Feb 27 12:29:59.558: INFO: Waiting up to 5m0s for pod "downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c" in namespace "downward-api-8164" to be "Succeeded or Failed"
Feb 27 12:29:59.566: INFO: Pod "downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.293611ms
Feb 27 12:30:01.583: INFO: Pod "downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024802062s
Feb 27 12:30:03.581: INFO: Pod "downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022353814s
STEP: Saw pod success 02/27/23 12:30:03.581
Feb 27 12:30:03.581: INFO: Pod "downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c" satisfied condition "Succeeded or Failed"
Feb 27 12:30:03.592: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c container dapi-container: <nil>
STEP: delete the pod 02/27/23 12:30:03.613
Feb 27 12:30:03.638: INFO: Waiting for pod downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c to disappear
Feb 27 12:30:03.648: INFO: Pod downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 27 12:30:03.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8164" for this suite. 02/27/23 12:30:03.662
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":268,"skipped":4985,"failed":0}
------------------------------
• [4.187 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:29:59.489
    Feb 27 12:29:59.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 12:29:59.49
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:29:59.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:29:59.533
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 02/27/23 12:29:59.541
    Feb 27 12:29:59.558: INFO: Waiting up to 5m0s for pod "downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c" in namespace "downward-api-8164" to be "Succeeded or Failed"
    Feb 27 12:29:59.566: INFO: Pod "downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.293611ms
    Feb 27 12:30:01.583: INFO: Pod "downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024802062s
    Feb 27 12:30:03.581: INFO: Pod "downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022353814s
    STEP: Saw pod success 02/27/23 12:30:03.581
    Feb 27 12:30:03.581: INFO: Pod "downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c" satisfied condition "Succeeded or Failed"
    Feb 27 12:30:03.592: INFO: Trying to get logs from node ip-172-31-11-159.eu-central-1.compute.internal pod downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c container dapi-container: <nil>
    STEP: delete the pod 02/27/23 12:30:03.613
    Feb 27 12:30:03.638: INFO: Waiting for pod downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c to disappear
    Feb 27 12:30:03.648: INFO: Pod downward-api-05cd6572-59a2-49da-ae98-c9e57ec1923c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 27 12:30:03.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8164" for this suite. 02/27/23 12:30:03.662
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:30:03.68
Feb 27 12:30:03.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename podtemplate 02/27/23 12:30:03.682
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:30:03.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:30:03.729
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 02/27/23 12:30:03.741
STEP: Replace a pod template 02/27/23 12:30:03.754
Feb 27 12:30:03.784: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb 27 12:30:03.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9245" for this suite. 02/27/23 12:30:03.812
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":269,"skipped":4986,"failed":0}
------------------------------
• [0.142 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:30:03.68
    Feb 27 12:30:03.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename podtemplate 02/27/23 12:30:03.682
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:30:03.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:30:03.729
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 02/27/23 12:30:03.741
    STEP: Replace a pod template 02/27/23 12:30:03.754
    Feb 27 12:30:03.784: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb 27 12:30:03.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9245" for this suite. 02/27/23 12:30:03.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:30:03.828
Feb 27 12:30:03.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-probe 02/27/23 12:30:03.829
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:30:03.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:30:03.88
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061 in namespace container-probe-3118 02/27/23 12:30:03.892
Feb 27 12:30:03.915: INFO: Waiting up to 5m0s for pod "liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061" in namespace "container-probe-3118" to be "not pending"
Feb 27 12:30:03.932: INFO: Pod "liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061": Phase="Pending", Reason="", readiness=false. Elapsed: 16.944012ms
Feb 27 12:30:05.942: INFO: Pod "liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061": Phase="Running", Reason="", readiness=true. Elapsed: 2.026459802s
Feb 27 12:30:05.942: INFO: Pod "liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061" satisfied condition "not pending"
Feb 27 12:30:05.942: INFO: Started pod liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061 in namespace container-probe-3118
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 12:30:05.942
Feb 27 12:30:05.949: INFO: Initial restart count of pod liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061 is 0
STEP: deleting the pod 02/27/23 12:34:07.51
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 27 12:34:07.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3118" for this suite. 02/27/23 12:34:07.548
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":270,"skipped":5034,"failed":0}
------------------------------
• [SLOW TEST] [243.734 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:30:03.828
    Feb 27 12:30:03.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-probe 02/27/23 12:30:03.829
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:30:03.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:30:03.88
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061 in namespace container-probe-3118 02/27/23 12:30:03.892
    Feb 27 12:30:03.915: INFO: Waiting up to 5m0s for pod "liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061" in namespace "container-probe-3118" to be "not pending"
    Feb 27 12:30:03.932: INFO: Pod "liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061": Phase="Pending", Reason="", readiness=false. Elapsed: 16.944012ms
    Feb 27 12:30:05.942: INFO: Pod "liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061": Phase="Running", Reason="", readiness=true. Elapsed: 2.026459802s
    Feb 27 12:30:05.942: INFO: Pod "liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061" satisfied condition "not pending"
    Feb 27 12:30:05.942: INFO: Started pod liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061 in namespace container-probe-3118
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 12:30:05.942
    Feb 27 12:30:05.949: INFO: Initial restart count of pod liveness-76cf0ee4-5e5d-41ce-84d2-efefccbff061 is 0
    STEP: deleting the pod 02/27/23 12:34:07.51
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 27 12:34:07.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3118" for this suite. 02/27/23 12:34:07.548
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:34:07.564
Feb 27 12:34:07.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 12:34:07.566
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:07.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:07.628
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 12:34:07.658
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 12:34:08.23
STEP: Deploying the webhook pod 02/27/23 12:34:08.243
STEP: Wait for the deployment to be ready 02/27/23 12:34:08.266
Feb 27 12:34:08.295: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 12:34:10.357
STEP: Verifying the service has paired with the endpoint 02/27/23 12:34:10.403
Feb 27 12:34:11.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/27/23 12:34:11.411
STEP: create a configmap that should be updated by the webhook 02/27/23 12:34:11.465
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:34:11.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7519" for this suite. 02/27/23 12:34:11.525
STEP: Destroying namespace "webhook-7519-markers" for this suite. 02/27/23 12:34:11.542
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":271,"skipped":5035,"failed":0}
------------------------------
• [4.173 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:34:07.564
    Feb 27 12:34:07.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 12:34:07.566
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:07.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:07.628
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 12:34:07.658
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 12:34:08.23
    STEP: Deploying the webhook pod 02/27/23 12:34:08.243
    STEP: Wait for the deployment to be ready 02/27/23 12:34:08.266
    Feb 27 12:34:08.295: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 12:34:10.357
    STEP: Verifying the service has paired with the endpoint 02/27/23 12:34:10.403
    Feb 27 12:34:11.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/27/23 12:34:11.411
    STEP: create a configmap that should be updated by the webhook 02/27/23 12:34:11.465
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:34:11.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7519" for this suite. 02/27/23 12:34:11.525
    STEP: Destroying namespace "webhook-7519-markers" for this suite. 02/27/23 12:34:11.542
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:34:11.741
Feb 27 12:34:11.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename gc 02/27/23 12:34:11.742
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:11.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:11.844
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 02/27/23 12:34:11.857
STEP: Wait for the Deployment to create new ReplicaSet 02/27/23 12:34:11.867
STEP: delete the deployment 02/27/23 12:34:11.877
STEP: wait for all rs to be garbage collected 02/27/23 12:34:11.895
STEP: expected 0 rs, got 1 rs 02/27/23 12:34:11.912
STEP: expected 0 pods, got 2 pods 02/27/23 12:34:11.943
STEP: Gathering metrics 02/27/23 12:34:12.464
W0227 12:34:12.485207      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 27 12:34:12.485: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 27 12:34:12.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8581" for this suite. 02/27/23 12:34:12.494
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":272,"skipped":5052,"failed":0}
------------------------------
• [0.767 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:34:11.741
    Feb 27 12:34:11.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename gc 02/27/23 12:34:11.742
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:11.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:11.844
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 02/27/23 12:34:11.857
    STEP: Wait for the Deployment to create new ReplicaSet 02/27/23 12:34:11.867
    STEP: delete the deployment 02/27/23 12:34:11.877
    STEP: wait for all rs to be garbage collected 02/27/23 12:34:11.895
    STEP: expected 0 rs, got 1 rs 02/27/23 12:34:11.912
    STEP: expected 0 pods, got 2 pods 02/27/23 12:34:11.943
    STEP: Gathering metrics 02/27/23 12:34:12.464
    W0227 12:34:12.485207      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 27 12:34:12.485: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 27 12:34:12.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8581" for this suite. 02/27/23 12:34:12.494
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:34:12.514
Feb 27 12:34:12.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename server-version 02/27/23 12:34:12.516
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:12.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:12.564
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 02/27/23 12:34:12.574
STEP: Confirm major version 02/27/23 12:34:12.578
Feb 27 12:34:12.578: INFO: Major version: 1
STEP: Confirm minor version 02/27/23 12:34:12.578
Feb 27 12:34:12.578: INFO: cleanMinorVersion: 25
Feb 27 12:34:12.578: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Feb 27 12:34:12.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2509" for this suite. 02/27/23 12:34:12.591
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":273,"skipped":5056,"failed":0}
------------------------------
• [0.088 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:34:12.514
    Feb 27 12:34:12.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename server-version 02/27/23 12:34:12.516
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:12.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:12.564
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 02/27/23 12:34:12.574
    STEP: Confirm major version 02/27/23 12:34:12.578
    Feb 27 12:34:12.578: INFO: Major version: 1
    STEP: Confirm minor version 02/27/23 12:34:12.578
    Feb 27 12:34:12.578: INFO: cleanMinorVersion: 25
    Feb 27 12:34:12.578: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Feb 27 12:34:12.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-2509" for this suite. 02/27/23 12:34:12.591
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:34:12.605
Feb 27 12:34:12.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 12:34:12.606
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:12.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:12.653
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 02/27/23 12:34:12.665
Feb 27 12:34:12.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-6892 cluster-info'
Feb 27 12:34:12.842: INFO: stderr: ""
Feb 27 12:34:12.842: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 12:34:12.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6892" for this suite. 02/27/23 12:34:12.855
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":274,"skipped":5060,"failed":0}
------------------------------
• [0.268 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:34:12.605
    Feb 27 12:34:12.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 12:34:12.606
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:12.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:12.653
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 02/27/23 12:34:12.665
    Feb 27 12:34:12.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-6892 cluster-info'
    Feb 27 12:34:12.842: INFO: stderr: ""
    Feb 27 12:34:12.842: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 12:34:12.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6892" for this suite. 02/27/23 12:34:12.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:34:12.899
Feb 27 12:34:12.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:34:12.9
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:12.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:12.967
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-14f53253-ce18-45da-8524-28d1cbd1eab7 02/27/23 12:34:12.991
STEP: Creating secret with name s-test-opt-upd-46f2cc56-aede-4037-abdd-6c00ec226014 02/27/23 12:34:13.002
STEP: Creating the pod 02/27/23 12:34:13.012
Feb 27 12:34:13.031: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4c7a2a2d-8026-4f3f-a168-66217633cf4c" in namespace "projected-9672" to be "running and ready"
Feb 27 12:34:13.042: INFO: Pod "pod-projected-secrets-4c7a2a2d-8026-4f3f-a168-66217633cf4c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.994185ms
Feb 27 12:34:13.042: INFO: The phase of Pod pod-projected-secrets-4c7a2a2d-8026-4f3f-a168-66217633cf4c is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:34:15.060: INFO: Pod "pod-projected-secrets-4c7a2a2d-8026-4f3f-a168-66217633cf4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.02927025s
Feb 27 12:34:15.060: INFO: The phase of Pod pod-projected-secrets-4c7a2a2d-8026-4f3f-a168-66217633cf4c is Running (Ready = true)
Feb 27 12:34:15.060: INFO: Pod "pod-projected-secrets-4c7a2a2d-8026-4f3f-a168-66217633cf4c" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-14f53253-ce18-45da-8524-28d1cbd1eab7 02/27/23 12:34:15.125
STEP: Updating secret s-test-opt-upd-46f2cc56-aede-4037-abdd-6c00ec226014 02/27/23 12:34:15.14
STEP: Creating secret with name s-test-opt-create-583e309c-356f-4b3a-ab1c-d6e0f26afd6a 02/27/23 12:34:15.148
STEP: waiting to observe update in volume 02/27/23 12:34:15.162
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 27 12:34:17.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9672" for this suite. 02/27/23 12:34:17.241
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":275,"skipped":5071,"failed":0}
------------------------------
• [4.358 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:34:12.899
    Feb 27 12:34:12.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:34:12.9
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:12.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:12.967
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-14f53253-ce18-45da-8524-28d1cbd1eab7 02/27/23 12:34:12.991
    STEP: Creating secret with name s-test-opt-upd-46f2cc56-aede-4037-abdd-6c00ec226014 02/27/23 12:34:13.002
    STEP: Creating the pod 02/27/23 12:34:13.012
    Feb 27 12:34:13.031: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4c7a2a2d-8026-4f3f-a168-66217633cf4c" in namespace "projected-9672" to be "running and ready"
    Feb 27 12:34:13.042: INFO: Pod "pod-projected-secrets-4c7a2a2d-8026-4f3f-a168-66217633cf4c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.994185ms
    Feb 27 12:34:13.042: INFO: The phase of Pod pod-projected-secrets-4c7a2a2d-8026-4f3f-a168-66217633cf4c is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:34:15.060: INFO: Pod "pod-projected-secrets-4c7a2a2d-8026-4f3f-a168-66217633cf4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.02927025s
    Feb 27 12:34:15.060: INFO: The phase of Pod pod-projected-secrets-4c7a2a2d-8026-4f3f-a168-66217633cf4c is Running (Ready = true)
    Feb 27 12:34:15.060: INFO: Pod "pod-projected-secrets-4c7a2a2d-8026-4f3f-a168-66217633cf4c" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-14f53253-ce18-45da-8524-28d1cbd1eab7 02/27/23 12:34:15.125
    STEP: Updating secret s-test-opt-upd-46f2cc56-aede-4037-abdd-6c00ec226014 02/27/23 12:34:15.14
    STEP: Creating secret with name s-test-opt-create-583e309c-356f-4b3a-ab1c-d6e0f26afd6a 02/27/23 12:34:15.148
    STEP: waiting to observe update in volume 02/27/23 12:34:15.162
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 27 12:34:17.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9672" for this suite. 02/27/23 12:34:17.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:34:17.264
Feb 27 12:34:17.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 12:34:17.265
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:17.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:17.308
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Feb 27 12:34:17.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 12:34:21.021
Feb 27 12:34:21.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-9786 --namespace=crd-publish-openapi-9786 create -f -'
Feb 27 12:34:21.995: INFO: stderr: ""
Feb 27 12:34:21.995: INFO: stdout: "e2e-test-crd-publish-openapi-5539-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 27 12:34:21.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-9786 --namespace=crd-publish-openapi-9786 delete e2e-test-crd-publish-openapi-5539-crds test-cr'
Feb 27 12:34:22.113: INFO: stderr: ""
Feb 27 12:34:22.114: INFO: stdout: "e2e-test-crd-publish-openapi-5539-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 27 12:34:22.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-9786 --namespace=crd-publish-openapi-9786 apply -f -'
Feb 27 12:34:22.516: INFO: stderr: ""
Feb 27 12:34:22.516: INFO: stdout: "e2e-test-crd-publish-openapi-5539-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 27 12:34:22.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-9786 --namespace=crd-publish-openapi-9786 delete e2e-test-crd-publish-openapi-5539-crds test-cr'
Feb 27 12:34:22.740: INFO: stderr: ""
Feb 27 12:34:22.740: INFO: stdout: "e2e-test-crd-publish-openapi-5539-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 02/27/23 12:34:22.74
Feb 27 12:34:22.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-9786 explain e2e-test-crd-publish-openapi-5539-crds'
Feb 27 12:34:23.098: INFO: stderr: ""
Feb 27 12:34:23.098: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5539-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:34:26.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9786" for this suite. 02/27/23 12:34:26.567
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":276,"skipped":5092,"failed":0}
------------------------------
• [SLOW TEST] [9.329 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:34:17.264
    Feb 27 12:34:17.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 12:34:17.265
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:17.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:17.308
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Feb 27 12:34:17.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 12:34:21.021
    Feb 27 12:34:21.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-9786 --namespace=crd-publish-openapi-9786 create -f -'
    Feb 27 12:34:21.995: INFO: stderr: ""
    Feb 27 12:34:21.995: INFO: stdout: "e2e-test-crd-publish-openapi-5539-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 27 12:34:21.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-9786 --namespace=crd-publish-openapi-9786 delete e2e-test-crd-publish-openapi-5539-crds test-cr'
    Feb 27 12:34:22.113: INFO: stderr: ""
    Feb 27 12:34:22.114: INFO: stdout: "e2e-test-crd-publish-openapi-5539-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Feb 27 12:34:22.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-9786 --namespace=crd-publish-openapi-9786 apply -f -'
    Feb 27 12:34:22.516: INFO: stderr: ""
    Feb 27 12:34:22.516: INFO: stdout: "e2e-test-crd-publish-openapi-5539-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 27 12:34:22.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-9786 --namespace=crd-publish-openapi-9786 delete e2e-test-crd-publish-openapi-5539-crds test-cr'
    Feb 27 12:34:22.740: INFO: stderr: ""
    Feb 27 12:34:22.740: INFO: stdout: "e2e-test-crd-publish-openapi-5539-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 02/27/23 12:34:22.74
    Feb 27 12:34:22.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=crd-publish-openapi-9786 explain e2e-test-crd-publish-openapi-5539-crds'
    Feb 27 12:34:23.098: INFO: stderr: ""
    Feb 27 12:34:23.098: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5539-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:34:26.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9786" for this suite. 02/27/23 12:34:26.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:34:26.597
Feb 27 12:34:26.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename deployment 02/27/23 12:34:26.599
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:26.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:26.635
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Feb 27 12:34:26.642: INFO: Creating deployment "webserver-deployment"
Feb 27 12:34:26.650: INFO: Waiting for observed generation 1
Feb 27 12:34:28.699: INFO: Waiting for all required pods to come up
Feb 27 12:34:28.729: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 02/27/23 12:34:28.729
Feb 27 12:34:28.729: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xb2nn" in namespace "deployment-536" to be "running"
Feb 27 12:34:28.730: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4dmz6" in namespace "deployment-536" to be "running"
Feb 27 12:34:28.730: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-l4bj7" in namespace "deployment-536" to be "running"
Feb 27 12:34:28.730: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-ndcfs" in namespace "deployment-536" to be "running"
Feb 27 12:34:28.730: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-r2h7h" in namespace "deployment-536" to be "running"
Feb 27 12:34:28.749: INFO: Pod "webserver-deployment-845c8977d9-l4bj7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.329549ms
Feb 27 12:34:28.752: INFO: Pod "webserver-deployment-845c8977d9-xb2nn": Phase="Pending", Reason="", readiness=false. Elapsed: 22.858403ms
Feb 27 12:34:28.753: INFO: Pod "webserver-deployment-845c8977d9-ndcfs": Phase="Pending", Reason="", readiness=false. Elapsed: 22.581152ms
Feb 27 12:34:28.753: INFO: Pod "webserver-deployment-845c8977d9-4dmz6": Phase="Pending", Reason="", readiness=false. Elapsed: 23.458987ms
Feb 27 12:34:28.753: INFO: Pod "webserver-deployment-845c8977d9-r2h7h": Phase="Pending", Reason="", readiness=false. Elapsed: 22.932455ms
Feb 27 12:34:30.757: INFO: Pod "webserver-deployment-845c8977d9-l4bj7": Phase="Running", Reason="", readiness=true. Elapsed: 2.027604244s
Feb 27 12:34:30.757: INFO: Pod "webserver-deployment-845c8977d9-l4bj7" satisfied condition "running"
Feb 27 12:34:30.761: INFO: Pod "webserver-deployment-845c8977d9-ndcfs": Phase="Running", Reason="", readiness=true. Elapsed: 2.031242778s
Feb 27 12:34:30.761: INFO: Pod "webserver-deployment-845c8977d9-ndcfs" satisfied condition "running"
Feb 27 12:34:30.762: INFO: Pod "webserver-deployment-845c8977d9-xb2nn": Phase="Running", Reason="", readiness=true. Elapsed: 2.032248982s
Feb 27 12:34:30.762: INFO: Pod "webserver-deployment-845c8977d9-xb2nn" satisfied condition "running"
Feb 27 12:34:30.762: INFO: Pod "webserver-deployment-845c8977d9-r2h7h": Phase="Running", Reason="", readiness=true. Elapsed: 2.031793111s
Feb 27 12:34:30.762: INFO: Pod "webserver-deployment-845c8977d9-r2h7h" satisfied condition "running"
Feb 27 12:34:30.762: INFO: Pod "webserver-deployment-845c8977d9-4dmz6": Phase="Running", Reason="", readiness=true. Elapsed: 2.032385414s
Feb 27 12:34:30.762: INFO: Pod "webserver-deployment-845c8977d9-4dmz6" satisfied condition "running"
Feb 27 12:34:30.762: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 27 12:34:30.780: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 27 12:34:30.804: INFO: Updating deployment webserver-deployment
Feb 27 12:34:30.804: INFO: Waiting for observed generation 2
Feb 27 12:34:32.828: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 27 12:34:32.837: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 27 12:34:32.846: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 27 12:34:32.879: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 27 12:34:32.879: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 27 12:34:32.890: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 27 12:34:32.908: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 27 12:34:32.908: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 27 12:34:32.941: INFO: Updating deployment webserver-deployment
Feb 27 12:34:32.941: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 27 12:34:32.963: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 27 12:34:35.007: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 12:34:35.053: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-536  9cc814ff-b10d-4465-94b9-5552f0b4ee59 93037 3 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c5d788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 12:34:32 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-02-27 12:34:33 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 27 12:34:35.128: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-536  2324d588-d44d-48d3-a0b3-63e90c4e2c25 93034 3 2023-02-27 12:34:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9cc814ff-b10d-4465-94b9-5552f0b4ee59 0xc004c5dc77 0xc004c5dc78}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9cc814ff-b10d-4465-94b9-5552f0b4ee59\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c5dd18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 12:34:35.129: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 27 12:34:35.129: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-536  937c01c5-ae8a-40d0-94f8-b0c3411c25ce 93022 3 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9cc814ff-b10d-4465-94b9-5552f0b4ee59 0xc004c5dd77 0xc004c5dd78}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9cc814ff-b10d-4465-94b9-5552f0b4ee59\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c5de48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 27 12:34:35.170: INFO: Pod "webserver-deployment-69b7448995-4lh2d" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-4lh2d webserver-deployment-69b7448995- deployment-536  adc39f25-04c7-4c2f-9f4b-d2967036da23 92930 0 2023-02-27 12:34:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:77a0eb6f3fac57638574490ba4656b165595d834e233672cf007c6f75a1329c4 cni.projectcalico.org/podIP:172.25.1.206/32 cni.projectcalico.org/podIPs:172.25.1.206/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9c857 0xc004c9c858}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2g42,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2g42,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.172: INFO: Pod "webserver-deployment-69b7448995-4vwt9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-4vwt9 webserver-deployment-69b7448995- deployment-536  2c8a6dcc-66d3-4e89-91cd-bd272f72c08b 93109 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5f44785ffbdf23826cc0ae22fe63a60c6c13ac78912c640359bb20f62eb1af3b cni.projectcalico.org/podIP:172.25.1.211/32 cni.projectcalico.org/podIPs:172.25.1.211/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9ca87 0xc004c9ca88}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxljq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxljq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.173: INFO: Pod "webserver-deployment-69b7448995-72qn9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-72qn9 webserver-deployment-69b7448995- deployment-536  61ad1aa9-f3bf-40dd-bdb1-8178cab4917d 93120 0 2023-02-27 12:34:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d82921fc9a3424e1be7c5aef458f1600acee800e9f9a5d662b6f485a4aad8fa3 cni.projectcalico.org/podIP:172.25.2.230/32 cni.projectcalico.org/podIPs:172.25.2.230/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9cd87 0xc004c9cd88}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lj8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lj8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.230,StartTime:2023-02-27 12:34:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.174: INFO: Pod "webserver-deployment-69b7448995-7xxt2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-7xxt2 webserver-deployment-69b7448995- deployment-536  8a38ccba-3894-457a-8337-ffc1b66767be 93122 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:75f12a75bc436574da3d5b9fd64ac12204a5e0c2ccf801e0b8f4636ada5a9384 cni.projectcalico.org/podIP:172.25.2.236/32 cni.projectcalico.org/podIPs:172.25.2.236/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9d0b7 0xc004c9d0b8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c99mg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c99mg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.174: INFO: Pod "webserver-deployment-69b7448995-9nb4c" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-9nb4c webserver-deployment-69b7448995- deployment-536  34890cfe-1a4d-4393-a87d-8cfb606064b9 93081 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:91aa412cf3b78388c3fe0687f3bccd5c63de814930be35f83e2c5a1eefe7680e cni.projectcalico.org/podIP:172.25.0.85/32 cni.projectcalico.org/podIPs:172.25.0.85/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9d3b7 0xc004c9d3b8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fzxzp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fzxzp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.175: INFO: Pod "webserver-deployment-69b7448995-blssq" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-blssq webserver-deployment-69b7448995- deployment-536  d6f24f50-bd02-41b5-8281-e9b914d1f7cd 93019 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9d657 0xc004c9d658}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bk2nw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bk2nw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.175: INFO: Pod "webserver-deployment-69b7448995-jhtjd" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-jhtjd webserver-deployment-69b7448995- deployment-536  850b60d5-8c69-425a-8a25-e8eb95d8d030 92925 0 2023-02-27 12:34:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:3b78f98f1106cba0e6f986593f53e95fbafe9b505891d67e9879b5be55627bda cni.projectcalico.org/podIP:172.25.2.231/32 cni.projectcalico.org/podIPs:172.25.2.231/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9d830 0xc004c9d831}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rvmbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rvmbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.176: INFO: Pod "webserver-deployment-69b7448995-kz4lj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kz4lj webserver-deployment-69b7448995- deployment-536  39a387e7-7274-433e-944f-baa0b5240818 93108 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:30bfc99671c00c8734033ca8a966b25b57dc5f50886bd5ba8c3bb88f61f19255 cni.projectcalico.org/podIP:172.25.0.87/32 cni.projectcalico.org/podIPs:172.25.0.87/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9daa7 0xc004c9daa8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lbbp9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lbbp9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.176: INFO: Pod "webserver-deployment-69b7448995-s622z" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-s622z webserver-deployment-69b7448995- deployment-536  a4e7236b-3b7f-4d8b-a537-72e24593f21e 93099 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:9581b01b4ce6822daf1db87ea4b09f2ba313bd3caa6a445cecd531aea913933e cni.projectcalico.org/podIP:172.25.2.234/32 cni.projectcalico.org/podIPs:172.25.2.234/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9dd17 0xc004c9dd18}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vqgv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vqgv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.177: INFO: Pod "webserver-deployment-69b7448995-swr22" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-swr22 webserver-deployment-69b7448995- deployment-536  c6e4f70b-9768-4d0f-8fa5-190e636f86fb 93111 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7124740808f8b8a2f20f7b037101c78caa6c55266e1ce88fcf5e3abf24b38045 cni.projectcalico.org/podIP:172.25.1.209/32 cni.projectcalico.org/podIPs:172.25.1.209/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9dfc7 0xc004c9dfc8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hcbdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hcbdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.177: INFO: Pod "webserver-deployment-69b7448995-sxdtf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-sxdtf webserver-deployment-69b7448995- deployment-536  001b1d25-6149-4436-8ca0-c93e0d71a3ab 93118 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5fe25904cf2580da0d27b1acf191f8b66f4394e7c4035381685f808fdb82aa5e cni.projectcalico.org/podIP:172.25.0.88/32 cni.projectcalico.org/podIPs:172.25.0.88/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004d04257 0xc004d04258}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ptx9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ptx9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.178: INFO: Pod "webserver-deployment-69b7448995-vw4f4" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-vw4f4 webserver-deployment-69b7448995- deployment-536  d2b6307f-75c6-4680-abab-f7bbbc7ef3b2 92921 0 2023-02-27 12:34:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d63bed429a3619cb96f580891cadacc3f74ad2c38156f14025872689dea40271 cni.projectcalico.org/podIP:172.25.1.205/32 cni.projectcalico.org/podIPs:172.25.1.205/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004d04477 0xc004d04478}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njq9m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njq9m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.178: INFO: Pod "webserver-deployment-69b7448995-w8c68" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-w8c68 webserver-deployment-69b7448995- deployment-536  7b46bdf7-76ba-4a0c-80da-ee01daf3123a 93119 0 2023-02-27 12:34:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:cf05526b5bccfe704c30cf4c7999b9f742cfc30cbd27097ed88ebd1a2f2ef997 cni.projectcalico.org/podIP:172.25.0.82/32 cni.projectcalico.org/podIPs:172.25.0.82/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004d047a7 0xc004d047a8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlvwq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlvwq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:172.25.0.82,StartTime:2023-02-27 12:34:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.179: INFO: Pod "webserver-deployment-845c8977d9-25vqn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-25vqn webserver-deployment-845c8977d9- deployment-536  e62adadc-3f91-4d74-a91f-0139d6c2e82e 92822 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3ee5bf568eec7a1e35debef9a13ec1d90af9cc4860aed5ac2c72450a551079a6 cni.projectcalico.org/podIP:172.25.2.228/32 cni.projectcalico.org/podIPs:172.25.2.228/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d04a20 0xc004d04a21}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbksg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbksg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.228,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bcdbbf81bdaff8440dbed2d3c73f9e5b07d78468562dfea01cce773af0fb8b67,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.206: INFO: Pod "webserver-deployment-845c8977d9-464mv" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-464mv webserver-deployment-845c8977d9- deployment-536  bae47572-efda-4de1-b2f8-26b9a4b49f2a 93025 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d04c17 0xc004d04c18}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gq9c2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gq9c2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.207: INFO: Pod "webserver-deployment-845c8977d9-4lhz2" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4lhz2 webserver-deployment-845c8977d9- deployment-536  e5560621-36b2-41f9-8571-cc358351d3a0 93093 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:ad2d939647981ec4bfee8bba593f09d68a0de91e4288908e14dfa26eaf00d132 cni.projectcalico.org/podIP:172.25.0.86/32 cni.projectcalico.org/podIPs:172.25.0.86/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d04df7 0xc004d04df8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmsk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmsk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.212: INFO: Pod "webserver-deployment-845c8977d9-4t4z8" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4t4z8 webserver-deployment-845c8977d9- deployment-536  12a4a611-5040-4fa9-8bf0-225afe8c4840 93121 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:676f55119001b4645c06c4b9cc6bc19cb03a02a33d571ab6300f8cddcf71774b cni.projectcalico.org/podIP:172.25.1.212/32 cni.projectcalico.org/podIPs:172.25.1.212/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d05057 0xc004d05058}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlbhw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlbhw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.235: INFO: Pod "webserver-deployment-845c8977d9-672g5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-672g5 webserver-deployment-845c8977d9- deployment-536  19572449-2a6e-48a3-b364-48075637d72f 93063 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:25459e0aa1bbe81c1f32d5a357173b5731da3ff36465cfb2637abc5168847f05 cni.projectcalico.org/podIP:172.25.1.207/32 cni.projectcalico.org/podIPs:172.25.1.207/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d05277 0xc004d05278}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xnmtl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xnmtl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.238: INFO: Pod "webserver-deployment-845c8977d9-6d9sf" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6d9sf webserver-deployment-845c8977d9- deployment-536  6c5c1b30-137c-4be5-a281-f2aa5134e330 92838 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8b1ffee10a5c86c9a0ea72ad08738ac458a83b860133517574064333cc00f1cd cni.projectcalico.org/podIP:172.25.1.202/32 cni.projectcalico.org/podIPs:172.25.1.202/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d054e7 0xc004d054e8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-564hx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-564hx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:172.25.1.202,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3079692681b8a76d2be8a7427d4c0cc66c3564698238d14c9921a2c2cec2a78b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.238: INFO: Pod "webserver-deployment-845c8977d9-chtwj" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-chtwj webserver-deployment-845c8977d9- deployment-536  4d64a0a1-fd5a-4e7b-b98a-fa2b0d78046d 92826 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d7f500e75ee63610fe4e7498107dfd00908f47fd9dbaa21688ade94e47d104f7 cni.projectcalico.org/podIP:172.25.2.227/32 cni.projectcalico.org/podIPs:172.25.2.227/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d05787 0xc004d05788}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lnfvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lnfvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.227,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://455ecbbe2fcabccbf88eb61ed908051bfe463998a73ceccff3704d0c2b0e33ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.239: INFO: Pod "webserver-deployment-845c8977d9-h9vnj" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-h9vnj webserver-deployment-845c8977d9- deployment-536  a2f241f3-33ea-4d8c-8712-c56d4e3e0ef7 93062 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:773ae256fdafa582443f60447247661fd45e29a3713bd1629785b36efeaeb051 cni.projectcalico.org/podIP:172.25.0.83/32 cni.projectcalico.org/podIPs:172.25.0.83/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d05a07 0xc004d05a08}] [] [{calico Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xr8nv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xr8nv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.239: INFO: Pod "webserver-deployment-845c8977d9-j2l8x" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-j2l8x webserver-deployment-845c8977d9- deployment-536  6d767b6b-cce1-4ffa-91cf-0e4a4cfbc57f 93033 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d05c47 0xc004d05c48}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bt8xf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bt8xf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.252: INFO: Pod "webserver-deployment-845c8977d9-jz44z" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jz44z webserver-deployment-845c8977d9- deployment-536  0bf13248-fd24-44bc-89e0-bc499027df27 92835 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:547c7e19739c26b18d9cd3e75dfacc2e1aa4238627069886bbeeff1c6971c5b1 cni.projectcalico.org/podIP:172.25.1.201/32 cni.projectcalico.org/podIPs:172.25.1.201/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d05eb7 0xc004d05eb8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dvtq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dvtq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:172.25.1.201,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4d7b7475d2999b70a560b0e916746bac9ab45721b17b1f11877b2569fc7d980c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.265: INFO: Pod "webserver-deployment-845c8977d9-l4bj7" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-l4bj7 webserver-deployment-845c8977d9- deployment-536  aa4831d4-6fc8-4cb1-ab04-f33f37720b14 92851 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9581cc6afaea60d694acc59242a2fca3557464b1806b75f62d9d468ab57ca773 cni.projectcalico.org/podIP:172.25.0.79/32 cni.projectcalico.org/podIPs:172.25.0.79/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d36137 0xc004d36138}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vk94,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vk94,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:172.25.0.79,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ec39dd33771ba3c4f06964fd6d984df6058f6e22ccfa835ba2848687b36aa6f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.266: INFO: Pod "webserver-deployment-845c8977d9-pwtbs" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pwtbs webserver-deployment-845c8977d9- deployment-536  70cdc1ef-1448-48e2-a6c8-75f2056497fc 93067 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:dcc0ef39386f75c589d9ff4b3371a98c266e4f66e0b8d0fd078899f437c29510 cni.projectcalico.org/podIP:172.25.2.232/32 cni.projectcalico.org/podIPs:172.25.2.232/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d363c0 0xc004d363c1}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sgkn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sgkn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.266: INFO: Pod "webserver-deployment-845c8977d9-r2h7h" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-r2h7h webserver-deployment-845c8977d9- deployment-536  c52d4216-7abe-476c-b664-c9dc07646447 92845 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:ae090828c75f17ca12b201bf39772ab8eaaf648cd8d2efe5dd4243fe5e533285 cni.projectcalico.org/podIP:172.25.0.80/32 cni.projectcalico.org/podIPs:172.25.0.80/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d36637 0xc004d36638}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42tvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42tvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:172.25.0.80,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a201a79fb271a2c65d6294b619ccad32b50cc034a172ab258312c5d9cea49cf7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.268: INFO: Pod "webserver-deployment-845c8977d9-s787h" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-s787h webserver-deployment-845c8977d9- deployment-536  0e5e7720-6396-41d2-8800-b49643425e54 93107 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:088717d9741d8d528db02a5cdffa87a80f407442672d41dad5df4be534ac08f8 cni.projectcalico.org/podIP:172.25.2.235/32 cni.projectcalico.org/podIPs:172.25.2.235/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d36880 0xc004d36881}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvw7n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvw7n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.271: INFO: Pod "webserver-deployment-845c8977d9-spkpn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-spkpn webserver-deployment-845c8977d9- deployment-536  61591b1d-3741-4285-a21f-f035cbdd296c 92828 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:49f359aca02ef940156cfd595f2f104a81e9380846cb5fb3c6aa1b3b6af254f1 cni.projectcalico.org/podIP:172.25.2.229/32 cni.projectcalico.org/podIPs:172.25.2.229/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d36a77 0xc004d36a78}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfsqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfsqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.229,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://29de44d2387e6d4d915e036fed6ad82498886ddac23f30ce7e5cbddbb0d078a1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.274: INFO: Pod "webserver-deployment-845c8977d9-svwc7" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-svwc7 webserver-deployment-845c8977d9- deployment-536  21cc744a-402b-41df-8788-93ce0e5c4aad 93084 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1666ac45564b71905fae6cc8ac0c7716febe20a212f26bab8539fcb4147091a6 cni.projectcalico.org/podIP:172.25.2.233/32 cni.projectcalico.org/podIPs:172.25.2.233/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d36d27 0xc004d36d28}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67x5t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67x5t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.276: INFO: Pod "webserver-deployment-845c8977d9-tp7x7" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-tp7x7 webserver-deployment-845c8977d9- deployment-536  f4e60582-07fc-4cac-8ebc-c2e5c58c07ff 93075 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b10fecf7270c35939d742dfad816de87d663f0452ae843bf19019bf96d73a71d cni.projectcalico.org/podIP:172.25.1.208/32 cni.projectcalico.org/podIPs:172.25.1.208/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d36fe7 0xc004d36fe8}] [] [{calico Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgpmv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgpmv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.308: INFO: Pod "webserver-deployment-845c8977d9-tsffm" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-tsffm webserver-deployment-845c8977d9- deployment-536  1360242c-f450-4469-b414-547f67a22ab3 93101 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:60a21bab3faac33271cfb5978db7320d2d96a28e9a044818d14fb7624adf0329 cni.projectcalico.org/podIP:172.25.1.210/32 cni.projectcalico.org/podIPs:172.25.1.210/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d371f0 0xc004d371f1}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qcwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qcwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.309: INFO: Pod "webserver-deployment-845c8977d9-xb2nn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xb2nn webserver-deployment-845c8977d9- deployment-536  3259ea1b-78ad-4eeb-954d-9a29ef514e58 92848 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a3fdb812e5e42c624dc20791bcbc30ee16916d1d229612491420f81d1e30b35d cni.projectcalico.org/podIP:172.25.0.81/32 cni.projectcalico.org/podIPs:172.25.0.81/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d37497 0xc004d37498}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xgmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xgmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:172.25.0.81,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d7ef7410e5cbaf35f3ea5cce94e5eb08138f10ceec80e869241cdc8f7eb9a7d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 12:34:35.309: INFO: Pod "webserver-deployment-845c8977d9-z68nw" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-z68nw webserver-deployment-845c8977d9- deployment-536  94bbe581-16c0-499a-850a-38ebf28a8a29 93077 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7abb2259251b4668aa4c58d4ebbc5eac006fae757d0b5d434a7752540e04408a cni.projectcalico.org/podIP:172.25.0.84/32 cni.projectcalico.org/podIPs:172.25.0.84/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d37720 0xc004d37721}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8z2n6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8z2n6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 27 12:34:35.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-536" for this suite. 02/27/23 12:34:35.417
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":277,"skipped":5108,"failed":0}
------------------------------
• [SLOW TEST] [8.836 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:34:26.597
    Feb 27 12:34:26.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename deployment 02/27/23 12:34:26.599
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:26.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:26.635
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Feb 27 12:34:26.642: INFO: Creating deployment "webserver-deployment"
    Feb 27 12:34:26.650: INFO: Waiting for observed generation 1
    Feb 27 12:34:28.699: INFO: Waiting for all required pods to come up
    Feb 27 12:34:28.729: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 02/27/23 12:34:28.729
    Feb 27 12:34:28.729: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xb2nn" in namespace "deployment-536" to be "running"
    Feb 27 12:34:28.730: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4dmz6" in namespace "deployment-536" to be "running"
    Feb 27 12:34:28.730: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-l4bj7" in namespace "deployment-536" to be "running"
    Feb 27 12:34:28.730: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-ndcfs" in namespace "deployment-536" to be "running"
    Feb 27 12:34:28.730: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-r2h7h" in namespace "deployment-536" to be "running"
    Feb 27 12:34:28.749: INFO: Pod "webserver-deployment-845c8977d9-l4bj7": Phase="Pending", Reason="", readiness=false. Elapsed: 19.329549ms
    Feb 27 12:34:28.752: INFO: Pod "webserver-deployment-845c8977d9-xb2nn": Phase="Pending", Reason="", readiness=false. Elapsed: 22.858403ms
    Feb 27 12:34:28.753: INFO: Pod "webserver-deployment-845c8977d9-ndcfs": Phase="Pending", Reason="", readiness=false. Elapsed: 22.581152ms
    Feb 27 12:34:28.753: INFO: Pod "webserver-deployment-845c8977d9-4dmz6": Phase="Pending", Reason="", readiness=false. Elapsed: 23.458987ms
    Feb 27 12:34:28.753: INFO: Pod "webserver-deployment-845c8977d9-r2h7h": Phase="Pending", Reason="", readiness=false. Elapsed: 22.932455ms
    Feb 27 12:34:30.757: INFO: Pod "webserver-deployment-845c8977d9-l4bj7": Phase="Running", Reason="", readiness=true. Elapsed: 2.027604244s
    Feb 27 12:34:30.757: INFO: Pod "webserver-deployment-845c8977d9-l4bj7" satisfied condition "running"
    Feb 27 12:34:30.761: INFO: Pod "webserver-deployment-845c8977d9-ndcfs": Phase="Running", Reason="", readiness=true. Elapsed: 2.031242778s
    Feb 27 12:34:30.761: INFO: Pod "webserver-deployment-845c8977d9-ndcfs" satisfied condition "running"
    Feb 27 12:34:30.762: INFO: Pod "webserver-deployment-845c8977d9-xb2nn": Phase="Running", Reason="", readiness=true. Elapsed: 2.032248982s
    Feb 27 12:34:30.762: INFO: Pod "webserver-deployment-845c8977d9-xb2nn" satisfied condition "running"
    Feb 27 12:34:30.762: INFO: Pod "webserver-deployment-845c8977d9-r2h7h": Phase="Running", Reason="", readiness=true. Elapsed: 2.031793111s
    Feb 27 12:34:30.762: INFO: Pod "webserver-deployment-845c8977d9-r2h7h" satisfied condition "running"
    Feb 27 12:34:30.762: INFO: Pod "webserver-deployment-845c8977d9-4dmz6": Phase="Running", Reason="", readiness=true. Elapsed: 2.032385414s
    Feb 27 12:34:30.762: INFO: Pod "webserver-deployment-845c8977d9-4dmz6" satisfied condition "running"
    Feb 27 12:34:30.762: INFO: Waiting for deployment "webserver-deployment" to complete
    Feb 27 12:34:30.780: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Feb 27 12:34:30.804: INFO: Updating deployment webserver-deployment
    Feb 27 12:34:30.804: INFO: Waiting for observed generation 2
    Feb 27 12:34:32.828: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Feb 27 12:34:32.837: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Feb 27 12:34:32.846: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 27 12:34:32.879: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Feb 27 12:34:32.879: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Feb 27 12:34:32.890: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 27 12:34:32.908: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Feb 27 12:34:32.908: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Feb 27 12:34:32.941: INFO: Updating deployment webserver-deployment
    Feb 27 12:34:32.941: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Feb 27 12:34:32.963: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Feb 27 12:34:35.007: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 12:34:35.053: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-536  9cc814ff-b10d-4465-94b9-5552f0b4ee59 93037 3 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c5d788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 12:34:32 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-02-27 12:34:33 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Feb 27 12:34:35.128: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-536  2324d588-d44d-48d3-a0b3-63e90c4e2c25 93034 3 2023-02-27 12:34:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9cc814ff-b10d-4465-94b9-5552f0b4ee59 0xc004c5dc77 0xc004c5dc78}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9cc814ff-b10d-4465-94b9-5552f0b4ee59\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c5dd18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 12:34:35.129: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Feb 27 12:34:35.129: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-536  937c01c5-ae8a-40d0-94f8-b0c3411c25ce 93022 3 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9cc814ff-b10d-4465-94b9-5552f0b4ee59 0xc004c5dd77 0xc004c5dd78}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9cc814ff-b10d-4465-94b9-5552f0b4ee59\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c5de48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 12:34:35.170: INFO: Pod "webserver-deployment-69b7448995-4lh2d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-4lh2d webserver-deployment-69b7448995- deployment-536  adc39f25-04c7-4c2f-9f4b-d2967036da23 92930 0 2023-02-27 12:34:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:77a0eb6f3fac57638574490ba4656b165595d834e233672cf007c6f75a1329c4 cni.projectcalico.org/podIP:172.25.1.206/32 cni.projectcalico.org/podIPs:172.25.1.206/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9c857 0xc004c9c858}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2g42,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2g42,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.172: INFO: Pod "webserver-deployment-69b7448995-4vwt9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-4vwt9 webserver-deployment-69b7448995- deployment-536  2c8a6dcc-66d3-4e89-91cd-bd272f72c08b 93109 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5f44785ffbdf23826cc0ae22fe63a60c6c13ac78912c640359bb20f62eb1af3b cni.projectcalico.org/podIP:172.25.1.211/32 cni.projectcalico.org/podIPs:172.25.1.211/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9ca87 0xc004c9ca88}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxljq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxljq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.173: INFO: Pod "webserver-deployment-69b7448995-72qn9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-72qn9 webserver-deployment-69b7448995- deployment-536  61ad1aa9-f3bf-40dd-bdb1-8178cab4917d 93120 0 2023-02-27 12:34:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d82921fc9a3424e1be7c5aef458f1600acee800e9f9a5d662b6f485a4aad8fa3 cni.projectcalico.org/podIP:172.25.2.230/32 cni.projectcalico.org/podIPs:172.25.2.230/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9cd87 0xc004c9cd88}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lj8v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lj8v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.230,StartTime:2023-02-27 12:34:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.174: INFO: Pod "webserver-deployment-69b7448995-7xxt2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-7xxt2 webserver-deployment-69b7448995- deployment-536  8a38ccba-3894-457a-8337-ffc1b66767be 93122 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:75f12a75bc436574da3d5b9fd64ac12204a5e0c2ccf801e0b8f4636ada5a9384 cni.projectcalico.org/podIP:172.25.2.236/32 cni.projectcalico.org/podIPs:172.25.2.236/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9d0b7 0xc004c9d0b8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c99mg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c99mg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.174: INFO: Pod "webserver-deployment-69b7448995-9nb4c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-9nb4c webserver-deployment-69b7448995- deployment-536  34890cfe-1a4d-4393-a87d-8cfb606064b9 93081 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:91aa412cf3b78388c3fe0687f3bccd5c63de814930be35f83e2c5a1eefe7680e cni.projectcalico.org/podIP:172.25.0.85/32 cni.projectcalico.org/podIPs:172.25.0.85/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9d3b7 0xc004c9d3b8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fzxzp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fzxzp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.175: INFO: Pod "webserver-deployment-69b7448995-blssq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-blssq webserver-deployment-69b7448995- deployment-536  d6f24f50-bd02-41b5-8281-e9b914d1f7cd 93019 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9d657 0xc004c9d658}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bk2nw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bk2nw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.175: INFO: Pod "webserver-deployment-69b7448995-jhtjd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-jhtjd webserver-deployment-69b7448995- deployment-536  850b60d5-8c69-425a-8a25-e8eb95d8d030 92925 0 2023-02-27 12:34:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:3b78f98f1106cba0e6f986593f53e95fbafe9b505891d67e9879b5be55627bda cni.projectcalico.org/podIP:172.25.2.231/32 cni.projectcalico.org/podIPs:172.25.2.231/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9d830 0xc004c9d831}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rvmbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rvmbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.176: INFO: Pod "webserver-deployment-69b7448995-kz4lj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kz4lj webserver-deployment-69b7448995- deployment-536  39a387e7-7274-433e-944f-baa0b5240818 93108 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:30bfc99671c00c8734033ca8a966b25b57dc5f50886bd5ba8c3bb88f61f19255 cni.projectcalico.org/podIP:172.25.0.87/32 cni.projectcalico.org/podIPs:172.25.0.87/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9daa7 0xc004c9daa8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lbbp9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lbbp9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.176: INFO: Pod "webserver-deployment-69b7448995-s622z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-s622z webserver-deployment-69b7448995- deployment-536  a4e7236b-3b7f-4d8b-a537-72e24593f21e 93099 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:9581b01b4ce6822daf1db87ea4b09f2ba313bd3caa6a445cecd531aea913933e cni.projectcalico.org/podIP:172.25.2.234/32 cni.projectcalico.org/podIPs:172.25.2.234/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9dd17 0xc004c9dd18}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vqgv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vqgv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.177: INFO: Pod "webserver-deployment-69b7448995-swr22" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-swr22 webserver-deployment-69b7448995- deployment-536  c6e4f70b-9768-4d0f-8fa5-190e636f86fb 93111 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7124740808f8b8a2f20f7b037101c78caa6c55266e1ce88fcf5e3abf24b38045 cni.projectcalico.org/podIP:172.25.1.209/32 cni.projectcalico.org/podIPs:172.25.1.209/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004c9dfc7 0xc004c9dfc8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hcbdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hcbdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.177: INFO: Pod "webserver-deployment-69b7448995-sxdtf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-sxdtf webserver-deployment-69b7448995- deployment-536  001b1d25-6149-4436-8ca0-c93e0d71a3ab 93118 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5fe25904cf2580da0d27b1acf191f8b66f4394e7c4035381685f808fdb82aa5e cni.projectcalico.org/podIP:172.25.0.88/32 cni.projectcalico.org/podIPs:172.25.0.88/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004d04257 0xc004d04258}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ptx9c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ptx9c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.178: INFO: Pod "webserver-deployment-69b7448995-vw4f4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-vw4f4 webserver-deployment-69b7448995- deployment-536  d2b6307f-75c6-4680-abab-f7bbbc7ef3b2 92921 0 2023-02-27 12:34:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d63bed429a3619cb96f580891cadacc3f74ad2c38156f14025872689dea40271 cni.projectcalico.org/podIP:172.25.1.205/32 cni.projectcalico.org/podIPs:172.25.1.205/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004d04477 0xc004d04478}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njq9m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njq9m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.178: INFO: Pod "webserver-deployment-69b7448995-w8c68" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-w8c68 webserver-deployment-69b7448995- deployment-536  7b46bdf7-76ba-4a0c-80da-ee01daf3123a 93119 0 2023-02-27 12:34:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:cf05526b5bccfe704c30cf4c7999b9f742cfc30cbd27097ed88ebd1a2f2ef997 cni.projectcalico.org/podIP:172.25.0.82/32 cni.projectcalico.org/podIPs:172.25.0.82/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2324d588-d44d-48d3-a0b3-63e90c4e2c25 0xc004d047a7 0xc004d047a8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2324d588-d44d-48d3-a0b3-63e90c4e2c25\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlvwq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlvwq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:172.25.0.82,StartTime:2023-02-27 12:34:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.179: INFO: Pod "webserver-deployment-845c8977d9-25vqn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-25vqn webserver-deployment-845c8977d9- deployment-536  e62adadc-3f91-4d74-a91f-0139d6c2e82e 92822 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3ee5bf568eec7a1e35debef9a13ec1d90af9cc4860aed5ac2c72450a551079a6 cni.projectcalico.org/podIP:172.25.2.228/32 cni.projectcalico.org/podIPs:172.25.2.228/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d04a20 0xc004d04a21}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbksg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbksg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.228,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bcdbbf81bdaff8440dbed2d3c73f9e5b07d78468562dfea01cce773af0fb8b67,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.206: INFO: Pod "webserver-deployment-845c8977d9-464mv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-464mv webserver-deployment-845c8977d9- deployment-536  bae47572-efda-4de1-b2f8-26b9a4b49f2a 93025 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d04c17 0xc004d04c18}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gq9c2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gq9c2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.207: INFO: Pod "webserver-deployment-845c8977d9-4lhz2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4lhz2 webserver-deployment-845c8977d9- deployment-536  e5560621-36b2-41f9-8571-cc358351d3a0 93093 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:ad2d939647981ec4bfee8bba593f09d68a0de91e4288908e14dfa26eaf00d132 cni.projectcalico.org/podIP:172.25.0.86/32 cni.projectcalico.org/podIPs:172.25.0.86/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d04df7 0xc004d04df8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmsk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmsk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.212: INFO: Pod "webserver-deployment-845c8977d9-4t4z8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4t4z8 webserver-deployment-845c8977d9- deployment-536  12a4a611-5040-4fa9-8bf0-225afe8c4840 93121 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:676f55119001b4645c06c4b9cc6bc19cb03a02a33d571ab6300f8cddcf71774b cni.projectcalico.org/podIP:172.25.1.212/32 cni.projectcalico.org/podIPs:172.25.1.212/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d05057 0xc004d05058}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlbhw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlbhw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.235: INFO: Pod "webserver-deployment-845c8977d9-672g5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-672g5 webserver-deployment-845c8977d9- deployment-536  19572449-2a6e-48a3-b364-48075637d72f 93063 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:25459e0aa1bbe81c1f32d5a357173b5731da3ff36465cfb2637abc5168847f05 cni.projectcalico.org/podIP:172.25.1.207/32 cni.projectcalico.org/podIPs:172.25.1.207/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d05277 0xc004d05278}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xnmtl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xnmtl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.238: INFO: Pod "webserver-deployment-845c8977d9-6d9sf" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6d9sf webserver-deployment-845c8977d9- deployment-536  6c5c1b30-137c-4be5-a281-f2aa5134e330 92838 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8b1ffee10a5c86c9a0ea72ad08738ac458a83b860133517574064333cc00f1cd cni.projectcalico.org/podIP:172.25.1.202/32 cni.projectcalico.org/podIPs:172.25.1.202/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d054e7 0xc004d054e8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-564hx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-564hx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:172.25.1.202,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3079692681b8a76d2be8a7427d4c0cc66c3564698238d14c9921a2c2cec2a78b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.238: INFO: Pod "webserver-deployment-845c8977d9-chtwj" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-chtwj webserver-deployment-845c8977d9- deployment-536  4d64a0a1-fd5a-4e7b-b98a-fa2b0d78046d 92826 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d7f500e75ee63610fe4e7498107dfd00908f47fd9dbaa21688ade94e47d104f7 cni.projectcalico.org/podIP:172.25.2.227/32 cni.projectcalico.org/podIPs:172.25.2.227/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d05787 0xc004d05788}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lnfvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lnfvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.227,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://455ecbbe2fcabccbf88eb61ed908051bfe463998a73ceccff3704d0c2b0e33ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.239: INFO: Pod "webserver-deployment-845c8977d9-h9vnj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-h9vnj webserver-deployment-845c8977d9- deployment-536  a2f241f3-33ea-4d8c-8712-c56d4e3e0ef7 93062 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:773ae256fdafa582443f60447247661fd45e29a3713bd1629785b36efeaeb051 cni.projectcalico.org/podIP:172.25.0.83/32 cni.projectcalico.org/podIPs:172.25.0.83/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d05a07 0xc004d05a08}] [] [{calico Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xr8nv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xr8nv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.239: INFO: Pod "webserver-deployment-845c8977d9-j2l8x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-j2l8x webserver-deployment-845c8977d9- deployment-536  6d767b6b-cce1-4ffa-91cf-0e4a4cfbc57f 93033 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d05c47 0xc004d05c48}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bt8xf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bt8xf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.252: INFO: Pod "webserver-deployment-845c8977d9-jz44z" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jz44z webserver-deployment-845c8977d9- deployment-536  0bf13248-fd24-44bc-89e0-bc499027df27 92835 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:547c7e19739c26b18d9cd3e75dfacc2e1aa4238627069886bbeeff1c6971c5b1 cni.projectcalico.org/podIP:172.25.1.201/32 cni.projectcalico.org/podIPs:172.25.1.201/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d05eb7 0xc004d05eb8}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dvtq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dvtq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:172.25.1.201,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4d7b7475d2999b70a560b0e916746bac9ab45721b17b1f11877b2569fc7d980c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.265: INFO: Pod "webserver-deployment-845c8977d9-l4bj7" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-l4bj7 webserver-deployment-845c8977d9- deployment-536  aa4831d4-6fc8-4cb1-ab04-f33f37720b14 92851 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9581cc6afaea60d694acc59242a2fca3557464b1806b75f62d9d468ab57ca773 cni.projectcalico.org/podIP:172.25.0.79/32 cni.projectcalico.org/podIPs:172.25.0.79/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d36137 0xc004d36138}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vk94,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vk94,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:172.25.0.79,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ec39dd33771ba3c4f06964fd6d984df6058f6e22ccfa835ba2848687b36aa6f7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.266: INFO: Pod "webserver-deployment-845c8977d9-pwtbs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pwtbs webserver-deployment-845c8977d9- deployment-536  70cdc1ef-1448-48e2-a6c8-75f2056497fc 93067 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:dcc0ef39386f75c589d9ff4b3371a98c266e4f66e0b8d0fd078899f437c29510 cni.projectcalico.org/podIP:172.25.2.232/32 cni.projectcalico.org/podIPs:172.25.2.232/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d363c0 0xc004d363c1}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sgkn9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sgkn9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.266: INFO: Pod "webserver-deployment-845c8977d9-r2h7h" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-r2h7h webserver-deployment-845c8977d9- deployment-536  c52d4216-7abe-476c-b664-c9dc07646447 92845 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:ae090828c75f17ca12b201bf39772ab8eaaf648cd8d2efe5dd4243fe5e533285 cni.projectcalico.org/podIP:172.25.0.80/32 cni.projectcalico.org/podIPs:172.25.0.80/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d36637 0xc004d36638}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42tvg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42tvg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:172.25.0.80,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a201a79fb271a2c65d6294b619ccad32b50cc034a172ab258312c5d9cea49cf7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.268: INFO: Pod "webserver-deployment-845c8977d9-s787h" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-s787h webserver-deployment-845c8977d9- deployment-536  0e5e7720-6396-41d2-8800-b49643425e54 93107 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:088717d9741d8d528db02a5cdffa87a80f407442672d41dad5df4be534ac08f8 cni.projectcalico.org/podIP:172.25.2.235/32 cni.projectcalico.org/podIPs:172.25.2.235/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d36880 0xc004d36881}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvw7n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvw7n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.271: INFO: Pod "webserver-deployment-845c8977d9-spkpn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-spkpn webserver-deployment-845c8977d9- deployment-536  61591b1d-3741-4285-a21f-f035cbdd296c 92828 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:49f359aca02ef940156cfd595f2f104a81e9380846cb5fb3c6aa1b3b6af254f1 cni.projectcalico.org/podIP:172.25.2.229/32 cni.projectcalico.org/podIPs:172.25.2.229/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d36a77 0xc004d36a78}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfsqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfsqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.229,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://29de44d2387e6d4d915e036fed6ad82498886ddac23f30ce7e5cbddbb0d078a1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.274: INFO: Pod "webserver-deployment-845c8977d9-svwc7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-svwc7 webserver-deployment-845c8977d9- deployment-536  21cc744a-402b-41df-8788-93ce0e5c4aad 93084 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1666ac45564b71905fae6cc8ac0c7716febe20a212f26bab8539fcb4147091a6 cni.projectcalico.org/podIP:172.25.2.233/32 cni.projectcalico.org/podIPs:172.25.2.233/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d36d27 0xc004d36d28}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67x5t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67x5t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.276: INFO: Pod "webserver-deployment-845c8977d9-tp7x7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-tp7x7 webserver-deployment-845c8977d9- deployment-536  f4e60582-07fc-4cac-8ebc-c2e5c58c07ff 93075 0 2023-02-27 12:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b10fecf7270c35939d742dfad816de87d663f0452ae843bf19019bf96d73a71d cni.projectcalico.org/podIP:172.25.1.208/32 cni.projectcalico.org/podIPs:172.25.1.208/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d36fe7 0xc004d36fe8}] [] [{calico Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgpmv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgpmv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.308: INFO: Pod "webserver-deployment-845c8977d9-tsffm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-tsffm webserver-deployment-845c8977d9- deployment-536  1360242c-f450-4469-b414-547f67a22ab3 93101 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:60a21bab3faac33271cfb5978db7320d2d96a28e9a044818d14fb7624adf0329 cni.projectcalico.org/podIP:172.25.1.210/32 cni.projectcalico.org/podIPs:172.25.1.210/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d371f0 0xc004d371f1}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qcwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qcwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-159.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.159,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.309: INFO: Pod "webserver-deployment-845c8977d9-xb2nn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xb2nn webserver-deployment-845c8977d9- deployment-536  3259ea1b-78ad-4eeb-954d-9a29ef514e58 92848 0 2023-02-27 12:34:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a3fdb812e5e42c624dc20791bcbc30ee16916d1d229612491420f81d1e30b35d cni.projectcalico.org/podIP:172.25.0.81/32 cni.projectcalico.org/podIPs:172.25.0.81/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d37497 0xc004d37498}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:34:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:34:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xgmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xgmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:172.25.0.81,StartTime:2023-02-27 12:34:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:34:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d7ef7410e5cbaf35f3ea5cce94e5eb08138f10ceec80e869241cdc8f7eb9a7d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 12:34:35.309: INFO: Pod "webserver-deployment-845c8977d9-z68nw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-z68nw webserver-deployment-845c8977d9- deployment-536  94bbe581-16c0-499a-850a-38ebf28a8a29 93077 0 2023-02-27 12:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7abb2259251b4668aa4c58d4ebbc5eac006fae757d0b5d434a7752540e04408a cni.projectcalico.org/podIP:172.25.0.84/32 cni.projectcalico.org/podIPs:172.25.0.84/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 937c01c5-ae8a-40d0-94f8-b0c3411c25ce 0xc004d37720 0xc004d37721}] [] [{kube-controller-manager Update v1 2023-02-27 12:34:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937c01c5-ae8a-40d0-94f8-b0c3411c25ce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:34:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-27 12:34:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8z2n6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8z2n6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-7-167.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.7.167,PodIP:,StartTime:2023-02-27 12:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 27 12:34:35.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-536" for this suite. 02/27/23 12:34:35.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:34:35.5
Feb 27 12:34:35.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename var-expansion 02/27/23 12:34:35.504
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:35.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:35.596
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 02/27/23 12:34:35.62
Feb 27 12:34:35.640: INFO: Waiting up to 2m0s for pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3" in namespace "var-expansion-7006" to be "running"
Feb 27 12:34:35.653: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.357755ms
Feb 27 12:34:37.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02240972s
Feb 27 12:34:39.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022177443s
Feb 27 12:34:41.696: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055847186s
Feb 27 12:34:43.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023631098s
Feb 27 12:34:45.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021271462s
Feb 27 12:34:47.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023333205s
Feb 27 12:34:49.665: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.025384896s
Feb 27 12:34:51.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.021422373s
Feb 27 12:34:53.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.022521941s
Feb 27 12:34:55.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.02218759s
Feb 27 12:34:57.669: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.029315262s
Feb 27 12:34:59.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.021770273s
Feb 27 12:35:01.665: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 26.024729281s
Feb 27 12:35:03.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 28.020906809s
Feb 27 12:35:05.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 30.022822548s
Feb 27 12:35:07.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 32.021477086s
Feb 27 12:35:09.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 34.021328421s
Feb 27 12:35:11.666: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 36.026105184s
Feb 27 12:35:13.665: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 38.02477982s
Feb 27 12:35:15.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 40.022917345s
Feb 27 12:35:17.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021747501s
Feb 27 12:35:19.668: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 44.027938633s
Feb 27 12:35:21.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023191376s
Feb 27 12:35:23.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 48.021568595s
Feb 27 12:35:25.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 50.024362469s
Feb 27 12:35:27.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 52.020979646s
Feb 27 12:35:29.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 54.022940849s
Feb 27 12:35:31.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 56.023611916s
Feb 27 12:35:33.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 58.022027215s
Feb 27 12:35:35.754: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.113702613s
Feb 27 12:35:37.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.021120471s
Feb 27 12:35:39.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.022435045s
Feb 27 12:35:41.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.022888634s
Feb 27 12:35:43.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.024097436s
Feb 27 12:35:45.665: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.025448992s
Feb 27 12:35:47.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.021303599s
Feb 27 12:35:49.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021646602s
Feb 27 12:35:51.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.022717808s
Feb 27 12:35:53.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.023592562s
Feb 27 12:35:55.670: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.029883492s
Feb 27 12:35:57.668: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.027826058s
Feb 27 12:35:59.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.020990036s
Feb 27 12:36:01.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.022117192s
Feb 27 12:36:03.665: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.024818997s
Feb 27 12:36:05.666: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.025839147s
Feb 27 12:36:07.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.022793441s
Feb 27 12:36:09.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.021183721s
Feb 27 12:36:11.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.023245285s
Feb 27 12:36:13.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.022663808s
Feb 27 12:36:15.667: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.02665348s
Feb 27 12:36:17.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.021532463s
Feb 27 12:36:19.666: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.025825952s
Feb 27 12:36:21.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.024285785s
Feb 27 12:36:23.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.02308344s
Feb 27 12:36:25.670: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.03030865s
Feb 27 12:36:27.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.021943127s
Feb 27 12:36:29.666: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.026143125s
Feb 27 12:36:31.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.020754034s
Feb 27 12:36:33.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.024017154s
Feb 27 12:36:35.674: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.034067897s
Feb 27 12:36:35.690: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.049904741s
STEP: updating the pod 02/27/23 12:36:35.69
Feb 27 12:36:36.225: INFO: Successfully updated pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3"
STEP: waiting for pod running 02/27/23 12:36:36.225
Feb 27 12:36:36.225: INFO: Waiting up to 2m0s for pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3" in namespace "var-expansion-7006" to be "running"
Feb 27 12:36:36.233: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.882832ms
Feb 27 12:36:38.243: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Running", Reason="", readiness=true. Elapsed: 2.017692863s
Feb 27 12:36:38.243: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3" satisfied condition "running"
STEP: deleting the pod gracefully 02/27/23 12:36:38.243
Feb 27 12:36:38.243: INFO: Deleting pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3" in namespace "var-expansion-7006"
Feb 27 12:36:38.265: INFO: Wait up to 5m0s for pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 27 12:37:10.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7006" for this suite. 02/27/23 12:37:10.294
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":278,"skipped":5113,"failed":0}
------------------------------
• [SLOW TEST] [154.808 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:34:35.5
    Feb 27 12:34:35.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename var-expansion 02/27/23 12:34:35.504
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:34:35.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:34:35.596
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 02/27/23 12:34:35.62
    Feb 27 12:34:35.640: INFO: Waiting up to 2m0s for pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3" in namespace "var-expansion-7006" to be "running"
    Feb 27 12:34:35.653: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.357755ms
    Feb 27 12:34:37.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02240972s
    Feb 27 12:34:39.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022177443s
    Feb 27 12:34:41.696: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055847186s
    Feb 27 12:34:43.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023631098s
    Feb 27 12:34:45.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021271462s
    Feb 27 12:34:47.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023333205s
    Feb 27 12:34:49.665: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.025384896s
    Feb 27 12:34:51.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.021422373s
    Feb 27 12:34:53.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.022521941s
    Feb 27 12:34:55.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.02218759s
    Feb 27 12:34:57.669: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.029315262s
    Feb 27 12:34:59.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.021770273s
    Feb 27 12:35:01.665: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 26.024729281s
    Feb 27 12:35:03.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 28.020906809s
    Feb 27 12:35:05.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 30.022822548s
    Feb 27 12:35:07.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 32.021477086s
    Feb 27 12:35:09.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 34.021328421s
    Feb 27 12:35:11.666: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 36.026105184s
    Feb 27 12:35:13.665: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 38.02477982s
    Feb 27 12:35:15.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 40.022917345s
    Feb 27 12:35:17.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021747501s
    Feb 27 12:35:19.668: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 44.027938633s
    Feb 27 12:35:21.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023191376s
    Feb 27 12:35:23.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 48.021568595s
    Feb 27 12:35:25.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 50.024362469s
    Feb 27 12:35:27.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 52.020979646s
    Feb 27 12:35:29.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 54.022940849s
    Feb 27 12:35:31.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 56.023611916s
    Feb 27 12:35:33.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 58.022027215s
    Feb 27 12:35:35.754: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.113702613s
    Feb 27 12:35:37.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.021120471s
    Feb 27 12:35:39.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.022435045s
    Feb 27 12:35:41.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.022888634s
    Feb 27 12:35:43.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.024097436s
    Feb 27 12:35:45.665: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.025448992s
    Feb 27 12:35:47.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.021303599s
    Feb 27 12:35:49.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021646602s
    Feb 27 12:35:51.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.022717808s
    Feb 27 12:35:53.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.023592562s
    Feb 27 12:35:55.670: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.029883492s
    Feb 27 12:35:57.668: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.027826058s
    Feb 27 12:35:59.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.020990036s
    Feb 27 12:36:01.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.022117192s
    Feb 27 12:36:03.665: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.024818997s
    Feb 27 12:36:05.666: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.025839147s
    Feb 27 12:36:07.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.022793441s
    Feb 27 12:36:09.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.021183721s
    Feb 27 12:36:11.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.023245285s
    Feb 27 12:36:13.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.022663808s
    Feb 27 12:36:15.667: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.02665348s
    Feb 27 12:36:17.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.021532463s
    Feb 27 12:36:19.666: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.025825952s
    Feb 27 12:36:21.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.024285785s
    Feb 27 12:36:23.663: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.02308344s
    Feb 27 12:36:25.670: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.03030865s
    Feb 27 12:36:27.662: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.021943127s
    Feb 27 12:36:29.666: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.026143125s
    Feb 27 12:36:31.661: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.020754034s
    Feb 27 12:36:33.664: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.024017154s
    Feb 27 12:36:35.674: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.034067897s
    Feb 27 12:36:35.690: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.049904741s
    STEP: updating the pod 02/27/23 12:36:35.69
    Feb 27 12:36:36.225: INFO: Successfully updated pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3"
    STEP: waiting for pod running 02/27/23 12:36:36.225
    Feb 27 12:36:36.225: INFO: Waiting up to 2m0s for pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3" in namespace "var-expansion-7006" to be "running"
    Feb 27 12:36:36.233: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.882832ms
    Feb 27 12:36:38.243: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3": Phase="Running", Reason="", readiness=true. Elapsed: 2.017692863s
    Feb 27 12:36:38.243: INFO: Pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3" satisfied condition "running"
    STEP: deleting the pod gracefully 02/27/23 12:36:38.243
    Feb 27 12:36:38.243: INFO: Deleting pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3" in namespace "var-expansion-7006"
    Feb 27 12:36:38.265: INFO: Wait up to 5m0s for pod "var-expansion-a7a11be7-0245-4736-a55d-bfdaa24f4cc3" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 27 12:37:10.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7006" for this suite. 02/27/23 12:37:10.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:37:10.315
Feb 27 12:37:10.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pods 02/27/23 12:37:10.316
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:37:10.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:37:10.361
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 02/27/23 12:37:10.372
Feb 27 12:37:10.394: INFO: Waiting up to 5m0s for pod "pod-jdn9k" in namespace "pods-6418" to be "running"
Feb 27 12:37:10.407: INFO: Pod "pod-jdn9k": Phase="Pending", Reason="", readiness=false. Elapsed: 12.794301ms
Feb 27 12:37:12.426: INFO: Pod "pod-jdn9k": Phase="Running", Reason="", readiness=true. Elapsed: 2.031886443s
Feb 27 12:37:12.427: INFO: Pod "pod-jdn9k" satisfied condition "running"
STEP: patching /status 02/27/23 12:37:12.427
Feb 27 12:37:12.446: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 27 12:37:12.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6418" for this suite. 02/27/23 12:37:12.46
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":279,"skipped":5126,"failed":0}
------------------------------
• [2.162 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:37:10.315
    Feb 27 12:37:10.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pods 02/27/23 12:37:10.316
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:37:10.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:37:10.361
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 02/27/23 12:37:10.372
    Feb 27 12:37:10.394: INFO: Waiting up to 5m0s for pod "pod-jdn9k" in namespace "pods-6418" to be "running"
    Feb 27 12:37:10.407: INFO: Pod "pod-jdn9k": Phase="Pending", Reason="", readiness=false. Elapsed: 12.794301ms
    Feb 27 12:37:12.426: INFO: Pod "pod-jdn9k": Phase="Running", Reason="", readiness=true. Elapsed: 2.031886443s
    Feb 27 12:37:12.427: INFO: Pod "pod-jdn9k" satisfied condition "running"
    STEP: patching /status 02/27/23 12:37:12.427
    Feb 27 12:37:12.446: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 27 12:37:12.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6418" for this suite. 02/27/23 12:37:12.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:37:12.48
Feb 27 12:37:12.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename replication-controller 02/27/23 12:37:12.481
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:37:12.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:37:12.531
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a 02/27/23 12:37:12.54
Feb 27 12:37:12.559: INFO: Pod name my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a: Found 0 pods out of 1
Feb 27 12:37:17.570: INFO: Pod name my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a: Found 1 pods out of 1
Feb 27 12:37:17.570: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a" are running
Feb 27 12:37:17.570: INFO: Waiting up to 5m0s for pod "my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a-hsmp7" in namespace "replication-controller-6988" to be "running"
Feb 27 12:37:17.577: INFO: Pod "my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a-hsmp7": Phase="Running", Reason="", readiness=true. Elapsed: 6.328015ms
Feb 27 12:37:17.577: INFO: Pod "my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a-hsmp7" satisfied condition "running"
Feb 27 12:37:17.577: INFO: Pod "my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a-hsmp7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 12:37:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 12:37:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 12:37:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 12:37:12 +0000 UTC Reason: Message:}])
Feb 27 12:37:17.577: INFO: Trying to dial the pod
Feb 27 12:37:22.609: INFO: Controller my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a: Got expected result from replica 1 [my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a-hsmp7]: "my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a-hsmp7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 27 12:37:22.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6988" for this suite. 02/27/23 12:37:22.623
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":280,"skipped":5132,"failed":0}
------------------------------
• [SLOW TEST] [10.157 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:37:12.48
    Feb 27 12:37:12.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename replication-controller 02/27/23 12:37:12.481
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:37:12.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:37:12.531
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a 02/27/23 12:37:12.54
    Feb 27 12:37:12.559: INFO: Pod name my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a: Found 0 pods out of 1
    Feb 27 12:37:17.570: INFO: Pod name my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a: Found 1 pods out of 1
    Feb 27 12:37:17.570: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a" are running
    Feb 27 12:37:17.570: INFO: Waiting up to 5m0s for pod "my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a-hsmp7" in namespace "replication-controller-6988" to be "running"
    Feb 27 12:37:17.577: INFO: Pod "my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a-hsmp7": Phase="Running", Reason="", readiness=true. Elapsed: 6.328015ms
    Feb 27 12:37:17.577: INFO: Pod "my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a-hsmp7" satisfied condition "running"
    Feb 27 12:37:17.577: INFO: Pod "my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a-hsmp7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 12:37:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 12:37:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 12:37:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 12:37:12 +0000 UTC Reason: Message:}])
    Feb 27 12:37:17.577: INFO: Trying to dial the pod
    Feb 27 12:37:22.609: INFO: Controller my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a: Got expected result from replica 1 [my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a-hsmp7]: "my-hostname-basic-417c3326-dbcf-4f84-9d3c-dc741b48874a-hsmp7", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 27 12:37:22.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6988" for this suite. 02/27/23 12:37:22.623
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:37:22.638
Feb 27 12:37:22.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pods 02/27/23 12:37:22.639
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:37:22.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:37:22.738
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 02/27/23 12:37:22.746
STEP: submitting the pod to kubernetes 02/27/23 12:37:22.747
Feb 27 12:37:22.772: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651" in namespace "pods-9708" to be "running and ready"
Feb 27 12:37:22.784: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651": Phase="Pending", Reason="", readiness=false. Elapsed: 11.918376ms
Feb 27 12:37:22.784: INFO: The phase of Pod pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:37:24.793: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651": Phase="Running", Reason="", readiness=true. Elapsed: 2.020409109s
Feb 27 12:37:24.793: INFO: The phase of Pod pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651 is Running (Ready = true)
Feb 27 12:37:24.793: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/27/23 12:37:24.8
STEP: updating the pod 02/27/23 12:37:24.808
Feb 27 12:37:25.332: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651"
Feb 27 12:37:25.334: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651" in namespace "pods-9708" to be "terminated with reason DeadlineExceeded"
Feb 27 12:37:25.352: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651": Phase="Running", Reason="", readiness=true. Elapsed: 16.700556ms
Feb 27 12:37:27.360: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651": Phase="Running", Reason="", readiness=true. Elapsed: 2.02555834s
Feb 27 12:37:29.360: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651": Phase="Running", Reason="", readiness=false. Elapsed: 4.02561441s
Feb 27 12:37:31.363: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.027720418s
Feb 27 12:37:31.363: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 27 12:37:31.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9708" for this suite. 02/27/23 12:37:31.385
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":281,"skipped":5134,"failed":0}
------------------------------
• [SLOW TEST] [8.766 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:37:22.638
    Feb 27 12:37:22.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pods 02/27/23 12:37:22.639
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:37:22.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:37:22.738
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 02/27/23 12:37:22.746
    STEP: submitting the pod to kubernetes 02/27/23 12:37:22.747
    Feb 27 12:37:22.772: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651" in namespace "pods-9708" to be "running and ready"
    Feb 27 12:37:22.784: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651": Phase="Pending", Reason="", readiness=false. Elapsed: 11.918376ms
    Feb 27 12:37:22.784: INFO: The phase of Pod pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:37:24.793: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651": Phase="Running", Reason="", readiness=true. Elapsed: 2.020409109s
    Feb 27 12:37:24.793: INFO: The phase of Pod pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651 is Running (Ready = true)
    Feb 27 12:37:24.793: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/27/23 12:37:24.8
    STEP: updating the pod 02/27/23 12:37:24.808
    Feb 27 12:37:25.332: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651"
    Feb 27 12:37:25.334: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651" in namespace "pods-9708" to be "terminated with reason DeadlineExceeded"
    Feb 27 12:37:25.352: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651": Phase="Running", Reason="", readiness=true. Elapsed: 16.700556ms
    Feb 27 12:37:27.360: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651": Phase="Running", Reason="", readiness=true. Elapsed: 2.02555834s
    Feb 27 12:37:29.360: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651": Phase="Running", Reason="", readiness=false. Elapsed: 4.02561441s
    Feb 27 12:37:31.363: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.027720418s
    Feb 27 12:37:31.363: INFO: Pod "pod-update-activedeadlineseconds-b6c566b3-dfcd-4e71-9df7-ec7c10e93651" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 27 12:37:31.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9708" for this suite. 02/27/23 12:37:31.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:37:31.41
Feb 27 12:37:31.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:37:31.412
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:37:31.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:37:31.473
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-431a8249-d204-43e8-841d-d0d8a6cf19a9 02/27/23 12:37:31.495
STEP: Creating configMap with name cm-test-opt-upd-c1564995-d3ba-4cf1-ab31-7104633583fe 02/27/23 12:37:31.51
STEP: Creating the pod 02/27/23 12:37:31.524
Feb 27 12:37:31.544: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8" in namespace "projected-9146" to be "running and ready"
Feb 27 12:37:31.558: INFO: Pod "pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.896475ms
Feb 27 12:37:31.558: INFO: The phase of Pod pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:37:33.570: INFO: Pod "pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026226494s
Feb 27 12:37:33.571: INFO: The phase of Pod pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:37:35.567: INFO: Pod "pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8": Phase="Running", Reason="", readiness=true. Elapsed: 4.023790801s
Feb 27 12:37:35.568: INFO: The phase of Pod pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8 is Running (Ready = true)
Feb 27 12:37:35.568: INFO: Pod "pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-431a8249-d204-43e8-841d-d0d8a6cf19a9 02/27/23 12:37:35.626
STEP: Updating configmap cm-test-opt-upd-c1564995-d3ba-4cf1-ab31-7104633583fe 02/27/23 12:37:35.643
STEP: Creating configMap with name cm-test-opt-create-dced9a9b-65d6-4b61-89ff-094ca8fa6cca 02/27/23 12:37:35.655
STEP: waiting to observe update in volume 02/27/23 12:37:35.672
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 27 12:38:48.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9146" for this suite. 02/27/23 12:38:48.495
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":282,"skipped":5139,"failed":0}
------------------------------
• [SLOW TEST] [77.100 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:37:31.41
    Feb 27 12:37:31.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:37:31.412
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:37:31.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:37:31.473
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-431a8249-d204-43e8-841d-d0d8a6cf19a9 02/27/23 12:37:31.495
    STEP: Creating configMap with name cm-test-opt-upd-c1564995-d3ba-4cf1-ab31-7104633583fe 02/27/23 12:37:31.51
    STEP: Creating the pod 02/27/23 12:37:31.524
    Feb 27 12:37:31.544: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8" in namespace "projected-9146" to be "running and ready"
    Feb 27 12:37:31.558: INFO: Pod "pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.896475ms
    Feb 27 12:37:31.558: INFO: The phase of Pod pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:37:33.570: INFO: Pod "pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026226494s
    Feb 27 12:37:33.571: INFO: The phase of Pod pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:37:35.567: INFO: Pod "pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8": Phase="Running", Reason="", readiness=true. Elapsed: 4.023790801s
    Feb 27 12:37:35.568: INFO: The phase of Pod pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8 is Running (Ready = true)
    Feb 27 12:37:35.568: INFO: Pod "pod-projected-configmaps-f11e8f1e-7a8f-4e58-ba4b-67509b1614a8" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-431a8249-d204-43e8-841d-d0d8a6cf19a9 02/27/23 12:37:35.626
    STEP: Updating configmap cm-test-opt-upd-c1564995-d3ba-4cf1-ab31-7104633583fe 02/27/23 12:37:35.643
    STEP: Creating configMap with name cm-test-opt-create-dced9a9b-65d6-4b61-89ff-094ca8fa6cca 02/27/23 12:37:35.655
    STEP: waiting to observe update in volume 02/27/23 12:37:35.672
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 27 12:38:48.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9146" for this suite. 02/27/23 12:38:48.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:38:48.516
Feb 27 12:38:48.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename ingressclass 02/27/23 12:38:48.517
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:38:48.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:38:48.571
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 02/27/23 12:38:48.589
STEP: getting /apis/networking.k8s.io 02/27/23 12:38:48.6
STEP: getting /apis/networking.k8s.iov1 02/27/23 12:38:48.604
STEP: creating 02/27/23 12:38:48.609
STEP: getting 02/27/23 12:38:48.641
STEP: listing 02/27/23 12:38:48.649
STEP: watching 02/27/23 12:38:48.659
Feb 27 12:38:48.660: INFO: starting watch
STEP: patching 02/27/23 12:38:48.664
STEP: updating 02/27/23 12:38:48.674
Feb 27 12:38:48.683: INFO: waiting for watch events with expected annotations
Feb 27 12:38:48.683: INFO: saw patched and updated annotations
STEP: deleting 02/27/23 12:38:48.684
STEP: deleting a collection 02/27/23 12:38:48.719
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Feb 27 12:38:48.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-3065" for this suite. 02/27/23 12:38:48.809
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":283,"skipped":5151,"failed":0}
------------------------------
• [0.314 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:38:48.516
    Feb 27 12:38:48.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename ingressclass 02/27/23 12:38:48.517
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:38:48.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:38:48.571
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 02/27/23 12:38:48.589
    STEP: getting /apis/networking.k8s.io 02/27/23 12:38:48.6
    STEP: getting /apis/networking.k8s.iov1 02/27/23 12:38:48.604
    STEP: creating 02/27/23 12:38:48.609
    STEP: getting 02/27/23 12:38:48.641
    STEP: listing 02/27/23 12:38:48.649
    STEP: watching 02/27/23 12:38:48.659
    Feb 27 12:38:48.660: INFO: starting watch
    STEP: patching 02/27/23 12:38:48.664
    STEP: updating 02/27/23 12:38:48.674
    Feb 27 12:38:48.683: INFO: waiting for watch events with expected annotations
    Feb 27 12:38:48.683: INFO: saw patched and updated annotations
    STEP: deleting 02/27/23 12:38:48.684
    STEP: deleting a collection 02/27/23 12:38:48.719
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Feb 27 12:38:48.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-3065" for this suite. 02/27/23 12:38:48.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:38:48.832
Feb 27 12:38:48.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename resourcequota 02/27/23 12:38:48.833
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:38:48.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:38:48.887
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 02/27/23 12:38:48.904
STEP: Counting existing ResourceQuota 02/27/23 12:38:53.928
STEP: Creating a ResourceQuota 02/27/23 12:38:58.953
STEP: Ensuring resource quota status is calculated 02/27/23 12:38:58.964
STEP: Creating a Secret 02/27/23 12:39:00.973
STEP: Ensuring resource quota status captures secret creation 02/27/23 12:39:00.994
STEP: Deleting a secret 02/27/23 12:39:03.005
STEP: Ensuring resource quota status released usage 02/27/23 12:39:03.023
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 27 12:39:05.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4848" for this suite. 02/27/23 12:39:05.05
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":284,"skipped":5165,"failed":0}
------------------------------
• [SLOW TEST] [16.243 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:38:48.832
    Feb 27 12:38:48.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename resourcequota 02/27/23 12:38:48.833
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:38:48.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:38:48.887
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 02/27/23 12:38:48.904
    STEP: Counting existing ResourceQuota 02/27/23 12:38:53.928
    STEP: Creating a ResourceQuota 02/27/23 12:38:58.953
    STEP: Ensuring resource quota status is calculated 02/27/23 12:38:58.964
    STEP: Creating a Secret 02/27/23 12:39:00.973
    STEP: Ensuring resource quota status captures secret creation 02/27/23 12:39:00.994
    STEP: Deleting a secret 02/27/23 12:39:03.005
    STEP: Ensuring resource quota status released usage 02/27/23 12:39:03.023
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 27 12:39:05.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4848" for this suite. 02/27/23 12:39:05.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:39:05.087
Feb 27 12:39:05.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename taint-multiple-pods 02/27/23 12:39:05.088
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:39:05.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:39:05.161
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Feb 27 12:39:05.193: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 12:40:05.278: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Feb 27 12:40:05.287: INFO: Starting informer...
STEP: Starting pods... 02/27/23 12:40:05.287
Feb 27 12:40:05.534: INFO: Pod1 is running on ip-172-31-11-159.eu-central-1.compute.internal. Tainting Node
Feb 27 12:40:05.561: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8854" to be "running"
Feb 27 12:40:05.580: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.357929ms
Feb 27 12:40:07.591: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.030313273s
Feb 27 12:40:07.591: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Feb 27 12:40:07.592: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8854" to be "running"
Feb 27 12:40:07.599: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 7.490581ms
Feb 27 12:40:07.599: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Feb 27 12:40:07.599: INFO: Pod2 is running on ip-172-31-11-159.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 02/27/23 12:40:07.599
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 12:40:07.627
STEP: Waiting for Pod1 and Pod2 to be deleted 02/27/23 12:40:07.643
Feb 27 12:40:14.202: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 27 12:40:33.554: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 12:40:33.581
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Feb 27 12:40:33.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8854" for this suite. 02/27/23 12:40:33.6
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":285,"skipped":5221,"failed":0}
------------------------------
• [SLOW TEST] [88.528 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:39:05.087
    Feb 27 12:39:05.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename taint-multiple-pods 02/27/23 12:39:05.088
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:39:05.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:39:05.161
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Feb 27 12:39:05.193: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 12:40:05.278: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Feb 27 12:40:05.287: INFO: Starting informer...
    STEP: Starting pods... 02/27/23 12:40:05.287
    Feb 27 12:40:05.534: INFO: Pod1 is running on ip-172-31-11-159.eu-central-1.compute.internal. Tainting Node
    Feb 27 12:40:05.561: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8854" to be "running"
    Feb 27 12:40:05.580: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.357929ms
    Feb 27 12:40:07.591: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.030313273s
    Feb 27 12:40:07.591: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Feb 27 12:40:07.592: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8854" to be "running"
    Feb 27 12:40:07.599: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 7.490581ms
    Feb 27 12:40:07.599: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Feb 27 12:40:07.599: INFO: Pod2 is running on ip-172-31-11-159.eu-central-1.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 02/27/23 12:40:07.599
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 12:40:07.627
    STEP: Waiting for Pod1 and Pod2 to be deleted 02/27/23 12:40:07.643
    Feb 27 12:40:14.202: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Feb 27 12:40:33.554: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 12:40:33.581
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 12:40:33.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-8854" for this suite. 02/27/23 12:40:33.6
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:40:33.615
Feb 27 12:40:33.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 12:40:33.618
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:40:33.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:40:33.654
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-4285 02/27/23 12:40:33.665
STEP: creating service affinity-clusterip in namespace services-4285 02/27/23 12:40:33.665
STEP: creating replication controller affinity-clusterip in namespace services-4285 02/27/23 12:40:33.689
I0227 12:40:33.711634      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4285, replica count: 3
I0227 12:40:36.762364      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 12:40:36.783: INFO: Creating new exec pod
Feb 27 12:40:36.792: INFO: Waiting up to 5m0s for pod "execpod-affinityphw9g" in namespace "services-4285" to be "running"
Feb 27 12:40:36.800: INFO: Pod "execpod-affinityphw9g": Phase="Pending", Reason="", readiness=false. Elapsed: 7.693201ms
Feb 27 12:40:38.852: INFO: Pod "execpod-affinityphw9g": Phase="Running", Reason="", readiness=true. Elapsed: 2.059509821s
Feb 27 12:40:38.852: INFO: Pod "execpod-affinityphw9g" satisfied condition "running"
Feb 27 12:40:39.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-4285 exec execpod-affinityphw9g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Feb 27 12:40:40.603: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb 27 12:40:40.603: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 12:40:40.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-4285 exec execpod-affinityphw9g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.24.20 80'
Feb 27 12:40:40.977: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.240.24.20 80\nConnection to 10.240.24.20 80 port [tcp/http] succeeded!\n"
Feb 27 12:40:40.977: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 27 12:40:40.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-4285 exec execpod-affinityphw9g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.24.20:80/ ; done'
Feb 27 12:40:41.514: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n"
Feb 27 12:40:41.514: INFO: stdout: "\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h"
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
Feb 27 12:40:41.514: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4285, will wait for the garbage collector to delete the pods 02/27/23 12:40:41.548
Feb 27 12:40:41.638: INFO: Deleting ReplicationController affinity-clusterip took: 29.65971ms
Feb 27 12:40:41.743: INFO: Terminating ReplicationController affinity-clusterip pods took: 105.526938ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 12:40:43.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4285" for this suite. 02/27/23 12:40:43.729
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":286,"skipped":5225,"failed":0}
------------------------------
• [SLOW TEST] [10.150 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:40:33.615
    Feb 27 12:40:33.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 12:40:33.618
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:40:33.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:40:33.654
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-4285 02/27/23 12:40:33.665
    STEP: creating service affinity-clusterip in namespace services-4285 02/27/23 12:40:33.665
    STEP: creating replication controller affinity-clusterip in namespace services-4285 02/27/23 12:40:33.689
    I0227 12:40:33.711634      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4285, replica count: 3
    I0227 12:40:36.762364      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 12:40:36.783: INFO: Creating new exec pod
    Feb 27 12:40:36.792: INFO: Waiting up to 5m0s for pod "execpod-affinityphw9g" in namespace "services-4285" to be "running"
    Feb 27 12:40:36.800: INFO: Pod "execpod-affinityphw9g": Phase="Pending", Reason="", readiness=false. Elapsed: 7.693201ms
    Feb 27 12:40:38.852: INFO: Pod "execpod-affinityphw9g": Phase="Running", Reason="", readiness=true. Elapsed: 2.059509821s
    Feb 27 12:40:38.852: INFO: Pod "execpod-affinityphw9g" satisfied condition "running"
    Feb 27 12:40:39.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-4285 exec execpod-affinityphw9g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Feb 27 12:40:40.603: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Feb 27 12:40:40.603: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 12:40:40.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-4285 exec execpod-affinityphw9g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.24.20 80'
    Feb 27 12:40:40.977: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.240.24.20 80\nConnection to 10.240.24.20 80 port [tcp/http] succeeded!\n"
    Feb 27 12:40:40.977: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 27 12:40:40.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-4285 exec execpod-affinityphw9g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.24.20:80/ ; done'
    Feb 27 12:40:41.514: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.24.20:80/\n"
    Feb 27 12:40:41.514: INFO: stdout: "\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h\naffinity-clusterip-hkn2h"
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Received response from host: affinity-clusterip-hkn2h
    Feb 27 12:40:41.514: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-4285, will wait for the garbage collector to delete the pods 02/27/23 12:40:41.548
    Feb 27 12:40:41.638: INFO: Deleting ReplicationController affinity-clusterip took: 29.65971ms
    Feb 27 12:40:41.743: INFO: Terminating ReplicationController affinity-clusterip pods took: 105.526938ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 12:40:43.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4285" for this suite. 02/27/23 12:40:43.729
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:40:43.769
Feb 27 12:40:43.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 12:40:43.771
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:40:43.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:40:43.837
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 02/27/23 12:40:43.857
Feb 27 12:40:43.881: INFO: Waiting up to 5m0s for pod "pod-d760dd03-c224-4563-b1ae-d5c240def85d" in namespace "emptydir-6568" to be "Succeeded or Failed"
Feb 27 12:40:43.901: INFO: Pod "pod-d760dd03-c224-4563-b1ae-d5c240def85d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.981881ms
Feb 27 12:40:45.914: INFO: Pod "pod-d760dd03-c224-4563-b1ae-d5c240def85d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032258489s
Feb 27 12:40:47.910: INFO: Pod "pod-d760dd03-c224-4563-b1ae-d5c240def85d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029021976s
STEP: Saw pod success 02/27/23 12:40:47.911
Feb 27 12:40:47.911: INFO: Pod "pod-d760dd03-c224-4563-b1ae-d5c240def85d" satisfied condition "Succeeded or Failed"
Feb 27 12:40:47.918: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-d760dd03-c224-4563-b1ae-d5c240def85d container test-container: <nil>
STEP: delete the pod 02/27/23 12:40:47.939
Feb 27 12:40:47.956: INFO: Waiting for pod pod-d760dd03-c224-4563-b1ae-d5c240def85d to disappear
Feb 27 12:40:47.963: INFO: Pod pod-d760dd03-c224-4563-b1ae-d5c240def85d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 12:40:47.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6568" for this suite. 02/27/23 12:40:47.974
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":287,"skipped":5230,"failed":0}
------------------------------
• [4.219 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:40:43.769
    Feb 27 12:40:43.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 12:40:43.771
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:40:43.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:40:43.837
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 02/27/23 12:40:43.857
    Feb 27 12:40:43.881: INFO: Waiting up to 5m0s for pod "pod-d760dd03-c224-4563-b1ae-d5c240def85d" in namespace "emptydir-6568" to be "Succeeded or Failed"
    Feb 27 12:40:43.901: INFO: Pod "pod-d760dd03-c224-4563-b1ae-d5c240def85d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.981881ms
    Feb 27 12:40:45.914: INFO: Pod "pod-d760dd03-c224-4563-b1ae-d5c240def85d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032258489s
    Feb 27 12:40:47.910: INFO: Pod "pod-d760dd03-c224-4563-b1ae-d5c240def85d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029021976s
    STEP: Saw pod success 02/27/23 12:40:47.911
    Feb 27 12:40:47.911: INFO: Pod "pod-d760dd03-c224-4563-b1ae-d5c240def85d" satisfied condition "Succeeded or Failed"
    Feb 27 12:40:47.918: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-d760dd03-c224-4563-b1ae-d5c240def85d container test-container: <nil>
    STEP: delete the pod 02/27/23 12:40:47.939
    Feb 27 12:40:47.956: INFO: Waiting for pod pod-d760dd03-c224-4563-b1ae-d5c240def85d to disappear
    Feb 27 12:40:47.963: INFO: Pod pod-d760dd03-c224-4563-b1ae-d5c240def85d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 12:40:47.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6568" for this suite. 02/27/23 12:40:47.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:40:47.99
Feb 27 12:40:47.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 12:40:47.992
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:40:48.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:40:48.038
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/27/23 12:40:48.048
Feb 27 12:40:48.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:40:51.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:41:07.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-934" for this suite. 02/27/23 12:41:07.041
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":288,"skipped":5238,"failed":0}
------------------------------
• [SLOW TEST] [19.065 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:40:47.99
    Feb 27 12:40:47.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 12:40:47.992
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:40:48.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:40:48.038
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/27/23 12:40:48.048
    Feb 27 12:40:48.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:40:51.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:41:07.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-934" for this suite. 02/27/23 12:41:07.041
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:41:07.056
Feb 27 12:41:07.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename cronjob 02/27/23 12:41:07.058
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:41:07.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:41:07.106
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 02/27/23 12:41:07.114
STEP: Ensuring a job is scheduled 02/27/23 12:41:07.124
STEP: Ensuring exactly one is scheduled 02/27/23 12:42:01.134
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/27/23 12:42:01.141
STEP: Ensuring the job is replaced with a new one 02/27/23 12:42:01.148
STEP: Removing cronjob 02/27/23 12:43:01.156
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 27 12:43:01.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-310" for this suite. 02/27/23 12:43:01.186
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":289,"skipped":5239,"failed":0}
------------------------------
• [SLOW TEST] [114.157 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:41:07.056
    Feb 27 12:41:07.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename cronjob 02/27/23 12:41:07.058
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:41:07.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:41:07.106
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 02/27/23 12:41:07.114
    STEP: Ensuring a job is scheduled 02/27/23 12:41:07.124
    STEP: Ensuring exactly one is scheduled 02/27/23 12:42:01.134
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/27/23 12:42:01.141
    STEP: Ensuring the job is replaced with a new one 02/27/23 12:42:01.148
    STEP: Removing cronjob 02/27/23 12:43:01.156
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 27 12:43:01.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-310" for this suite. 02/27/23 12:43:01.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:43:01.22
Feb 27 12:43:01.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename dns 02/27/23 12:43:01.222
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:43:01.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:43:01.264
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 02/27/23 12:43:01.273
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9478.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local; sleep 1; done
 02/27/23 12:43:01.296
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9478.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local; sleep 1; done
 02/27/23 12:43:01.297
STEP: creating a pod to probe DNS 02/27/23 12:43:01.297
STEP: submitting the pod to kubernetes 02/27/23 12:43:01.297
Feb 27 12:43:01.327: INFO: Waiting up to 15m0s for pod "dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454" in namespace "dns-9478" to be "running"
Feb 27 12:43:01.342: INFO: Pod "dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454": Phase="Pending", Reason="", readiness=false. Elapsed: 15.381602ms
Feb 27 12:43:03.377: INFO: Pod "dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454": Phase="Running", Reason="", readiness=true. Elapsed: 2.050477487s
Feb 27 12:43:03.377: INFO: Pod "dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454" satisfied condition "running"
STEP: retrieving the pod 02/27/23 12:43:03.377
STEP: looking for the results for each expected name from probers 02/27/23 12:43:03.392
Feb 27 12:43:03.438: INFO: File jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local from pod  dns-9478/dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454 contains '' instead of 'foo.example.com.'
Feb 27 12:43:03.438: INFO: Lookups using dns-9478/dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454 failed for: [jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local]

Feb 27 12:43:08.457: INFO: File wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local from pod  dns-9478/dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454 contains '' instead of 'foo.example.com.'
Feb 27 12:43:08.477: INFO: Lookups using dns-9478/dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454 failed for: [wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local]

Feb 27 12:43:13.465: INFO: DNS probes using dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454 succeeded

STEP: deleting the pod 02/27/23 12:43:13.465
STEP: changing the externalName to bar.example.com 02/27/23 12:43:13.504
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9478.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local; sleep 1; done
 02/27/23 12:43:13.519
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9478.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local; sleep 1; done
 02/27/23 12:43:13.52
STEP: creating a second pod to probe DNS 02/27/23 12:43:13.52
STEP: submitting the pod to kubernetes 02/27/23 12:43:13.52
Feb 27 12:43:13.535: INFO: Waiting up to 15m0s for pod "dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62" in namespace "dns-9478" to be "running"
Feb 27 12:43:13.548: INFO: Pod "dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62": Phase="Pending", Reason="", readiness=false. Elapsed: 13.479504ms
Feb 27 12:43:15.561: INFO: Pod "dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62": Phase="Running", Reason="", readiness=true. Elapsed: 2.02590072s
Feb 27 12:43:15.561: INFO: Pod "dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62" satisfied condition "running"
STEP: retrieving the pod 02/27/23 12:43:15.561
STEP: looking for the results for each expected name from probers 02/27/23 12:43:15.573
Feb 27 12:43:15.592: INFO: File wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local from pod  dns-9478/dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 27 12:43:15.604: INFO: File jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local from pod  dns-9478/dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 27 12:43:15.604: INFO: Lookups using dns-9478/dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62 failed for: [wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local]

Feb 27 12:43:20.635: INFO: DNS probes using dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62 succeeded

STEP: deleting the pod 02/27/23 12:43:20.635
STEP: changing the service to type=ClusterIP 02/27/23 12:43:20.661
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9478.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local; sleep 1; done
 02/27/23 12:43:20.692
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9478.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local; sleep 1; done
 02/27/23 12:43:20.692
STEP: creating a third pod to probe DNS 02/27/23 12:43:20.693
STEP: submitting the pod to kubernetes 02/27/23 12:43:20.725
Feb 27 12:43:20.743: INFO: Waiting up to 15m0s for pod "dns-test-fb1e62d0-0f24-4103-8176-16ca51ffe1d4" in namespace "dns-9478" to be "running"
Feb 27 12:43:20.760: INFO: Pod "dns-test-fb1e62d0-0f24-4103-8176-16ca51ffe1d4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.477446ms
Feb 27 12:43:22.782: INFO: Pod "dns-test-fb1e62d0-0f24-4103-8176-16ca51ffe1d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.038254217s
Feb 27 12:43:22.782: INFO: Pod "dns-test-fb1e62d0-0f24-4103-8176-16ca51ffe1d4" satisfied condition "running"
STEP: retrieving the pod 02/27/23 12:43:22.782
STEP: looking for the results for each expected name from probers 02/27/23 12:43:22.799
Feb 27 12:43:22.829: INFO: DNS probes using dns-test-fb1e62d0-0f24-4103-8176-16ca51ffe1d4 succeeded

STEP: deleting the pod 02/27/23 12:43:22.829
STEP: deleting the test externalName service 02/27/23 12:43:22.853
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 27 12:43:22.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9478" for this suite. 02/27/23 12:43:22.9
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":290,"skipped":5245,"failed":0}
------------------------------
• [SLOW TEST] [21.698 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:43:01.22
    Feb 27 12:43:01.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename dns 02/27/23 12:43:01.222
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:43:01.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:43:01.264
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 02/27/23 12:43:01.273
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9478.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local; sleep 1; done
     02/27/23 12:43:01.296
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9478.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local; sleep 1; done
     02/27/23 12:43:01.297
    STEP: creating a pod to probe DNS 02/27/23 12:43:01.297
    STEP: submitting the pod to kubernetes 02/27/23 12:43:01.297
    Feb 27 12:43:01.327: INFO: Waiting up to 15m0s for pod "dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454" in namespace "dns-9478" to be "running"
    Feb 27 12:43:01.342: INFO: Pod "dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454": Phase="Pending", Reason="", readiness=false. Elapsed: 15.381602ms
    Feb 27 12:43:03.377: INFO: Pod "dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454": Phase="Running", Reason="", readiness=true. Elapsed: 2.050477487s
    Feb 27 12:43:03.377: INFO: Pod "dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 12:43:03.377
    STEP: looking for the results for each expected name from probers 02/27/23 12:43:03.392
    Feb 27 12:43:03.438: INFO: File jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local from pod  dns-9478/dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454 contains '' instead of 'foo.example.com.'
    Feb 27 12:43:03.438: INFO: Lookups using dns-9478/dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454 failed for: [jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local]

    Feb 27 12:43:08.457: INFO: File wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local from pod  dns-9478/dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454 contains '' instead of 'foo.example.com.'
    Feb 27 12:43:08.477: INFO: Lookups using dns-9478/dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454 failed for: [wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local]

    Feb 27 12:43:13.465: INFO: DNS probes using dns-test-cbc026b3-d7f2-4632-baa5-e7c98109a454 succeeded

    STEP: deleting the pod 02/27/23 12:43:13.465
    STEP: changing the externalName to bar.example.com 02/27/23 12:43:13.504
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9478.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local; sleep 1; done
     02/27/23 12:43:13.519
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9478.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local; sleep 1; done
     02/27/23 12:43:13.52
    STEP: creating a second pod to probe DNS 02/27/23 12:43:13.52
    STEP: submitting the pod to kubernetes 02/27/23 12:43:13.52
    Feb 27 12:43:13.535: INFO: Waiting up to 15m0s for pod "dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62" in namespace "dns-9478" to be "running"
    Feb 27 12:43:13.548: INFO: Pod "dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62": Phase="Pending", Reason="", readiness=false. Elapsed: 13.479504ms
    Feb 27 12:43:15.561: INFO: Pod "dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62": Phase="Running", Reason="", readiness=true. Elapsed: 2.02590072s
    Feb 27 12:43:15.561: INFO: Pod "dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 12:43:15.561
    STEP: looking for the results for each expected name from probers 02/27/23 12:43:15.573
    Feb 27 12:43:15.592: INFO: File wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local from pod  dns-9478/dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 27 12:43:15.604: INFO: File jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local from pod  dns-9478/dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 27 12:43:15.604: INFO: Lookups using dns-9478/dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62 failed for: [wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local]

    Feb 27 12:43:20.635: INFO: DNS probes using dns-test-8bb85bd7-0233-4c22-86e7-25810fd72c62 succeeded

    STEP: deleting the pod 02/27/23 12:43:20.635
    STEP: changing the service to type=ClusterIP 02/27/23 12:43:20.661
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9478.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9478.svc.cluster.local; sleep 1; done
     02/27/23 12:43:20.692
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9478.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9478.svc.cluster.local; sleep 1; done
     02/27/23 12:43:20.692
    STEP: creating a third pod to probe DNS 02/27/23 12:43:20.693
    STEP: submitting the pod to kubernetes 02/27/23 12:43:20.725
    Feb 27 12:43:20.743: INFO: Waiting up to 15m0s for pod "dns-test-fb1e62d0-0f24-4103-8176-16ca51ffe1d4" in namespace "dns-9478" to be "running"
    Feb 27 12:43:20.760: INFO: Pod "dns-test-fb1e62d0-0f24-4103-8176-16ca51ffe1d4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.477446ms
    Feb 27 12:43:22.782: INFO: Pod "dns-test-fb1e62d0-0f24-4103-8176-16ca51ffe1d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.038254217s
    Feb 27 12:43:22.782: INFO: Pod "dns-test-fb1e62d0-0f24-4103-8176-16ca51ffe1d4" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 12:43:22.782
    STEP: looking for the results for each expected name from probers 02/27/23 12:43:22.799
    Feb 27 12:43:22.829: INFO: DNS probes using dns-test-fb1e62d0-0f24-4103-8176-16ca51ffe1d4 succeeded

    STEP: deleting the pod 02/27/23 12:43:22.829
    STEP: deleting the test externalName service 02/27/23 12:43:22.853
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 27 12:43:22.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9478" for this suite. 02/27/23 12:43:22.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:43:22.923
Feb 27 12:43:22.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 12:43:22.924
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:43:22.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:43:22.982
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 02/27/23 12:43:23.005
Feb 27 12:43:23.005: INFO: namespace kubectl-4464
Feb 27 12:43:23.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4464 create -f -'
Feb 27 12:43:24.153: INFO: stderr: ""
Feb 27 12:43:24.153: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/27/23 12:43:24.153
Feb 27 12:43:25.161: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 12:43:25.161: INFO: Found 1 / 1
Feb 27 12:43:25.161: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 27 12:43:25.170: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 12:43:25.170: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 27 12:43:25.170: INFO: wait on agnhost-primary startup in kubectl-4464 
Feb 27 12:43:25.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4464 logs agnhost-primary-v8qz7 agnhost-primary'
Feb 27 12:43:25.302: INFO: stderr: ""
Feb 27 12:43:25.302: INFO: stdout: "Paused\n"
STEP: exposing RC 02/27/23 12:43:25.302
Feb 27 12:43:25.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4464 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb 27 12:43:25.449: INFO: stderr: ""
Feb 27 12:43:25.449: INFO: stdout: "service/rm2 exposed\n"
Feb 27 12:43:25.460: INFO: Service rm2 in namespace kubectl-4464 found.
STEP: exposing service 02/27/23 12:43:27.477
Feb 27 12:43:27.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4464 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb 27 12:43:27.606: INFO: stderr: ""
Feb 27 12:43:27.606: INFO: stdout: "service/rm3 exposed\n"
Feb 27 12:43:27.617: INFO: Service rm3 in namespace kubectl-4464 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 12:43:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4464" for this suite. 02/27/23 12:43:29.653
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":291,"skipped":5277,"failed":0}
------------------------------
• [SLOW TEST] [6.745 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:43:22.923
    Feb 27 12:43:22.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 12:43:22.924
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:43:22.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:43:22.982
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 02/27/23 12:43:23.005
    Feb 27 12:43:23.005: INFO: namespace kubectl-4464
    Feb 27 12:43:23.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4464 create -f -'
    Feb 27 12:43:24.153: INFO: stderr: ""
    Feb 27 12:43:24.153: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/27/23 12:43:24.153
    Feb 27 12:43:25.161: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 12:43:25.161: INFO: Found 1 / 1
    Feb 27 12:43:25.161: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 27 12:43:25.170: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 12:43:25.170: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 27 12:43:25.170: INFO: wait on agnhost-primary startup in kubectl-4464 
    Feb 27 12:43:25.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4464 logs agnhost-primary-v8qz7 agnhost-primary'
    Feb 27 12:43:25.302: INFO: stderr: ""
    Feb 27 12:43:25.302: INFO: stdout: "Paused\n"
    STEP: exposing RC 02/27/23 12:43:25.302
    Feb 27 12:43:25.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4464 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Feb 27 12:43:25.449: INFO: stderr: ""
    Feb 27 12:43:25.449: INFO: stdout: "service/rm2 exposed\n"
    Feb 27 12:43:25.460: INFO: Service rm2 in namespace kubectl-4464 found.
    STEP: exposing service 02/27/23 12:43:27.477
    Feb 27 12:43:27.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-4464 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Feb 27 12:43:27.606: INFO: stderr: ""
    Feb 27 12:43:27.606: INFO: stdout: "service/rm3 exposed\n"
    Feb 27 12:43:27.617: INFO: Service rm3 in namespace kubectl-4464 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 12:43:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4464" for this suite. 02/27/23 12:43:29.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:43:29.669
Feb 27 12:43:29.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename subpath 02/27/23 12:43:29.67
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:43:29.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:43:29.704
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 12:43:29.717
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-5nkn 02/27/23 12:43:29.74
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 12:43:29.741
Feb 27 12:43:29.759: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-5nkn" in namespace "subpath-3214" to be "Succeeded or Failed"
Feb 27 12:43:29.768: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Pending", Reason="", readiness=false. Elapsed: 9.027686ms
Feb 27 12:43:31.776: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 2.017450012s
Feb 27 12:43:33.776: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 4.017408332s
Feb 27 12:43:36.295: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 6.536476259s
Feb 27 12:43:37.776: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 8.017127135s
Feb 27 12:43:39.776: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 10.017277817s
Feb 27 12:43:41.779: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 12.02012221s
Feb 27 12:43:43.783: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 14.024324298s
Feb 27 12:43:45.778: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 16.018788061s
Feb 27 12:43:47.780: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 18.02111567s
Feb 27 12:43:49.777: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 20.018439205s
Feb 27 12:43:51.777: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=false. Elapsed: 22.018422075s
Feb 27 12:43:53.777: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018270641s
STEP: Saw pod success 02/27/23 12:43:53.777
Feb 27 12:43:53.778: INFO: Pod "pod-subpath-test-secret-5nkn" satisfied condition "Succeeded or Failed"
Feb 27 12:43:53.784: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-subpath-test-secret-5nkn container test-container-subpath-secret-5nkn: <nil>
STEP: delete the pod 02/27/23 12:43:53.8
Feb 27 12:43:53.822: INFO: Waiting for pod pod-subpath-test-secret-5nkn to disappear
Feb 27 12:43:53.833: INFO: Pod pod-subpath-test-secret-5nkn no longer exists
STEP: Deleting pod pod-subpath-test-secret-5nkn 02/27/23 12:43:53.833
Feb 27 12:43:53.833: INFO: Deleting pod "pod-subpath-test-secret-5nkn" in namespace "subpath-3214"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 27 12:43:53.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3214" for this suite. 02/27/23 12:43:53.853
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":292,"skipped":5283,"failed":0}
------------------------------
• [SLOW TEST] [24.199 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:43:29.669
    Feb 27 12:43:29.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename subpath 02/27/23 12:43:29.67
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:43:29.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:43:29.704
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 12:43:29.717
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-5nkn 02/27/23 12:43:29.74
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 12:43:29.741
    Feb 27 12:43:29.759: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-5nkn" in namespace "subpath-3214" to be "Succeeded or Failed"
    Feb 27 12:43:29.768: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Pending", Reason="", readiness=false. Elapsed: 9.027686ms
    Feb 27 12:43:31.776: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 2.017450012s
    Feb 27 12:43:33.776: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 4.017408332s
    Feb 27 12:43:36.295: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 6.536476259s
    Feb 27 12:43:37.776: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 8.017127135s
    Feb 27 12:43:39.776: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 10.017277817s
    Feb 27 12:43:41.779: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 12.02012221s
    Feb 27 12:43:43.783: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 14.024324298s
    Feb 27 12:43:45.778: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 16.018788061s
    Feb 27 12:43:47.780: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 18.02111567s
    Feb 27 12:43:49.777: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=true. Elapsed: 20.018439205s
    Feb 27 12:43:51.777: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Running", Reason="", readiness=false. Elapsed: 22.018422075s
    Feb 27 12:43:53.777: INFO: Pod "pod-subpath-test-secret-5nkn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018270641s
    STEP: Saw pod success 02/27/23 12:43:53.777
    Feb 27 12:43:53.778: INFO: Pod "pod-subpath-test-secret-5nkn" satisfied condition "Succeeded or Failed"
    Feb 27 12:43:53.784: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-subpath-test-secret-5nkn container test-container-subpath-secret-5nkn: <nil>
    STEP: delete the pod 02/27/23 12:43:53.8
    Feb 27 12:43:53.822: INFO: Waiting for pod pod-subpath-test-secret-5nkn to disappear
    Feb 27 12:43:53.833: INFO: Pod pod-subpath-test-secret-5nkn no longer exists
    STEP: Deleting pod pod-subpath-test-secret-5nkn 02/27/23 12:43:53.833
    Feb 27 12:43:53.833: INFO: Deleting pod "pod-subpath-test-secret-5nkn" in namespace "subpath-3214"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 27 12:43:53.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3214" for this suite. 02/27/23 12:43:53.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:43:53.873
Feb 27 12:43:53.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename deployment 02/27/23 12:43:53.875
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:43:53.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:43:53.918
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 02/27/23 12:43:53.947
Feb 27 12:43:53.947: INFO: Creating simple deployment test-deployment-tzccn
Feb 27 12:43:53.987: INFO: deployment "test-deployment-tzccn" doesn't have the required revision set
STEP: Getting /status 02/27/23 12:43:56.02
Feb 27 12:43:56.028: INFO: Deployment test-deployment-tzccn has Conditions: [{Available True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tzccn-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 02/27/23 12:43:56.028
Feb 27 12:43:56.048: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 43, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 43, 55, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 43, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 43, 53, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-tzccn-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 02/27/23 12:43:56.048
Feb 27 12:43:56.056: INFO: Observed &Deployment event: ADDED
Feb 27 12:43:56.056: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tzccn-777898ffcc"}
Feb 27 12:43:56.056: INFO: Observed &Deployment event: MODIFIED
Feb 27 12:43:56.056: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tzccn-777898ffcc"}
Feb 27 12:43:56.056: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 27 12:43:56.056: INFO: Observed &Deployment event: MODIFIED
Feb 27 12:43:56.056: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 27 12:43:56.056: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:54 +0000 UTC 2023-02-27 12:43:53 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tzccn-777898ffcc" is progressing.}
Feb 27 12:43:56.057: INFO: Observed &Deployment event: MODIFIED
Feb 27 12:43:56.057: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 27 12:43:56.057: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tzccn-777898ffcc" has successfully progressed.}
Feb 27 12:43:56.057: INFO: Observed &Deployment event: MODIFIED
Feb 27 12:43:56.057: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 27 12:43:56.057: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tzccn-777898ffcc" has successfully progressed.}
Feb 27 12:43:56.057: INFO: Found Deployment test-deployment-tzccn in namespace deployment-8004 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 12:43:56.057: INFO: Deployment test-deployment-tzccn has an updated status
STEP: patching the Statefulset Status 02/27/23 12:43:56.057
Feb 27 12:43:56.057: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 27 12:43:56.084: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 02/27/23 12:43:56.084
Feb 27 12:43:56.100: INFO: Observed &Deployment event: ADDED
Feb 27 12:43:56.100: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tzccn-777898ffcc"}
Feb 27 12:43:56.100: INFO: Observed &Deployment event: MODIFIED
Feb 27 12:43:56.101: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tzccn-777898ffcc"}
Feb 27 12:43:56.101: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 27 12:43:56.101: INFO: Observed &Deployment event: MODIFIED
Feb 27 12:43:56.101: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 27 12:43:56.102: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:54 +0000 UTC 2023-02-27 12:43:53 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tzccn-777898ffcc" is progressing.}
Feb 27 12:43:56.102: INFO: Observed &Deployment event: MODIFIED
Feb 27 12:43:56.102: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 27 12:43:56.102: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tzccn-777898ffcc" has successfully progressed.}
Feb 27 12:43:56.103: INFO: Observed &Deployment event: MODIFIED
Feb 27 12:43:56.103: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 27 12:43:56.103: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tzccn-777898ffcc" has successfully progressed.}
Feb 27 12:43:56.104: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 12:43:56.104: INFO: Observed &Deployment event: MODIFIED
Feb 27 12:43:56.105: INFO: Found deployment test-deployment-tzccn in namespace deployment-8004 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Feb 27 12:43:56.106: INFO: Deployment test-deployment-tzccn has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 12:43:56.127: INFO: Deployment "test-deployment-tzccn":
&Deployment{ObjectMeta:{test-deployment-tzccn  deployment-8004  7ef9678d-f914-422a-9b64-f60c5977a9e9 96491 1 2023-02-27 12:43:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-27 12:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-27 12:43:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-27 12:43:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006fba6a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-tzccn-777898ffcc",LastUpdateTime:2023-02-27 12:43:56 +0000 UTC,LastTransitionTime:2023-02-27 12:43:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 12:43:56.138: INFO: New ReplicaSet "test-deployment-tzccn-777898ffcc" of Deployment "test-deployment-tzccn":
&ReplicaSet{ObjectMeta:{test-deployment-tzccn-777898ffcc  deployment-8004  2f89ecaa-b7bd-4768-a7a8-cc02dfc12f3d 96483 1 2023-02-27 12:43:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-tzccn 7ef9678d-f914-422a-9b64-f60c5977a9e9 0xc006fbaa90 0xc006fbaa91}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ef9678d-f914-422a-9b64-f60c5977a9e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:43:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006fbab38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 12:43:56.151: INFO: Pod "test-deployment-tzccn-777898ffcc-9df8v" is available:
&Pod{ObjectMeta:{test-deployment-tzccn-777898ffcc-9df8v test-deployment-tzccn-777898ffcc- deployment-8004  080f78ca-6f9e-457f-8433-69a37ac3102a 96482 0 2023-02-27 12:43:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:533d59d1b050255a06a4205bedc6ce941abeb935bea865753cd784895bc61cd1 cni.projectcalico.org/podIP:172.25.2.248/32 cni.projectcalico.org/podIPs:172.25.2.248/32] [{apps/v1 ReplicaSet test-deployment-tzccn-777898ffcc 2f89ecaa-b7bd-4768-a7a8-cc02dfc12f3d 0xc006fbaf00 0xc006fbaf01}] [] [{kube-controller-manager Update v1 2023-02-27 12:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f89ecaa-b7bd-4768-a7a8-cc02dfc12f3d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:43:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:43:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9bw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9bw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:43:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:43:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:43:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:43:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.248,StartTime:2023-02-27 12:43:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:43:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c404695fee089d4c6b70fdb5e6a50883c2733936467b8e43544ae809e0f9aefa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 27 12:43:56.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8004" for this suite. 02/27/23 12:43:56.161
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":293,"skipped":5298,"failed":0}
------------------------------
• [2.322 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:43:53.873
    Feb 27 12:43:53.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename deployment 02/27/23 12:43:53.875
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:43:53.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:43:53.918
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 02/27/23 12:43:53.947
    Feb 27 12:43:53.947: INFO: Creating simple deployment test-deployment-tzccn
    Feb 27 12:43:53.987: INFO: deployment "test-deployment-tzccn" doesn't have the required revision set
    STEP: Getting /status 02/27/23 12:43:56.02
    Feb 27 12:43:56.028: INFO: Deployment test-deployment-tzccn has Conditions: [{Available True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tzccn-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 02/27/23 12:43:56.028
    Feb 27 12:43:56.048: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 43, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 43, 55, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 43, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 43, 53, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-tzccn-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 02/27/23 12:43:56.048
    Feb 27 12:43:56.056: INFO: Observed &Deployment event: ADDED
    Feb 27 12:43:56.056: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tzccn-777898ffcc"}
    Feb 27 12:43:56.056: INFO: Observed &Deployment event: MODIFIED
    Feb 27 12:43:56.056: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tzccn-777898ffcc"}
    Feb 27 12:43:56.056: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 27 12:43:56.056: INFO: Observed &Deployment event: MODIFIED
    Feb 27 12:43:56.056: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 27 12:43:56.056: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:54 +0000 UTC 2023-02-27 12:43:53 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tzccn-777898ffcc" is progressing.}
    Feb 27 12:43:56.057: INFO: Observed &Deployment event: MODIFIED
    Feb 27 12:43:56.057: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 27 12:43:56.057: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tzccn-777898ffcc" has successfully progressed.}
    Feb 27 12:43:56.057: INFO: Observed &Deployment event: MODIFIED
    Feb 27 12:43:56.057: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 27 12:43:56.057: INFO: Observed Deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tzccn-777898ffcc" has successfully progressed.}
    Feb 27 12:43:56.057: INFO: Found Deployment test-deployment-tzccn in namespace deployment-8004 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 12:43:56.057: INFO: Deployment test-deployment-tzccn has an updated status
    STEP: patching the Statefulset Status 02/27/23 12:43:56.057
    Feb 27 12:43:56.057: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 27 12:43:56.084: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 02/27/23 12:43:56.084
    Feb 27 12:43:56.100: INFO: Observed &Deployment event: ADDED
    Feb 27 12:43:56.100: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tzccn-777898ffcc"}
    Feb 27 12:43:56.100: INFO: Observed &Deployment event: MODIFIED
    Feb 27 12:43:56.101: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tzccn-777898ffcc"}
    Feb 27 12:43:56.101: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 27 12:43:56.101: INFO: Observed &Deployment event: MODIFIED
    Feb 27 12:43:56.101: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 12:43:53 +0000 UTC 2023-02-27 12:43:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 27 12:43:56.102: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:54 +0000 UTC 2023-02-27 12:43:53 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tzccn-777898ffcc" is progressing.}
    Feb 27 12:43:56.102: INFO: Observed &Deployment event: MODIFIED
    Feb 27 12:43:56.102: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 27 12:43:56.102: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tzccn-777898ffcc" has successfully progressed.}
    Feb 27 12:43:56.103: INFO: Observed &Deployment event: MODIFIED
    Feb 27 12:43:56.103: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 27 12:43:56.103: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 12:43:55 +0000 UTC 2023-02-27 12:43:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tzccn-777898ffcc" has successfully progressed.}
    Feb 27 12:43:56.104: INFO: Observed deployment test-deployment-tzccn in namespace deployment-8004 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 12:43:56.104: INFO: Observed &Deployment event: MODIFIED
    Feb 27 12:43:56.105: INFO: Found deployment test-deployment-tzccn in namespace deployment-8004 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Feb 27 12:43:56.106: INFO: Deployment test-deployment-tzccn has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 12:43:56.127: INFO: Deployment "test-deployment-tzccn":
    &Deployment{ObjectMeta:{test-deployment-tzccn  deployment-8004  7ef9678d-f914-422a-9b64-f60c5977a9e9 96491 1 2023-02-27 12:43:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-27 12:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-27 12:43:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-27 12:43:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006fba6a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-tzccn-777898ffcc",LastUpdateTime:2023-02-27 12:43:56 +0000 UTC,LastTransitionTime:2023-02-27 12:43:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 12:43:56.138: INFO: New ReplicaSet "test-deployment-tzccn-777898ffcc" of Deployment "test-deployment-tzccn":
    &ReplicaSet{ObjectMeta:{test-deployment-tzccn-777898ffcc  deployment-8004  2f89ecaa-b7bd-4768-a7a8-cc02dfc12f3d 96483 1 2023-02-27 12:43:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-tzccn 7ef9678d-f914-422a-9b64-f60c5977a9e9 0xc006fbaa90 0xc006fbaa91}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ef9678d-f914-422a-9b64-f60c5977a9e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:43:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006fbab38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 12:43:56.151: INFO: Pod "test-deployment-tzccn-777898ffcc-9df8v" is available:
    &Pod{ObjectMeta:{test-deployment-tzccn-777898ffcc-9df8v test-deployment-tzccn-777898ffcc- deployment-8004  080f78ca-6f9e-457f-8433-69a37ac3102a 96482 0 2023-02-27 12:43:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:533d59d1b050255a06a4205bedc6ce941abeb935bea865753cd784895bc61cd1 cni.projectcalico.org/podIP:172.25.2.248/32 cni.projectcalico.org/podIPs:172.25.2.248/32] [{apps/v1 ReplicaSet test-deployment-tzccn-777898ffcc 2f89ecaa-b7bd-4768-a7a8-cc02dfc12f3d 0xc006fbaf00 0xc006fbaf01}] [] [{kube-controller-manager Update v1 2023-02-27 12:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f89ecaa-b7bd-4768-a7a8-cc02dfc12f3d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-27 12:43:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-27 12:43:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9bw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9bw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:43:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:43:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:43:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:43:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.248,StartTime:2023-02-27 12:43:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:43:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c404695fee089d4c6b70fdb5e6a50883c2733936467b8e43544ae809e0f9aefa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 27 12:43:56.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8004" for this suite. 02/27/23 12:43:56.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:43:56.195
Feb 27 12:43:56.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 12:43:56.196
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:43:56.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:43:56.245
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 02/27/23 12:43:56.258
Feb 27 12:43:56.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: mark a version not serverd 02/27/23 12:44:07.752
STEP: check the unserved version gets removed 02/27/23 12:44:07.789
STEP: check the other version is not changed 02/27/23 12:44:12.308
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:44:20.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6384" for this suite. 02/27/23 12:44:20.161
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":294,"skipped":5303,"failed":0}
------------------------------
• [SLOW TEST] [23.980 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:43:56.195
    Feb 27 12:43:56.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 12:43:56.196
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:43:56.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:43:56.245
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 02/27/23 12:43:56.258
    Feb 27 12:43:56.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: mark a version not serverd 02/27/23 12:44:07.752
    STEP: check the unserved version gets removed 02/27/23 12:44:07.789
    STEP: check the other version is not changed 02/27/23 12:44:12.308
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:44:20.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6384" for this suite. 02/27/23 12:44:20.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:44:20.177
Feb 27 12:44:20.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename job 02/27/23 12:44:20.179
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:44:20.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:44:20.229
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 02/27/23 12:44:20.24
STEP: Ensuring job reaches completions 02/27/23 12:44:20.25
STEP: Ensuring pods with index for job exist 02/27/23 12:44:30.265
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 27 12:44:30.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3903" for this suite. 02/27/23 12:44:30.289
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":295,"skipped":5316,"failed":0}
------------------------------
• [SLOW TEST] [10.129 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:44:20.177
    Feb 27 12:44:20.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename job 02/27/23 12:44:20.179
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:44:20.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:44:20.229
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 02/27/23 12:44:20.24
    STEP: Ensuring job reaches completions 02/27/23 12:44:20.25
    STEP: Ensuring pods with index for job exist 02/27/23 12:44:30.265
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 27 12:44:30.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3903" for this suite. 02/27/23 12:44:30.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:44:30.336
Feb 27 12:44:30.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pod-network-test 02/27/23 12:44:30.339
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:44:30.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:44:30.389
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-5024 02/27/23 12:44:30.4
STEP: creating a selector 02/27/23 12:44:30.4
STEP: Creating the service pods in kubernetes 02/27/23 12:44:30.4
Feb 27 12:44:30.400: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 27 12:44:30.509: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5024" to be "running and ready"
Feb 27 12:44:30.519: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.489058ms
Feb 27 12:44:30.520: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:44:32.528: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.018186124s
Feb 27 12:44:32.528: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:44:34.531: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.021145018s
Feb 27 12:44:34.531: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:44:36.528: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017865176s
Feb 27 12:44:36.528: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:44:38.529: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018686225s
Feb 27 12:44:38.529: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:44:40.528: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01857636s
Feb 27 12:44:40.529: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:44:42.541: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.030734763s
Feb 27 12:44:42.541: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:44:44.527: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017404395s
Feb 27 12:44:44.527: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:44:46.529: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019295329s
Feb 27 12:44:46.529: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:44:48.529: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018678952s
Feb 27 12:44:48.529: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:44:50.534: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.023651308s
Feb 27 12:44:50.534: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 12:44:52.531: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.021181981s
Feb 27 12:44:52.531: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 27 12:44:52.531: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 27 12:44:52.546: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5024" to be "running and ready"
Feb 27 12:44:52.559: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.797961ms
Feb 27 12:44:52.560: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 27 12:44:52.560: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 27 12:44:52.588: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5024" to be "running and ready"
Feb 27 12:44:52.597: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.029807ms
Feb 27 12:44:52.597: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 27 12:44:52.597: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/27/23 12:44:52.604
Feb 27 12:44:52.626: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5024" to be "running"
Feb 27 12:44:52.635: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.107482ms
Feb 27 12:44:54.642: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015809219s
Feb 27 12:44:54.642: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 27 12:44:54.651: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5024" to be "running"
Feb 27 12:44:54.658: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.543691ms
Feb 27 12:44:54.658: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 27 12:44:54.667: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 27 12:44:54.667: INFO: Going to poll 172.25.1.220 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 27 12:44:54.679: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.220:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:44:54.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:44:54.680: INFO: ExecWithOptions: Clientset creation
Feb 27 12:44:54.680: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-5024/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.1.220%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 12:44:54.820: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 27 12:44:54.820: INFO: Going to poll 172.25.2.252 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 27 12:44:54.828: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.252:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:44:54.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:44:54.829: INFO: ExecWithOptions: Clientset creation
Feb 27 12:44:54.829: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-5024/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.2.252%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 12:44:55.040: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 27 12:44:55.041: INFO: Going to poll 172.25.0.98 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 27 12:44:55.052: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.98:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:44:55.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:44:55.053: INFO: ExecWithOptions: Clientset creation
Feb 27 12:44:55.053: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-5024/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.0.98%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 12:44:55.202: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 27 12:44:55.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5024" for this suite. 02/27/23 12:44:55.213
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":296,"skipped":5335,"failed":0}
------------------------------
• [SLOW TEST] [24.890 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:44:30.336
    Feb 27 12:44:30.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pod-network-test 02/27/23 12:44:30.339
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:44:30.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:44:30.389
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-5024 02/27/23 12:44:30.4
    STEP: creating a selector 02/27/23 12:44:30.4
    STEP: Creating the service pods in kubernetes 02/27/23 12:44:30.4
    Feb 27 12:44:30.400: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 27 12:44:30.509: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5024" to be "running and ready"
    Feb 27 12:44:30.519: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.489058ms
    Feb 27 12:44:30.520: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:44:32.528: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.018186124s
    Feb 27 12:44:32.528: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:44:34.531: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.021145018s
    Feb 27 12:44:34.531: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:44:36.528: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017865176s
    Feb 27 12:44:36.528: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:44:38.529: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018686225s
    Feb 27 12:44:38.529: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:44:40.528: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01857636s
    Feb 27 12:44:40.529: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:44:42.541: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.030734763s
    Feb 27 12:44:42.541: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:44:44.527: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.017404395s
    Feb 27 12:44:44.527: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:44:46.529: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019295329s
    Feb 27 12:44:46.529: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:44:48.529: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018678952s
    Feb 27 12:44:48.529: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:44:50.534: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.023651308s
    Feb 27 12:44:50.534: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 12:44:52.531: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.021181981s
    Feb 27 12:44:52.531: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 27 12:44:52.531: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 27 12:44:52.546: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5024" to be "running and ready"
    Feb 27 12:44:52.559: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.797961ms
    Feb 27 12:44:52.560: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 27 12:44:52.560: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 27 12:44:52.588: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5024" to be "running and ready"
    Feb 27 12:44:52.597: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.029807ms
    Feb 27 12:44:52.597: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 27 12:44:52.597: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/27/23 12:44:52.604
    Feb 27 12:44:52.626: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5024" to be "running"
    Feb 27 12:44:52.635: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.107482ms
    Feb 27 12:44:54.642: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015809219s
    Feb 27 12:44:54.642: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 27 12:44:54.651: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5024" to be "running"
    Feb 27 12:44:54.658: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.543691ms
    Feb 27 12:44:54.658: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 27 12:44:54.667: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 27 12:44:54.667: INFO: Going to poll 172.25.1.220 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 27 12:44:54.679: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.220:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:44:54.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:44:54.680: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:44:54.680: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-5024/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.1.220%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 12:44:54.820: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 27 12:44:54.820: INFO: Going to poll 172.25.2.252 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 27 12:44:54.828: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.252:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:44:54.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:44:54.829: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:44:54.829: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-5024/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.2.252%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 12:44:55.040: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 27 12:44:55.041: INFO: Going to poll 172.25.0.98 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 27 12:44:55.052: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.98:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:44:55.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:44:55.053: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:44:55.053: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-5024/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.0.98%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 12:44:55.202: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 27 12:44:55.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5024" for this suite. 02/27/23 12:44:55.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:44:55.227
Feb 27 12:44:55.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename var-expansion 02/27/23 12:44:55.232
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:44:55.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:44:55.268
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 02/27/23 12:44:55.277
STEP: waiting for pod running 02/27/23 12:44:55.297
Feb 27 12:44:55.297: INFO: Waiting up to 2m0s for pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46" in namespace "var-expansion-1773" to be "running"
Feb 27 12:44:55.309: INFO: Pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46": Phase="Pending", Reason="", readiness=false. Elapsed: 11.615757ms
Feb 27 12:44:57.316: INFO: Pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46": Phase="Running", Reason="", readiness=true. Elapsed: 2.019439244s
Feb 27 12:44:57.317: INFO: Pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46" satisfied condition "running"
STEP: creating a file in subpath 02/27/23 12:44:57.317
Feb 27 12:44:57.324: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1773 PodName:var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:44:57.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:44:57.325: INFO: ExecWithOptions: Clientset creation
Feb 27 12:44:57.325: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-1773/pods/var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 02/27/23 12:44:57.503
Feb 27 12:44:57.519: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1773 PodName:var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:44:57.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:44:57.520: INFO: ExecWithOptions: Clientset creation
Feb 27 12:44:57.520: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-1773/pods/var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 02/27/23 12:44:57.641
Feb 27 12:44:58.168: INFO: Successfully updated pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46"
STEP: waiting for annotated pod running 02/27/23 12:44:58.168
Feb 27 12:44:58.168: INFO: Waiting up to 2m0s for pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46" in namespace "var-expansion-1773" to be "running"
Feb 27 12:44:58.175: INFO: Pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46": Phase="Running", Reason="", readiness=true. Elapsed: 7.497591ms
Feb 27 12:44:58.175: INFO: Pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46" satisfied condition "running"
STEP: deleting the pod gracefully 02/27/23 12:44:58.175
Feb 27 12:44:58.176: INFO: Deleting pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46" in namespace "var-expansion-1773"
Feb 27 12:44:58.204: INFO: Wait up to 5m0s for pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 27 12:45:32.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1773" for this suite. 02/27/23 12:45:32.23
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":297,"skipped":5342,"failed":0}
------------------------------
• [SLOW TEST] [37.015 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:44:55.227
    Feb 27 12:44:55.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename var-expansion 02/27/23 12:44:55.232
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:44:55.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:44:55.268
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 02/27/23 12:44:55.277
    STEP: waiting for pod running 02/27/23 12:44:55.297
    Feb 27 12:44:55.297: INFO: Waiting up to 2m0s for pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46" in namespace "var-expansion-1773" to be "running"
    Feb 27 12:44:55.309: INFO: Pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46": Phase="Pending", Reason="", readiness=false. Elapsed: 11.615757ms
    Feb 27 12:44:57.316: INFO: Pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46": Phase="Running", Reason="", readiness=true. Elapsed: 2.019439244s
    Feb 27 12:44:57.317: INFO: Pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46" satisfied condition "running"
    STEP: creating a file in subpath 02/27/23 12:44:57.317
    Feb 27 12:44:57.324: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1773 PodName:var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:44:57.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:44:57.325: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:44:57.325: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-1773/pods/var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 02/27/23 12:44:57.503
    Feb 27 12:44:57.519: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1773 PodName:var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:44:57.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:44:57.520: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:44:57.520: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-1773/pods/var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 02/27/23 12:44:57.641
    Feb 27 12:44:58.168: INFO: Successfully updated pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46"
    STEP: waiting for annotated pod running 02/27/23 12:44:58.168
    Feb 27 12:44:58.168: INFO: Waiting up to 2m0s for pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46" in namespace "var-expansion-1773" to be "running"
    Feb 27 12:44:58.175: INFO: Pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46": Phase="Running", Reason="", readiness=true. Elapsed: 7.497591ms
    Feb 27 12:44:58.175: INFO: Pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46" satisfied condition "running"
    STEP: deleting the pod gracefully 02/27/23 12:44:58.175
    Feb 27 12:44:58.176: INFO: Deleting pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46" in namespace "var-expansion-1773"
    Feb 27 12:44:58.204: INFO: Wait up to 5m0s for pod "var-expansion-7b1d61fc-3cad-4d54-a729-5fad0f6cdc46" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 27 12:45:32.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1773" for this suite. 02/27/23 12:45:32.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:45:32.246
Feb 27 12:45:32.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 12:45:32.247
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:45:32.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:45:32.29
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 12:45:32.338
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 12:45:33.199
STEP: Deploying the webhook pod 02/27/23 12:45:33.227
STEP: Wait for the deployment to be ready 02/27/23 12:45:33.265
Feb 27 12:45:33.292: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 12:45:35.329
STEP: Verifying the service has paired with the endpoint 02/27/23 12:45:35.347
Feb 27 12:45:36.348: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 02/27/23 12:45:36.356
STEP: Creating a custom resource definition that should be denied by the webhook 02/27/23 12:45:36.404
Feb 27 12:45:36.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:45:36.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5363" for this suite. 02/27/23 12:45:36.462
STEP: Destroying namespace "webhook-5363-markers" for this suite. 02/27/23 12:45:36.477
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":298,"skipped":5368,"failed":0}
------------------------------
• [4.310 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:45:32.246
    Feb 27 12:45:32.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 12:45:32.247
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:45:32.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:45:32.29
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 12:45:32.338
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 12:45:33.199
    STEP: Deploying the webhook pod 02/27/23 12:45:33.227
    STEP: Wait for the deployment to be ready 02/27/23 12:45:33.265
    Feb 27 12:45:33.292: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 12:45:35.329
    STEP: Verifying the service has paired with the endpoint 02/27/23 12:45:35.347
    Feb 27 12:45:36.348: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 02/27/23 12:45:36.356
    STEP: Creating a custom resource definition that should be denied by the webhook 02/27/23 12:45:36.404
    Feb 27 12:45:36.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:45:36.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5363" for this suite. 02/27/23 12:45:36.462
    STEP: Destroying namespace "webhook-5363-markers" for this suite. 02/27/23 12:45:36.477
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:45:36.56
Feb 27 12:45:36.560: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename sched-preemption 02/27/23 12:45:36.561
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:45:36.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:45:36.628
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 27 12:45:36.680: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 12:46:36.783: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:46:36.796
Feb 27 12:46:36.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename sched-preemption-path 02/27/23 12:46:36.797
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:46:36.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:46:36.846
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Feb 27 12:46:36.912: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Feb 27 12:46:36.924: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Feb 27 12:46:37.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1155" for this suite. 02/27/23 12:46:37.022
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 27 12:46:37.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9960" for this suite. 02/27/23 12:46:37.131
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":299,"skipped":5376,"failed":0}
------------------------------
• [SLOW TEST] [60.728 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:45:36.56
    Feb 27 12:45:36.560: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename sched-preemption 02/27/23 12:45:36.561
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:45:36.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:45:36.628
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 27 12:45:36.680: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 12:46:36.783: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:46:36.796
    Feb 27 12:46:36.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename sched-preemption-path 02/27/23 12:46:36.797
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:46:36.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:46:36.846
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Feb 27 12:46:36.912: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Feb 27 12:46:36.924: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Feb 27 12:46:37.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-1155" for this suite. 02/27/23 12:46:37.022
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 12:46:37.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9960" for this suite. 02/27/23 12:46:37.131
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:46:37.306
Feb 27 12:46:37.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 12:46:37.307
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:46:37.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:46:37.38
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-2970372f-0f7f-44ba-b2e0-64d7337443f6 02/27/23 12:46:37.396
STEP: Creating a pod to test consume configMaps 02/27/23 12:46:37.408
Feb 27 12:46:37.441: INFO: Waiting up to 5m0s for pod "pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862" in namespace "configmap-7856" to be "Succeeded or Failed"
Feb 27 12:46:37.469: INFO: Pod "pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862": Phase="Pending", Reason="", readiness=false. Elapsed: 27.28333ms
Feb 27 12:46:39.477: INFO: Pod "pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035557296s
Feb 27 12:46:41.545: INFO: Pod "pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103604187s
Feb 27 12:46:43.485: INFO: Pod "pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043985378s
STEP: Saw pod success 02/27/23 12:46:43.485
Feb 27 12:46:43.486: INFO: Pod "pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862" satisfied condition "Succeeded or Failed"
Feb 27 12:46:43.503: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 12:46:43.565
Feb 27 12:46:43.589: INFO: Waiting for pod pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862 to disappear
Feb 27 12:46:43.616: INFO: Pod pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 12:46:43.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7856" for this suite. 02/27/23 12:46:43.636
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":300,"skipped":5393,"failed":0}
------------------------------
• [SLOW TEST] [6.362 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:46:37.306
    Feb 27 12:46:37.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 12:46:37.307
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:46:37.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:46:37.38
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-2970372f-0f7f-44ba-b2e0-64d7337443f6 02/27/23 12:46:37.396
    STEP: Creating a pod to test consume configMaps 02/27/23 12:46:37.408
    Feb 27 12:46:37.441: INFO: Waiting up to 5m0s for pod "pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862" in namespace "configmap-7856" to be "Succeeded or Failed"
    Feb 27 12:46:37.469: INFO: Pod "pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862": Phase="Pending", Reason="", readiness=false. Elapsed: 27.28333ms
    Feb 27 12:46:39.477: INFO: Pod "pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035557296s
    Feb 27 12:46:41.545: INFO: Pod "pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103604187s
    Feb 27 12:46:43.485: INFO: Pod "pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043985378s
    STEP: Saw pod success 02/27/23 12:46:43.485
    Feb 27 12:46:43.486: INFO: Pod "pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862" satisfied condition "Succeeded or Failed"
    Feb 27 12:46:43.503: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 12:46:43.565
    Feb 27 12:46:43.589: INFO: Waiting for pod pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862 to disappear
    Feb 27 12:46:43.616: INFO: Pod pod-configmaps-fab42b52-5e9e-44a8-b7b6-02820d05d862 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 12:46:43.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7856" for this suite. 02/27/23 12:46:43.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:46:43.672
Feb 27 12:46:43.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename security-context-test 02/27/23 12:46:43.674
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:46:43.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:46:43.717
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Feb 27 12:46:43.748: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2e9eed1c-015e-460d-b373-4fe3efcf2322" in namespace "security-context-test-7378" to be "Succeeded or Failed"
Feb 27 12:46:43.767: INFO: Pod "busybox-readonly-false-2e9eed1c-015e-460d-b373-4fe3efcf2322": Phase="Pending", Reason="", readiness=false. Elapsed: 19.238798ms
Feb 27 12:46:45.803: INFO: Pod "busybox-readonly-false-2e9eed1c-015e-460d-b373-4fe3efcf2322": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054874508s
Feb 27 12:46:47.777: INFO: Pod "busybox-readonly-false-2e9eed1c-015e-460d-b373-4fe3efcf2322": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029642068s
Feb 27 12:46:47.777: INFO: Pod "busybox-readonly-false-2e9eed1c-015e-460d-b373-4fe3efcf2322" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 27 12:46:47.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7378" for this suite. 02/27/23 12:46:47.802
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":301,"skipped":5412,"failed":0}
------------------------------
• [4.147 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:46:43.672
    Feb 27 12:46:43.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename security-context-test 02/27/23 12:46:43.674
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:46:43.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:46:43.717
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Feb 27 12:46:43.748: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2e9eed1c-015e-460d-b373-4fe3efcf2322" in namespace "security-context-test-7378" to be "Succeeded or Failed"
    Feb 27 12:46:43.767: INFO: Pod "busybox-readonly-false-2e9eed1c-015e-460d-b373-4fe3efcf2322": Phase="Pending", Reason="", readiness=false. Elapsed: 19.238798ms
    Feb 27 12:46:45.803: INFO: Pod "busybox-readonly-false-2e9eed1c-015e-460d-b373-4fe3efcf2322": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054874508s
    Feb 27 12:46:47.777: INFO: Pod "busybox-readonly-false-2e9eed1c-015e-460d-b373-4fe3efcf2322": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029642068s
    Feb 27 12:46:47.777: INFO: Pod "busybox-readonly-false-2e9eed1c-015e-460d-b373-4fe3efcf2322" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 27 12:46:47.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7378" for this suite. 02/27/23 12:46:47.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:46:47.841
Feb 27 12:46:47.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 12:46:47.842
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:46:47.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:46:47.934
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/27/23 12:46:47.971
Feb 27 12:46:47.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:46:53.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:47:12.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6153" for this suite. 02/27/23 12:47:12.4
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":302,"skipped":5470,"failed":0}
------------------------------
• [SLOW TEST] [24.572 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:46:47.841
    Feb 27 12:46:47.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 12:46:47.842
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:46:47.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:46:47.934
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/27/23 12:46:47.971
    Feb 27 12:46:47.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:46:53.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:47:12.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6153" for this suite. 02/27/23 12:47:12.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:47:12.417
Feb 27 12:47:12.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename events 02/27/23 12:47:12.419
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:12.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:12.456
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 02/27/23 12:47:12.464
STEP: listing events in all namespaces 02/27/23 12:47:12.475
STEP: listing events in test namespace 02/27/23 12:47:12.488
STEP: listing events with field selection filtering on source 02/27/23 12:47:12.494
STEP: listing events with field selection filtering on reportingController 02/27/23 12:47:12.505
STEP: getting the test event 02/27/23 12:47:12.513
STEP: patching the test event 02/27/23 12:47:12.521
STEP: getting the test event 02/27/23 12:47:12.538
STEP: updating the test event 02/27/23 12:47:12.547
STEP: getting the test event 02/27/23 12:47:12.565
STEP: deleting the test event 02/27/23 12:47:12.572
STEP: listing events in all namespaces 02/27/23 12:47:12.59
STEP: listing events in test namespace 02/27/23 12:47:12.601
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Feb 27 12:47:12.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7718" for this suite. 02/27/23 12:47:12.623
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":303,"skipped":5490,"failed":0}
------------------------------
• [0.218 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:47:12.417
    Feb 27 12:47:12.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename events 02/27/23 12:47:12.419
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:12.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:12.456
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 02/27/23 12:47:12.464
    STEP: listing events in all namespaces 02/27/23 12:47:12.475
    STEP: listing events in test namespace 02/27/23 12:47:12.488
    STEP: listing events with field selection filtering on source 02/27/23 12:47:12.494
    STEP: listing events with field selection filtering on reportingController 02/27/23 12:47:12.505
    STEP: getting the test event 02/27/23 12:47:12.513
    STEP: patching the test event 02/27/23 12:47:12.521
    STEP: getting the test event 02/27/23 12:47:12.538
    STEP: updating the test event 02/27/23 12:47:12.547
    STEP: getting the test event 02/27/23 12:47:12.565
    STEP: deleting the test event 02/27/23 12:47:12.572
    STEP: listing events in all namespaces 02/27/23 12:47:12.59
    STEP: listing events in test namespace 02/27/23 12:47:12.601
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Feb 27 12:47:12.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-7718" for this suite. 02/27/23 12:47:12.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:47:12.637
Feb 27 12:47:12.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename gc 02/27/23 12:47:12.638
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:12.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:12.673
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 02/27/23 12:47:13.149
STEP: create the rc2 02/27/23 12:47:13.583
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/27/23 12:47:18.625
STEP: delete the rc simpletest-rc-to-be-deleted 02/27/23 12:47:19.891
STEP: wait for the rc to be deleted 02/27/23 12:47:19.92
Feb 27 12:47:25.317: INFO: 65 pods remaining
Feb 27 12:47:25.317: INFO: 65 pods has nil DeletionTimestamp
Feb 27 12:47:25.317: INFO: 
STEP: Gathering metrics 02/27/23 12:47:29.949
W0227 12:47:30.000096      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 27 12:47:30.000: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 27 12:47:30.000: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dxg6" in namespace "gc-8462"
Feb 27 12:47:30.043: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lktg" in namespace "gc-8462"
Feb 27 12:47:30.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-4frjc" in namespace "gc-8462"
Feb 27 12:47:30.097: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mqn7" in namespace "gc-8462"
Feb 27 12:47:30.139: INFO: Deleting pod "simpletest-rc-to-be-deleted-55lkt" in namespace "gc-8462"
Feb 27 12:47:30.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-68w7s" in namespace "gc-8462"
Feb 27 12:47:30.250: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cm8d" in namespace "gc-8462"
Feb 27 12:47:30.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xkwk" in namespace "gc-8462"
Feb 27 12:47:30.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-74zp2" in namespace "gc-8462"
Feb 27 12:47:30.376: INFO: Deleting pod "simpletest-rc-to-be-deleted-76g8j" in namespace "gc-8462"
Feb 27 12:47:30.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-76x8x" in namespace "gc-8462"
Feb 27 12:47:30.466: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gb96" in namespace "gc-8462"
Feb 27 12:47:30.508: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wjmw" in namespace "gc-8462"
Feb 27 12:47:30.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fb6z" in namespace "gc-8462"
Feb 27 12:47:30.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t9k9" in namespace "gc-8462"
Feb 27 12:47:30.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-8thrr" in namespace "gc-8462"
Feb 27 12:47:30.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vv9v" in namespace "gc-8462"
Feb 27 12:47:30.664: INFO: Deleting pod "simpletest-rc-to-be-deleted-97h84" in namespace "gc-8462"
Feb 27 12:47:30.711: INFO: Deleting pod "simpletest-rc-to-be-deleted-97n78" in namespace "gc-8462"
Feb 27 12:47:30.785: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f9nb" in namespace "gc-8462"
Feb 27 12:47:30.812: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6qbt" in namespace "gc-8462"
Feb 27 12:47:30.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbcxt" in namespace "gc-8462"
Feb 27 12:47:30.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-bh6v9" in namespace "gc-8462"
Feb 27 12:47:30.969: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhdrz" in namespace "gc-8462"
Feb 27 12:47:30.993: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnczn" in namespace "gc-8462"
Feb 27 12:47:31.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqw97" in namespace "gc-8462"
Feb 27 12:47:31.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvfc5" in namespace "gc-8462"
Feb 27 12:47:31.110: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxfhp" in namespace "gc-8462"
Feb 27 12:47:31.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4ptn" in namespace "gc-8462"
Feb 27 12:47:31.212: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4rlj" in namespace "gc-8462"
Feb 27 12:47:31.239: INFO: Deleting pod "simpletest-rc-to-be-deleted-cncng" in namespace "gc-8462"
Feb 27 12:47:31.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-crlls" in namespace "gc-8462"
Feb 27 12:47:31.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwpr9" in namespace "gc-8462"
Feb 27 12:47:31.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-cz9jh" in namespace "gc-8462"
Feb 27 12:47:31.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmnld" in namespace "gc-8462"
Feb 27 12:47:31.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqr85" in namespace "gc-8462"
Feb 27 12:47:31.539: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwvrx" in namespace "gc-8462"
Feb 27 12:47:31.572: INFO: Deleting pod "simpletest-rc-to-be-deleted-f988v" in namespace "gc-8462"
Feb 27 12:47:31.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbqh8" in namespace "gc-8462"
Feb 27 12:47:31.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnxcs" in namespace "gc-8462"
Feb 27 12:47:31.677: INFO: Deleting pod "simpletest-rc-to-be-deleted-frzl4" in namespace "gc-8462"
Feb 27 12:47:31.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzwtl" in namespace "gc-8462"
Feb 27 12:47:31.740: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5lk6" in namespace "gc-8462"
Feb 27 12:47:31.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkhhl" in namespace "gc-8462"
Feb 27 12:47:31.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkmph" in namespace "gc-8462"
Feb 27 12:47:31.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-glhxb" in namespace "gc-8462"
Feb 27 12:47:31.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvvxt" in namespace "gc-8462"
Feb 27 12:47:31.909: INFO: Deleting pod "simpletest-rc-to-be-deleted-h45mp" in namespace "gc-8462"
Feb 27 12:47:31.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-h5ct5" in namespace "gc-8462"
Feb 27 12:47:31.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8xh7" in namespace "gc-8462"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 27 12:47:31.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8462" for this suite. 02/27/23 12:47:31.981
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":304,"skipped":5496,"failed":0}
------------------------------
• [SLOW TEST] [19.357 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:47:12.637
    Feb 27 12:47:12.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename gc 02/27/23 12:47:12.638
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:12.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:12.673
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 02/27/23 12:47:13.149
    STEP: create the rc2 02/27/23 12:47:13.583
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/27/23 12:47:18.625
    STEP: delete the rc simpletest-rc-to-be-deleted 02/27/23 12:47:19.891
    STEP: wait for the rc to be deleted 02/27/23 12:47:19.92
    Feb 27 12:47:25.317: INFO: 65 pods remaining
    Feb 27 12:47:25.317: INFO: 65 pods has nil DeletionTimestamp
    Feb 27 12:47:25.317: INFO: 
    STEP: Gathering metrics 02/27/23 12:47:29.949
    W0227 12:47:30.000096      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 27 12:47:30.000: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 27 12:47:30.000: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dxg6" in namespace "gc-8462"
    Feb 27 12:47:30.043: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lktg" in namespace "gc-8462"
    Feb 27 12:47:30.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-4frjc" in namespace "gc-8462"
    Feb 27 12:47:30.097: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mqn7" in namespace "gc-8462"
    Feb 27 12:47:30.139: INFO: Deleting pod "simpletest-rc-to-be-deleted-55lkt" in namespace "gc-8462"
    Feb 27 12:47:30.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-68w7s" in namespace "gc-8462"
    Feb 27 12:47:30.250: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cm8d" in namespace "gc-8462"
    Feb 27 12:47:30.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xkwk" in namespace "gc-8462"
    Feb 27 12:47:30.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-74zp2" in namespace "gc-8462"
    Feb 27 12:47:30.376: INFO: Deleting pod "simpletest-rc-to-be-deleted-76g8j" in namespace "gc-8462"
    Feb 27 12:47:30.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-76x8x" in namespace "gc-8462"
    Feb 27 12:47:30.466: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gb96" in namespace "gc-8462"
    Feb 27 12:47:30.508: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wjmw" in namespace "gc-8462"
    Feb 27 12:47:30.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fb6z" in namespace "gc-8462"
    Feb 27 12:47:30.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t9k9" in namespace "gc-8462"
    Feb 27 12:47:30.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-8thrr" in namespace "gc-8462"
    Feb 27 12:47:30.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vv9v" in namespace "gc-8462"
    Feb 27 12:47:30.664: INFO: Deleting pod "simpletest-rc-to-be-deleted-97h84" in namespace "gc-8462"
    Feb 27 12:47:30.711: INFO: Deleting pod "simpletest-rc-to-be-deleted-97n78" in namespace "gc-8462"
    Feb 27 12:47:30.785: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f9nb" in namespace "gc-8462"
    Feb 27 12:47:30.812: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6qbt" in namespace "gc-8462"
    Feb 27 12:47:30.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbcxt" in namespace "gc-8462"
    Feb 27 12:47:30.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-bh6v9" in namespace "gc-8462"
    Feb 27 12:47:30.969: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhdrz" in namespace "gc-8462"
    Feb 27 12:47:30.993: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnczn" in namespace "gc-8462"
    Feb 27 12:47:31.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqw97" in namespace "gc-8462"
    Feb 27 12:47:31.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvfc5" in namespace "gc-8462"
    Feb 27 12:47:31.110: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxfhp" in namespace "gc-8462"
    Feb 27 12:47:31.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4ptn" in namespace "gc-8462"
    Feb 27 12:47:31.212: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4rlj" in namespace "gc-8462"
    Feb 27 12:47:31.239: INFO: Deleting pod "simpletest-rc-to-be-deleted-cncng" in namespace "gc-8462"
    Feb 27 12:47:31.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-crlls" in namespace "gc-8462"
    Feb 27 12:47:31.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwpr9" in namespace "gc-8462"
    Feb 27 12:47:31.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-cz9jh" in namespace "gc-8462"
    Feb 27 12:47:31.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmnld" in namespace "gc-8462"
    Feb 27 12:47:31.505: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqr85" in namespace "gc-8462"
    Feb 27 12:47:31.539: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwvrx" in namespace "gc-8462"
    Feb 27 12:47:31.572: INFO: Deleting pod "simpletest-rc-to-be-deleted-f988v" in namespace "gc-8462"
    Feb 27 12:47:31.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbqh8" in namespace "gc-8462"
    Feb 27 12:47:31.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnxcs" in namespace "gc-8462"
    Feb 27 12:47:31.677: INFO: Deleting pod "simpletest-rc-to-be-deleted-frzl4" in namespace "gc-8462"
    Feb 27 12:47:31.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzwtl" in namespace "gc-8462"
    Feb 27 12:47:31.740: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5lk6" in namespace "gc-8462"
    Feb 27 12:47:31.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkhhl" in namespace "gc-8462"
    Feb 27 12:47:31.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkmph" in namespace "gc-8462"
    Feb 27 12:47:31.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-glhxb" in namespace "gc-8462"
    Feb 27 12:47:31.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvvxt" in namespace "gc-8462"
    Feb 27 12:47:31.909: INFO: Deleting pod "simpletest-rc-to-be-deleted-h45mp" in namespace "gc-8462"
    Feb 27 12:47:31.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-h5ct5" in namespace "gc-8462"
    Feb 27 12:47:31.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8xh7" in namespace "gc-8462"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 27 12:47:31.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8462" for this suite. 02/27/23 12:47:31.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:47:31.996
Feb 27 12:47:31.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 12:47:32
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:32.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:32.038
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Feb 27 12:47:32.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:47:38.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1410" for this suite. 02/27/23 12:47:38.646
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":305,"skipped":5517,"failed":0}
------------------------------
• [SLOW TEST] [6.666 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:47:31.996
    Feb 27 12:47:31.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 12:47:32
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:32.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:32.038
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Feb 27 12:47:32.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:47:38.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1410" for this suite. 02/27/23 12:47:38.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:47:38.663
Feb 27 12:47:38.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:47:38.664
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:38.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:38.717
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 02/27/23 12:47:38.728
Feb 27 12:47:38.750: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a" in namespace "projected-9777" to be "Succeeded or Failed"
Feb 27 12:47:38.760: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.539588ms
Feb 27 12:47:40.769: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018800402s
Feb 27 12:47:42.767: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016683321s
Feb 27 12:47:44.769: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a": Phase="Running", Reason="", readiness=false. Elapsed: 6.018589545s
Feb 27 12:47:46.769: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a": Phase="Running", Reason="", readiness=false. Elapsed: 8.018839213s
Feb 27 12:47:48.767: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.016363981s
STEP: Saw pod success 02/27/23 12:47:48.767
Feb 27 12:47:48.767: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a" satisfied condition "Succeeded or Failed"
Feb 27 12:47:48.774: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a container client-container: <nil>
STEP: delete the pod 02/27/23 12:47:48.789
Feb 27 12:47:48.808: INFO: Waiting for pod downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a to disappear
Feb 27 12:47:48.816: INFO: Pod downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 27 12:47:48.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9777" for this suite. 02/27/23 12:47:48.826
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":306,"skipped":5532,"failed":0}
------------------------------
• [SLOW TEST] [10.177 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:47:38.663
    Feb 27 12:47:38.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:47:38.664
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:38.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:38.717
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 02/27/23 12:47:38.728
    Feb 27 12:47:38.750: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a" in namespace "projected-9777" to be "Succeeded or Failed"
    Feb 27 12:47:38.760: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.539588ms
    Feb 27 12:47:40.769: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018800402s
    Feb 27 12:47:42.767: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016683321s
    Feb 27 12:47:44.769: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a": Phase="Running", Reason="", readiness=false. Elapsed: 6.018589545s
    Feb 27 12:47:46.769: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a": Phase="Running", Reason="", readiness=false. Elapsed: 8.018839213s
    Feb 27 12:47:48.767: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.016363981s
    STEP: Saw pod success 02/27/23 12:47:48.767
    Feb 27 12:47:48.767: INFO: Pod "downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a" satisfied condition "Succeeded or Failed"
    Feb 27 12:47:48.774: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a container client-container: <nil>
    STEP: delete the pod 02/27/23 12:47:48.789
    Feb 27 12:47:48.808: INFO: Waiting for pod downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a to disappear
    Feb 27 12:47:48.816: INFO: Pod downwardapi-volume-5563dfbf-652c-46c8-a510-3595e342574a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 27 12:47:48.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9777" for this suite. 02/27/23 12:47:48.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:47:48.849
Feb 27 12:47:48.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename endpointslice 02/27/23 12:47:48.85
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:48.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:48.888
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 27 12:47:51.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1347" for this suite. 02/27/23 12:47:51.062
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":307,"skipped":5558,"failed":0}
------------------------------
• [2.250 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:47:48.849
    Feb 27 12:47:48.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename endpointslice 02/27/23 12:47:48.85
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:48.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:48.888
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 27 12:47:51.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-1347" for this suite. 02/27/23 12:47:51.062
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:47:51.108
Feb 27 12:47:51.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 12:47:51.11
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:51.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:51.157
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/27/23 12:47:51.169
Feb 27 12:47:51.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1268 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 27 12:47:51.367: INFO: stderr: ""
Feb 27 12:47:51.367: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 02/27/23 12:47:51.367
Feb 27 12:47:51.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1268 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Feb 27 12:47:52.980: INFO: stderr: ""
Feb 27 12:47:52.980: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/27/23 12:47:52.98
Feb 27 12:47:52.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1268 delete pods e2e-test-httpd-pod'
Feb 27 12:47:55.591: INFO: stderr: ""
Feb 27 12:47:55.591: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 12:47:55.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1268" for this suite. 02/27/23 12:47:55.606
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":308,"skipped":5562,"failed":0}
------------------------------
• [4.514 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:47:51.108
    Feb 27 12:47:51.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 12:47:51.11
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:51.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:51.157
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/27/23 12:47:51.169
    Feb 27 12:47:51.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1268 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 27 12:47:51.367: INFO: stderr: ""
    Feb 27 12:47:51.367: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 02/27/23 12:47:51.367
    Feb 27 12:47:51.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1268 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Feb 27 12:47:52.980: INFO: stderr: ""
    Feb 27 12:47:52.980: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/27/23 12:47:52.98
    Feb 27 12:47:52.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1268 delete pods e2e-test-httpd-pod'
    Feb 27 12:47:55.591: INFO: stderr: ""
    Feb 27 12:47:55.591: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 12:47:55.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1268" for this suite. 02/27/23 12:47:55.606
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:47:55.622
Feb 27 12:47:55.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename sched-pred 02/27/23 12:47:55.624
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:55.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:55.671
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 27 12:47:55.692: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 27 12:47:55.719: INFO: Waiting for terminating namespaces to be deleted...
Feb 27 12:47:55.737: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-11-159.eu-central-1.compute.internal before test
Feb 27 12:47:55.756: INFO: canal-4q9m8 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
Feb 27 12:47:55.756: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 12:47:55.756: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 27 12:47:55.756: INFO: ebs-csi-node-b6z5h from kube-system started at 2023-02-27 09:22:11 +0000 UTC (3 container statuses recorded)
Feb 27 12:47:55.756: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 12:47:55.756: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 12:47:55.756: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 12:47:55.756: INFO: envoy-agent-2wwht from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
Feb 27 12:47:55.756: INFO: 	Container assign-address ready: true, restart count 0
Feb 27 12:47:55.756: INFO: 	Container envoy-agent ready: true, restart count 0
Feb 27 12:47:55.756: INFO: kube-proxy-cz9zt from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.756: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 27 12:47:55.757: INFO: node-local-dns-r6r4k from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.757: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 12:47:55.757: INFO: user-ssh-keys-agent-rk5t9 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.757: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 27 12:47:55.757: INFO: sonobuoy from sonobuoy started at 2023-02-27 11:19:26 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.757: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 27 12:47:55.757: INFO: sonobuoy-e2e-job-18131847dfcd49d5 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 12:47:55.757: INFO: 	Container e2e ready: true, restart count 0
Feb 27 12:47:55.757: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 12:47:55.757: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-rlhml from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 12:47:55.757: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 12:47:55.757: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 12:47:55.757: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-15-17.eu-central-1.compute.internal before test
Feb 27 12:47:55.779: INFO: calico-kube-controllers-55d99d998f-f5ngs from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.779: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 27 12:47:55.779: INFO: canal-bxq4m from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
Feb 27 12:47:55.779: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 12:47:55.779: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 27 12:47:55.779: INFO: coredns-bf8668b4f-5h5v9 from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.779: INFO: 	Container coredns ready: true, restart count 0
Feb 27 12:47:55.779: INFO: ebs-csi-node-z9l5x from kube-system started at 2023-02-27 09:22:52 +0000 UTC (3 container statuses recorded)
Feb 27 12:47:55.779: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 12:47:55.779: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 12:47:55.779: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 12:47:55.792: INFO: envoy-agent-7xhs5 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
Feb 27 12:47:55.792: INFO: 	Container assign-address ready: true, restart count 0
Feb 27 12:47:55.792: INFO: 	Container envoy-agent ready: true, restart count 0
Feb 27 12:47:55.792: INFO: konnectivity-agent-76c848fdd6-56hgq from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.792: INFO: 	Container konnectivity-agent ready: true, restart count 0
Feb 27 12:47:55.792: INFO: kube-proxy-hxmbw from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.792: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 27 12:47:55.792: INFO: metrics-server-5f7c5d4b9-xlr2c from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.792: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 12:47:55.792: INFO: node-local-dns-8z787 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.792: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 12:47:55.792: INFO: user-ssh-keys-agent-fvhdn from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.792: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 27 12:47:55.792: INFO: dashboard-metrics-scraper-85f6dd84d5-2qtjs from kubernetes-dashboard started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.792: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 27 12:47:55.792: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-94vkz from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 12:47:55.792: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 12:47:55.792: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 12:47:55.792: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-7-167.eu-central-1.compute.internal before test
Feb 27 12:47:55.809: INFO: canal-mbg4r from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 27 12:47:55.809: INFO: coredns-bf8668b4f-wttzt from kube-system started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container coredns ready: true, restart count 0
Feb 27 12:47:55.809: INFO: ebs-csi-controller-54c5c66b84-mkj54 from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container csi-resizer ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 12:47:55.809: INFO: ebs-csi-controller-54c5c66b84-rxhdh from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container csi-resizer ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 12:47:55.809: INFO: ebs-csi-node-7dvrp from kube-system started at 2023-02-27 09:22:08 +0000 UTC (3 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 12:47:55.809: INFO: envoy-agent-scd88 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container assign-address ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container envoy-agent ready: true, restart count 0
Feb 27 12:47:55.809: INFO: konnectivity-agent-76c848fdd6-bn8rz from kube-system started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container konnectivity-agent ready: true, restart count 0
Feb 27 12:47:55.809: INFO: kube-proxy-ghd44 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 27 12:47:55.809: INFO: metrics-server-5f7c5d4b9-89jfg from kube-system started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 12:47:55.809: INFO: node-local-dns-kfv2k from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 12:47:55.809: INFO: user-ssh-keys-agent-gjs99 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 27 12:47:55.809: INFO: dashboard-metrics-scraper-85f6dd84d5-hdh84 from kubernetes-dashboard started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 27 12:47:55.809: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-k5l74 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 12:47:55.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 12:47:55.809: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node ip-172-31-11-159.eu-central-1.compute.internal 02/27/23 12:47:55.873
STEP: verifying the node has the label node ip-172-31-15-17.eu-central-1.compute.internal 02/27/23 12:47:55.924
STEP: verifying the node has the label node ip-172-31-7-167.eu-central-1.compute.internal 02/27/23 12:47:55.957
Feb 27 12:47:55.991: INFO: Pod calico-kube-controllers-55d99d998f-f5ngs requesting resource cpu=0m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:55.992: INFO: Pod canal-4q9m8 requesting resource cpu=250m on Node ip-172-31-11-159.eu-central-1.compute.internal
Feb 27 12:47:55.992: INFO: Pod canal-bxq4m requesting resource cpu=250m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:55.992: INFO: Pod canal-mbg4r requesting resource cpu=250m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.992: INFO: Pod coredns-bf8668b4f-5h5v9 requesting resource cpu=50m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:55.992: INFO: Pod coredns-bf8668b4f-wttzt requesting resource cpu=50m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.992: INFO: Pod ebs-csi-controller-54c5c66b84-mkj54 requesting resource cpu=0m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.993: INFO: Pod ebs-csi-controller-54c5c66b84-rxhdh requesting resource cpu=0m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.993: INFO: Pod ebs-csi-node-7dvrp requesting resource cpu=0m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.993: INFO: Pod ebs-csi-node-b6z5h requesting resource cpu=0m on Node ip-172-31-11-159.eu-central-1.compute.internal
Feb 27 12:47:55.993: INFO: Pod ebs-csi-node-z9l5x requesting resource cpu=0m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:55.993: INFO: Pod envoy-agent-2wwht requesting resource cpu=50m on Node ip-172-31-11-159.eu-central-1.compute.internal
Feb 27 12:47:55.993: INFO: Pod envoy-agent-7xhs5 requesting resource cpu=50m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:55.994: INFO: Pod envoy-agent-scd88 requesting resource cpu=50m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.994: INFO: Pod konnectivity-agent-76c848fdd6-56hgq requesting resource cpu=10m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:55.994: INFO: Pod konnectivity-agent-76c848fdd6-bn8rz requesting resource cpu=10m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.994: INFO: Pod kube-proxy-cz9zt requesting resource cpu=75m on Node ip-172-31-11-159.eu-central-1.compute.internal
Feb 27 12:47:55.994: INFO: Pod kube-proxy-ghd44 requesting resource cpu=75m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.995: INFO: Pod kube-proxy-hxmbw requesting resource cpu=75m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:55.995: INFO: Pod metrics-server-5f7c5d4b9-89jfg requesting resource cpu=100m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.995: INFO: Pod metrics-server-5f7c5d4b9-xlr2c requesting resource cpu=100m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:55.995: INFO: Pod node-local-dns-8z787 requesting resource cpu=0m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:55.995: INFO: Pod node-local-dns-kfv2k requesting resource cpu=0m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.995: INFO: Pod node-local-dns-r6r4k requesting resource cpu=0m on Node ip-172-31-11-159.eu-central-1.compute.internal
Feb 27 12:47:55.996: INFO: Pod user-ssh-keys-agent-fvhdn requesting resource cpu=0m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:55.996: INFO: Pod user-ssh-keys-agent-gjs99 requesting resource cpu=0m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.996: INFO: Pod user-ssh-keys-agent-rk5t9 requesting resource cpu=0m on Node ip-172-31-11-159.eu-central-1.compute.internal
Feb 27 12:47:55.996: INFO: Pod dashboard-metrics-scraper-85f6dd84d5-2qtjs requesting resource cpu=50m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:55.996: INFO: Pod dashboard-metrics-scraper-85f6dd84d5-hdh84 requesting resource cpu=50m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.996: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-11-159.eu-central-1.compute.internal
Feb 27 12:47:55.996: INFO: Pod sonobuoy-e2e-job-18131847dfcd49d5 requesting resource cpu=0m on Node ip-172-31-11-159.eu-central-1.compute.internal
Feb 27 12:47:55.997: INFO: Pod sonobuoy-systemd-logs-daemon-set-52382428c7df4090-94vkz requesting resource cpu=0m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:55.997: INFO: Pod sonobuoy-systemd-logs-daemon-set-52382428c7df4090-k5l74 requesting resource cpu=0m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:55.997: INFO: Pod sonobuoy-systemd-logs-daemon-set-52382428c7df4090-rlhml requesting resource cpu=0m on Node ip-172-31-11-159.eu-central-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU. 02/27/23 12:47:55.997
Feb 27 12:47:55.997: INFO: Creating a pod which consumes cpu=710m on Node ip-172-31-7-167.eu-central-1.compute.internal
Feb 27 12:47:56.014: INFO: Creating a pod which consumes cpu=857m on Node ip-172-31-11-159.eu-central-1.compute.internal
Feb 27 12:47:56.031: INFO: Creating a pod which consumes cpu=710m on Node ip-172-31-15-17.eu-central-1.compute.internal
Feb 27 12:47:56.045: INFO: Waiting up to 5m0s for pod "filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399" in namespace "sched-pred-5808" to be "running"
Feb 27 12:47:56.068: INFO: Pod "filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399": Phase="Pending", Reason="", readiness=false. Elapsed: 23.203714ms
Feb 27 12:47:58.223: INFO: Pod "filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399": Phase="Running", Reason="", readiness=true. Elapsed: 2.177910677s
Feb 27 12:47:58.223: INFO: Pod "filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399" satisfied condition "running"
Feb 27 12:47:58.223: INFO: Waiting up to 5m0s for pod "filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4" in namespace "sched-pred-5808" to be "running"
Feb 27 12:47:58.233: INFO: Pod "filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4": Phase="Running", Reason="", readiness=true. Elapsed: 10.378102ms
Feb 27 12:47:58.233: INFO: Pod "filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4" satisfied condition "running"
Feb 27 12:47:58.233: INFO: Waiting up to 5m0s for pod "filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626" in namespace "sched-pred-5808" to be "running"
Feb 27 12:47:58.242: INFO: Pod "filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626": Phase="Running", Reason="", readiness=true. Elapsed: 8.346574ms
Feb 27 12:47:58.242: INFO: Pod "filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 02/27/23 12:47:58.242
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399.1747af40178797d1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5808/filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399 to ip-172-31-7-167.eu-central-1.compute.internal] 02/27/23 12:47:58.267
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399.1747af40459cae6d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/27/23 12:47:58.278
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399.1747af40472b189f], Reason = [Created], Message = [Created container filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399] 02/27/23 12:47:58.279
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399.1747af404ed217a6], Reason = [Started], Message = [Started container filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399] 02/27/23 12:47:58.279
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626.1747af4019389539], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5808/filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626 to ip-172-31-15-17.eu-central-1.compute.internal] 02/27/23 12:47:58.279
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626.1747af404515a17f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/27/23 12:47:58.28
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626.1747af4048bd3b21], Reason = [Created], Message = [Created container filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626] 02/27/23 12:47:58.28
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626.1747af404f93fe5d], Reason = [Started], Message = [Started container filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626] 02/27/23 12:47:58.282
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4.1747af40180681e6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5808/filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4 to ip-172-31-11-159.eu-central-1.compute.internal] 02/27/23 12:47:58.283
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4.1747af4042a75b6b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/27/23 12:47:58.283
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4.1747af40445f4560], Reason = [Created], Message = [Created container filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4] 02/27/23 12:47:58.283
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4.1747af404a9ed017], Reason = [Started], Message = [Started container filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4] 02/27/23 12:47:58.283
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1747af409de155b1], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 02/27/23 12:47:58.295
STEP: removing the label node off the node ip-172-31-11-159.eu-central-1.compute.internal 02/27/23 12:47:59.296
STEP: verifying the node doesn't have the label node 02/27/23 12:47:59.361
STEP: removing the label node off the node ip-172-31-15-17.eu-central-1.compute.internal 02/27/23 12:47:59.372
STEP: verifying the node doesn't have the label node 02/27/23 12:47:59.412
STEP: removing the label node off the node ip-172-31-7-167.eu-central-1.compute.internal 02/27/23 12:47:59.421
STEP: verifying the node doesn't have the label node 02/27/23 12:47:59.471
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 27 12:47:59.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5808" for this suite. 02/27/23 12:47:59.493
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":309,"skipped":5565,"failed":0}
------------------------------
• [3.893 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:47:55.622
    Feb 27 12:47:55.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename sched-pred 02/27/23 12:47:55.624
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:55.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:55.671
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 27 12:47:55.692: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 27 12:47:55.719: INFO: Waiting for terminating namespaces to be deleted...
    Feb 27 12:47:55.737: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-11-159.eu-central-1.compute.internal before test
    Feb 27 12:47:55.756: INFO: canal-4q9m8 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
    Feb 27 12:47:55.756: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 12:47:55.756: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 27 12:47:55.756: INFO: ebs-csi-node-b6z5h from kube-system started at 2023-02-27 09:22:11 +0000 UTC (3 container statuses recorded)
    Feb 27 12:47:55.756: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 12:47:55.756: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 12:47:55.756: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 12:47:55.756: INFO: envoy-agent-2wwht from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
    Feb 27 12:47:55.756: INFO: 	Container assign-address ready: true, restart count 0
    Feb 27 12:47:55.756: INFO: 	Container envoy-agent ready: true, restart count 0
    Feb 27 12:47:55.756: INFO: kube-proxy-cz9zt from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.756: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 27 12:47:55.757: INFO: node-local-dns-r6r4k from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.757: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 12:47:55.757: INFO: user-ssh-keys-agent-rk5t9 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.757: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Feb 27 12:47:55.757: INFO: sonobuoy from sonobuoy started at 2023-02-27 11:19:26 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.757: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 27 12:47:55.757: INFO: sonobuoy-e2e-job-18131847dfcd49d5 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 12:47:55.757: INFO: 	Container e2e ready: true, restart count 0
    Feb 27 12:47:55.757: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 12:47:55.757: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-rlhml from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 12:47:55.757: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 12:47:55.757: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 12:47:55.757: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-15-17.eu-central-1.compute.internal before test
    Feb 27 12:47:55.779: INFO: calico-kube-controllers-55d99d998f-f5ngs from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.779: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 27 12:47:55.779: INFO: canal-bxq4m from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
    Feb 27 12:47:55.779: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 12:47:55.779: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 27 12:47:55.779: INFO: coredns-bf8668b4f-5h5v9 from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.779: INFO: 	Container coredns ready: true, restart count 0
    Feb 27 12:47:55.779: INFO: ebs-csi-node-z9l5x from kube-system started at 2023-02-27 09:22:52 +0000 UTC (3 container statuses recorded)
    Feb 27 12:47:55.779: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 12:47:55.779: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 12:47:55.779: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 12:47:55.792: INFO: envoy-agent-7xhs5 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
    Feb 27 12:47:55.792: INFO: 	Container assign-address ready: true, restart count 0
    Feb 27 12:47:55.792: INFO: 	Container envoy-agent ready: true, restart count 0
    Feb 27 12:47:55.792: INFO: konnectivity-agent-76c848fdd6-56hgq from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.792: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Feb 27 12:47:55.792: INFO: kube-proxy-hxmbw from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.792: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 27 12:47:55.792: INFO: metrics-server-5f7c5d4b9-xlr2c from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.792: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 12:47:55.792: INFO: node-local-dns-8z787 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.792: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 12:47:55.792: INFO: user-ssh-keys-agent-fvhdn from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.792: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Feb 27 12:47:55.792: INFO: dashboard-metrics-scraper-85f6dd84d5-2qtjs from kubernetes-dashboard started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.792: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Feb 27 12:47:55.792: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-94vkz from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 12:47:55.792: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 12:47:55.792: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 12:47:55.792: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-7-167.eu-central-1.compute.internal before test
    Feb 27 12:47:55.809: INFO: canal-mbg4r from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: coredns-bf8668b4f-wttzt from kube-system started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container coredns ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: ebs-csi-controller-54c5c66b84-mkj54 from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container csi-attacher ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container csi-provisioner ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container csi-resizer ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: ebs-csi-controller-54c5c66b84-rxhdh from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container csi-attacher ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container csi-provisioner ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container csi-resizer ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: ebs-csi-node-7dvrp from kube-system started at 2023-02-27 09:22:08 +0000 UTC (3 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: envoy-agent-scd88 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container assign-address ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container envoy-agent ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: konnectivity-agent-76c848fdd6-bn8rz from kube-system started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: kube-proxy-ghd44 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: metrics-server-5f7c5d4b9-89jfg from kube-system started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: node-local-dns-kfv2k from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: user-ssh-keys-agent-gjs99 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: dashboard-metrics-scraper-85f6dd84d5-hdh84 from kubernetes-dashboard started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-k5l74 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 12:47:55.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 12:47:55.809: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node ip-172-31-11-159.eu-central-1.compute.internal 02/27/23 12:47:55.873
    STEP: verifying the node has the label node ip-172-31-15-17.eu-central-1.compute.internal 02/27/23 12:47:55.924
    STEP: verifying the node has the label node ip-172-31-7-167.eu-central-1.compute.internal 02/27/23 12:47:55.957
    Feb 27 12:47:55.991: INFO: Pod calico-kube-controllers-55d99d998f-f5ngs requesting resource cpu=0m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:55.992: INFO: Pod canal-4q9m8 requesting resource cpu=250m on Node ip-172-31-11-159.eu-central-1.compute.internal
    Feb 27 12:47:55.992: INFO: Pod canal-bxq4m requesting resource cpu=250m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:55.992: INFO: Pod canal-mbg4r requesting resource cpu=250m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.992: INFO: Pod coredns-bf8668b4f-5h5v9 requesting resource cpu=50m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:55.992: INFO: Pod coredns-bf8668b4f-wttzt requesting resource cpu=50m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.992: INFO: Pod ebs-csi-controller-54c5c66b84-mkj54 requesting resource cpu=0m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.993: INFO: Pod ebs-csi-controller-54c5c66b84-rxhdh requesting resource cpu=0m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.993: INFO: Pod ebs-csi-node-7dvrp requesting resource cpu=0m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.993: INFO: Pod ebs-csi-node-b6z5h requesting resource cpu=0m on Node ip-172-31-11-159.eu-central-1.compute.internal
    Feb 27 12:47:55.993: INFO: Pod ebs-csi-node-z9l5x requesting resource cpu=0m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:55.993: INFO: Pod envoy-agent-2wwht requesting resource cpu=50m on Node ip-172-31-11-159.eu-central-1.compute.internal
    Feb 27 12:47:55.993: INFO: Pod envoy-agent-7xhs5 requesting resource cpu=50m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:55.994: INFO: Pod envoy-agent-scd88 requesting resource cpu=50m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.994: INFO: Pod konnectivity-agent-76c848fdd6-56hgq requesting resource cpu=10m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:55.994: INFO: Pod konnectivity-agent-76c848fdd6-bn8rz requesting resource cpu=10m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.994: INFO: Pod kube-proxy-cz9zt requesting resource cpu=75m on Node ip-172-31-11-159.eu-central-1.compute.internal
    Feb 27 12:47:55.994: INFO: Pod kube-proxy-ghd44 requesting resource cpu=75m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.995: INFO: Pod kube-proxy-hxmbw requesting resource cpu=75m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:55.995: INFO: Pod metrics-server-5f7c5d4b9-89jfg requesting resource cpu=100m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.995: INFO: Pod metrics-server-5f7c5d4b9-xlr2c requesting resource cpu=100m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:55.995: INFO: Pod node-local-dns-8z787 requesting resource cpu=0m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:55.995: INFO: Pod node-local-dns-kfv2k requesting resource cpu=0m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.995: INFO: Pod node-local-dns-r6r4k requesting resource cpu=0m on Node ip-172-31-11-159.eu-central-1.compute.internal
    Feb 27 12:47:55.996: INFO: Pod user-ssh-keys-agent-fvhdn requesting resource cpu=0m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:55.996: INFO: Pod user-ssh-keys-agent-gjs99 requesting resource cpu=0m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.996: INFO: Pod user-ssh-keys-agent-rk5t9 requesting resource cpu=0m on Node ip-172-31-11-159.eu-central-1.compute.internal
    Feb 27 12:47:55.996: INFO: Pod dashboard-metrics-scraper-85f6dd84d5-2qtjs requesting resource cpu=50m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:55.996: INFO: Pod dashboard-metrics-scraper-85f6dd84d5-hdh84 requesting resource cpu=50m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.996: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-11-159.eu-central-1.compute.internal
    Feb 27 12:47:55.996: INFO: Pod sonobuoy-e2e-job-18131847dfcd49d5 requesting resource cpu=0m on Node ip-172-31-11-159.eu-central-1.compute.internal
    Feb 27 12:47:55.997: INFO: Pod sonobuoy-systemd-logs-daemon-set-52382428c7df4090-94vkz requesting resource cpu=0m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:55.997: INFO: Pod sonobuoy-systemd-logs-daemon-set-52382428c7df4090-k5l74 requesting resource cpu=0m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:55.997: INFO: Pod sonobuoy-systemd-logs-daemon-set-52382428c7df4090-rlhml requesting resource cpu=0m on Node ip-172-31-11-159.eu-central-1.compute.internal
    STEP: Starting Pods to consume most of the cluster CPU. 02/27/23 12:47:55.997
    Feb 27 12:47:55.997: INFO: Creating a pod which consumes cpu=710m on Node ip-172-31-7-167.eu-central-1.compute.internal
    Feb 27 12:47:56.014: INFO: Creating a pod which consumes cpu=857m on Node ip-172-31-11-159.eu-central-1.compute.internal
    Feb 27 12:47:56.031: INFO: Creating a pod which consumes cpu=710m on Node ip-172-31-15-17.eu-central-1.compute.internal
    Feb 27 12:47:56.045: INFO: Waiting up to 5m0s for pod "filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399" in namespace "sched-pred-5808" to be "running"
    Feb 27 12:47:56.068: INFO: Pod "filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399": Phase="Pending", Reason="", readiness=false. Elapsed: 23.203714ms
    Feb 27 12:47:58.223: INFO: Pod "filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399": Phase="Running", Reason="", readiness=true. Elapsed: 2.177910677s
    Feb 27 12:47:58.223: INFO: Pod "filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399" satisfied condition "running"
    Feb 27 12:47:58.223: INFO: Waiting up to 5m0s for pod "filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4" in namespace "sched-pred-5808" to be "running"
    Feb 27 12:47:58.233: INFO: Pod "filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4": Phase="Running", Reason="", readiness=true. Elapsed: 10.378102ms
    Feb 27 12:47:58.233: INFO: Pod "filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4" satisfied condition "running"
    Feb 27 12:47:58.233: INFO: Waiting up to 5m0s for pod "filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626" in namespace "sched-pred-5808" to be "running"
    Feb 27 12:47:58.242: INFO: Pod "filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626": Phase="Running", Reason="", readiness=true. Elapsed: 8.346574ms
    Feb 27 12:47:58.242: INFO: Pod "filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 02/27/23 12:47:58.242
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399.1747af40178797d1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5808/filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399 to ip-172-31-7-167.eu-central-1.compute.internal] 02/27/23 12:47:58.267
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399.1747af40459cae6d], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/27/23 12:47:58.278
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399.1747af40472b189f], Reason = [Created], Message = [Created container filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399] 02/27/23 12:47:58.279
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399.1747af404ed217a6], Reason = [Started], Message = [Started container filler-pod-1f39e5b3-525e-4e7c-89fc-274b4cbdf399] 02/27/23 12:47:58.279
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626.1747af4019389539], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5808/filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626 to ip-172-31-15-17.eu-central-1.compute.internal] 02/27/23 12:47:58.279
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626.1747af404515a17f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/27/23 12:47:58.28
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626.1747af4048bd3b21], Reason = [Created], Message = [Created container filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626] 02/27/23 12:47:58.28
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626.1747af404f93fe5d], Reason = [Started], Message = [Started container filler-pod-2aaf6de3-af7e-4ebd-83ee-0fa75ea97626] 02/27/23 12:47:58.282
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4.1747af40180681e6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5808/filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4 to ip-172-31-11-159.eu-central-1.compute.internal] 02/27/23 12:47:58.283
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4.1747af4042a75b6b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/27/23 12:47:58.283
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4.1747af40445f4560], Reason = [Created], Message = [Created container filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4] 02/27/23 12:47:58.283
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4.1747af404a9ed017], Reason = [Started], Message = [Started container filler-pod-6b5d0def-a7a0-4d19-aac3-68f25e43e4e4] 02/27/23 12:47:58.283
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1747af409de155b1], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 02/27/23 12:47:58.295
    STEP: removing the label node off the node ip-172-31-11-159.eu-central-1.compute.internal 02/27/23 12:47:59.296
    STEP: verifying the node doesn't have the label node 02/27/23 12:47:59.361
    STEP: removing the label node off the node ip-172-31-15-17.eu-central-1.compute.internal 02/27/23 12:47:59.372
    STEP: verifying the node doesn't have the label node 02/27/23 12:47:59.412
    STEP: removing the label node off the node ip-172-31-7-167.eu-central-1.compute.internal 02/27/23 12:47:59.421
    STEP: verifying the node doesn't have the label node 02/27/23 12:47:59.471
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 12:47:59.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5808" for this suite. 02/27/23 12:47:59.493
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:47:59.516
Feb 27 12:47:59.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename init-container 02/27/23 12:47:59.518
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:59.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:59.562
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 02/27/23 12:47:59.571
Feb 27 12:47:59.571: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 27 12:48:05.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8387" for this suite. 02/27/23 12:48:05.604
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":310,"skipped":5572,"failed":0}
------------------------------
• [SLOW TEST] [6.105 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:47:59.516
    Feb 27 12:47:59.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename init-container 02/27/23 12:47:59.518
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:47:59.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:47:59.562
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 02/27/23 12:47:59.571
    Feb 27 12:47:59.571: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 27 12:48:05.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-8387" for this suite. 02/27/23 12:48:05.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:48:05.628
Feb 27 12:48:05.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 12:48:05.631
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:48:05.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:48:05.686
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 27 12:48:05.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4970" for this suite. 02/27/23 12:48:05.811
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":311,"skipped":5603,"failed":0}
------------------------------
• [0.201 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:48:05.628
    Feb 27 12:48:05.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 12:48:05.631
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:48:05.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:48:05.686
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 12:48:05.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4970" for this suite. 02/27/23 12:48:05.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:48:05.831
Feb 27 12:48:05.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename statefulset 02/27/23 12:48:05.832
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:48:05.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:48:05.879
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-825 02/27/23 12:48:05.895
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 02/27/23 12:48:05.91
STEP: Creating stateful set ss in namespace statefulset-825 02/27/23 12:48:05.923
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-825 02/27/23 12:48:05.933
Feb 27 12:48:05.940: INFO: Found 0 stateful pods, waiting for 1
Feb 27 12:48:15.953: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/27/23 12:48:15.953
Feb 27 12:48:15.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 12:48:16.212: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 12:48:16.212: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 12:48:16.212: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 12:48:16.225: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 27 12:48:26.236: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 12:48:26.236: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 12:48:26.281: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999964s
Feb 27 12:48:27.288: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990266055s
Feb 27 12:48:28.298: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.982907561s
Feb 27 12:48:29.305: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.973456549s
Feb 27 12:48:30.314: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.966473785s
Feb 27 12:48:31.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.95656224s
Feb 27 12:48:32.360: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.919997377s
Feb 27 12:48:33.397: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.911288487s
Feb 27 12:48:34.407: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.872892696s
Feb 27 12:48:35.431: INFO: Verifying statefulset ss doesn't scale past 1 for another 864.160535ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-825 02/27/23 12:48:36.431
Feb 27 12:48:36.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 12:48:36.697: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 12:48:36.698: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 12:48:36.698: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 12:48:36.709: INFO: Found 1 stateful pods, waiting for 3
Feb 27 12:48:46.720: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 12:48:46.720: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 12:48:46.720: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 02/27/23 12:48:46.72
STEP: Scale down will halt with unhealthy stateful pod 02/27/23 12:48:46.721
Feb 27 12:48:46.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 12:48:47.060: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 12:48:47.060: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 12:48:47.060: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 12:48:47.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 12:48:47.311: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 12:48:47.311: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 12:48:47.311: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 12:48:47.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 12:48:47.554: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 12:48:47.554: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 12:48:47.554: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 12:48:47.554: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 12:48:47.563: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 27 12:48:57.583: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 12:48:57.583: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 12:48:57.583: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 12:48:57.612: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999955s
Feb 27 12:48:58.627: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991205437s
Feb 27 12:48:59.645: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972469891s
Feb 27 12:49:00.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.957404341s
Feb 27 12:49:01.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.947296214s
Feb 27 12:49:02.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.936152471s
Feb 27 12:49:03.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.910677467s
Feb 27 12:49:04.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.900241527s
Feb 27 12:49:05.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.891482103s
Feb 27 12:49:06.730: INFO: Verifying statefulset ss doesn't scale past 3 for another 882.04916ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-825 02/27/23 12:49:07.731
Feb 27 12:49:07.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 12:49:07.978: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 12:49:07.978: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 12:49:07.978: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 12:49:08.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 12:49:08.385: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 12:49:08.385: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 12:49:08.385: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 12:49:08.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 12:49:08.674: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 12:49:08.674: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 12:49:08.674: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 12:49:08.674: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 02/27/23 12:49:18.716
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 27 12:49:18.717: INFO: Deleting all statefulset in ns statefulset-825
Feb 27 12:49:18.730: INFO: Scaling statefulset ss to 0
Feb 27 12:49:18.762: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 12:49:18.771: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 27 12:49:18.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-825" for this suite. 02/27/23 12:49:18.82
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":312,"skipped":5609,"failed":0}
------------------------------
• [SLOW TEST] [73.009 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:48:05.831
    Feb 27 12:48:05.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename statefulset 02/27/23 12:48:05.832
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:48:05.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:48:05.879
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-825 02/27/23 12:48:05.895
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 02/27/23 12:48:05.91
    STEP: Creating stateful set ss in namespace statefulset-825 02/27/23 12:48:05.923
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-825 02/27/23 12:48:05.933
    Feb 27 12:48:05.940: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 12:48:15.953: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/27/23 12:48:15.953
    Feb 27 12:48:15.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 12:48:16.212: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 12:48:16.212: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 12:48:16.212: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 12:48:16.225: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 27 12:48:26.236: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 12:48:26.236: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 12:48:26.281: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999964s
    Feb 27 12:48:27.288: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990266055s
    Feb 27 12:48:28.298: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.982907561s
    Feb 27 12:48:29.305: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.973456549s
    Feb 27 12:48:30.314: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.966473785s
    Feb 27 12:48:31.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.95656224s
    Feb 27 12:48:32.360: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.919997377s
    Feb 27 12:48:33.397: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.911288487s
    Feb 27 12:48:34.407: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.872892696s
    Feb 27 12:48:35.431: INFO: Verifying statefulset ss doesn't scale past 1 for another 864.160535ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-825 02/27/23 12:48:36.431
    Feb 27 12:48:36.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 12:48:36.697: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 12:48:36.698: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 12:48:36.698: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 12:48:36.709: INFO: Found 1 stateful pods, waiting for 3
    Feb 27 12:48:46.720: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 12:48:46.720: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 12:48:46.720: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 02/27/23 12:48:46.72
    STEP: Scale down will halt with unhealthy stateful pod 02/27/23 12:48:46.721
    Feb 27 12:48:46.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 12:48:47.060: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 12:48:47.060: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 12:48:47.060: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 12:48:47.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 12:48:47.311: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 12:48:47.311: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 12:48:47.311: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 12:48:47.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 12:48:47.554: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 12:48:47.554: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 12:48:47.554: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 12:48:47.554: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 12:48:47.563: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Feb 27 12:48:57.583: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 12:48:57.583: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 12:48:57.583: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 12:48:57.612: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999955s
    Feb 27 12:48:58.627: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991205437s
    Feb 27 12:48:59.645: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972469891s
    Feb 27 12:49:00.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.957404341s
    Feb 27 12:49:01.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.947296214s
    Feb 27 12:49:02.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.936152471s
    Feb 27 12:49:03.702: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.910677467s
    Feb 27 12:49:04.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.900241527s
    Feb 27 12:49:05.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.891482103s
    Feb 27 12:49:06.730: INFO: Verifying statefulset ss doesn't scale past 3 for another 882.04916ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-825 02/27/23 12:49:07.731
    Feb 27 12:49:07.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 12:49:07.978: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 12:49:07.978: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 12:49:07.978: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 12:49:08.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 12:49:08.385: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 12:49:08.385: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 12:49:08.385: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 12:49:08.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=statefulset-825 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 12:49:08.674: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 12:49:08.674: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 12:49:08.674: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 12:49:08.674: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 02/27/23 12:49:18.716
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 27 12:49:18.717: INFO: Deleting all statefulset in ns statefulset-825
    Feb 27 12:49:18.730: INFO: Scaling statefulset ss to 0
    Feb 27 12:49:18.762: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 12:49:18.771: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 27 12:49:18.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-825" for this suite. 02/27/23 12:49:18.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:49:18.841
Feb 27 12:49:18.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename daemonsets 02/27/23 12:49:18.842
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:49:18.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:49:18.905
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Feb 27 12:49:18.995: INFO: Create a RollingUpdate DaemonSet
Feb 27 12:49:19.012: INFO: Check that daemon pods launch on every node of the cluster
Feb 27 12:49:19.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 12:49:19.053: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 12:49:20.080: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 12:49:20.080: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
Feb 27 12:49:21.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 12:49:21.076: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Feb 27 12:49:21.076: INFO: Update the DaemonSet to trigger a rollout
Feb 27 12:49:21.099: INFO: Updating DaemonSet daemon-set
Feb 27 12:49:23.133: INFO: Roll back the DaemonSet before rollout is complete
Feb 27 12:49:23.153: INFO: Updating DaemonSet daemon-set
Feb 27 12:49:23.153: INFO: Make sure DaemonSet rollback is complete
Feb 27 12:49:23.160: INFO: Wrong image for pod: daemon-set-j5lkf. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Feb 27 12:49:23.160: INFO: Pod daemon-set-j5lkf is not available
Feb 27 12:49:27.543: INFO: Pod daemon-set-fbxzp is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/27/23 12:49:27.577
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3048, will wait for the garbage collector to delete the pods 02/27/23 12:49:27.578
Feb 27 12:49:27.655: INFO: Deleting DaemonSet.extensions daemon-set took: 20.427416ms
Feb 27 12:49:27.756: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.615291ms
Feb 27 12:49:30.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 12:49:30.064: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 12:49:30.076: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"100664"},"items":null}

Feb 27 12:49:30.093: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"100664"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 27 12:49:30.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3048" for this suite. 02/27/23 12:49:30.144
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":313,"skipped":5618,"failed":0}
------------------------------
• [SLOW TEST] [11.318 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:49:18.841
    Feb 27 12:49:18.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename daemonsets 02/27/23 12:49:18.842
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:49:18.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:49:18.905
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Feb 27 12:49:18.995: INFO: Create a RollingUpdate DaemonSet
    Feb 27 12:49:19.012: INFO: Check that daemon pods launch on every node of the cluster
    Feb 27 12:49:19.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 12:49:19.053: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 12:49:20.080: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 12:49:20.080: INFO: Node ip-172-31-11-159.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    Feb 27 12:49:21.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 12:49:21.076: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Feb 27 12:49:21.076: INFO: Update the DaemonSet to trigger a rollout
    Feb 27 12:49:21.099: INFO: Updating DaemonSet daemon-set
    Feb 27 12:49:23.133: INFO: Roll back the DaemonSet before rollout is complete
    Feb 27 12:49:23.153: INFO: Updating DaemonSet daemon-set
    Feb 27 12:49:23.153: INFO: Make sure DaemonSet rollback is complete
    Feb 27 12:49:23.160: INFO: Wrong image for pod: daemon-set-j5lkf. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Feb 27 12:49:23.160: INFO: Pod daemon-set-j5lkf is not available
    Feb 27 12:49:27.543: INFO: Pod daemon-set-fbxzp is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 12:49:27.577
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3048, will wait for the garbage collector to delete the pods 02/27/23 12:49:27.578
    Feb 27 12:49:27.655: INFO: Deleting DaemonSet.extensions daemon-set took: 20.427416ms
    Feb 27 12:49:27.756: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.615291ms
    Feb 27 12:49:30.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 12:49:30.064: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 12:49:30.076: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"100664"},"items":null}

    Feb 27 12:49:30.093: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"100664"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 12:49:30.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3048" for this suite. 02/27/23 12:49:30.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:49:30.163
Feb 27 12:49:30.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:49:30.164
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:49:30.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:49:30.207
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-d1fd8543-cb0b-481a-b76c-e91a43d6a1c7 02/27/23 12:49:30.237
STEP: Creating a pod to test consume configMaps 02/27/23 12:49:30.248
Feb 27 12:49:30.261: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d" in namespace "projected-3234" to be "Succeeded or Failed"
Feb 27 12:49:30.268: INFO: Pod "pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.31726ms
Feb 27 12:49:32.278: INFO: Pod "pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017454295s
Feb 27 12:49:34.279: INFO: Pod "pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017920086s
STEP: Saw pod success 02/27/23 12:49:34.279
Feb 27 12:49:34.279: INFO: Pod "pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d" satisfied condition "Succeeded or Failed"
Feb 27 12:49:34.286: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d container projected-configmap-volume-test: <nil>
STEP: delete the pod 02/27/23 12:49:34.303
Feb 27 12:49:34.327: INFO: Waiting for pod pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d to disappear
Feb 27 12:49:34.334: INFO: Pod pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 27 12:49:34.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3234" for this suite. 02/27/23 12:49:34.344
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":314,"skipped":5625,"failed":0}
------------------------------
• [4.199 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:49:30.163
    Feb 27 12:49:30.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:49:30.164
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:49:30.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:49:30.207
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-d1fd8543-cb0b-481a-b76c-e91a43d6a1c7 02/27/23 12:49:30.237
    STEP: Creating a pod to test consume configMaps 02/27/23 12:49:30.248
    Feb 27 12:49:30.261: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d" in namespace "projected-3234" to be "Succeeded or Failed"
    Feb 27 12:49:30.268: INFO: Pod "pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.31726ms
    Feb 27 12:49:32.278: INFO: Pod "pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017454295s
    Feb 27 12:49:34.279: INFO: Pod "pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017920086s
    STEP: Saw pod success 02/27/23 12:49:34.279
    Feb 27 12:49:34.279: INFO: Pod "pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d" satisfied condition "Succeeded or Failed"
    Feb 27 12:49:34.286: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d container projected-configmap-volume-test: <nil>
    STEP: delete the pod 02/27/23 12:49:34.303
    Feb 27 12:49:34.327: INFO: Waiting for pod pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d to disappear
    Feb 27 12:49:34.334: INFO: Pod pod-projected-configmaps-40d68552-78c2-4317-bfb5-9aa82826df2d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 27 12:49:34.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3234" for this suite. 02/27/23 12:49:34.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:49:34.364
Feb 27 12:49:34.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename podtemplate 02/27/23 12:49:34.365
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:49:34.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:49:34.41
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb 27 12:49:34.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1453" for this suite. 02/27/23 12:49:34.498
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":315,"skipped":5632,"failed":0}
------------------------------
• [0.150 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:49:34.364
    Feb 27 12:49:34.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename podtemplate 02/27/23 12:49:34.365
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:49:34.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:49:34.41
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb 27 12:49:34.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1453" for this suite. 02/27/23 12:49:34.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:49:34.517
Feb 27 12:49:34.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename container-runtime 02/27/23 12:49:34.518
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:49:34.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:49:34.571
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/27/23 12:49:34.636
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/27/23 12:49:49.84
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/27/23 12:49:49.847
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/27/23 12:49:49.861
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/27/23 12:49:49.861
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/27/23 12:49:49.905
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/27/23 12:49:52.942
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/27/23 12:49:54.968
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/27/23 12:49:54.984
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/27/23 12:49:54.984
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/27/23 12:49:55.328
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/27/23 12:49:56.353
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/27/23 12:49:59.385
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/27/23 12:49:59.404
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/27/23 12:49:59.404
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 27 12:49:59.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6711" for this suite. 02/27/23 12:49:59.488
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":316,"skipped":5647,"failed":0}
------------------------------
• [SLOW TEST] [24.991 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:49:34.517
    Feb 27 12:49:34.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename container-runtime 02/27/23 12:49:34.518
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:49:34.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:49:34.571
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/27/23 12:49:34.636
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/27/23 12:49:49.84
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/27/23 12:49:49.847
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/27/23 12:49:49.861
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/27/23 12:49:49.861
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/27/23 12:49:49.905
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/27/23 12:49:52.942
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/27/23 12:49:54.968
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/27/23 12:49:54.984
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/27/23 12:49:54.984
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/27/23 12:49:55.328
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/27/23 12:49:56.353
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/27/23 12:49:59.385
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/27/23 12:49:59.404
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/27/23 12:49:59.404
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 27 12:49:59.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6711" for this suite. 02/27/23 12:49:59.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:49:59.508
Feb 27 12:49:59.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename emptydir 02/27/23 12:49:59.509
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:49:59.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:49:59.548
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 02/27/23 12:49:59.557
Feb 27 12:49:59.574: INFO: Waiting up to 5m0s for pod "pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f" in namespace "emptydir-9924" to be "Succeeded or Failed"
Feb 27 12:49:59.589: INFO: Pod "pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.203818ms
Feb 27 12:50:01.604: INFO: Pod "pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030053851s
Feb 27 12:50:03.601: INFO: Pod "pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026860655s
STEP: Saw pod success 02/27/23 12:50:03.601
Feb 27 12:50:03.602: INFO: Pod "pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f" satisfied condition "Succeeded or Failed"
Feb 27 12:50:03.610: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f container test-container: <nil>
STEP: delete the pod 02/27/23 12:50:03.631
Feb 27 12:50:03.658: INFO: Waiting for pod pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f to disappear
Feb 27 12:50:03.668: INFO: Pod pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 27 12:50:03.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9924" for this suite. 02/27/23 12:50:03.678
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":317,"skipped":5662,"failed":0}
------------------------------
• [4.184 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:49:59.508
    Feb 27 12:49:59.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename emptydir 02/27/23 12:49:59.509
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:49:59.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:49:59.548
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/27/23 12:49:59.557
    Feb 27 12:49:59.574: INFO: Waiting up to 5m0s for pod "pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f" in namespace "emptydir-9924" to be "Succeeded or Failed"
    Feb 27 12:49:59.589: INFO: Pod "pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.203818ms
    Feb 27 12:50:01.604: INFO: Pod "pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030053851s
    Feb 27 12:50:03.601: INFO: Pod "pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026860655s
    STEP: Saw pod success 02/27/23 12:50:03.601
    Feb 27 12:50:03.602: INFO: Pod "pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f" satisfied condition "Succeeded or Failed"
    Feb 27 12:50:03.610: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f container test-container: <nil>
    STEP: delete the pod 02/27/23 12:50:03.631
    Feb 27 12:50:03.658: INFO: Waiting for pod pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f to disappear
    Feb 27 12:50:03.668: INFO: Pod pod-c4223e7e-c9d4-4cd3-b62a-d6900beaed2f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 27 12:50:03.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9924" for this suite. 02/27/23 12:50:03.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:50:03.693
Feb 27 12:50:03.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename resourcequota 02/27/23 12:50:03.695
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:03.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:03.742
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 02/27/23 12:50:03.753
STEP: Getting a ResourceQuota 02/27/23 12:50:03.767
STEP: Listing all ResourceQuotas with LabelSelector 02/27/23 12:50:03.777
STEP: Patching the ResourceQuota 02/27/23 12:50:03.795
STEP: Deleting a Collection of ResourceQuotas 02/27/23 12:50:03.812
STEP: Verifying the deleted ResourceQuota 02/27/23 12:50:03.836
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 27 12:50:03.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7341" for this suite. 02/27/23 12:50:03.859
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":318,"skipped":5667,"failed":0}
------------------------------
• [0.182 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:50:03.693
    Feb 27 12:50:03.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename resourcequota 02/27/23 12:50:03.695
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:03.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:03.742
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 02/27/23 12:50:03.753
    STEP: Getting a ResourceQuota 02/27/23 12:50:03.767
    STEP: Listing all ResourceQuotas with LabelSelector 02/27/23 12:50:03.777
    STEP: Patching the ResourceQuota 02/27/23 12:50:03.795
    STEP: Deleting a Collection of ResourceQuotas 02/27/23 12:50:03.812
    STEP: Verifying the deleted ResourceQuota 02/27/23 12:50:03.836
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 27 12:50:03.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7341" for this suite. 02/27/23 12:50:03.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:50:03.889
Feb 27 12:50:03.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 12:50:03.901
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:03.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:03.945
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-8e648807-93f1-45a8-b1ed-90fde5f242a4 02/27/23 12:50:03.957
STEP: Creating a pod to test consume secrets 02/27/23 12:50:03.972
Feb 27 12:50:03.991: INFO: Waiting up to 5m0s for pod "pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f" in namespace "secrets-1204" to be "Succeeded or Failed"
Feb 27 12:50:03.999: INFO: Pod "pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.906311ms
Feb 27 12:50:06.015: INFO: Pod "pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023784057s
Feb 27 12:50:08.008: INFO: Pod "pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016769689s
STEP: Saw pod success 02/27/23 12:50:08.008
Feb 27 12:50:08.008: INFO: Pod "pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f" satisfied condition "Succeeded or Failed"
Feb 27 12:50:08.016: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f container secret-env-test: <nil>
STEP: delete the pod 02/27/23 12:50:08.032
Feb 27 12:50:08.061: INFO: Waiting for pod pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f to disappear
Feb 27 12:50:08.070: INFO: Pod pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 27 12:50:08.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1204" for this suite. 02/27/23 12:50:08.082
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":319,"skipped":5764,"failed":0}
------------------------------
• [4.208 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:50:03.889
    Feb 27 12:50:03.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 12:50:03.901
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:03.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:03.945
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-8e648807-93f1-45a8-b1ed-90fde5f242a4 02/27/23 12:50:03.957
    STEP: Creating a pod to test consume secrets 02/27/23 12:50:03.972
    Feb 27 12:50:03.991: INFO: Waiting up to 5m0s for pod "pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f" in namespace "secrets-1204" to be "Succeeded or Failed"
    Feb 27 12:50:03.999: INFO: Pod "pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.906311ms
    Feb 27 12:50:06.015: INFO: Pod "pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023784057s
    Feb 27 12:50:08.008: INFO: Pod "pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016769689s
    STEP: Saw pod success 02/27/23 12:50:08.008
    Feb 27 12:50:08.008: INFO: Pod "pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f" satisfied condition "Succeeded or Failed"
    Feb 27 12:50:08.016: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f container secret-env-test: <nil>
    STEP: delete the pod 02/27/23 12:50:08.032
    Feb 27 12:50:08.061: INFO: Waiting for pod pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f to disappear
    Feb 27 12:50:08.070: INFO: Pod pod-secrets-2c0c9828-ff06-4781-9ebe-afe869773d3f no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 12:50:08.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1204" for this suite. 02/27/23 12:50:08.082
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:50:08.097
Feb 27 12:50:08.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename sysctl 02/27/23 12:50:08.099
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:08.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:08.14
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/27/23 12:50:08.152
STEP: Watching for error events or started pod 02/27/23 12:50:08.164
STEP: Waiting for pod completion 02/27/23 12:50:10.179
Feb 27 12:50:10.180: INFO: Waiting up to 3m0s for pod "sysctl-bb3f9cda-8f3e-4522-87ac-5faf04f16541" in namespace "sysctl-5076" to be "completed"
Feb 27 12:50:10.233: INFO: Pod "sysctl-bb3f9cda-8f3e-4522-87ac-5faf04f16541": Phase="Pending", Reason="", readiness=false. Elapsed: 53.347578ms
Feb 27 12:50:12.244: INFO: Pod "sysctl-bb3f9cda-8f3e-4522-87ac-5faf04f16541": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.064190943s
Feb 27 12:50:12.244: INFO: Pod "sysctl-bb3f9cda-8f3e-4522-87ac-5faf04f16541" satisfied condition "completed"
STEP: Checking that the pod succeeded 02/27/23 12:50:12.252
STEP: Getting logs from the pod 02/27/23 12:50:12.252
STEP: Checking that the sysctl is actually updated 02/27/23 12:50:12.265
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 27 12:50:12.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5076" for this suite. 02/27/23 12:50:12.276
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":320,"skipped":5764,"failed":0}
------------------------------
• [4.192 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:50:08.097
    Feb 27 12:50:08.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename sysctl 02/27/23 12:50:08.099
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:08.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:08.14
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/27/23 12:50:08.152
    STEP: Watching for error events or started pod 02/27/23 12:50:08.164
    STEP: Waiting for pod completion 02/27/23 12:50:10.179
    Feb 27 12:50:10.180: INFO: Waiting up to 3m0s for pod "sysctl-bb3f9cda-8f3e-4522-87ac-5faf04f16541" in namespace "sysctl-5076" to be "completed"
    Feb 27 12:50:10.233: INFO: Pod "sysctl-bb3f9cda-8f3e-4522-87ac-5faf04f16541": Phase="Pending", Reason="", readiness=false. Elapsed: 53.347578ms
    Feb 27 12:50:12.244: INFO: Pod "sysctl-bb3f9cda-8f3e-4522-87ac-5faf04f16541": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.064190943s
    Feb 27 12:50:12.244: INFO: Pod "sysctl-bb3f9cda-8f3e-4522-87ac-5faf04f16541" satisfied condition "completed"
    STEP: Checking that the pod succeeded 02/27/23 12:50:12.252
    STEP: Getting logs from the pod 02/27/23 12:50:12.252
    STEP: Checking that the sysctl is actually updated 02/27/23 12:50:12.265
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 27 12:50:12.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-5076" for this suite. 02/27/23 12:50:12.276
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:50:12.291
Feb 27 12:50:12.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename containers 02/27/23 12:50:12.292
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:12.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:12.348
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Feb 27 12:50:12.383: INFO: Waiting up to 5m0s for pod "client-containers-08294592-0bdf-4c03-988b-a13abc92128b" in namespace "containers-396" to be "running"
Feb 27 12:50:12.395: INFO: Pod "client-containers-08294592-0bdf-4c03-988b-a13abc92128b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.726797ms
Feb 27 12:50:14.403: INFO: Pod "client-containers-08294592-0bdf-4c03-988b-a13abc92128b": Phase="Running", Reason="", readiness=true. Elapsed: 2.019532388s
Feb 27 12:50:14.403: INFO: Pod "client-containers-08294592-0bdf-4c03-988b-a13abc92128b" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 27 12:50:14.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-396" for this suite. 02/27/23 12:50:14.434
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":321,"skipped":5764,"failed":0}
------------------------------
• [2.157 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:50:12.291
    Feb 27 12:50:12.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename containers 02/27/23 12:50:12.292
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:12.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:12.348
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Feb 27 12:50:12.383: INFO: Waiting up to 5m0s for pod "client-containers-08294592-0bdf-4c03-988b-a13abc92128b" in namespace "containers-396" to be "running"
    Feb 27 12:50:12.395: INFO: Pod "client-containers-08294592-0bdf-4c03-988b-a13abc92128b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.726797ms
    Feb 27 12:50:14.403: INFO: Pod "client-containers-08294592-0bdf-4c03-988b-a13abc92128b": Phase="Running", Reason="", readiness=true. Elapsed: 2.019532388s
    Feb 27 12:50:14.403: INFO: Pod "client-containers-08294592-0bdf-4c03-988b-a13abc92128b" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 27 12:50:14.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-396" for this suite. 02/27/23 12:50:14.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:50:14.454
Feb 27 12:50:14.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pods 02/27/23 12:50:14.455
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:14.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:14.495
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 02/27/23 12:50:14.508
Feb 27 12:50:14.524: INFO: created test-pod-1
Feb 27 12:50:14.536: INFO: created test-pod-2
Feb 27 12:50:14.550: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 02/27/23 12:50:14.55
Feb 27 12:50:14.551: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7533' to be running and ready
Feb 27 12:50:14.576: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 27 12:50:14.576: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 27 12:50:14.576: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 27 12:50:14.576: INFO: 0 / 3 pods in namespace 'pods-7533' are running and ready (0 seconds elapsed)
Feb 27 12:50:14.576: INFO: expected 0 pod replicas in namespace 'pods-7533', 0 are Running and Ready.
Feb 27 12:50:14.577: INFO: POD         NODE                                           PHASE    GRACE  CONDITIONS
Feb 27 12:50:14.577: INFO: test-pod-1  ip-172-31-7-167.eu-central-1.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:50:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:50:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:50:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:50:14 +0000 UTC  }]
Feb 27 12:50:14.577: INFO: test-pod-2  ip-172-31-7-167.eu-central-1.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:50:14 +0000 UTC  }]
Feb 27 12:50:14.577: INFO: test-pod-3  ip-172-31-7-167.eu-central-1.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:50:14 +0000 UTC  }]
Feb 27 12:50:14.577: INFO: 
Feb 27 12:50:16.600: INFO: 3 / 3 pods in namespace 'pods-7533' are running and ready (2 seconds elapsed)
Feb 27 12:50:16.600: INFO: expected 0 pod replicas in namespace 'pods-7533', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 02/27/23 12:50:16.643
Feb 27 12:50:16.655: INFO: Pod quantity 3 is different from expected quantity 0
Feb 27 12:50:17.675: INFO: Pod quantity 3 is different from expected quantity 0
Feb 27 12:50:18.667: INFO: Pod quantity 3 is different from expected quantity 0
Feb 27 12:50:19.664: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 27 12:50:20.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7533" for this suite. 02/27/23 12:50:20.672
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":322,"skipped":5769,"failed":0}
------------------------------
• [SLOW TEST] [6.232 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:50:14.454
    Feb 27 12:50:14.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pods 02/27/23 12:50:14.455
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:14.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:14.495
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 02/27/23 12:50:14.508
    Feb 27 12:50:14.524: INFO: created test-pod-1
    Feb 27 12:50:14.536: INFO: created test-pod-2
    Feb 27 12:50:14.550: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 02/27/23 12:50:14.55
    Feb 27 12:50:14.551: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7533' to be running and ready
    Feb 27 12:50:14.576: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 27 12:50:14.576: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 27 12:50:14.576: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 27 12:50:14.576: INFO: 0 / 3 pods in namespace 'pods-7533' are running and ready (0 seconds elapsed)
    Feb 27 12:50:14.576: INFO: expected 0 pod replicas in namespace 'pods-7533', 0 are Running and Ready.
    Feb 27 12:50:14.577: INFO: POD         NODE                                           PHASE    GRACE  CONDITIONS
    Feb 27 12:50:14.577: INFO: test-pod-1  ip-172-31-7-167.eu-central-1.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:50:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:50:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:50:14 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:50:14 +0000 UTC  }]
    Feb 27 12:50:14.577: INFO: test-pod-2  ip-172-31-7-167.eu-central-1.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:50:14 +0000 UTC  }]
    Feb 27 12:50:14.577: INFO: test-pod-3  ip-172-31-7-167.eu-central-1.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:50:14 +0000 UTC  }]
    Feb 27 12:50:14.577: INFO: 
    Feb 27 12:50:16.600: INFO: 3 / 3 pods in namespace 'pods-7533' are running and ready (2 seconds elapsed)
    Feb 27 12:50:16.600: INFO: expected 0 pod replicas in namespace 'pods-7533', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 02/27/23 12:50:16.643
    Feb 27 12:50:16.655: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 27 12:50:17.675: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 27 12:50:18.667: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 27 12:50:19.664: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 27 12:50:20.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7533" for this suite. 02/27/23 12:50:20.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:50:20.688
Feb 27 12:50:20.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename sched-pred 02/27/23 12:50:20.689
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:20.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:20.73
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 27 12:50:20.745: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 27 12:50:20.778: INFO: Waiting for terminating namespaces to be deleted...
Feb 27 12:50:20.792: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-11-159.eu-central-1.compute.internal before test
Feb 27 12:50:20.821: INFO: canal-4q9m8 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
Feb 27 12:50:20.821: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 12:50:20.821: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 27 12:50:20.821: INFO: ebs-csi-node-b6z5h from kube-system started at 2023-02-27 09:22:11 +0000 UTC (3 container statuses recorded)
Feb 27 12:50:20.821: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 12:50:20.821: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 12:50:20.821: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 12:50:20.821: INFO: envoy-agent-2wwht from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
Feb 27 12:50:20.821: INFO: 	Container assign-address ready: true, restart count 0
Feb 27 12:50:20.821: INFO: 	Container envoy-agent ready: true, restart count 0
Feb 27 12:50:20.821: INFO: kube-proxy-cz9zt from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.821: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 27 12:50:20.821: INFO: node-local-dns-r6r4k from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.821: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 12:50:20.821: INFO: user-ssh-keys-agent-rk5t9 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.821: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 27 12:50:20.821: INFO: sonobuoy from sonobuoy started at 2023-02-27 11:19:26 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.821: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 27 12:50:20.821: INFO: sonobuoy-e2e-job-18131847dfcd49d5 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 12:50:20.821: INFO: 	Container e2e ready: true, restart count 0
Feb 27 12:50:20.821: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 12:50:20.821: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-rlhml from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 12:50:20.821: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 12:50:20.821: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 12:50:20.821: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-15-17.eu-central-1.compute.internal before test
Feb 27 12:50:20.840: INFO: calico-kube-controllers-55d99d998f-f5ngs from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.841: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 27 12:50:20.841: INFO: canal-bxq4m from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
Feb 27 12:50:20.841: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 12:50:20.841: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 27 12:50:20.841: INFO: coredns-bf8668b4f-5h5v9 from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.841: INFO: 	Container coredns ready: true, restart count 0
Feb 27 12:50:20.841: INFO: ebs-csi-node-z9l5x from kube-system started at 2023-02-27 09:22:52 +0000 UTC (3 container statuses recorded)
Feb 27 12:50:20.841: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 12:50:20.841: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 12:50:20.841: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 12:50:20.841: INFO: envoy-agent-7xhs5 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
Feb 27 12:50:20.841: INFO: 	Container assign-address ready: true, restart count 0
Feb 27 12:50:20.841: INFO: 	Container envoy-agent ready: true, restart count 0
Feb 27 12:50:20.841: INFO: konnectivity-agent-76c848fdd6-56hgq from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.841: INFO: 	Container konnectivity-agent ready: true, restart count 0
Feb 27 12:50:20.841: INFO: kube-proxy-hxmbw from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.841: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 27 12:50:20.841: INFO: metrics-server-5f7c5d4b9-xlr2c from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.841: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 12:50:20.841: INFO: node-local-dns-8z787 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.841: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 12:50:20.841: INFO: user-ssh-keys-agent-fvhdn from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.841: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 27 12:50:20.841: INFO: dashboard-metrics-scraper-85f6dd84d5-2qtjs from kubernetes-dashboard started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.841: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 27 12:50:20.841: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-94vkz from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 12:50:20.841: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 12:50:20.841: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 12:50:20.841: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-7-167.eu-central-1.compute.internal before test
Feb 27 12:50:20.930: INFO: canal-mbg4r from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
Feb 27 12:50:20.930: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 12:50:20.930: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 27 12:50:20.930: INFO: coredns-bf8668b4f-wttzt from kube-system started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.930: INFO: 	Container coredns ready: true, restart count 0
Feb 27 12:50:20.930: INFO: ebs-csi-controller-54c5c66b84-mkj54 from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
Feb 27 12:50:20.930: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 27 12:50:20.930: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 27 12:50:20.931: INFO: 	Container csi-resizer ready: true, restart count 0
Feb 27 12:50:20.931: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 12:50:20.931: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 12:50:20.931: INFO: ebs-csi-controller-54c5c66b84-rxhdh from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
Feb 27 12:50:20.931: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 27 12:50:20.931: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 27 12:50:20.931: INFO: 	Container csi-resizer ready: true, restart count 0
Feb 27 12:50:20.931: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 12:50:20.931: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 12:50:20.931: INFO: ebs-csi-node-7dvrp from kube-system started at 2023-02-27 09:22:08 +0000 UTC (3 container statuses recorded)
Feb 27 12:50:20.931: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 27 12:50:20.931: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 12:50:20.931: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 12:50:20.931: INFO: envoy-agent-scd88 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
Feb 27 12:50:20.931: INFO: 	Container assign-address ready: true, restart count 0
Feb 27 12:50:20.931: INFO: 	Container envoy-agent ready: true, restart count 0
Feb 27 12:50:20.931: INFO: konnectivity-agent-76c848fdd6-bn8rz from kube-system started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.931: INFO: 	Container konnectivity-agent ready: true, restart count 0
Feb 27 12:50:20.931: INFO: kube-proxy-ghd44 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.931: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 27 12:50:20.931: INFO: metrics-server-5f7c5d4b9-89jfg from kube-system started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.931: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 12:50:20.931: INFO: node-local-dns-kfv2k from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.931: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 12:50:20.931: INFO: user-ssh-keys-agent-gjs99 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.931: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
Feb 27 12:50:20.931: INFO: dashboard-metrics-scraper-85f6dd84d5-hdh84 from kubernetes-dashboard started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
Feb 27 12:50:20.931: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 27 12:50:20.931: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-k5l74 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
Feb 27 12:50:20.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 12:50:20.931: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 12:50:20.931
Feb 27 12:50:20.953: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1997" to be "running"
Feb 27 12:50:21.037: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 84.515515ms
Feb 27 12:50:23.052: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.099181029s
Feb 27 12:50:25.045: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.092463927s
Feb 27 12:50:25.045: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 12:50:25.063
STEP: Trying to apply a random label on the found node. 02/27/23 12:50:25.094
STEP: verifying the node has the label kubernetes.io/e2e-ee1913e8-4f7b-499c-a0da-eaab750b4ab0 42 02/27/23 12:50:25.126
STEP: Trying to relaunch the pod, now with labels. 02/27/23 12:50:25.136
Feb 27 12:50:25.145: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1997" to be "not pending"
Feb 27 12:50:25.155: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 9.530208ms
Feb 27 12:50:27.163: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.017913266s
Feb 27 12:50:27.163: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-ee1913e8-4f7b-499c-a0da-eaab750b4ab0 off the node ip-172-31-15-17.eu-central-1.compute.internal 02/27/23 12:50:27.171
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ee1913e8-4f7b-499c-a0da-eaab750b4ab0 02/27/23 12:50:27.197
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 27 12:50:27.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1997" for this suite. 02/27/23 12:50:27.226
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":323,"skipped":5790,"failed":0}
------------------------------
• [SLOW TEST] [6.563 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:50:20.688
    Feb 27 12:50:20.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename sched-pred 02/27/23 12:50:20.689
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:20.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:20.73
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 27 12:50:20.745: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 27 12:50:20.778: INFO: Waiting for terminating namespaces to be deleted...
    Feb 27 12:50:20.792: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-11-159.eu-central-1.compute.internal before test
    Feb 27 12:50:20.821: INFO: canal-4q9m8 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
    Feb 27 12:50:20.821: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: ebs-csi-node-b6z5h from kube-system started at 2023-02-27 09:22:11 +0000 UTC (3 container statuses recorded)
    Feb 27 12:50:20.821: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: envoy-agent-2wwht from kube-system started at 2023-02-27 09:22:11 +0000 UTC (2 container statuses recorded)
    Feb 27 12:50:20.821: INFO: 	Container assign-address ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: 	Container envoy-agent ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: kube-proxy-cz9zt from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.821: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: node-local-dns-r6r4k from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.821: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: user-ssh-keys-agent-rk5t9 from kube-system started at 2023-02-27 09:22:11 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.821: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: sonobuoy from sonobuoy started at 2023-02-27 11:19:26 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.821: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: sonobuoy-e2e-job-18131847dfcd49d5 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 12:50:20.821: INFO: 	Container e2e ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-rlhml from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 12:50:20.821: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 12:50:20.821: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-15-17.eu-central-1.compute.internal before test
    Feb 27 12:50:20.840: INFO: calico-kube-controllers-55d99d998f-f5ngs from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.841: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: canal-bxq4m from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
    Feb 27 12:50:20.841: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: coredns-bf8668b4f-5h5v9 from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.841: INFO: 	Container coredns ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: ebs-csi-node-z9l5x from kube-system started at 2023-02-27 09:22:52 +0000 UTC (3 container statuses recorded)
    Feb 27 12:50:20.841: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: envoy-agent-7xhs5 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (2 container statuses recorded)
    Feb 27 12:50:20.841: INFO: 	Container assign-address ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: 	Container envoy-agent ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: konnectivity-agent-76c848fdd6-56hgq from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.841: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: kube-proxy-hxmbw from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.841: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: metrics-server-5f7c5d4b9-xlr2c from kube-system started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.841: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: node-local-dns-8z787 from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.841: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: user-ssh-keys-agent-fvhdn from kube-system started at 2023-02-27 09:22:52 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.841: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: dashboard-metrics-scraper-85f6dd84d5-2qtjs from kubernetes-dashboard started at 2023-02-27 09:55:21 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.841: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-94vkz from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 12:50:20.841: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 12:50:20.841: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-7-167.eu-central-1.compute.internal before test
    Feb 27 12:50:20.930: INFO: canal-mbg4r from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
    Feb 27 12:50:20.930: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 12:50:20.930: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 27 12:50:20.930: INFO: coredns-bf8668b4f-wttzt from kube-system started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.930: INFO: 	Container coredns ready: true, restart count 0
    Feb 27 12:50:20.930: INFO: ebs-csi-controller-54c5c66b84-mkj54 from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
    Feb 27 12:50:20.930: INFO: 	Container csi-attacher ready: true, restart count 0
    Feb 27 12:50:20.930: INFO: 	Container csi-provisioner ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: 	Container csi-resizer ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: ebs-csi-controller-54c5c66b84-rxhdh from kube-system started at 2023-02-27 09:22:51 +0000 UTC (5 container statuses recorded)
    Feb 27 12:50:20.931: INFO: 	Container csi-attacher ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: 	Container csi-provisioner ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: 	Container csi-resizer ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: ebs-csi-node-7dvrp from kube-system started at 2023-02-27 09:22:08 +0000 UTC (3 container statuses recorded)
    Feb 27 12:50:20.931: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: envoy-agent-scd88 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (2 container statuses recorded)
    Feb 27 12:50:20.931: INFO: 	Container assign-address ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: 	Container envoy-agent ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: konnectivity-agent-76c848fdd6-bn8rz from kube-system started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.931: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: kube-proxy-ghd44 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.931: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: metrics-server-5f7c5d4b9-89jfg from kube-system started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.931: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: node-local-dns-kfv2k from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.931: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: user-ssh-keys-agent-gjs99 from kube-system started at 2023-02-27 09:22:08 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.931: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: dashboard-metrics-scraper-85f6dd84d5-hdh84 from kubernetes-dashboard started at 2023-02-27 12:40:07 +0000 UTC (1 container statuses recorded)
    Feb 27 12:50:20.931: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: sonobuoy-systemd-logs-daemon-set-52382428c7df4090-k5l74 from sonobuoy started at 2023-02-27 11:19:27 +0000 UTC (2 container statuses recorded)
    Feb 27 12:50:20.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 12:50:20.931: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 12:50:20.931
    Feb 27 12:50:20.953: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1997" to be "running"
    Feb 27 12:50:21.037: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 84.515515ms
    Feb 27 12:50:23.052: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.099181029s
    Feb 27 12:50:25.045: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.092463927s
    Feb 27 12:50:25.045: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 12:50:25.063
    STEP: Trying to apply a random label on the found node. 02/27/23 12:50:25.094
    STEP: verifying the node has the label kubernetes.io/e2e-ee1913e8-4f7b-499c-a0da-eaab750b4ab0 42 02/27/23 12:50:25.126
    STEP: Trying to relaunch the pod, now with labels. 02/27/23 12:50:25.136
    Feb 27 12:50:25.145: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1997" to be "not pending"
    Feb 27 12:50:25.155: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 9.530208ms
    Feb 27 12:50:27.163: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.017913266s
    Feb 27 12:50:27.163: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-ee1913e8-4f7b-499c-a0da-eaab750b4ab0 off the node ip-172-31-15-17.eu-central-1.compute.internal 02/27/23 12:50:27.171
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-ee1913e8-4f7b-499c-a0da-eaab750b4ab0 02/27/23 12:50:27.197
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 12:50:27.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1997" for this suite. 02/27/23 12:50:27.226
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:50:27.252
Feb 27 12:50:27.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 12:50:27.253
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:27.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:27.321
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-dfa3c9b9-f6c7-479f-9ffd-f92947027917 02/27/23 12:50:27.33
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 27 12:50:27.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2810" for this suite. 02/27/23 12:50:27.344
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":324,"skipped":5807,"failed":0}
------------------------------
• [0.105 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:50:27.252
    Feb 27 12:50:27.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 12:50:27.253
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:27.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:27.321
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-dfa3c9b9-f6c7-479f-9ffd-f92947027917 02/27/23 12:50:27.33
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 12:50:27.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2810" for this suite. 02/27/23 12:50:27.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:50:27.363
Feb 27 12:50:27.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 12:50:27.365
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:27.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:27.412
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4222 02/27/23 12:50:27.427
STEP: changing the ExternalName service to type=ClusterIP 02/27/23 12:50:27.448
STEP: creating replication controller externalname-service in namespace services-4222 02/27/23 12:50:27.483
I0227 12:50:27.496576      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4222, replica count: 2
I0227 12:50:30.549468      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 12:50:30.549: INFO: Creating new exec pod
Feb 27 12:50:30.568: INFO: Waiting up to 5m0s for pod "execpodnxnqh" in namespace "services-4222" to be "running"
Feb 27 12:50:30.577: INFO: Pod "execpodnxnqh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.809976ms
Feb 27 12:50:32.589: INFO: Pod "execpodnxnqh": Phase="Running", Reason="", readiness=true. Elapsed: 2.020574838s
Feb 27 12:50:32.589: INFO: Pod "execpodnxnqh" satisfied condition "running"
Feb 27 12:50:33.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-4222 exec execpodnxnqh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 27 12:50:33.855: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 27 12:50:33.855: INFO: stdout: "externalname-service-kmd9n"
Feb 27 12:50:33.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-4222 exec execpodnxnqh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.28.152 80'
Feb 27 12:50:34.096: INFO: stderr: "+ nc -v -t -w 2 10.240.28.152 80\nConnection to 10.240.28.152 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Feb 27 12:50:34.096: INFO: stdout: "externalname-service-wb8vc"
Feb 27 12:50:34.096: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 12:50:34.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4222" for this suite. 02/27/23 12:50:34.17
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":325,"skipped":5832,"failed":0}
------------------------------
• [SLOW TEST] [6.818 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:50:27.363
    Feb 27 12:50:27.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 12:50:27.365
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:27.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:27.412
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4222 02/27/23 12:50:27.427
    STEP: changing the ExternalName service to type=ClusterIP 02/27/23 12:50:27.448
    STEP: creating replication controller externalname-service in namespace services-4222 02/27/23 12:50:27.483
    I0227 12:50:27.496576      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4222, replica count: 2
    I0227 12:50:30.549468      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 12:50:30.549: INFO: Creating new exec pod
    Feb 27 12:50:30.568: INFO: Waiting up to 5m0s for pod "execpodnxnqh" in namespace "services-4222" to be "running"
    Feb 27 12:50:30.577: INFO: Pod "execpodnxnqh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.809976ms
    Feb 27 12:50:32.589: INFO: Pod "execpodnxnqh": Phase="Running", Reason="", readiness=true. Elapsed: 2.020574838s
    Feb 27 12:50:32.589: INFO: Pod "execpodnxnqh" satisfied condition "running"
    Feb 27 12:50:33.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-4222 exec execpodnxnqh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 27 12:50:33.855: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 27 12:50:33.855: INFO: stdout: "externalname-service-kmd9n"
    Feb 27 12:50:33.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-4222 exec execpodnxnqh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.28.152 80'
    Feb 27 12:50:34.096: INFO: stderr: "+ nc -v -t -w 2 10.240.28.152 80\nConnection to 10.240.28.152 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Feb 27 12:50:34.096: INFO: stdout: "externalname-service-wb8vc"
    Feb 27 12:50:34.096: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 12:50:34.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4222" for this suite. 02/27/23 12:50:34.17
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:50:34.182
Feb 27 12:50:34.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename gc 02/27/23 12:50:34.184
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:34.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:34.232
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 02/27/23 12:50:34.251
STEP: delete the rc 02/27/23 12:50:39.287
STEP: wait for the rc to be deleted 02/27/23 12:50:39.313
Feb 27 12:50:40.446: INFO: 80 pods remaining
Feb 27 12:50:40.446: INFO: 80 pods has nil DeletionTimestamp
Feb 27 12:50:40.446: INFO: 
Feb 27 12:50:41.391: INFO: 69 pods remaining
Feb 27 12:50:41.391: INFO: 69 pods has nil DeletionTimestamp
Feb 27 12:50:41.391: INFO: 
Feb 27 12:50:42.484: INFO: 60 pods remaining
Feb 27 12:50:42.485: INFO: 60 pods has nil DeletionTimestamp
Feb 27 12:50:42.485: INFO: 
Feb 27 12:50:43.334: INFO: 40 pods remaining
Feb 27 12:50:43.334: INFO: 40 pods has nil DeletionTimestamp
Feb 27 12:50:43.334: INFO: 
Feb 27 12:50:44.588: INFO: 25 pods remaining
Feb 27 12:50:44.588: INFO: 25 pods has nil DeletionTimestamp
Feb 27 12:50:44.588: INFO: 
Feb 27 12:50:45.344: INFO: 20 pods remaining
Feb 27 12:50:45.344: INFO: 20 pods has nil DeletionTimestamp
Feb 27 12:50:45.344: INFO: 
STEP: Gathering metrics 02/27/23 12:50:46.327
W0227 12:50:46.349058      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 27 12:50:46.349: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 27 12:50:46.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5605" for this suite. 02/27/23 12:50:46.36
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":326,"skipped":5842,"failed":0}
------------------------------
• [SLOW TEST] [12.194 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:50:34.182
    Feb 27 12:50:34.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename gc 02/27/23 12:50:34.184
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:34.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:34.232
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 02/27/23 12:50:34.251
    STEP: delete the rc 02/27/23 12:50:39.287
    STEP: wait for the rc to be deleted 02/27/23 12:50:39.313
    Feb 27 12:50:40.446: INFO: 80 pods remaining
    Feb 27 12:50:40.446: INFO: 80 pods has nil DeletionTimestamp
    Feb 27 12:50:40.446: INFO: 
    Feb 27 12:50:41.391: INFO: 69 pods remaining
    Feb 27 12:50:41.391: INFO: 69 pods has nil DeletionTimestamp
    Feb 27 12:50:41.391: INFO: 
    Feb 27 12:50:42.484: INFO: 60 pods remaining
    Feb 27 12:50:42.485: INFO: 60 pods has nil DeletionTimestamp
    Feb 27 12:50:42.485: INFO: 
    Feb 27 12:50:43.334: INFO: 40 pods remaining
    Feb 27 12:50:43.334: INFO: 40 pods has nil DeletionTimestamp
    Feb 27 12:50:43.334: INFO: 
    Feb 27 12:50:44.588: INFO: 25 pods remaining
    Feb 27 12:50:44.588: INFO: 25 pods has nil DeletionTimestamp
    Feb 27 12:50:44.588: INFO: 
    Feb 27 12:50:45.344: INFO: 20 pods remaining
    Feb 27 12:50:45.344: INFO: 20 pods has nil DeletionTimestamp
    Feb 27 12:50:45.344: INFO: 
    STEP: Gathering metrics 02/27/23 12:50:46.327
    W0227 12:50:46.349058      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 27 12:50:46.349: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 27 12:50:46.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5605" for this suite. 02/27/23 12:50:46.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:50:46.378
Feb 27 12:50:46.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename crd-webhook 02/27/23 12:50:46.77
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:46.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:46.805
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/27/23 12:50:46.813
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/27/23 12:50:49.608
STEP: Deploying the custom resource conversion webhook pod 02/27/23 12:50:49.881
STEP: Wait for the deployment to be ready 02/27/23 12:50:49.908
Feb 27 12:50:49.938: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 27 12:50:51.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 12, 50, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 50, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 50, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 50, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 12:50:53.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 12, 50, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 50, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 50, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 50, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/27/23 12:50:55.989
STEP: Verifying the service has paired with the endpoint 02/27/23 12:50:56.009
Feb 27 12:50:57.011: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Feb 27 12:50:57.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Creating a v1 custom resource 02/27/23 12:50:59.8
STEP: Create a v2 custom resource 02/27/23 12:50:59.828
STEP: List CRs in v1 02/27/23 12:50:59.923
STEP: List CRs in v2 02/27/23 12:50:59.944
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:51:00.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5675" for this suite. 02/27/23 12:51:00.517
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":327,"skipped":5864,"failed":0}
------------------------------
• [SLOW TEST] [14.272 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:50:46.378
    Feb 27 12:50:46.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename crd-webhook 02/27/23 12:50:46.77
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:50:46.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:50:46.805
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/27/23 12:50:46.813
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/27/23 12:50:49.608
    STEP: Deploying the custom resource conversion webhook pod 02/27/23 12:50:49.881
    STEP: Wait for the deployment to be ready 02/27/23 12:50:49.908
    Feb 27 12:50:49.938: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Feb 27 12:50:51.981: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 12, 50, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 50, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 50, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 50, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 12:50:53.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 12, 50, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 50, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 12, 50, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 12, 50, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/27/23 12:50:55.989
    STEP: Verifying the service has paired with the endpoint 02/27/23 12:50:56.009
    Feb 27 12:50:57.011: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Feb 27 12:50:57.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Creating a v1 custom resource 02/27/23 12:50:59.8
    STEP: Create a v2 custom resource 02/27/23 12:50:59.828
    STEP: List CRs in v1 02/27/23 12:50:59.923
    STEP: List CRs in v2 02/27/23 12:50:59.944
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:51:00.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-5675" for this suite. 02/27/23 12:51:00.517
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:51:00.651
Feb 27 12:51:00.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename webhook 02/27/23 12:51:00.652
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:00.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:00.72
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/27/23 12:51:00.784
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 12:51:01.153
STEP: Deploying the webhook pod 02/27/23 12:51:01.164
STEP: Wait for the deployment to be ready 02/27/23 12:51:01.188
Feb 27 12:51:01.213: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 12:51:03.271
STEP: Verifying the service has paired with the endpoint 02/27/23 12:51:03.309
Feb 27 12:51:04.310: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/27/23 12:51:04.32
STEP: create a pod that should be updated by the webhook 02/27/23 12:51:04.359
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 27 12:51:04.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-512" for this suite. 02/27/23 12:51:04.419
STEP: Destroying namespace "webhook-512-markers" for this suite. 02/27/23 12:51:04.43
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":328,"skipped":5871,"failed":0}
------------------------------
• [3.894 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:51:00.651
    Feb 27 12:51:00.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename webhook 02/27/23 12:51:00.652
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:00.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:00.72
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/27/23 12:51:00.784
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 12:51:01.153
    STEP: Deploying the webhook pod 02/27/23 12:51:01.164
    STEP: Wait for the deployment to be ready 02/27/23 12:51:01.188
    Feb 27 12:51:01.213: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 12:51:03.271
    STEP: Verifying the service has paired with the endpoint 02/27/23 12:51:03.309
    Feb 27 12:51:04.310: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/27/23 12:51:04.32
    STEP: create a pod that should be updated by the webhook 02/27/23 12:51:04.359
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 27 12:51:04.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-512" for this suite. 02/27/23 12:51:04.419
    STEP: Destroying namespace "webhook-512-markers" for this suite. 02/27/23 12:51:04.43
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:51:04.549
Feb 27 12:51:04.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 12:51:04.55
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:04.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:04.588
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 02/27/23 12:51:04.608
Feb 27 12:51:04.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1274 create -f -'
Feb 27 12:51:06.078: INFO: stderr: ""
Feb 27 12:51:06.078: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 02/27/23 12:51:06.078
Feb 27 12:51:06.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1274 diff -f -'
Feb 27 12:51:07.432: INFO: rc: 1
Feb 27 12:51:07.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1274 delete -f -'
Feb 27 12:51:07.539: INFO: stderr: ""
Feb 27 12:51:07.539: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 12:51:07.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1274" for this suite. 02/27/23 12:51:07.551
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":329,"skipped":5888,"failed":0}
------------------------------
• [3.024 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:51:04.549
    Feb 27 12:51:04.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 12:51:04.55
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:04.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:04.588
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 02/27/23 12:51:04.608
    Feb 27 12:51:04.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1274 create -f -'
    Feb 27 12:51:06.078: INFO: stderr: ""
    Feb 27 12:51:06.078: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 02/27/23 12:51:06.078
    Feb 27 12:51:06.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1274 diff -f -'
    Feb 27 12:51:07.432: INFO: rc: 1
    Feb 27 12:51:07.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1274 delete -f -'
    Feb 27 12:51:07.539: INFO: stderr: ""
    Feb 27 12:51:07.539: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 12:51:07.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1274" for this suite. 02/27/23 12:51:07.551
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:51:07.573
Feb 27 12:51:07.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:51:07.575
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:07.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:07.605
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 02/27/23 12:51:07.612
Feb 27 12:51:07.628: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2" in namespace "projected-952" to be "Succeeded or Failed"
Feb 27 12:51:07.636: INFO: Pod "downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011973ms
Feb 27 12:51:09.654: INFO: Pod "downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026155591s
Feb 27 12:51:11.647: INFO: Pod "downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019233576s
STEP: Saw pod success 02/27/23 12:51:11.648
Feb 27 12:51:11.648: INFO: Pod "downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2" satisfied condition "Succeeded or Failed"
Feb 27 12:51:11.656: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2 container client-container: <nil>
STEP: delete the pod 02/27/23 12:51:11.672
Feb 27 12:51:11.698: INFO: Waiting for pod downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2 to disappear
Feb 27 12:51:11.714: INFO: Pod downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 27 12:51:11.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-952" for this suite. 02/27/23 12:51:11.725
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":330,"skipped":5889,"failed":0}
------------------------------
• [4.183 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:51:07.573
    Feb 27 12:51:07.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:51:07.575
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:07.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:07.605
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 02/27/23 12:51:07.612
    Feb 27 12:51:07.628: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2" in namespace "projected-952" to be "Succeeded or Failed"
    Feb 27 12:51:07.636: INFO: Pod "downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011973ms
    Feb 27 12:51:09.654: INFO: Pod "downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026155591s
    Feb 27 12:51:11.647: INFO: Pod "downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019233576s
    STEP: Saw pod success 02/27/23 12:51:11.648
    Feb 27 12:51:11.648: INFO: Pod "downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2" satisfied condition "Succeeded or Failed"
    Feb 27 12:51:11.656: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2 container client-container: <nil>
    STEP: delete the pod 02/27/23 12:51:11.672
    Feb 27 12:51:11.698: INFO: Waiting for pod downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2 to disappear
    Feb 27 12:51:11.714: INFO: Pod downwardapi-volume-82813b4b-7227-4d69-aff2-4259ea8051d2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 27 12:51:11.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-952" for this suite. 02/27/23 12:51:11.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:51:11.765
Feb 27 12:51:11.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename projected 02/27/23 12:51:11.766
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:11.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:11.819
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 02/27/23 12:51:11.833
Feb 27 12:51:11.856: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f" in namespace "projected-8093" to be "Succeeded or Failed"
Feb 27 12:51:11.877: INFO: Pod "downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.631688ms
Feb 27 12:51:13.886: INFO: Pod "downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f": Phase="Running", Reason="", readiness=false. Elapsed: 2.029928902s
Feb 27 12:51:15.891: INFO: Pod "downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035027225s
STEP: Saw pod success 02/27/23 12:51:15.891
Feb 27 12:51:15.891: INFO: Pod "downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f" satisfied condition "Succeeded or Failed"
Feb 27 12:51:15.903: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f container client-container: <nil>
STEP: delete the pod 02/27/23 12:51:15.922
Feb 27 12:51:15.939: INFO: Waiting for pod downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f to disappear
Feb 27 12:51:15.946: INFO: Pod downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 27 12:51:15.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8093" for this suite. 02/27/23 12:51:15.957
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":331,"skipped":5939,"failed":0}
------------------------------
• [4.204 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:51:11.765
    Feb 27 12:51:11.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename projected 02/27/23 12:51:11.766
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:11.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:11.819
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 02/27/23 12:51:11.833
    Feb 27 12:51:11.856: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f" in namespace "projected-8093" to be "Succeeded or Failed"
    Feb 27 12:51:11.877: INFO: Pod "downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.631688ms
    Feb 27 12:51:13.886: INFO: Pod "downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f": Phase="Running", Reason="", readiness=false. Elapsed: 2.029928902s
    Feb 27 12:51:15.891: INFO: Pod "downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035027225s
    STEP: Saw pod success 02/27/23 12:51:15.891
    Feb 27 12:51:15.891: INFO: Pod "downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f" satisfied condition "Succeeded or Failed"
    Feb 27 12:51:15.903: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f container client-container: <nil>
    STEP: delete the pod 02/27/23 12:51:15.922
    Feb 27 12:51:15.939: INFO: Waiting for pod downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f to disappear
    Feb 27 12:51:15.946: INFO: Pod downwardapi-volume-fa1298b6-2637-4afc-8b4e-368aaa65358f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 27 12:51:15.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8093" for this suite. 02/27/23 12:51:15.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:51:15.973
Feb 27 12:51:15.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename pods 02/27/23 12:51:15.974
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:16.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:16.016
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 02/27/23 12:51:16.039
STEP: watching for Pod to be ready 02/27/23 12:51:16.055
Feb 27 12:51:16.059: INFO: observed Pod pod-test in namespace pods-499 in phase Pending with labels: map[test-pod-static:true] & conditions []
Feb 27 12:51:16.062: INFO: observed Pod pod-test in namespace pods-499 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  }]
Feb 27 12:51:16.085: INFO: observed Pod pod-test in namespace pods-499 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  }]
Feb 27 12:51:16.539: INFO: observed Pod pod-test in namespace pods-499 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  }]
Feb 27 12:51:17.887: INFO: Found Pod pod-test in namespace pods-499 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 02/27/23 12:51:17.899
STEP: getting the Pod and ensuring that it's patched 02/27/23 12:51:17.918
STEP: replacing the Pod's status Ready condition to False 02/27/23 12:51:17.926
STEP: check the Pod again to ensure its Ready conditions are False 02/27/23 12:51:17.948
STEP: deleting the Pod via a Collection with a LabelSelector 02/27/23 12:51:17.948
STEP: watching for the Pod to be deleted 02/27/23 12:51:17.972
Feb 27 12:51:17.980: INFO: observed event type MODIFIED
Feb 27 12:51:18.850: INFO: observed event type MODIFIED
Feb 27 12:51:20.157: INFO: observed event type MODIFIED
Feb 27 12:51:20.895: INFO: observed event type MODIFIED
Feb 27 12:51:20.932: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 27 12:51:20.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-499" for this suite. 02/27/23 12:51:20.989
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":332,"skipped":5946,"failed":0}
------------------------------
• [SLOW TEST] [5.043 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:51:15.973
    Feb 27 12:51:15.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename pods 02/27/23 12:51:15.974
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:16.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:16.016
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 02/27/23 12:51:16.039
    STEP: watching for Pod to be ready 02/27/23 12:51:16.055
    Feb 27 12:51:16.059: INFO: observed Pod pod-test in namespace pods-499 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Feb 27 12:51:16.062: INFO: observed Pod pod-test in namespace pods-499 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  }]
    Feb 27 12:51:16.085: INFO: observed Pod pod-test in namespace pods-499 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  }]
    Feb 27 12:51:16.539: INFO: observed Pod pod-test in namespace pods-499 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  }]
    Feb 27 12:51:17.887: INFO: Found Pod pod-test in namespace pods-499 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 12:51:16 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 02/27/23 12:51:17.899
    STEP: getting the Pod and ensuring that it's patched 02/27/23 12:51:17.918
    STEP: replacing the Pod's status Ready condition to False 02/27/23 12:51:17.926
    STEP: check the Pod again to ensure its Ready conditions are False 02/27/23 12:51:17.948
    STEP: deleting the Pod via a Collection with a LabelSelector 02/27/23 12:51:17.948
    STEP: watching for the Pod to be deleted 02/27/23 12:51:17.972
    Feb 27 12:51:17.980: INFO: observed event type MODIFIED
    Feb 27 12:51:18.850: INFO: observed event type MODIFIED
    Feb 27 12:51:20.157: INFO: observed event type MODIFIED
    Feb 27 12:51:20.895: INFO: observed event type MODIFIED
    Feb 27 12:51:20.932: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 27 12:51:20.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-499" for this suite. 02/27/23 12:51:20.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:51:21.019
Feb 27 12:51:21.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename proxy 02/27/23 12:51:21.02
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:21.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:21.088
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 02/27/23 12:51:21.138
STEP: creating replication controller proxy-service-vkpkl in namespace proxy-5861 02/27/23 12:51:21.138
I0227 12:51:21.157051      20 runners.go:193] Created replication controller with name: proxy-service-vkpkl, namespace: proxy-5861, replica count: 1
I0227 12:51:22.209065      20 runners.go:193] proxy-service-vkpkl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0227 12:51:23.210542      20 runners.go:193] proxy-service-vkpkl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0227 12:51:24.214369      20 runners.go:193] proxy-service-vkpkl Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 12:51:24.221: INFO: setup took 3.116647406s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/27/23 12:51:24.221
Feb 27 12:51:24.253: INFO: (0) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 29.115308ms)
Feb 27 12:51:24.265: INFO: (0) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 41.312918ms)
Feb 27 12:51:24.268: INFO: (0) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 44.585611ms)
Feb 27 12:51:24.268: INFO: (0) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 44.469061ms)
Feb 27 12:51:24.268: INFO: (0) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 44.466071ms)
Feb 27 12:51:24.268: INFO: (0) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 44.709432ms)
Feb 27 12:51:24.273: INFO: (0) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 49.730413ms)
Feb 27 12:51:24.273: INFO: (0) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 49.909653ms)
Feb 27 12:51:24.274: INFO: (0) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 50.757616ms)
Feb 27 12:51:24.274: INFO: (0) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 50.745627ms)
Feb 27 12:51:24.276: INFO: (0) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 53.205817ms)
Feb 27 12:51:24.276: INFO: (0) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 54.644622ms)
Feb 27 12:51:24.278: INFO: (0) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 54.854424ms)
Feb 27 12:51:24.278: INFO: (0) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 54.798623ms)
Feb 27 12:51:24.285: INFO: (0) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 60.911998ms)
Feb 27 12:51:24.285: INFO: (0) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 63.125497ms)
Feb 27 12:51:24.305: INFO: (1) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 19.888491ms)
Feb 27 12:51:24.305: INFO: (1) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 20.219132ms)
Feb 27 12:51:24.308: INFO: (1) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 23.422966ms)
Feb 27 12:51:24.308: INFO: (1) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 23.143564ms)
Feb 27 12:51:24.310: INFO: (1) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 24.65355ms)
Feb 27 12:51:24.310: INFO: (1) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 24.866221ms)
Feb 27 12:51:24.310: INFO: (1) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 24.855671ms)
Feb 27 12:51:24.314: INFO: (1) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 28.422165ms)
Feb 27 12:51:24.314: INFO: (1) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 28.585916ms)
Feb 27 12:51:24.314: INFO: (1) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 28.491526ms)
Feb 27 12:51:24.315: INFO: (1) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 30.484844ms)
Feb 27 12:51:24.315: INFO: (1) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 30.667555ms)
Feb 27 12:51:24.315: INFO: (1) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 30.468294ms)
Feb 27 12:51:24.315: INFO: (1) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 30.356614ms)
Feb 27 12:51:24.315: INFO: (1) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 30.864245ms)
Feb 27 12:51:24.316: INFO: (1) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 30.650514ms)
Feb 27 12:51:24.339: INFO: (2) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 22.09958ms)
Feb 27 12:51:24.339: INFO: (2) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 23.228234ms)
Feb 27 12:51:24.339: INFO: (2) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 21.110877ms)
Feb 27 12:51:24.341: INFO: (2) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 23.463876ms)
Feb 27 12:51:24.341: INFO: (2) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 21.489868ms)
Feb 27 12:51:24.341: INFO: (2) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 24.760641ms)
Feb 27 12:51:24.343: INFO: (2) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 24.345229ms)
Feb 27 12:51:24.343: INFO: (2) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 24.090058ms)
Feb 27 12:51:24.345: INFO: (2) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 27.278911ms)
Feb 27 12:51:24.345: INFO: (2) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 25.616055ms)
Feb 27 12:51:24.345: INFO: (2) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 25.194623ms)
Feb 27 12:51:24.345: INFO: (2) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 24.674291ms)
Feb 27 12:51:24.347: INFO: (2) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 27.423832ms)
Feb 27 12:51:24.347: INFO: (2) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 27.550483ms)
Feb 27 12:51:24.351: INFO: (2) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 30.830776ms)
Feb 27 12:51:24.352: INFO: (2) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 32.332162ms)
Feb 27 12:51:24.368: INFO: (3) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 14.82033ms)
Feb 27 12:51:24.372: INFO: (3) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 18.632936ms)
Feb 27 12:51:24.391: INFO: (3) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 34.589671ms)
Feb 27 12:51:24.394: INFO: (3) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 38.303606ms)
Feb 27 12:51:24.394: INFO: (3) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 39.729712ms)
Feb 27 12:51:24.411: INFO: (3) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 56.682671ms)
Feb 27 12:51:24.411: INFO: (3) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 54.999284ms)
Feb 27 12:51:24.411: INFO: (3) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 58.346867ms)
Feb 27 12:51:24.411: INFO: (3) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 58.6826ms)
Feb 27 12:51:24.411: INFO: (3) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 54.657733ms)
Feb 27 12:51:24.411: INFO: (3) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 55.890298ms)
Feb 27 12:51:24.415: INFO: (3) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 59.644983ms)
Feb 27 12:51:24.421: INFO: (3) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 66.465581ms)
Feb 27 12:51:24.436: INFO: (3) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 79.863975ms)
Feb 27 12:51:24.436: INFO: (3) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 81.04736ms)
Feb 27 12:51:24.436: INFO: (3) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 79.240883ms)
Feb 27 12:51:24.462: INFO: (4) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 25.256773ms)
Feb 27 12:51:24.466: INFO: (4) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 27.863643ms)
Feb 27 12:51:24.467: INFO: (4) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 29.174488ms)
Feb 27 12:51:24.468: INFO: (4) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 30.056063ms)
Feb 27 12:51:24.491: INFO: (4) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 52.767135ms)
Feb 27 12:51:24.492: INFO: (4) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 54.681103ms)
Feb 27 12:51:24.492: INFO: (4) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 54.502962ms)
Feb 27 12:51:24.493: INFO: (4) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 55.060404ms)
Feb 27 12:51:24.493: INFO: (4) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 54.606502ms)
Feb 27 12:51:24.493: INFO: (4) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 54.619882ms)
Feb 27 12:51:24.495: INFO: (4) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 57.360684ms)
Feb 27 12:51:24.499: INFO: (4) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 60.948059ms)
Feb 27 12:51:24.512: INFO: (4) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 74.727504ms)
Feb 27 12:51:24.513: INFO: (4) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 74.827484ms)
Feb 27 12:51:24.525: INFO: (4) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 87.925999ms)
Feb 27 12:51:24.544: INFO: (4) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 106.819966ms)
Feb 27 12:51:24.580: INFO: (5) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 34.747551ms)
Feb 27 12:51:24.580: INFO: (5) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 34.849612ms)
Feb 27 12:51:24.625: INFO: (5) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 80.459797ms)
Feb 27 12:51:24.625: INFO: (5) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 80.551797ms)
Feb 27 12:51:24.625: INFO: (5) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 80.209036ms)
Feb 27 12:51:24.625: INFO: (5) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 80.291117ms)
Feb 27 12:51:24.625: INFO: (5) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 80.390317ms)
Feb 27 12:51:24.631: INFO: (5) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 86.437831ms)
Feb 27 12:51:24.632: INFO: (5) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 87.753466ms)
Feb 27 12:51:24.632: INFO: (5) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 87.561515ms)
Feb 27 12:51:24.633: INFO: (5) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 88.293839ms)
Feb 27 12:51:24.657: INFO: (5) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 112.246946ms)
Feb 27 12:51:24.658: INFO: (5) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 112.876129ms)
Feb 27 12:51:24.658: INFO: (5) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 113.24939ms)
Feb 27 12:51:24.659: INFO: (5) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 113.753113ms)
Feb 27 12:51:24.659: INFO: (5) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 113.907573ms)
Feb 27 12:51:24.724: INFO: (6) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 63.985051ms)
Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 61.796711ms)
Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 64.952554ms)
Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 64.308152ms)
Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 63.566339ms)
Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 65.435957ms)
Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 64.951055ms)
Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 64.566273ms)
Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 62.425614ms)
Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 62.027602ms)
Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 65.453587ms)
Feb 27 12:51:24.726: INFO: (6) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 67.278794ms)
Feb 27 12:51:24.728: INFO: (6) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 68.259518ms)
Feb 27 12:51:24.728: INFO: (6) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 68.129528ms)
Feb 27 12:51:24.728: INFO: (6) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 67.267514ms)
Feb 27 12:51:24.728: INFO: (6) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 67.249114ms)
Feb 27 12:51:24.753: INFO: (7) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 24.480179ms)
Feb 27 12:51:24.754: INFO: (7) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 23.648706ms)
Feb 27 12:51:24.754: INFO: (7) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 24.284958ms)
Feb 27 12:51:24.754: INFO: (7) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 23.872047ms)
Feb 27 12:51:24.755: INFO: (7) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 25.124492ms)
Feb 27 12:51:24.757: INFO: (7) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 27.734702ms)
Feb 27 12:51:24.763: INFO: (7) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 33.551917ms)
Feb 27 12:51:24.763: INFO: (7) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 33.304546ms)
Feb 27 12:51:24.763: INFO: (7) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 33.182015ms)
Feb 27 12:51:24.768: INFO: (7) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 38.143965ms)
Feb 27 12:51:24.768: INFO: (7) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 38.512226ms)
Feb 27 12:51:24.768: INFO: (7) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 37.728014ms)
Feb 27 12:51:24.768: INFO: (7) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 38.455287ms)
Feb 27 12:51:24.768: INFO: (7) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 37.425993ms)
Feb 27 12:51:24.768: INFO: (7) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 38.040315ms)
Feb 27 12:51:24.773: INFO: (7) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 43.149605ms)
Feb 27 12:51:24.816: INFO: (8) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 41.312269ms)
Feb 27 12:51:24.816: INFO: (8) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 42.863205ms)
Feb 27 12:51:24.816: INFO: (8) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 42.620574ms)
Feb 27 12:51:24.816: INFO: (8) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 42.834584ms)
Feb 27 12:51:24.816: INFO: (8) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 42.153622ms)
Feb 27 12:51:24.819: INFO: (8) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 44.959853ms)
Feb 27 12:51:24.820: INFO: (8) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 45.257994ms)
Feb 27 12:51:24.820: INFO: (8) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 46.464679ms)
Feb 27 12:51:24.820: INFO: (8) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 45.440656ms)
Feb 27 12:51:24.821: INFO: (8) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 47.444383ms)
Feb 27 12:51:24.824: INFO: (8) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 49.606412ms)
Feb 27 12:51:24.824: INFO: (8) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 49.654462ms)
Feb 27 12:51:24.824: INFO: (8) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 49.739292ms)
Feb 27 12:51:24.825: INFO: (8) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 49.791873ms)
Feb 27 12:51:24.825: INFO: (8) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 50.069674ms)
Feb 27 12:51:24.830: INFO: (8) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 55.216015ms)
Feb 27 12:51:24.860: INFO: (9) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 29.597731ms)
Feb 27 12:51:24.860: INFO: (9) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 29.313569ms)
Feb 27 12:51:24.860: INFO: (9) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 29.38922ms)
Feb 27 12:51:24.865: INFO: (9) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 34.853782ms)
Feb 27 12:51:24.865: INFO: (9) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 35.029042ms)
Feb 27 12:51:24.865: INFO: (9) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 34.816492ms)
Feb 27 12:51:24.865: INFO: (9) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 34.981193ms)
Feb 27 12:51:24.868: INFO: (9) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 37.165412ms)
Feb 27 12:51:24.868: INFO: (9) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 37.069571ms)
Feb 27 12:51:24.868: INFO: (9) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 37.229861ms)
Feb 27 12:51:24.868: INFO: (9) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 37.06412ms)
Feb 27 12:51:24.868: INFO: (9) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 37.228652ms)
Feb 27 12:51:24.872: INFO: (9) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 41.256458ms)
Feb 27 12:51:24.874: INFO: (9) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 43.657217ms)
Feb 27 12:51:24.875: INFO: (9) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 44.31004ms)
Feb 27 12:51:24.875: INFO: (9) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 44.734681ms)
Feb 27 12:51:24.892: INFO: (10) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 15.599184ms)
Feb 27 12:51:24.902: INFO: (10) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 26.316888ms)
Feb 27 12:51:24.902: INFO: (10) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 25.879556ms)
Feb 27 12:51:24.902: INFO: (10) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 25.444244ms)
Feb 27 12:51:24.902: INFO: (10) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 25.899576ms)
Feb 27 12:51:24.903: INFO: (10) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 27.05555ms)
Feb 27 12:51:24.903: INFO: (10) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 25.909215ms)
Feb 27 12:51:24.903: INFO: (10) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 27.049261ms)
Feb 27 12:51:24.905: INFO: (10) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 28.315676ms)
Feb 27 12:51:24.905: INFO: (10) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 28.571146ms)
Feb 27 12:51:24.905: INFO: (10) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 29.240189ms)
Feb 27 12:51:24.906: INFO: (10) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 29.41635ms)
Feb 27 12:51:24.906: INFO: (10) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 29.256799ms)
Feb 27 12:51:24.906: INFO: (10) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 29.829202ms)
Feb 27 12:51:24.906: INFO: (10) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 28.961378ms)
Feb 27 12:51:24.914: INFO: (10) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 37.345972ms)
Feb 27 12:51:24.929: INFO: (11) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 14.95536ms)
Feb 27 12:51:24.937: INFO: (11) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 22.777442ms)
Feb 27 12:51:24.937: INFO: (11) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 22.660652ms)
Feb 27 12:51:24.937: INFO: (11) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 22.083989ms)
Feb 27 12:51:24.937: INFO: (11) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 22.679702ms)
Feb 27 12:51:24.939: INFO: (11) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 24.69778ms)
Feb 27 12:51:24.939: INFO: (11) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 25.260533ms)
Feb 27 12:51:24.939: INFO: (11) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 24.432349ms)
Feb 27 12:51:24.946: INFO: (11) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 31.523928ms)
Feb 27 12:51:24.946: INFO: (11) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 32.343121ms)
Feb 27 12:51:24.947: INFO: (11) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 32.05661ms)
Feb 27 12:51:24.947: INFO: (11) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 32.516182ms)
Feb 27 12:51:24.947: INFO: (11) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 31.829339ms)
Feb 27 12:51:24.947: INFO: (11) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 32.810793ms)
Feb 27 12:51:24.947: INFO: (11) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 32.995834ms)
Feb 27 12:51:24.947: INFO: (11) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 32.785764ms)
Feb 27 12:51:24.964: INFO: (12) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 15.854655ms)
Feb 27 12:51:24.966: INFO: (12) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 19.022817ms)
Feb 27 12:51:24.967: INFO: (12) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 18.998818ms)
Feb 27 12:51:24.976: INFO: (12) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 28.047864ms)
Feb 27 12:51:24.977: INFO: (12) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 29.40089ms)
Feb 27 12:51:24.977: INFO: (12) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 28.882817ms)
Feb 27 12:51:24.977: INFO: (12) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 28.754497ms)
Feb 27 12:51:24.983: INFO: (12) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 35.598355ms)
Feb 27 12:51:24.984: INFO: (12) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 35.847506ms)
Feb 27 12:51:24.984: INFO: (12) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 35.581955ms)
Feb 27 12:51:24.986: INFO: (12) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 37.998845ms)
Feb 27 12:51:24.986: INFO: (12) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 37.701404ms)
Feb 27 12:51:24.986: INFO: (12) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 37.693083ms)
Feb 27 12:51:24.988: INFO: (12) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 39.881312ms)
Feb 27 12:51:24.992: INFO: (12) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 44.372401ms)
Feb 27 12:51:24.992: INFO: (12) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 44.304201ms)
Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 59.814554ms)
Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 60.164695ms)
Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 60.269245ms)
Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 61.635472ms)
Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 60.260016ms)
Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 60.541287ms)
Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 60.597417ms)
Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 61.5074ms)
Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 60.767747ms)
Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 61.38677ms)
Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 60.717347ms)
Feb 27 12:51:25.055: INFO: (13) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 61.26969ms)
Feb 27 12:51:25.055: INFO: (13) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 60.959559ms)
Feb 27 12:51:25.057: INFO: (13) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 63.68089ms)
Feb 27 12:51:25.057: INFO: (13) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 64.005841ms)
Feb 27 12:51:25.057: INFO: (13) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 62.980467ms)
Feb 27 12:51:25.074: INFO: (14) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 17.307941ms)
Feb 27 12:51:25.077: INFO: (14) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 20.003612ms)
Feb 27 12:51:25.077: INFO: (14) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 20.122862ms)
Feb 27 12:51:25.077: INFO: (14) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 19.6137ms)
Feb 27 12:51:25.078: INFO: (14) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 20.318433ms)
Feb 27 12:51:25.078: INFO: (14) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 20.733094ms)
Feb 27 12:51:25.078: INFO: (14) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 20.963136ms)
Feb 27 12:51:25.079: INFO: (14) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 21.322517ms)
Feb 27 12:51:25.083: INFO: (14) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 25.494944ms)
Feb 27 12:51:25.083: INFO: (14) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 25.754175ms)
Feb 27 12:51:25.085: INFO: (14) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 27.846394ms)
Feb 27 12:51:25.085: INFO: (14) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 28.026205ms)
Feb 27 12:51:25.085: INFO: (14) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 27.995494ms)
Feb 27 12:51:25.086: INFO: (14) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 28.452126ms)
Feb 27 12:51:25.089: INFO: (14) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 31.261038ms)
Feb 27 12:51:25.091: INFO: (14) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 33.669257ms)
Feb 27 12:51:25.111: INFO: (15) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 19.970662ms)
Feb 27 12:51:25.113: INFO: (15) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 18.462375ms)
Feb 27 12:51:25.114: INFO: (15) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 22.683712ms)
Feb 27 12:51:25.119: INFO: (15) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 24.160878ms)
Feb 27 12:51:25.119: INFO: (15) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 27.848164ms)
Feb 27 12:51:25.119: INFO: (15) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 24.254828ms)
Feb 27 12:51:25.120: INFO: (15) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 27.288841ms)
Feb 27 12:51:25.120: INFO: (15) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 25.773115ms)
Feb 27 12:51:25.121: INFO: (15) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 27.06251ms)
Feb 27 12:51:25.121: INFO: (15) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 28.094134ms)
Feb 27 12:51:25.121: INFO: (15) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 27.01265ms)
Feb 27 12:51:25.121: INFO: (15) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 27.731673ms)
Feb 27 12:51:25.121: INFO: (15) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 28.481016ms)
Feb 27 12:51:25.122: INFO: (15) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 29.660291ms)
Feb 27 12:51:25.122: INFO: (15) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 30.593885ms)
Feb 27 12:51:25.124: INFO: (15) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 32.774673ms)
Feb 27 12:51:25.147: INFO: (16) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 22.32082ms)
Feb 27 12:51:25.148: INFO: (16) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 20.884035ms)
Feb 27 12:51:25.151: INFO: (16) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 25.806795ms)
Feb 27 12:51:25.151: INFO: (16) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 25.116202ms)
Feb 27 12:51:25.152: INFO: (16) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 26.534418ms)
Feb 27 12:51:25.152: INFO: (16) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 24.52427ms)
Feb 27 12:51:25.152: INFO: (16) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 24.469449ms)
Feb 27 12:51:25.152: INFO: (16) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 24.45715ms)
Feb 27 12:51:25.152: INFO: (16) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 25.890336ms)
Feb 27 12:51:25.152: INFO: (16) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 26.478608ms)
Feb 27 12:51:25.157: INFO: (16) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 32.012811ms)
Feb 27 12:51:25.157: INFO: (16) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 29.147799ms)
Feb 27 12:51:25.158: INFO: (16) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 30.639575ms)
Feb 27 12:51:25.161: INFO: (16) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 33.272136ms)
Feb 27 12:51:25.162: INFO: (16) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 34.882482ms)
Feb 27 12:51:25.168: INFO: (16) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 40.740126ms)
Feb 27 12:51:25.180: INFO: (17) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 12.034659ms)
Feb 27 12:51:25.188: INFO: (17) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 19.254788ms)
Feb 27 12:51:25.188: INFO: (17) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 18.773176ms)
Feb 27 12:51:25.188: INFO: (17) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 17.34474ms)
Feb 27 12:51:25.190: INFO: (17) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 18.577096ms)
Feb 27 12:51:25.190: INFO: (17) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 18.168483ms)
Feb 27 12:51:25.191: INFO: (17) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 18.236044ms)
Feb 27 12:51:25.191: INFO: (17) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 18.145643ms)
Feb 27 12:51:25.191: INFO: (17) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 18.786996ms)
Feb 27 12:51:25.191: INFO: (17) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 19.003367ms)
Feb 27 12:51:25.191: INFO: (17) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 21.073405ms)
Feb 27 12:51:25.194: INFO: (17) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 24.275689ms)
Feb 27 12:51:25.194: INFO: (17) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 23.570346ms)
Feb 27 12:51:25.197: INFO: (17) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 24.71304ms)
Feb 27 12:51:25.200: INFO: (17) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 27.783643ms)
Feb 27 12:51:25.200: INFO: (17) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 28.013214ms)
Feb 27 12:51:25.221: INFO: (18) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 20.764015ms)
Feb 27 12:51:25.221: INFO: (18) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 20.449593ms)
Feb 27 12:51:25.221: INFO: (18) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 20.699894ms)
Feb 27 12:51:25.221: INFO: (18) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 17.699592ms)
Feb 27 12:51:25.221: INFO: (18) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 18.948677ms)
Feb 27 12:51:25.221: INFO: (18) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 19.192878ms)
Feb 27 12:51:25.222: INFO: (18) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 17.804272ms)
Feb 27 12:51:25.222: INFO: (18) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 18.368995ms)
Feb 27 12:51:25.222: INFO: (18) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 19.5807ms)
Feb 27 12:51:25.222: INFO: (18) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 21.847599ms)
Feb 27 12:51:25.222: INFO: (18) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 18.280165ms)
Feb 27 12:51:25.224: INFO: (18) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 22.734082ms)
Feb 27 12:51:25.225: INFO: (18) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 23.863207ms)
Feb 27 12:51:25.226: INFO: (18) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 23.037504ms)
Feb 27 12:51:25.226: INFO: (18) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 23.012733ms)
Feb 27 12:51:25.229: INFO: (18) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 25.522984ms)
Feb 27 12:51:25.278: INFO: (19) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 48.390877ms)
Feb 27 12:51:25.278: INFO: (19) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 49.0983ms)
Feb 27 12:51:25.280: INFO: (19) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 50.751917ms)
Feb 27 12:51:25.281: INFO: (19) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 51.768211ms)
Feb 27 12:51:25.281: INFO: (19) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 51.885962ms)
Feb 27 12:51:25.283: INFO: (19) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 53.325907ms)
Feb 27 12:51:25.283: INFO: (19) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 53.668889ms)
Feb 27 12:51:25.283: INFO: (19) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 53.543979ms)
Feb 27 12:51:25.283: INFO: (19) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 53.667348ms)
Feb 27 12:51:25.284: INFO: (19) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 54.371092ms)
Feb 27 12:51:25.284: INFO: (19) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 54.704913ms)
Feb 27 12:51:25.285: INFO: (19) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 56.3168ms)
Feb 27 12:51:25.285: INFO: (19) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 56.204459ms)
Feb 27 12:51:25.285: INFO: (19) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 56.025588ms)
Feb 27 12:51:25.285: INFO: (19) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 55.949348ms)
Feb 27 12:51:25.287: INFO: (19) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 57.074262ms)
STEP: deleting ReplicationController proxy-service-vkpkl in namespace proxy-5861, will wait for the garbage collector to delete the pods 02/27/23 12:51:25.287
Feb 27 12:51:25.362: INFO: Deleting ReplicationController proxy-service-vkpkl took: 17.320841ms
Feb 27 12:51:25.463: INFO: Terminating ReplicationController proxy-service-vkpkl pods took: 101.028651ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb 27 12:51:27.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5861" for this suite. 02/27/23 12:51:27.275
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":333,"skipped":5968,"failed":0}
------------------------------
• [SLOW TEST] [6.276 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:51:21.019
    Feb 27 12:51:21.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename proxy 02/27/23 12:51:21.02
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:21.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:21.088
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 02/27/23 12:51:21.138
    STEP: creating replication controller proxy-service-vkpkl in namespace proxy-5861 02/27/23 12:51:21.138
    I0227 12:51:21.157051      20 runners.go:193] Created replication controller with name: proxy-service-vkpkl, namespace: proxy-5861, replica count: 1
    I0227 12:51:22.209065      20 runners.go:193] proxy-service-vkpkl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0227 12:51:23.210542      20 runners.go:193] proxy-service-vkpkl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0227 12:51:24.214369      20 runners.go:193] proxy-service-vkpkl Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 12:51:24.221: INFO: setup took 3.116647406s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/27/23 12:51:24.221
    Feb 27 12:51:24.253: INFO: (0) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 29.115308ms)
    Feb 27 12:51:24.265: INFO: (0) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 41.312918ms)
    Feb 27 12:51:24.268: INFO: (0) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 44.585611ms)
    Feb 27 12:51:24.268: INFO: (0) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 44.469061ms)
    Feb 27 12:51:24.268: INFO: (0) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 44.466071ms)
    Feb 27 12:51:24.268: INFO: (0) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 44.709432ms)
    Feb 27 12:51:24.273: INFO: (0) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 49.730413ms)
    Feb 27 12:51:24.273: INFO: (0) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 49.909653ms)
    Feb 27 12:51:24.274: INFO: (0) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 50.757616ms)
    Feb 27 12:51:24.274: INFO: (0) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 50.745627ms)
    Feb 27 12:51:24.276: INFO: (0) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 53.205817ms)
    Feb 27 12:51:24.276: INFO: (0) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 54.644622ms)
    Feb 27 12:51:24.278: INFO: (0) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 54.854424ms)
    Feb 27 12:51:24.278: INFO: (0) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 54.798623ms)
    Feb 27 12:51:24.285: INFO: (0) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 60.911998ms)
    Feb 27 12:51:24.285: INFO: (0) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 63.125497ms)
    Feb 27 12:51:24.305: INFO: (1) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 19.888491ms)
    Feb 27 12:51:24.305: INFO: (1) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 20.219132ms)
    Feb 27 12:51:24.308: INFO: (1) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 23.422966ms)
    Feb 27 12:51:24.308: INFO: (1) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 23.143564ms)
    Feb 27 12:51:24.310: INFO: (1) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 24.65355ms)
    Feb 27 12:51:24.310: INFO: (1) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 24.866221ms)
    Feb 27 12:51:24.310: INFO: (1) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 24.855671ms)
    Feb 27 12:51:24.314: INFO: (1) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 28.422165ms)
    Feb 27 12:51:24.314: INFO: (1) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 28.585916ms)
    Feb 27 12:51:24.314: INFO: (1) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 28.491526ms)
    Feb 27 12:51:24.315: INFO: (1) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 30.484844ms)
    Feb 27 12:51:24.315: INFO: (1) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 30.667555ms)
    Feb 27 12:51:24.315: INFO: (1) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 30.468294ms)
    Feb 27 12:51:24.315: INFO: (1) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 30.356614ms)
    Feb 27 12:51:24.315: INFO: (1) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 30.864245ms)
    Feb 27 12:51:24.316: INFO: (1) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 30.650514ms)
    Feb 27 12:51:24.339: INFO: (2) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 22.09958ms)
    Feb 27 12:51:24.339: INFO: (2) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 23.228234ms)
    Feb 27 12:51:24.339: INFO: (2) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 21.110877ms)
    Feb 27 12:51:24.341: INFO: (2) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 23.463876ms)
    Feb 27 12:51:24.341: INFO: (2) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 21.489868ms)
    Feb 27 12:51:24.341: INFO: (2) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 24.760641ms)
    Feb 27 12:51:24.343: INFO: (2) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 24.345229ms)
    Feb 27 12:51:24.343: INFO: (2) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 24.090058ms)
    Feb 27 12:51:24.345: INFO: (2) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 27.278911ms)
    Feb 27 12:51:24.345: INFO: (2) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 25.616055ms)
    Feb 27 12:51:24.345: INFO: (2) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 25.194623ms)
    Feb 27 12:51:24.345: INFO: (2) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 24.674291ms)
    Feb 27 12:51:24.347: INFO: (2) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 27.423832ms)
    Feb 27 12:51:24.347: INFO: (2) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 27.550483ms)
    Feb 27 12:51:24.351: INFO: (2) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 30.830776ms)
    Feb 27 12:51:24.352: INFO: (2) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 32.332162ms)
    Feb 27 12:51:24.368: INFO: (3) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 14.82033ms)
    Feb 27 12:51:24.372: INFO: (3) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 18.632936ms)
    Feb 27 12:51:24.391: INFO: (3) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 34.589671ms)
    Feb 27 12:51:24.394: INFO: (3) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 38.303606ms)
    Feb 27 12:51:24.394: INFO: (3) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 39.729712ms)
    Feb 27 12:51:24.411: INFO: (3) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 56.682671ms)
    Feb 27 12:51:24.411: INFO: (3) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 54.999284ms)
    Feb 27 12:51:24.411: INFO: (3) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 58.346867ms)
    Feb 27 12:51:24.411: INFO: (3) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 58.6826ms)
    Feb 27 12:51:24.411: INFO: (3) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 54.657733ms)
    Feb 27 12:51:24.411: INFO: (3) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 55.890298ms)
    Feb 27 12:51:24.415: INFO: (3) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 59.644983ms)
    Feb 27 12:51:24.421: INFO: (3) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 66.465581ms)
    Feb 27 12:51:24.436: INFO: (3) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 79.863975ms)
    Feb 27 12:51:24.436: INFO: (3) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 81.04736ms)
    Feb 27 12:51:24.436: INFO: (3) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 79.240883ms)
    Feb 27 12:51:24.462: INFO: (4) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 25.256773ms)
    Feb 27 12:51:24.466: INFO: (4) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 27.863643ms)
    Feb 27 12:51:24.467: INFO: (4) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 29.174488ms)
    Feb 27 12:51:24.468: INFO: (4) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 30.056063ms)
    Feb 27 12:51:24.491: INFO: (4) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 52.767135ms)
    Feb 27 12:51:24.492: INFO: (4) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 54.681103ms)
    Feb 27 12:51:24.492: INFO: (4) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 54.502962ms)
    Feb 27 12:51:24.493: INFO: (4) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 55.060404ms)
    Feb 27 12:51:24.493: INFO: (4) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 54.606502ms)
    Feb 27 12:51:24.493: INFO: (4) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 54.619882ms)
    Feb 27 12:51:24.495: INFO: (4) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 57.360684ms)
    Feb 27 12:51:24.499: INFO: (4) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 60.948059ms)
    Feb 27 12:51:24.512: INFO: (4) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 74.727504ms)
    Feb 27 12:51:24.513: INFO: (4) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 74.827484ms)
    Feb 27 12:51:24.525: INFO: (4) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 87.925999ms)
    Feb 27 12:51:24.544: INFO: (4) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 106.819966ms)
    Feb 27 12:51:24.580: INFO: (5) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 34.747551ms)
    Feb 27 12:51:24.580: INFO: (5) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 34.849612ms)
    Feb 27 12:51:24.625: INFO: (5) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 80.459797ms)
    Feb 27 12:51:24.625: INFO: (5) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 80.551797ms)
    Feb 27 12:51:24.625: INFO: (5) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 80.209036ms)
    Feb 27 12:51:24.625: INFO: (5) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 80.291117ms)
    Feb 27 12:51:24.625: INFO: (5) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 80.390317ms)
    Feb 27 12:51:24.631: INFO: (5) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 86.437831ms)
    Feb 27 12:51:24.632: INFO: (5) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 87.753466ms)
    Feb 27 12:51:24.632: INFO: (5) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 87.561515ms)
    Feb 27 12:51:24.633: INFO: (5) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 88.293839ms)
    Feb 27 12:51:24.657: INFO: (5) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 112.246946ms)
    Feb 27 12:51:24.658: INFO: (5) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 112.876129ms)
    Feb 27 12:51:24.658: INFO: (5) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 113.24939ms)
    Feb 27 12:51:24.659: INFO: (5) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 113.753113ms)
    Feb 27 12:51:24.659: INFO: (5) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 113.907573ms)
    Feb 27 12:51:24.724: INFO: (6) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 63.985051ms)
    Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 61.796711ms)
    Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 64.952554ms)
    Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 64.308152ms)
    Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 63.566339ms)
    Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 65.435957ms)
    Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 64.951055ms)
    Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 64.566273ms)
    Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 62.425614ms)
    Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 62.027602ms)
    Feb 27 12:51:24.725: INFO: (6) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 65.453587ms)
    Feb 27 12:51:24.726: INFO: (6) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 67.278794ms)
    Feb 27 12:51:24.728: INFO: (6) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 68.259518ms)
    Feb 27 12:51:24.728: INFO: (6) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 68.129528ms)
    Feb 27 12:51:24.728: INFO: (6) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 67.267514ms)
    Feb 27 12:51:24.728: INFO: (6) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 67.249114ms)
    Feb 27 12:51:24.753: INFO: (7) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 24.480179ms)
    Feb 27 12:51:24.754: INFO: (7) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 23.648706ms)
    Feb 27 12:51:24.754: INFO: (7) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 24.284958ms)
    Feb 27 12:51:24.754: INFO: (7) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 23.872047ms)
    Feb 27 12:51:24.755: INFO: (7) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 25.124492ms)
    Feb 27 12:51:24.757: INFO: (7) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 27.734702ms)
    Feb 27 12:51:24.763: INFO: (7) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 33.551917ms)
    Feb 27 12:51:24.763: INFO: (7) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 33.304546ms)
    Feb 27 12:51:24.763: INFO: (7) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 33.182015ms)
    Feb 27 12:51:24.768: INFO: (7) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 38.143965ms)
    Feb 27 12:51:24.768: INFO: (7) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 38.512226ms)
    Feb 27 12:51:24.768: INFO: (7) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 37.728014ms)
    Feb 27 12:51:24.768: INFO: (7) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 38.455287ms)
    Feb 27 12:51:24.768: INFO: (7) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 37.425993ms)
    Feb 27 12:51:24.768: INFO: (7) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 38.040315ms)
    Feb 27 12:51:24.773: INFO: (7) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 43.149605ms)
    Feb 27 12:51:24.816: INFO: (8) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 41.312269ms)
    Feb 27 12:51:24.816: INFO: (8) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 42.863205ms)
    Feb 27 12:51:24.816: INFO: (8) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 42.620574ms)
    Feb 27 12:51:24.816: INFO: (8) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 42.834584ms)
    Feb 27 12:51:24.816: INFO: (8) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 42.153622ms)
    Feb 27 12:51:24.819: INFO: (8) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 44.959853ms)
    Feb 27 12:51:24.820: INFO: (8) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 45.257994ms)
    Feb 27 12:51:24.820: INFO: (8) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 46.464679ms)
    Feb 27 12:51:24.820: INFO: (8) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 45.440656ms)
    Feb 27 12:51:24.821: INFO: (8) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 47.444383ms)
    Feb 27 12:51:24.824: INFO: (8) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 49.606412ms)
    Feb 27 12:51:24.824: INFO: (8) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 49.654462ms)
    Feb 27 12:51:24.824: INFO: (8) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 49.739292ms)
    Feb 27 12:51:24.825: INFO: (8) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 49.791873ms)
    Feb 27 12:51:24.825: INFO: (8) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 50.069674ms)
    Feb 27 12:51:24.830: INFO: (8) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 55.216015ms)
    Feb 27 12:51:24.860: INFO: (9) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 29.597731ms)
    Feb 27 12:51:24.860: INFO: (9) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 29.313569ms)
    Feb 27 12:51:24.860: INFO: (9) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 29.38922ms)
    Feb 27 12:51:24.865: INFO: (9) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 34.853782ms)
    Feb 27 12:51:24.865: INFO: (9) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 35.029042ms)
    Feb 27 12:51:24.865: INFO: (9) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 34.816492ms)
    Feb 27 12:51:24.865: INFO: (9) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 34.981193ms)
    Feb 27 12:51:24.868: INFO: (9) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 37.165412ms)
    Feb 27 12:51:24.868: INFO: (9) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 37.069571ms)
    Feb 27 12:51:24.868: INFO: (9) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 37.229861ms)
    Feb 27 12:51:24.868: INFO: (9) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 37.06412ms)
    Feb 27 12:51:24.868: INFO: (9) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 37.228652ms)
    Feb 27 12:51:24.872: INFO: (9) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 41.256458ms)
    Feb 27 12:51:24.874: INFO: (9) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 43.657217ms)
    Feb 27 12:51:24.875: INFO: (9) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 44.31004ms)
    Feb 27 12:51:24.875: INFO: (9) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 44.734681ms)
    Feb 27 12:51:24.892: INFO: (10) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 15.599184ms)
    Feb 27 12:51:24.902: INFO: (10) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 26.316888ms)
    Feb 27 12:51:24.902: INFO: (10) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 25.879556ms)
    Feb 27 12:51:24.902: INFO: (10) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 25.444244ms)
    Feb 27 12:51:24.902: INFO: (10) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 25.899576ms)
    Feb 27 12:51:24.903: INFO: (10) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 27.05555ms)
    Feb 27 12:51:24.903: INFO: (10) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 25.909215ms)
    Feb 27 12:51:24.903: INFO: (10) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 27.049261ms)
    Feb 27 12:51:24.905: INFO: (10) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 28.315676ms)
    Feb 27 12:51:24.905: INFO: (10) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 28.571146ms)
    Feb 27 12:51:24.905: INFO: (10) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 29.240189ms)
    Feb 27 12:51:24.906: INFO: (10) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 29.41635ms)
    Feb 27 12:51:24.906: INFO: (10) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 29.256799ms)
    Feb 27 12:51:24.906: INFO: (10) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 29.829202ms)
    Feb 27 12:51:24.906: INFO: (10) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 28.961378ms)
    Feb 27 12:51:24.914: INFO: (10) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 37.345972ms)
    Feb 27 12:51:24.929: INFO: (11) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 14.95536ms)
    Feb 27 12:51:24.937: INFO: (11) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 22.777442ms)
    Feb 27 12:51:24.937: INFO: (11) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 22.660652ms)
    Feb 27 12:51:24.937: INFO: (11) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 22.083989ms)
    Feb 27 12:51:24.937: INFO: (11) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 22.679702ms)
    Feb 27 12:51:24.939: INFO: (11) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 24.69778ms)
    Feb 27 12:51:24.939: INFO: (11) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 25.260533ms)
    Feb 27 12:51:24.939: INFO: (11) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 24.432349ms)
    Feb 27 12:51:24.946: INFO: (11) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 31.523928ms)
    Feb 27 12:51:24.946: INFO: (11) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 32.343121ms)
    Feb 27 12:51:24.947: INFO: (11) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 32.05661ms)
    Feb 27 12:51:24.947: INFO: (11) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 32.516182ms)
    Feb 27 12:51:24.947: INFO: (11) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 31.829339ms)
    Feb 27 12:51:24.947: INFO: (11) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 32.810793ms)
    Feb 27 12:51:24.947: INFO: (11) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 32.995834ms)
    Feb 27 12:51:24.947: INFO: (11) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 32.785764ms)
    Feb 27 12:51:24.964: INFO: (12) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 15.854655ms)
    Feb 27 12:51:24.966: INFO: (12) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 19.022817ms)
    Feb 27 12:51:24.967: INFO: (12) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 18.998818ms)
    Feb 27 12:51:24.976: INFO: (12) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 28.047864ms)
    Feb 27 12:51:24.977: INFO: (12) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 29.40089ms)
    Feb 27 12:51:24.977: INFO: (12) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 28.882817ms)
    Feb 27 12:51:24.977: INFO: (12) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 28.754497ms)
    Feb 27 12:51:24.983: INFO: (12) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 35.598355ms)
    Feb 27 12:51:24.984: INFO: (12) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 35.847506ms)
    Feb 27 12:51:24.984: INFO: (12) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 35.581955ms)
    Feb 27 12:51:24.986: INFO: (12) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 37.998845ms)
    Feb 27 12:51:24.986: INFO: (12) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 37.701404ms)
    Feb 27 12:51:24.986: INFO: (12) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 37.693083ms)
    Feb 27 12:51:24.988: INFO: (12) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 39.881312ms)
    Feb 27 12:51:24.992: INFO: (12) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 44.372401ms)
    Feb 27 12:51:24.992: INFO: (12) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 44.304201ms)
    Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 59.814554ms)
    Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 60.164695ms)
    Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 60.269245ms)
    Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 61.635472ms)
    Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 60.260016ms)
    Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 60.541287ms)
    Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 60.597417ms)
    Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 61.5074ms)
    Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 60.767747ms)
    Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 61.38677ms)
    Feb 27 12:51:25.054: INFO: (13) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 60.717347ms)
    Feb 27 12:51:25.055: INFO: (13) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 61.26969ms)
    Feb 27 12:51:25.055: INFO: (13) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 60.959559ms)
    Feb 27 12:51:25.057: INFO: (13) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 63.68089ms)
    Feb 27 12:51:25.057: INFO: (13) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 64.005841ms)
    Feb 27 12:51:25.057: INFO: (13) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 62.980467ms)
    Feb 27 12:51:25.074: INFO: (14) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 17.307941ms)
    Feb 27 12:51:25.077: INFO: (14) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 20.003612ms)
    Feb 27 12:51:25.077: INFO: (14) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 20.122862ms)
    Feb 27 12:51:25.077: INFO: (14) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 19.6137ms)
    Feb 27 12:51:25.078: INFO: (14) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 20.318433ms)
    Feb 27 12:51:25.078: INFO: (14) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 20.733094ms)
    Feb 27 12:51:25.078: INFO: (14) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 20.963136ms)
    Feb 27 12:51:25.079: INFO: (14) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 21.322517ms)
    Feb 27 12:51:25.083: INFO: (14) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 25.494944ms)
    Feb 27 12:51:25.083: INFO: (14) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 25.754175ms)
    Feb 27 12:51:25.085: INFO: (14) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 27.846394ms)
    Feb 27 12:51:25.085: INFO: (14) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 28.026205ms)
    Feb 27 12:51:25.085: INFO: (14) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 27.995494ms)
    Feb 27 12:51:25.086: INFO: (14) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 28.452126ms)
    Feb 27 12:51:25.089: INFO: (14) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 31.261038ms)
    Feb 27 12:51:25.091: INFO: (14) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 33.669257ms)
    Feb 27 12:51:25.111: INFO: (15) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 19.970662ms)
    Feb 27 12:51:25.113: INFO: (15) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 18.462375ms)
    Feb 27 12:51:25.114: INFO: (15) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 22.683712ms)
    Feb 27 12:51:25.119: INFO: (15) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 24.160878ms)
    Feb 27 12:51:25.119: INFO: (15) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 27.848164ms)
    Feb 27 12:51:25.119: INFO: (15) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 24.254828ms)
    Feb 27 12:51:25.120: INFO: (15) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 27.288841ms)
    Feb 27 12:51:25.120: INFO: (15) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 25.773115ms)
    Feb 27 12:51:25.121: INFO: (15) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 27.06251ms)
    Feb 27 12:51:25.121: INFO: (15) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 28.094134ms)
    Feb 27 12:51:25.121: INFO: (15) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 27.01265ms)
    Feb 27 12:51:25.121: INFO: (15) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 27.731673ms)
    Feb 27 12:51:25.121: INFO: (15) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 28.481016ms)
    Feb 27 12:51:25.122: INFO: (15) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 29.660291ms)
    Feb 27 12:51:25.122: INFO: (15) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 30.593885ms)
    Feb 27 12:51:25.124: INFO: (15) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 32.774673ms)
    Feb 27 12:51:25.147: INFO: (16) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 22.32082ms)
    Feb 27 12:51:25.148: INFO: (16) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 20.884035ms)
    Feb 27 12:51:25.151: INFO: (16) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 25.806795ms)
    Feb 27 12:51:25.151: INFO: (16) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 25.116202ms)
    Feb 27 12:51:25.152: INFO: (16) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 26.534418ms)
    Feb 27 12:51:25.152: INFO: (16) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 24.52427ms)
    Feb 27 12:51:25.152: INFO: (16) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 24.469449ms)
    Feb 27 12:51:25.152: INFO: (16) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 24.45715ms)
    Feb 27 12:51:25.152: INFO: (16) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 25.890336ms)
    Feb 27 12:51:25.152: INFO: (16) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 26.478608ms)
    Feb 27 12:51:25.157: INFO: (16) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 32.012811ms)
    Feb 27 12:51:25.157: INFO: (16) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 29.147799ms)
    Feb 27 12:51:25.158: INFO: (16) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 30.639575ms)
    Feb 27 12:51:25.161: INFO: (16) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 33.272136ms)
    Feb 27 12:51:25.162: INFO: (16) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 34.882482ms)
    Feb 27 12:51:25.168: INFO: (16) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 40.740126ms)
    Feb 27 12:51:25.180: INFO: (17) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 12.034659ms)
    Feb 27 12:51:25.188: INFO: (17) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 19.254788ms)
    Feb 27 12:51:25.188: INFO: (17) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 18.773176ms)
    Feb 27 12:51:25.188: INFO: (17) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 17.34474ms)
    Feb 27 12:51:25.190: INFO: (17) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 18.577096ms)
    Feb 27 12:51:25.190: INFO: (17) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 18.168483ms)
    Feb 27 12:51:25.191: INFO: (17) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 18.236044ms)
    Feb 27 12:51:25.191: INFO: (17) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 18.145643ms)
    Feb 27 12:51:25.191: INFO: (17) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 18.786996ms)
    Feb 27 12:51:25.191: INFO: (17) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 19.003367ms)
    Feb 27 12:51:25.191: INFO: (17) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 21.073405ms)
    Feb 27 12:51:25.194: INFO: (17) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 24.275689ms)
    Feb 27 12:51:25.194: INFO: (17) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 23.570346ms)
    Feb 27 12:51:25.197: INFO: (17) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 24.71304ms)
    Feb 27 12:51:25.200: INFO: (17) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 27.783643ms)
    Feb 27 12:51:25.200: INFO: (17) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 28.013214ms)
    Feb 27 12:51:25.221: INFO: (18) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 20.764015ms)
    Feb 27 12:51:25.221: INFO: (18) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 20.449593ms)
    Feb 27 12:51:25.221: INFO: (18) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 20.699894ms)
    Feb 27 12:51:25.221: INFO: (18) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 17.699592ms)
    Feb 27 12:51:25.221: INFO: (18) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 18.948677ms)
    Feb 27 12:51:25.221: INFO: (18) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 19.192878ms)
    Feb 27 12:51:25.222: INFO: (18) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 17.804272ms)
    Feb 27 12:51:25.222: INFO: (18) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 18.368995ms)
    Feb 27 12:51:25.222: INFO: (18) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 19.5807ms)
    Feb 27 12:51:25.222: INFO: (18) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 21.847599ms)
    Feb 27 12:51:25.222: INFO: (18) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 18.280165ms)
    Feb 27 12:51:25.224: INFO: (18) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 22.734082ms)
    Feb 27 12:51:25.225: INFO: (18) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 23.863207ms)
    Feb 27 12:51:25.226: INFO: (18) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 23.037504ms)
    Feb 27 12:51:25.226: INFO: (18) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 23.012733ms)
    Feb 27 12:51:25.229: INFO: (18) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 25.522984ms)
    Feb 27 12:51:25.278: INFO: (19) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 48.390877ms)
    Feb 27 12:51:25.278: INFO: (19) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname1/proxy/: foo (200; 49.0983ms)
    Feb 27 12:51:25.280: INFO: (19) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname1/proxy/: foo (200; 50.751917ms)
    Feb 27 12:51:25.281: INFO: (19) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">... (200; 51.768211ms)
    Feb 27 12:51:25.281: INFO: (19) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:1080/proxy/rewriteme">test<... (200; 51.885962ms)
    Feb 27 12:51:25.283: INFO: (19) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn/proxy/rewriteme">test</a> (200; 53.325907ms)
    Feb 27 12:51:25.283: INFO: (19) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:462/proxy/: tls qux (200; 53.668889ms)
    Feb 27 12:51:25.283: INFO: (19) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:162/proxy/: bar (200; 53.543979ms)
    Feb 27 12:51:25.283: INFO: (19) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/: <a href="/api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:443/proxy/tlsrewritem... (200; 53.667348ms)
    Feb 27 12:51:25.284: INFO: (19) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname2/proxy/: tls qux (200; 54.371092ms)
    Feb 27 12:51:25.284: INFO: (19) /api/v1/namespaces/proxy-5861/pods/proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 54.704913ms)
    Feb 27 12:51:25.285: INFO: (19) /api/v1/namespaces/proxy-5861/pods/https:proxy-service-vkpkl-xz6dn:460/proxy/: tls baz (200; 56.3168ms)
    Feb 27 12:51:25.285: INFO: (19) /api/v1/namespaces/proxy-5861/services/proxy-service-vkpkl:portname2/proxy/: bar (200; 56.204459ms)
    Feb 27 12:51:25.285: INFO: (19) /api/v1/namespaces/proxy-5861/pods/http:proxy-service-vkpkl-xz6dn:160/proxy/: foo (200; 56.025588ms)
    Feb 27 12:51:25.285: INFO: (19) /api/v1/namespaces/proxy-5861/services/https:proxy-service-vkpkl:tlsportname1/proxy/: tls baz (200; 55.949348ms)
    Feb 27 12:51:25.287: INFO: (19) /api/v1/namespaces/proxy-5861/services/http:proxy-service-vkpkl:portname2/proxy/: bar (200; 57.074262ms)
    STEP: deleting ReplicationController proxy-service-vkpkl in namespace proxy-5861, will wait for the garbage collector to delete the pods 02/27/23 12:51:25.287
    Feb 27 12:51:25.362: INFO: Deleting ReplicationController proxy-service-vkpkl took: 17.320841ms
    Feb 27 12:51:25.463: INFO: Terminating ReplicationController proxy-service-vkpkl pods took: 101.028651ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb 27 12:51:27.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5861" for this suite. 02/27/23 12:51:27.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:51:27.317
Feb 27 12:51:27.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename var-expansion 02/27/23 12:51:27.319
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:27.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:27.369
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 02/27/23 12:51:27.382
Feb 27 12:51:27.401: INFO: Waiting up to 5m0s for pod "var-expansion-e6a65959-189d-4173-ac5e-c57254420237" in namespace "var-expansion-9747" to be "Succeeded or Failed"
Feb 27 12:51:27.412: INFO: Pod "var-expansion-e6a65959-189d-4173-ac5e-c57254420237": Phase="Pending", Reason="", readiness=false. Elapsed: 11.567517ms
Feb 27 12:51:29.420: INFO: Pod "var-expansion-e6a65959-189d-4173-ac5e-c57254420237": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01938694s
Feb 27 12:51:31.438: INFO: Pod "var-expansion-e6a65959-189d-4173-ac5e-c57254420237": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037268464s
STEP: Saw pod success 02/27/23 12:51:31.438
Feb 27 12:51:31.438: INFO: Pod "var-expansion-e6a65959-189d-4173-ac5e-c57254420237" satisfied condition "Succeeded or Failed"
Feb 27 12:51:31.452: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod var-expansion-e6a65959-189d-4173-ac5e-c57254420237 container dapi-container: <nil>
STEP: delete the pod 02/27/23 12:51:31.496
Feb 27 12:51:31.525: INFO: Waiting for pod var-expansion-e6a65959-189d-4173-ac5e-c57254420237 to disappear
Feb 27 12:51:31.547: INFO: Pod var-expansion-e6a65959-189d-4173-ac5e-c57254420237 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 27 12:51:31.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9747" for this suite. 02/27/23 12:51:31.561
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":334,"skipped":6020,"failed":0}
------------------------------
• [4.259 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:51:27.317
    Feb 27 12:51:27.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename var-expansion 02/27/23 12:51:27.319
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:27.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:27.369
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 02/27/23 12:51:27.382
    Feb 27 12:51:27.401: INFO: Waiting up to 5m0s for pod "var-expansion-e6a65959-189d-4173-ac5e-c57254420237" in namespace "var-expansion-9747" to be "Succeeded or Failed"
    Feb 27 12:51:27.412: INFO: Pod "var-expansion-e6a65959-189d-4173-ac5e-c57254420237": Phase="Pending", Reason="", readiness=false. Elapsed: 11.567517ms
    Feb 27 12:51:29.420: INFO: Pod "var-expansion-e6a65959-189d-4173-ac5e-c57254420237": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01938694s
    Feb 27 12:51:31.438: INFO: Pod "var-expansion-e6a65959-189d-4173-ac5e-c57254420237": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037268464s
    STEP: Saw pod success 02/27/23 12:51:31.438
    Feb 27 12:51:31.438: INFO: Pod "var-expansion-e6a65959-189d-4173-ac5e-c57254420237" satisfied condition "Succeeded or Failed"
    Feb 27 12:51:31.452: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod var-expansion-e6a65959-189d-4173-ac5e-c57254420237 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 12:51:31.496
    Feb 27 12:51:31.525: INFO: Waiting for pod var-expansion-e6a65959-189d-4173-ac5e-c57254420237 to disappear
    Feb 27 12:51:31.547: INFO: Pod var-expansion-e6a65959-189d-4173-ac5e-c57254420237 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 27 12:51:31.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9747" for this suite. 02/27/23 12:51:31.561
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:51:31.578
Feb 27 12:51:31.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename containers 02/27/23 12:51:31.579
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:31.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:31.612
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 02/27/23 12:51:31.622
Feb 27 12:51:31.639: INFO: Waiting up to 5m0s for pod "client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653" in namespace "containers-7952" to be "Succeeded or Failed"
Feb 27 12:51:31.647: INFO: Pod "client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653": Phase="Pending", Reason="", readiness=false. Elapsed: 7.770952ms
Feb 27 12:51:33.658: INFO: Pod "client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018166806s
Feb 27 12:51:35.658: INFO: Pod "client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01818395s
STEP: Saw pod success 02/27/23 12:51:35.658
Feb 27 12:51:35.659: INFO: Pod "client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653" satisfied condition "Succeeded or Failed"
Feb 27 12:51:35.682: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 12:51:35.698
Feb 27 12:51:35.723: INFO: Waiting for pod client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653 to disappear
Feb 27 12:51:35.742: INFO: Pod client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 27 12:51:35.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7952" for this suite. 02/27/23 12:51:35.753
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":335,"skipped":6023,"failed":0}
------------------------------
• [4.188 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:51:31.578
    Feb 27 12:51:31.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename containers 02/27/23 12:51:31.579
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:31.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:31.612
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 02/27/23 12:51:31.622
    Feb 27 12:51:31.639: INFO: Waiting up to 5m0s for pod "client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653" in namespace "containers-7952" to be "Succeeded or Failed"
    Feb 27 12:51:31.647: INFO: Pod "client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653": Phase="Pending", Reason="", readiness=false. Elapsed: 7.770952ms
    Feb 27 12:51:33.658: INFO: Pod "client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018166806s
    Feb 27 12:51:35.658: INFO: Pod "client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01818395s
    STEP: Saw pod success 02/27/23 12:51:35.658
    Feb 27 12:51:35.659: INFO: Pod "client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653" satisfied condition "Succeeded or Failed"
    Feb 27 12:51:35.682: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 12:51:35.698
    Feb 27 12:51:35.723: INFO: Waiting for pod client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653 to disappear
    Feb 27 12:51:35.742: INFO: Pod client-containers-54b83bbe-ffe5-4390-9ff0-6453d241b653 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 27 12:51:35.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7952" for this suite. 02/27/23 12:51:35.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:51:35.775
Feb 27 12:51:35.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 12:51:35.776
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:35.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:35.808
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-8020/secret-test-7b374e5e-1063-45cc-8671-98850e2959d9 02/27/23 12:51:35.82
STEP: Creating a pod to test consume secrets 02/27/23 12:51:35.831
Feb 27 12:51:35.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8" in namespace "secrets-8020" to be "Succeeded or Failed"
Feb 27 12:51:35.855: INFO: Pod "pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.723565ms
Feb 27 12:51:37.867: INFO: Pod "pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020441527s
Feb 27 12:51:39.864: INFO: Pod "pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018115009s
STEP: Saw pod success 02/27/23 12:51:39.865
Feb 27 12:51:39.865: INFO: Pod "pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8" satisfied condition "Succeeded or Failed"
Feb 27 12:51:39.874: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8 container env-test: <nil>
STEP: delete the pod 02/27/23 12:51:39.892
Feb 27 12:51:39.913: INFO: Waiting for pod pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8 to disappear
Feb 27 12:51:39.924: INFO: Pod pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 27 12:51:39.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8020" for this suite. 02/27/23 12:51:39.933
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":336,"skipped":6063,"failed":0}
------------------------------
• [4.170 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:51:35.775
    Feb 27 12:51:35.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 12:51:35.776
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:35.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:35.808
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-8020/secret-test-7b374e5e-1063-45cc-8671-98850e2959d9 02/27/23 12:51:35.82
    STEP: Creating a pod to test consume secrets 02/27/23 12:51:35.831
    Feb 27 12:51:35.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8" in namespace "secrets-8020" to be "Succeeded or Failed"
    Feb 27 12:51:35.855: INFO: Pod "pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.723565ms
    Feb 27 12:51:37.867: INFO: Pod "pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020441527s
    Feb 27 12:51:39.864: INFO: Pod "pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018115009s
    STEP: Saw pod success 02/27/23 12:51:39.865
    Feb 27 12:51:39.865: INFO: Pod "pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8" satisfied condition "Succeeded or Failed"
    Feb 27 12:51:39.874: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8 container env-test: <nil>
    STEP: delete the pod 02/27/23 12:51:39.892
    Feb 27 12:51:39.913: INFO: Waiting for pod pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8 to disappear
    Feb 27 12:51:39.924: INFO: Pod pod-configmaps-c7a6f2d4-fa73-4427-af30-902d34c703d8 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 12:51:39.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8020" for this suite. 02/27/23 12:51:39.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:51:39.949
Feb 27 12:51:39.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename downward-api 02/27/23 12:51:39.951
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:39.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:39.988
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 02/27/23 12:51:39.996
Feb 27 12:51:40.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a" in namespace "downward-api-3825" to be "Succeeded or Failed"
Feb 27 12:51:40.028: INFO: Pod "downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.15886ms
Feb 27 12:51:42.035: INFO: Pod "downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a": Phase="Running", Reason="", readiness=true. Elapsed: 2.019169715s
Feb 27 12:51:44.040: INFO: Pod "downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a": Phase="Running", Reason="", readiness=false. Elapsed: 4.024280046s
Feb 27 12:51:46.037: INFO: Pod "downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021548336s
STEP: Saw pod success 02/27/23 12:51:46.038
Feb 27 12:51:46.038: INFO: Pod "downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a" satisfied condition "Succeeded or Failed"
Feb 27 12:51:46.046: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a container client-container: <nil>
STEP: delete the pod 02/27/23 12:51:46.064
Feb 27 12:51:46.081: INFO: Waiting for pod downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a to disappear
Feb 27 12:51:46.088: INFO: Pod downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 27 12:51:46.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3825" for this suite. 02/27/23 12:51:46.1
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":337,"skipped":6091,"failed":0}
------------------------------
• [SLOW TEST] [6.163 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:51:39.949
    Feb 27 12:51:39.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename downward-api 02/27/23 12:51:39.951
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:39.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:39.988
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 02/27/23 12:51:39.996
    Feb 27 12:51:40.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a" in namespace "downward-api-3825" to be "Succeeded or Failed"
    Feb 27 12:51:40.028: INFO: Pod "downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.15886ms
    Feb 27 12:51:42.035: INFO: Pod "downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a": Phase="Running", Reason="", readiness=true. Elapsed: 2.019169715s
    Feb 27 12:51:44.040: INFO: Pod "downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a": Phase="Running", Reason="", readiness=false. Elapsed: 4.024280046s
    Feb 27 12:51:46.037: INFO: Pod "downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021548336s
    STEP: Saw pod success 02/27/23 12:51:46.038
    Feb 27 12:51:46.038: INFO: Pod "downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a" satisfied condition "Succeeded or Failed"
    Feb 27 12:51:46.046: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a container client-container: <nil>
    STEP: delete the pod 02/27/23 12:51:46.064
    Feb 27 12:51:46.081: INFO: Waiting for pod downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a to disappear
    Feb 27 12:51:46.088: INFO: Pod downwardapi-volume-21c64b10-4d85-43ef-b5d7-65120ae2903a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 27 12:51:46.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3825" for this suite. 02/27/23 12:51:46.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:51:46.112
Feb 27 12:51:46.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename namespaces 02/27/23 12:51:46.114
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:46.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:46.156
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 02/27/23 12:51:46.165
Feb 27 12:51:46.174: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 02/27/23 12:51:46.174
Feb 27 12:51:46.188: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 02/27/23 12:51:46.188
Feb 27 12:51:46.210: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 27 12:51:46.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1198" for this suite. 02/27/23 12:51:46.225
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":338,"skipped":6098,"failed":0}
------------------------------
• [0.129 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:51:46.112
    Feb 27 12:51:46.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename namespaces 02/27/23 12:51:46.114
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:46.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:46.156
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 02/27/23 12:51:46.165
    Feb 27 12:51:46.174: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 02/27/23 12:51:46.174
    Feb 27 12:51:46.188: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 02/27/23 12:51:46.188
    Feb 27 12:51:46.210: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 27 12:51:46.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1198" for this suite. 02/27/23 12:51:46.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:51:46.246
Feb 27 12:51:46.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename cronjob 02/27/23 12:51:46.247
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:46.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:46.284
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 02/27/23 12:51:46.293
STEP: Ensuring a job is scheduled 02/27/23 12:51:46.307
STEP: Ensuring exactly one is scheduled 02/27/23 12:52:00.316
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/27/23 12:52:00.324
STEP: Ensuring no more jobs are scheduled 02/27/23 12:52:00.332
STEP: Removing cronjob 02/27/23 12:57:00.366
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 27 12:57:00.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-910" for this suite. 02/27/23 12:57:00.416
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":339,"skipped":6107,"failed":0}
------------------------------
• [SLOW TEST] [314.209 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:51:46.246
    Feb 27 12:51:46.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename cronjob 02/27/23 12:51:46.247
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:51:46.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:51:46.284
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 02/27/23 12:51:46.293
    STEP: Ensuring a job is scheduled 02/27/23 12:51:46.307
    STEP: Ensuring exactly one is scheduled 02/27/23 12:52:00.316
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/27/23 12:52:00.324
    STEP: Ensuring no more jobs are scheduled 02/27/23 12:52:00.332
    STEP: Removing cronjob 02/27/23 12:57:00.366
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 27 12:57:00.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-910" for this suite. 02/27/23 12:57:00.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:57:00.508
Feb 27 12:57:00.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename init-container 02/27/23 12:57:00.509
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:00.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:00.554
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 02/27/23 12:57:00.565
Feb 27 12:57:00.565: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 27 12:57:04.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3648" for this suite. 02/27/23 12:57:04.223
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":340,"skipped":6147,"failed":0}
------------------------------
• [3.727 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:57:00.508
    Feb 27 12:57:00.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename init-container 02/27/23 12:57:00.509
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:00.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:00.554
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 02/27/23 12:57:00.565
    Feb 27 12:57:00.565: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 27 12:57:04.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3648" for this suite. 02/27/23 12:57:04.223
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:57:04.239
Feb 27 12:57:04.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 12:57:04.241
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:04.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:04.285
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9066 02/27/23 12:57:04.297
STEP: changing the ExternalName service to type=NodePort 02/27/23 12:57:04.317
STEP: creating replication controller externalname-service in namespace services-9066 02/27/23 12:57:04.377
I0227 12:57:04.395297      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9066, replica count: 2
I0227 12:57:07.448864      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 12:57:07.449: INFO: Creating new exec pod
Feb 27 12:57:07.458: INFO: Waiting up to 5m0s for pod "execpod86ghg" in namespace "services-9066" to be "running"
Feb 27 12:57:07.469: INFO: Pod "execpod86ghg": Phase="Pending", Reason="", readiness=false. Elapsed: 10.709323ms
Feb 27 12:57:09.478: INFO: Pod "execpod86ghg": Phase="Running", Reason="", readiness=true. Elapsed: 2.019874665s
Feb 27 12:57:09.478: INFO: Pod "execpod86ghg" satisfied condition "running"
Feb 27 12:57:10.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-9066 exec execpod86ghg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 27 12:57:11.092: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 27 12:57:11.092: INFO: stdout: "externalname-service-7ncmj"
Feb 27 12:57:11.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-9066 exec execpod86ghg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.22.170 80'
Feb 27 12:57:11.356: INFO: stderr: "+ nc -v -t -w 2 10.240.22.170 80\nConnection to 10.240.22.170 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Feb 27 12:57:11.356: INFO: stdout: "externalname-service-l88q8"
Feb 27 12:57:11.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-9066 exec execpod86ghg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.167 31653'
Feb 27 12:57:11.621: INFO: stderr: "+ nc -v -t -w 2 172.31.7.167 31653\n+ echo hostName\nConnection to 172.31.7.167 31653 port [tcp/*] succeeded!\n"
Feb 27 12:57:11.621: INFO: stdout: "externalname-service-7ncmj"
Feb 27 12:57:11.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-9066 exec execpod86ghg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.159 31653'
Feb 27 12:57:11.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.11.159 31653\nConnection to 172.31.11.159 31653 port [tcp/*] succeeded!\n"
Feb 27 12:57:11.865: INFO: stdout: ""
Feb 27 12:57:12.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-9066 exec execpod86ghg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.159 31653'
Feb 27 12:57:13.133: INFO: stderr: "+ nc -v -t -w 2 172.31.11.159 31653\n+ echo hostName\nConnection to 172.31.11.159 31653 port [tcp/*] succeeded!\n"
Feb 27 12:57:13.133: INFO: stdout: "externalname-service-l88q8"
Feb 27 12:57:13.133: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 12:57:13.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9066" for this suite. 02/27/23 12:57:13.208
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":341,"skipped":6151,"failed":0}
------------------------------
• [SLOW TEST] [9.004 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:57:04.239
    Feb 27 12:57:04.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 12:57:04.241
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:04.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:04.285
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9066 02/27/23 12:57:04.297
    STEP: changing the ExternalName service to type=NodePort 02/27/23 12:57:04.317
    STEP: creating replication controller externalname-service in namespace services-9066 02/27/23 12:57:04.377
    I0227 12:57:04.395297      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9066, replica count: 2
    I0227 12:57:07.448864      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 12:57:07.449: INFO: Creating new exec pod
    Feb 27 12:57:07.458: INFO: Waiting up to 5m0s for pod "execpod86ghg" in namespace "services-9066" to be "running"
    Feb 27 12:57:07.469: INFO: Pod "execpod86ghg": Phase="Pending", Reason="", readiness=false. Elapsed: 10.709323ms
    Feb 27 12:57:09.478: INFO: Pod "execpod86ghg": Phase="Running", Reason="", readiness=true. Elapsed: 2.019874665s
    Feb 27 12:57:09.478: INFO: Pod "execpod86ghg" satisfied condition "running"
    Feb 27 12:57:10.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-9066 exec execpod86ghg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 27 12:57:11.092: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 27 12:57:11.092: INFO: stdout: "externalname-service-7ncmj"
    Feb 27 12:57:11.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-9066 exec execpod86ghg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.22.170 80'
    Feb 27 12:57:11.356: INFO: stderr: "+ nc -v -t -w 2 10.240.22.170 80\nConnection to 10.240.22.170 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Feb 27 12:57:11.356: INFO: stdout: "externalname-service-l88q8"
    Feb 27 12:57:11.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-9066 exec execpod86ghg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.7.167 31653'
    Feb 27 12:57:11.621: INFO: stderr: "+ nc -v -t -w 2 172.31.7.167 31653\n+ echo hostName\nConnection to 172.31.7.167 31653 port [tcp/*] succeeded!\n"
    Feb 27 12:57:11.621: INFO: stdout: "externalname-service-7ncmj"
    Feb 27 12:57:11.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-9066 exec execpod86ghg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.159 31653'
    Feb 27 12:57:11.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.11.159 31653\nConnection to 172.31.11.159 31653 port [tcp/*] succeeded!\n"
    Feb 27 12:57:11.865: INFO: stdout: ""
    Feb 27 12:57:12.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=services-9066 exec execpod86ghg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.159 31653'
    Feb 27 12:57:13.133: INFO: stderr: "+ nc -v -t -w 2 172.31.11.159 31653\n+ echo hostName\nConnection to 172.31.11.159 31653 port [tcp/*] succeeded!\n"
    Feb 27 12:57:13.133: INFO: stdout: "externalname-service-l88q8"
    Feb 27 12:57:13.133: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 12:57:13.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9066" for this suite. 02/27/23 12:57:13.208
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:57:13.252
Feb 27 12:57:13.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename statefulset 02/27/23 12:57:13.254
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:13.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:13.344
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1991 02/27/23 12:57:13.352
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 02/27/23 12:57:13.363
STEP: Creating pod with conflicting port in namespace statefulset-1991 02/27/23 12:57:13.38
STEP: Waiting until pod test-pod will start running in namespace statefulset-1991 02/27/23 12:57:13.406
Feb 27 12:57:13.406: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-1991" to be "running"
Feb 27 12:57:13.415: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.268424ms
Feb 27 12:57:15.428: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02116173s
Feb 27 12:57:15.428: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-1991 02/27/23 12:57:15.428
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1991 02/27/23 12:57:15.447
Feb 27 12:57:15.465: INFO: Observed stateful pod in namespace: statefulset-1991, name: ss-0, uid: d197b8ab-188b-447c-9ddd-1e1c342dc374, status phase: Pending. Waiting for statefulset controller to delete.
Feb 27 12:57:15.497: INFO: Observed stateful pod in namespace: statefulset-1991, name: ss-0, uid: d197b8ab-188b-447c-9ddd-1e1c342dc374, status phase: Failed. Waiting for statefulset controller to delete.
Feb 27 12:57:15.526: INFO: Observed stateful pod in namespace: statefulset-1991, name: ss-0, uid: d197b8ab-188b-447c-9ddd-1e1c342dc374, status phase: Failed. Waiting for statefulset controller to delete.
Feb 27 12:57:15.533: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1991
STEP: Removing pod with conflicting port in namespace statefulset-1991 02/27/23 12:57:15.533
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1991 and will be in running state 02/27/23 12:57:15.569
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 27 12:57:17.601: INFO: Deleting all statefulset in ns statefulset-1991
Feb 27 12:57:17.614: INFO: Scaling statefulset ss to 0
Feb 27 12:57:27.657: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 12:57:27.664: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 27 12:57:27.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1991" for this suite. 02/27/23 12:57:27.717
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":342,"skipped":6180,"failed":0}
------------------------------
• [SLOW TEST] [14.481 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:57:13.252
    Feb 27 12:57:13.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename statefulset 02/27/23 12:57:13.254
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:13.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:13.344
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1991 02/27/23 12:57:13.352
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 02/27/23 12:57:13.363
    STEP: Creating pod with conflicting port in namespace statefulset-1991 02/27/23 12:57:13.38
    STEP: Waiting until pod test-pod will start running in namespace statefulset-1991 02/27/23 12:57:13.406
    Feb 27 12:57:13.406: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-1991" to be "running"
    Feb 27 12:57:13.415: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.268424ms
    Feb 27 12:57:15.428: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02116173s
    Feb 27 12:57:15.428: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-1991 02/27/23 12:57:15.428
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1991 02/27/23 12:57:15.447
    Feb 27 12:57:15.465: INFO: Observed stateful pod in namespace: statefulset-1991, name: ss-0, uid: d197b8ab-188b-447c-9ddd-1e1c342dc374, status phase: Pending. Waiting for statefulset controller to delete.
    Feb 27 12:57:15.497: INFO: Observed stateful pod in namespace: statefulset-1991, name: ss-0, uid: d197b8ab-188b-447c-9ddd-1e1c342dc374, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 27 12:57:15.526: INFO: Observed stateful pod in namespace: statefulset-1991, name: ss-0, uid: d197b8ab-188b-447c-9ddd-1e1c342dc374, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 27 12:57:15.533: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1991
    STEP: Removing pod with conflicting port in namespace statefulset-1991 02/27/23 12:57:15.533
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1991 and will be in running state 02/27/23 12:57:15.569
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 27 12:57:17.601: INFO: Deleting all statefulset in ns statefulset-1991
    Feb 27 12:57:17.614: INFO: Scaling statefulset ss to 0
    Feb 27 12:57:27.657: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 12:57:27.664: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 27 12:57:27.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1991" for this suite. 02/27/23 12:57:27.717
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:57:27.735
Feb 27 12:57:27.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename var-expansion 02/27/23 12:57:27.737
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:27.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:27.777
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 02/27/23 12:57:27.788
Feb 27 12:57:27.803: INFO: Waiting up to 5m0s for pod "var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87" in namespace "var-expansion-8092" to be "Succeeded or Failed"
Feb 27 12:57:27.824: INFO: Pod "var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87": Phase="Pending", Reason="", readiness=false. Elapsed: 20.891864ms
Feb 27 12:57:29.833: INFO: Pod "var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030307835s
Feb 27 12:57:31.833: INFO: Pod "var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029710707s
STEP: Saw pod success 02/27/23 12:57:31.833
Feb 27 12:57:31.833: INFO: Pod "var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87" satisfied condition "Succeeded or Failed"
Feb 27 12:57:31.840: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87 container dapi-container: <nil>
STEP: delete the pod 02/27/23 12:57:31.858
Feb 27 12:57:31.883: INFO: Waiting for pod var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87 to disappear
Feb 27 12:57:31.889: INFO: Pod var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 27 12:57:31.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8092" for this suite. 02/27/23 12:57:31.898
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":343,"skipped":6180,"failed":0}
------------------------------
• [4.178 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:57:27.735
    Feb 27 12:57:27.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename var-expansion 02/27/23 12:57:27.737
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:27.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:27.777
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 02/27/23 12:57:27.788
    Feb 27 12:57:27.803: INFO: Waiting up to 5m0s for pod "var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87" in namespace "var-expansion-8092" to be "Succeeded or Failed"
    Feb 27 12:57:27.824: INFO: Pod "var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87": Phase="Pending", Reason="", readiness=false. Elapsed: 20.891864ms
    Feb 27 12:57:29.833: INFO: Pod "var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030307835s
    Feb 27 12:57:31.833: INFO: Pod "var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029710707s
    STEP: Saw pod success 02/27/23 12:57:31.833
    Feb 27 12:57:31.833: INFO: Pod "var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87" satisfied condition "Succeeded or Failed"
    Feb 27 12:57:31.840: INFO: Trying to get logs from node ip-172-31-7-167.eu-central-1.compute.internal pod var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 12:57:31.858
    Feb 27 12:57:31.883: INFO: Waiting for pod var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87 to disappear
    Feb 27 12:57:31.889: INFO: Pod var-expansion-6bffad6f-1dc5-4f7b-9267-5164cbbd1d87 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 27 12:57:31.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8092" for this suite. 02/27/23 12:57:31.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:57:31.915
Feb 27 12:57:31.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 12:57:31.917
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:31.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:31.956
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-1463223c-3580-43b2-8b29-f3ff7b2a11cb 02/27/23 12:57:31.989
STEP: Creating the pod 02/27/23 12:57:32.004
Feb 27 12:57:32.037: INFO: Waiting up to 5m0s for pod "pod-configmaps-72312bb8-92b5-40d3-8ef2-7414ca89c7eb" in namespace "configmap-7912" to be "running and ready"
Feb 27 12:57:32.047: INFO: Pod "pod-configmaps-72312bb8-92b5-40d3-8ef2-7414ca89c7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.643699ms
Feb 27 12:57:32.047: INFO: The phase of Pod pod-configmaps-72312bb8-92b5-40d3-8ef2-7414ca89c7eb is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:57:34.056: INFO: Pod "pod-configmaps-72312bb8-92b5-40d3-8ef2-7414ca89c7eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.018772868s
Feb 27 12:57:34.056: INFO: The phase of Pod pod-configmaps-72312bb8-92b5-40d3-8ef2-7414ca89c7eb is Running (Ready = true)
Feb 27 12:57:34.056: INFO: Pod "pod-configmaps-72312bb8-92b5-40d3-8ef2-7414ca89c7eb" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-1463223c-3580-43b2-8b29-f3ff7b2a11cb 02/27/23 12:57:34.09
STEP: waiting to observe update in volume 02/27/23 12:57:34.104
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 12:57:36.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7912" for this suite. 02/27/23 12:57:36.157
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":344,"skipped":6204,"failed":0}
------------------------------
• [4.256 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:57:31.915
    Feb 27 12:57:31.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 12:57:31.917
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:31.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:31.956
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-1463223c-3580-43b2-8b29-f3ff7b2a11cb 02/27/23 12:57:31.989
    STEP: Creating the pod 02/27/23 12:57:32.004
    Feb 27 12:57:32.037: INFO: Waiting up to 5m0s for pod "pod-configmaps-72312bb8-92b5-40d3-8ef2-7414ca89c7eb" in namespace "configmap-7912" to be "running and ready"
    Feb 27 12:57:32.047: INFO: Pod "pod-configmaps-72312bb8-92b5-40d3-8ef2-7414ca89c7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.643699ms
    Feb 27 12:57:32.047: INFO: The phase of Pod pod-configmaps-72312bb8-92b5-40d3-8ef2-7414ca89c7eb is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:57:34.056: INFO: Pod "pod-configmaps-72312bb8-92b5-40d3-8ef2-7414ca89c7eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.018772868s
    Feb 27 12:57:34.056: INFO: The phase of Pod pod-configmaps-72312bb8-92b5-40d3-8ef2-7414ca89c7eb is Running (Ready = true)
    Feb 27 12:57:34.056: INFO: Pod "pod-configmaps-72312bb8-92b5-40d3-8ef2-7414ca89c7eb" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-1463223c-3580-43b2-8b29-f3ff7b2a11cb 02/27/23 12:57:34.09
    STEP: waiting to observe update in volume 02/27/23 12:57:34.104
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 12:57:36.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7912" for this suite. 02/27/23 12:57:36.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:57:36.182
Feb 27 12:57:36.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename deployment 02/27/23 12:57:36.191
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:36.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:36.236
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Feb 27 12:57:36.246: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 27 12:57:36.267: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 27 12:57:41.276: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 12:57:41.276
Feb 27 12:57:41.277: INFO: Creating deployment "test-rolling-update-deployment"
Feb 27 12:57:41.290: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 27 12:57:41.333: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Feb 27 12:57:43.362: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 27 12:57:43.377: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 12:57:43.405: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1156  75ea67e8-443d-4e0f-a781-a512e9875477 105168 1 2023-02-27 12:57:41 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-27 12:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:57:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006034b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 12:57:41 +0000 UTC,LastTransitionTime:2023-02-27 12:57:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-02-27 12:57:42 +0000 UTC,LastTransitionTime:2023-02-27 12:57:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 12:57:43.412: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-1156  42347c0e-1efc-43d9-b960-39d06ed89d2f 105158 1 2023-02-27 12:57:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 75ea67e8-443d-4e0f-a781-a512e9875477 0xc006035007 0xc006035008}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75ea67e8-443d-4e0f-a781-a512e9875477\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:57:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060350b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 12:57:43.412: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 27 12:57:43.412: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1156  b7b30e13-6e4d-46b4-9a88-490d9ceabcb9 105167 2 2023-02-27 12:57:36 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 75ea67e8-443d-4e0f-a781-a512e9875477 0xc006034ed7 0xc006034ed8}] [] [{e2e.test Update apps/v1 2023-02-27 12:57:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:57:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75ea67e8-443d-4e0f-a781-a512e9875477\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:57:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006034f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 12:57:43.423: INFO: Pod "test-rolling-update-deployment-78f575d8ff-hhbq4" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-hhbq4 test-rolling-update-deployment-78f575d8ff- deployment-1156  28a98ff1-a604-43d2-bcbf-fa6a68b911ba 105157 0 2023-02-27 12:57:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:fda6150eeedcefb03cde7b32a21a5bf89fd19b0a4a6f5548c647c96a6b26de34 cni.projectcalico.org/podIP:172.25.2.88/32 cni.projectcalico.org/podIPs:172.25.2.88/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 42347c0e-1efc-43d9-b960-39d06ed89d2f 0xc006035537 0xc006035538}] [] [{calico Update v1 2023-02-27 12:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"42347c0e-1efc-43d9-b960-39d06ed89d2f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:57:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d556p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d556p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:57:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:57:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:57:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:57:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.88,StartTime:2023-02-27 12:57:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:57:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://ccba41f0929c998cffa926661bf2f0565ae345db0fcdfe809d495950bfa17ba3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 27 12:57:43.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1156" for this suite. 02/27/23 12:57:43.451
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":345,"skipped":6231,"failed":0}
------------------------------
• [SLOW TEST] [7.283 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:57:36.182
    Feb 27 12:57:36.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename deployment 02/27/23 12:57:36.191
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:36.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:36.236
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Feb 27 12:57:36.246: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Feb 27 12:57:36.267: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 27 12:57:41.276: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 12:57:41.276
    Feb 27 12:57:41.277: INFO: Creating deployment "test-rolling-update-deployment"
    Feb 27 12:57:41.290: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Feb 27 12:57:41.333: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
    Feb 27 12:57:43.362: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Feb 27 12:57:43.377: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 12:57:43.405: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1156  75ea67e8-443d-4e0f-a781-a512e9875477 105168 1 2023-02-27 12:57:41 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-27 12:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:57:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006034b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 12:57:41 +0000 UTC,LastTransitionTime:2023-02-27 12:57:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-02-27 12:57:42 +0000 UTC,LastTransitionTime:2023-02-27 12:57:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 12:57:43.412: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-1156  42347c0e-1efc-43d9-b960-39d06ed89d2f 105158 1 2023-02-27 12:57:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 75ea67e8-443d-4e0f-a781-a512e9875477 0xc006035007 0xc006035008}] [] [{kube-controller-manager Update apps/v1 2023-02-27 12:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75ea67e8-443d-4e0f-a781-a512e9875477\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:57:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0060350b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 12:57:43.412: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Feb 27 12:57:43.412: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1156  b7b30e13-6e4d-46b4-9a88-490d9ceabcb9 105167 2 2023-02-27 12:57:36 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 75ea67e8-443d-4e0f-a781-a512e9875477 0xc006034ed7 0xc006034ed8}] [] [{e2e.test Update apps/v1 2023-02-27 12:57:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:57:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75ea67e8-443d-4e0f-a781-a512e9875477\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-27 12:57:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006034f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 12:57:43.423: INFO: Pod "test-rolling-update-deployment-78f575d8ff-hhbq4" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-hhbq4 test-rolling-update-deployment-78f575d8ff- deployment-1156  28a98ff1-a604-43d2-bcbf-fa6a68b911ba 105157 0 2023-02-27 12:57:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:fda6150eeedcefb03cde7b32a21a5bf89fd19b0a4a6f5548c647c96a6b26de34 cni.projectcalico.org/podIP:172.25.2.88/32 cni.projectcalico.org/podIPs:172.25.2.88/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 42347c0e-1efc-43d9-b960-39d06ed89d2f 0xc006035537 0xc006035538}] [] [{calico Update v1 2023-02-27 12:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-27 12:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"42347c0e-1efc-43d9-b960-39d06ed89d2f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 12:57:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d556p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d556p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-15-17.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:57:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:57:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:57:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 12:57:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.15.17,PodIP:172.25.2.88,StartTime:2023-02-27 12:57:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 12:57:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://ccba41f0929c998cffa926661bf2f0565ae345db0fcdfe809d495950bfa17ba3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 27 12:57:43.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1156" for this suite. 02/27/23 12:57:43.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:57:43.475
Feb 27 12:57:43.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename security-context-test 02/27/23 12:57:43.478
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:43.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:43.515
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Feb 27 12:57:43.558: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6d74b9a0-703f-450c-9bc8-0b7e5a0355c0" in namespace "security-context-test-5167" to be "Succeeded or Failed"
Feb 27 12:57:43.571: INFO: Pod "busybox-privileged-false-6d74b9a0-703f-450c-9bc8-0b7e5a0355c0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.2821ms
Feb 27 12:57:45.622: INFO: Pod "busybox-privileged-false-6d74b9a0-703f-450c-9bc8-0b7e5a0355c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06328222s
Feb 27 12:57:47.589: INFO: Pod "busybox-privileged-false-6d74b9a0-703f-450c-9bc8-0b7e5a0355c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030241256s
Feb 27 12:57:47.589: INFO: Pod "busybox-privileged-false-6d74b9a0-703f-450c-9bc8-0b7e5a0355c0" satisfied condition "Succeeded or Failed"
Feb 27 12:57:47.614: INFO: Got logs for pod "busybox-privileged-false-6d74b9a0-703f-450c-9bc8-0b7e5a0355c0": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 27 12:57:47.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5167" for this suite. 02/27/23 12:57:47.633
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":346,"skipped":6247,"failed":0}
------------------------------
• [4.174 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:57:43.475
    Feb 27 12:57:43.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename security-context-test 02/27/23 12:57:43.478
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:43.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:43.515
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Feb 27 12:57:43.558: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6d74b9a0-703f-450c-9bc8-0b7e5a0355c0" in namespace "security-context-test-5167" to be "Succeeded or Failed"
    Feb 27 12:57:43.571: INFO: Pod "busybox-privileged-false-6d74b9a0-703f-450c-9bc8-0b7e5a0355c0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.2821ms
    Feb 27 12:57:45.622: INFO: Pod "busybox-privileged-false-6d74b9a0-703f-450c-9bc8-0b7e5a0355c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06328222s
    Feb 27 12:57:47.589: INFO: Pod "busybox-privileged-false-6d74b9a0-703f-450c-9bc8-0b7e5a0355c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030241256s
    Feb 27 12:57:47.589: INFO: Pod "busybox-privileged-false-6d74b9a0-703f-450c-9bc8-0b7e5a0355c0" satisfied condition "Succeeded or Failed"
    Feb 27 12:57:47.614: INFO: Got logs for pod "busybox-privileged-false-6d74b9a0-703f-450c-9bc8-0b7e5a0355c0": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 27 12:57:47.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5167" for this suite. 02/27/23 12:57:47.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:57:47.658
Feb 27 12:57:47.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename endpointslice 02/27/23 12:57:47.662
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:47.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:47.731
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 02/27/23 12:57:47.75
STEP: getting /apis/discovery.k8s.io 02/27/23 12:57:47.774
STEP: getting /apis/discovery.k8s.iov1 02/27/23 12:57:47.784
STEP: creating 02/27/23 12:57:47.794
STEP: getting 02/27/23 12:57:47.887
STEP: listing 02/27/23 12:57:47.912
STEP: watching 02/27/23 12:57:47.928
Feb 27 12:57:47.928: INFO: starting watch
STEP: cluster-wide listing 02/27/23 12:57:47.941
STEP: cluster-wide watching 02/27/23 12:57:47.954
Feb 27 12:57:47.954: INFO: starting watch
STEP: patching 02/27/23 12:57:47.97
STEP: updating 02/27/23 12:57:48.041
Feb 27 12:57:48.078: INFO: waiting for watch events with expected annotations
Feb 27 12:57:48.078: INFO: saw patched and updated annotations
STEP: deleting 02/27/23 12:57:48.08
STEP: deleting a collection 02/27/23 12:57:48.169
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 27 12:57:48.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9439" for this suite. 02/27/23 12:57:48.238
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":347,"skipped":6271,"failed":0}
------------------------------
• [0.592 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:57:47.658
    Feb 27 12:57:47.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename endpointslice 02/27/23 12:57:47.662
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:47.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:47.731
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 02/27/23 12:57:47.75
    STEP: getting /apis/discovery.k8s.io 02/27/23 12:57:47.774
    STEP: getting /apis/discovery.k8s.iov1 02/27/23 12:57:47.784
    STEP: creating 02/27/23 12:57:47.794
    STEP: getting 02/27/23 12:57:47.887
    STEP: listing 02/27/23 12:57:47.912
    STEP: watching 02/27/23 12:57:47.928
    Feb 27 12:57:47.928: INFO: starting watch
    STEP: cluster-wide listing 02/27/23 12:57:47.941
    STEP: cluster-wide watching 02/27/23 12:57:47.954
    Feb 27 12:57:47.954: INFO: starting watch
    STEP: patching 02/27/23 12:57:47.97
    STEP: updating 02/27/23 12:57:48.041
    Feb 27 12:57:48.078: INFO: waiting for watch events with expected annotations
    Feb 27 12:57:48.078: INFO: saw patched and updated annotations
    STEP: deleting 02/27/23 12:57:48.08
    STEP: deleting a collection 02/27/23 12:57:48.169
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 27 12:57:48.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9439" for this suite. 02/27/23 12:57:48.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:57:48.258
Feb 27 12:57:48.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename disruption 02/27/23 12:57:48.261
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:48.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:48.312
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 02/27/23 12:57:48.344
STEP: Waiting for all pods to be running 02/27/23 12:57:50.432
Feb 27 12:57:50.463: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 27 12:57:52.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4720" for this suite. 02/27/23 12:57:52.493
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":348,"skipped":6287,"failed":0}
------------------------------
• [4.255 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:57:48.258
    Feb 27 12:57:48.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename disruption 02/27/23 12:57:48.261
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:48.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:48.312
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 02/27/23 12:57:48.344
    STEP: Waiting for all pods to be running 02/27/23 12:57:50.432
    Feb 27 12:57:50.463: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 27 12:57:52.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4720" for this suite. 02/27/23 12:57:52.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:57:52.518
Feb 27 12:57:52.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename proxy 02/27/23 12:57:52.52
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:52.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:52.57
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Feb 27 12:57:52.586: INFO: Creating pod...
Feb 27 12:57:52.618: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9620" to be "running"
Feb 27 12:57:52.633: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 14.357088ms
Feb 27 12:57:54.642: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.023715703s
Feb 27 12:57:54.642: INFO: Pod "agnhost" satisfied condition "running"
Feb 27 12:57:54.642: INFO: Creating service...
Feb 27 12:57:54.669: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=DELETE
Feb 27 12:57:54.716: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 27 12:57:54.716: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=OPTIONS
Feb 27 12:57:54.759: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 27 12:57:54.759: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=PATCH
Feb 27 12:57:54.777: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 27 12:57:54.777: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=POST
Feb 27 12:57:54.797: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 27 12:57:54.797: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=PUT
Feb 27 12:57:54.814: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 27 12:57:54.816: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=DELETE
Feb 27 12:57:54.837: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 27 12:57:54.837: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=OPTIONS
Feb 27 12:57:54.861: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 27 12:57:54.861: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=PATCH
Feb 27 12:57:54.891: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 27 12:57:54.891: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=POST
Feb 27 12:57:54.911: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 27 12:57:54.911: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=PUT
Feb 27 12:57:54.928: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 27 12:57:54.928: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=GET
Feb 27 12:57:54.939: INFO: http.Client request:GET StatusCode:301
Feb 27 12:57:54.939: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=GET
Feb 27 12:57:54.954: INFO: http.Client request:GET StatusCode:301
Feb 27 12:57:54.954: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=HEAD
Feb 27 12:57:54.960: INFO: http.Client request:HEAD StatusCode:301
Feb 27 12:57:54.960: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=HEAD
Feb 27 12:57:54.986: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb 27 12:57:54.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9620" for this suite. 02/27/23 12:57:54.999
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":349,"skipped":6300,"failed":0}
------------------------------
• [2.506 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:57:52.518
    Feb 27 12:57:52.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename proxy 02/27/23 12:57:52.52
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:52.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:52.57
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Feb 27 12:57:52.586: INFO: Creating pod...
    Feb 27 12:57:52.618: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9620" to be "running"
    Feb 27 12:57:52.633: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 14.357088ms
    Feb 27 12:57:54.642: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.023715703s
    Feb 27 12:57:54.642: INFO: Pod "agnhost" satisfied condition "running"
    Feb 27 12:57:54.642: INFO: Creating service...
    Feb 27 12:57:54.669: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=DELETE
    Feb 27 12:57:54.716: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 27 12:57:54.716: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=OPTIONS
    Feb 27 12:57:54.759: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 27 12:57:54.759: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=PATCH
    Feb 27 12:57:54.777: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 27 12:57:54.777: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=POST
    Feb 27 12:57:54.797: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 27 12:57:54.797: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=PUT
    Feb 27 12:57:54.814: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 27 12:57:54.816: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=DELETE
    Feb 27 12:57:54.837: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 27 12:57:54.837: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Feb 27 12:57:54.861: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 27 12:57:54.861: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=PATCH
    Feb 27 12:57:54.891: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 27 12:57:54.891: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=POST
    Feb 27 12:57:54.911: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 27 12:57:54.911: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=PUT
    Feb 27 12:57:54.928: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 27 12:57:54.928: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=GET
    Feb 27 12:57:54.939: INFO: http.Client request:GET StatusCode:301
    Feb 27 12:57:54.939: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=GET
    Feb 27 12:57:54.954: INFO: http.Client request:GET StatusCode:301
    Feb 27 12:57:54.954: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/pods/agnhost/proxy?method=HEAD
    Feb 27 12:57:54.960: INFO: http.Client request:HEAD StatusCode:301
    Feb 27 12:57:54.960: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-9620/services/e2e-proxy-test-service/proxy?method=HEAD
    Feb 27 12:57:54.986: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb 27 12:57:54.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-9620" for this suite. 02/27/23 12:57:54.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:57:55.03
Feb 27 12:57:55.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename statefulset 02/27/23 12:57:55.031
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:55.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:55.107
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1075 02/27/23 12:57:55.13
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-1075 02/27/23 12:57:55.165
Feb 27 12:57:55.197: INFO: Found 0 stateful pods, waiting for 1
Feb 27 12:58:05.205: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 02/27/23 12:58:05.232
STEP: Getting /status 02/27/23 12:58:05.25
Feb 27 12:58:05.373: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 02/27/23 12:58:05.373
Feb 27 12:58:05.398: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 02/27/23 12:58:05.398
Feb 27 12:58:05.406: INFO: Observed &StatefulSet event: ADDED
Feb 27 12:58:05.406: INFO: Found Statefulset ss in namespace statefulset-1075 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 12:58:05.406: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 02/27/23 12:58:05.407
Feb 27 12:58:05.407: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 27 12:58:05.423: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 02/27/23 12:58:05.423
Feb 27 12:58:05.435: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 27 12:58:05.436: INFO: Deleting all statefulset in ns statefulset-1075
Feb 27 12:58:05.450: INFO: Scaling statefulset ss to 0
Feb 27 12:58:15.525: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 12:58:15.536: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 27 12:58:15.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1075" for this suite. 02/27/23 12:58:15.592
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":350,"skipped":6320,"failed":0}
------------------------------
• [SLOW TEST] [20.580 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:57:55.03
    Feb 27 12:57:55.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename statefulset 02/27/23 12:57:55.031
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:57:55.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:57:55.107
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1075 02/27/23 12:57:55.13
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-1075 02/27/23 12:57:55.165
    Feb 27 12:57:55.197: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 12:58:05.205: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 02/27/23 12:58:05.232
    STEP: Getting /status 02/27/23 12:58:05.25
    Feb 27 12:58:05.373: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 02/27/23 12:58:05.373
    Feb 27 12:58:05.398: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 02/27/23 12:58:05.398
    Feb 27 12:58:05.406: INFO: Observed &StatefulSet event: ADDED
    Feb 27 12:58:05.406: INFO: Found Statefulset ss in namespace statefulset-1075 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 12:58:05.406: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 02/27/23 12:58:05.407
    Feb 27 12:58:05.407: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 27 12:58:05.423: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 02/27/23 12:58:05.423
    Feb 27 12:58:05.435: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 27 12:58:05.436: INFO: Deleting all statefulset in ns statefulset-1075
    Feb 27 12:58:05.450: INFO: Scaling statefulset ss to 0
    Feb 27 12:58:15.525: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 12:58:15.536: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 27 12:58:15.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1075" for this suite. 02/27/23 12:58:15.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:58:15.62
Feb 27 12:58:15.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubelet-test 02/27/23 12:58:15.621
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:15.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:15.693
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Feb 27 12:58:15.727: INFO: Waiting up to 5m0s for pod "busybox-scheduling-3bf78159-c1c9-4fba-bf28-dd9dabd969ea" in namespace "kubelet-test-9180" to be "running and ready"
Feb 27 12:58:15.739: INFO: Pod "busybox-scheduling-3bf78159-c1c9-4fba-bf28-dd9dabd969ea": Phase="Pending", Reason="", readiness=false. Elapsed: 11.583686ms
Feb 27 12:58:15.739: INFO: The phase of Pod busybox-scheduling-3bf78159-c1c9-4fba-bf28-dd9dabd969ea is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:58:17.748: INFO: Pod "busybox-scheduling-3bf78159-c1c9-4fba-bf28-dd9dabd969ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.020140993s
Feb 27 12:58:17.748: INFO: The phase of Pod busybox-scheduling-3bf78159-c1c9-4fba-bf28-dd9dabd969ea is Running (Ready = true)
Feb 27 12:58:17.748: INFO: Pod "busybox-scheduling-3bf78159-c1c9-4fba-bf28-dd9dabd969ea" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 27 12:58:17.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9180" for this suite. 02/27/23 12:58:17.796
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":351,"skipped":6325,"failed":0}
------------------------------
• [2.194 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:58:15.62
    Feb 27 12:58:15.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 12:58:15.621
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:15.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:15.693
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Feb 27 12:58:15.727: INFO: Waiting up to 5m0s for pod "busybox-scheduling-3bf78159-c1c9-4fba-bf28-dd9dabd969ea" in namespace "kubelet-test-9180" to be "running and ready"
    Feb 27 12:58:15.739: INFO: Pod "busybox-scheduling-3bf78159-c1c9-4fba-bf28-dd9dabd969ea": Phase="Pending", Reason="", readiness=false. Elapsed: 11.583686ms
    Feb 27 12:58:15.739: INFO: The phase of Pod busybox-scheduling-3bf78159-c1c9-4fba-bf28-dd9dabd969ea is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:58:17.748: INFO: Pod "busybox-scheduling-3bf78159-c1c9-4fba-bf28-dd9dabd969ea": Phase="Running", Reason="", readiness=true. Elapsed: 2.020140993s
    Feb 27 12:58:17.748: INFO: The phase of Pod busybox-scheduling-3bf78159-c1c9-4fba-bf28-dd9dabd969ea is Running (Ready = true)
    Feb 27 12:58:17.748: INFO: Pod "busybox-scheduling-3bf78159-c1c9-4fba-bf28-dd9dabd969ea" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 27 12:58:17.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9180" for this suite. 02/27/23 12:58:17.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:58:17.828
Feb 27 12:58:17.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename disruption 02/27/23 12:58:17.829
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:17.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:17.947
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 02/27/23 12:58:17.983
STEP: Waiting for the pdb to be processed 02/27/23 12:58:17.996
STEP: First trying to evict a pod which shouldn't be evictable 02/27/23 12:58:18.035
STEP: Waiting for all pods to be running 02/27/23 12:58:18.035
Feb 27 12:58:18.053: INFO: pods: 1 < 3
Feb 27 12:58:20.062: INFO: running pods: 2 < 3
STEP: locating a running pod 02/27/23 12:58:22.065
STEP: Updating the pdb to allow a pod to be evicted 02/27/23 12:58:22.106
STEP: Waiting for the pdb to be processed 02/27/23 12:58:22.184
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/27/23 12:58:22.204
STEP: Waiting for all pods to be running 02/27/23 12:58:22.205
STEP: Waiting for the pdb to observed all healthy pods 02/27/23 12:58:22.221
STEP: Patching the pdb to disallow a pod to be evicted 02/27/23 12:58:22.283
STEP: Waiting for the pdb to be processed 02/27/23 12:58:22.344
STEP: Waiting for all pods to be running 02/27/23 12:58:24.375
STEP: locating a running pod 02/27/23 12:58:24.386
STEP: Deleting the pdb to allow a pod to be evicted 02/27/23 12:58:24.403
STEP: Waiting for the pdb to be deleted 02/27/23 12:58:24.422
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/27/23 12:58:24.44
STEP: Waiting for all pods to be running 02/27/23 12:58:24.44
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 27 12:58:24.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2285" for this suite. 02/27/23 12:58:24.505
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":352,"skipped":6349,"failed":0}
------------------------------
• [SLOW TEST] [6.713 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:58:17.828
    Feb 27 12:58:17.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename disruption 02/27/23 12:58:17.829
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:17.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:17.947
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 02/27/23 12:58:17.983
    STEP: Waiting for the pdb to be processed 02/27/23 12:58:17.996
    STEP: First trying to evict a pod which shouldn't be evictable 02/27/23 12:58:18.035
    STEP: Waiting for all pods to be running 02/27/23 12:58:18.035
    Feb 27 12:58:18.053: INFO: pods: 1 < 3
    Feb 27 12:58:20.062: INFO: running pods: 2 < 3
    STEP: locating a running pod 02/27/23 12:58:22.065
    STEP: Updating the pdb to allow a pod to be evicted 02/27/23 12:58:22.106
    STEP: Waiting for the pdb to be processed 02/27/23 12:58:22.184
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/27/23 12:58:22.204
    STEP: Waiting for all pods to be running 02/27/23 12:58:22.205
    STEP: Waiting for the pdb to observed all healthy pods 02/27/23 12:58:22.221
    STEP: Patching the pdb to disallow a pod to be evicted 02/27/23 12:58:22.283
    STEP: Waiting for the pdb to be processed 02/27/23 12:58:22.344
    STEP: Waiting for all pods to be running 02/27/23 12:58:24.375
    STEP: locating a running pod 02/27/23 12:58:24.386
    STEP: Deleting the pdb to allow a pod to be evicted 02/27/23 12:58:24.403
    STEP: Waiting for the pdb to be deleted 02/27/23 12:58:24.422
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/27/23 12:58:24.44
    STEP: Waiting for all pods to be running 02/27/23 12:58:24.44
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 27 12:58:24.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2285" for this suite. 02/27/23 12:58:24.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:58:24.543
Feb 27 12:58:24.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename configmap 02/27/23 12:58:24.544
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:24.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:24.592
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-bd71c71e-e066-49a1-bf25-4614acf522b9 02/27/23 12:58:24.602
STEP: Creating a pod to test consume configMaps 02/27/23 12:58:24.611
Feb 27 12:58:24.632: INFO: Waiting up to 5m0s for pod "pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69" in namespace "configmap-2825" to be "Succeeded or Failed"
Feb 27 12:58:24.663: INFO: Pod "pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69": Phase="Pending", Reason="", readiness=false. Elapsed: 31.317887ms
Feb 27 12:58:26.687: INFO: Pod "pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05478276s
Feb 27 12:58:28.676: INFO: Pod "pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043610013s
STEP: Saw pod success 02/27/23 12:58:28.676
Feb 27 12:58:28.676: INFO: Pod "pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69" satisfied condition "Succeeded or Failed"
Feb 27 12:58:28.714: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 12:58:28.747
Feb 27 12:58:28.785: INFO: Waiting for pod pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69 to disappear
Feb 27 12:58:28.793: INFO: Pod pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 27 12:58:28.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2825" for this suite. 02/27/23 12:58:28.806
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":353,"skipped":6365,"failed":0}
------------------------------
• [4.279 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:58:24.543
    Feb 27 12:58:24.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename configmap 02/27/23 12:58:24.544
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:24.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:24.592
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-bd71c71e-e066-49a1-bf25-4614acf522b9 02/27/23 12:58:24.602
    STEP: Creating a pod to test consume configMaps 02/27/23 12:58:24.611
    Feb 27 12:58:24.632: INFO: Waiting up to 5m0s for pod "pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69" in namespace "configmap-2825" to be "Succeeded or Failed"
    Feb 27 12:58:24.663: INFO: Pod "pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69": Phase="Pending", Reason="", readiness=false. Elapsed: 31.317887ms
    Feb 27 12:58:26.687: INFO: Pod "pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05478276s
    Feb 27 12:58:28.676: INFO: Pod "pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043610013s
    STEP: Saw pod success 02/27/23 12:58:28.676
    Feb 27 12:58:28.676: INFO: Pod "pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69" satisfied condition "Succeeded or Failed"
    Feb 27 12:58:28.714: INFO: Trying to get logs from node ip-172-31-15-17.eu-central-1.compute.internal pod pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 12:58:28.747
    Feb 27 12:58:28.785: INFO: Waiting for pod pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69 to disappear
    Feb 27 12:58:28.793: INFO: Pod pod-configmaps-78cbe986-4c38-4035-a400-7e4af144de69 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 27 12:58:28.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2825" for this suite. 02/27/23 12:58:28.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:58:28.835
Feb 27 12:58:28.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename security-context-test 02/27/23 12:58:28.836
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:28.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:28.897
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Feb 27 12:58:28.938: INFO: Waiting up to 5m0s for pod "busybox-user-65534-64bd7593-8e45-41e7-93e9-525d607f72d5" in namespace "security-context-test-6355" to be "Succeeded or Failed"
Feb 27 12:58:28.960: INFO: Pod "busybox-user-65534-64bd7593-8e45-41e7-93e9-525d607f72d5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.449472ms
Feb 27 12:58:30.976: INFO: Pod "busybox-user-65534-64bd7593-8e45-41e7-93e9-525d607f72d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038027661s
Feb 27 12:58:32.970: INFO: Pod "busybox-user-65534-64bd7593-8e45-41e7-93e9-525d607f72d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032308397s
Feb 27 12:58:32.970: INFO: Pod "busybox-user-65534-64bd7593-8e45-41e7-93e9-525d607f72d5" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 27 12:58:32.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6355" for this suite. 02/27/23 12:58:32.983
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":354,"skipped":6401,"failed":0}
------------------------------
• [4.167 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:58:28.835
    Feb 27 12:58:28.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename security-context-test 02/27/23 12:58:28.836
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:28.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:28.897
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Feb 27 12:58:28.938: INFO: Waiting up to 5m0s for pod "busybox-user-65534-64bd7593-8e45-41e7-93e9-525d607f72d5" in namespace "security-context-test-6355" to be "Succeeded or Failed"
    Feb 27 12:58:28.960: INFO: Pod "busybox-user-65534-64bd7593-8e45-41e7-93e9-525d607f72d5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.449472ms
    Feb 27 12:58:30.976: INFO: Pod "busybox-user-65534-64bd7593-8e45-41e7-93e9-525d607f72d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038027661s
    Feb 27 12:58:32.970: INFO: Pod "busybox-user-65534-64bd7593-8e45-41e7-93e9-525d607f72d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032308397s
    Feb 27 12:58:32.970: INFO: Pod "busybox-user-65534-64bd7593-8e45-41e7-93e9-525d607f72d5" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 27 12:58:32.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-6355" for this suite. 02/27/23 12:58:32.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:58:33.043
Feb 27 12:58:33.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename resourcequota 02/27/23 12:58:33.045
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:33.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:33.146
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 02/27/23 12:58:33.162
STEP: Creating a ResourceQuota 02/27/23 12:58:38.172
STEP: Ensuring resource quota status is calculated 02/27/23 12:58:38.184
STEP: Creating a ReplicaSet 02/27/23 12:58:40.2
STEP: Ensuring resource quota status captures replicaset creation 02/27/23 12:58:40.224
STEP: Deleting a ReplicaSet 02/27/23 12:58:42.233
STEP: Ensuring resource quota status released usage 02/27/23 12:58:42.247
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 27 12:58:44.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-657" for this suite. 02/27/23 12:58:44.27
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":355,"skipped":6485,"failed":0}
------------------------------
• [SLOW TEST] [11.240 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:58:33.043
    Feb 27 12:58:33.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename resourcequota 02/27/23 12:58:33.045
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:33.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:33.146
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 02/27/23 12:58:33.162
    STEP: Creating a ResourceQuota 02/27/23 12:58:38.172
    STEP: Ensuring resource quota status is calculated 02/27/23 12:58:38.184
    STEP: Creating a ReplicaSet 02/27/23 12:58:40.2
    STEP: Ensuring resource quota status captures replicaset creation 02/27/23 12:58:40.224
    STEP: Deleting a ReplicaSet 02/27/23 12:58:42.233
    STEP: Ensuring resource quota status released usage 02/27/23 12:58:42.247
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 27 12:58:44.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-657" for this suite. 02/27/23 12:58:44.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:58:44.289
Feb 27 12:58:44.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename kubectl 02/27/23 12:58:44.291
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:44.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:44.349
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 02/27/23 12:58:44.36
Feb 27 12:58:44.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 create -f -'
Feb 27 12:58:44.721: INFO: stderr: ""
Feb 27 12:58:44.721: INFO: stdout: "pod/pause created\n"
Feb 27 12:58:44.721: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 27 12:58:44.721: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1269" to be "running and ready"
Feb 27 12:58:44.732: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.50241ms
Feb 27 12:58:44.732: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-172-31-7-167.eu-central-1.compute.internal' to be 'Running' but was 'Pending'
Feb 27 12:58:46.791: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.070509652s
Feb 27 12:58:46.791: INFO: Pod "pause" satisfied condition "running and ready"
Feb 27 12:58:46.791: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 02/27/23 12:58:46.792
Feb 27 12:58:46.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 label pods pause testing-label=testing-label-value'
Feb 27 12:58:46.994: INFO: stderr: ""
Feb 27 12:58:46.994: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 02/27/23 12:58:46.994
Feb 27 12:58:46.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 get pod pause -L testing-label'
Feb 27 12:58:47.136: INFO: stderr: ""
Feb 27 12:58:47.136: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 02/27/23 12:58:47.136
Feb 27 12:58:47.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 label pods pause testing-label-'
Feb 27 12:58:47.285: INFO: stderr: ""
Feb 27 12:58:47.285: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 02/27/23 12:58:47.285
Feb 27 12:58:47.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 get pod pause -L testing-label'
Feb 27 12:58:47.410: INFO: stderr: ""
Feb 27 12:58:47.410: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 02/27/23 12:58:47.41
Feb 27 12:58:47.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 delete --grace-period=0 --force -f -'
Feb 27 12:58:47.547: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 12:58:47.547: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 27 12:58:47.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 get rc,svc -l name=pause --no-headers'
Feb 27 12:58:47.679: INFO: stderr: "No resources found in kubectl-1269 namespace.\n"
Feb 27 12:58:47.679: INFO: stdout: ""
Feb 27 12:58:47.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 27 12:58:47.789: INFO: stderr: ""
Feb 27 12:58:47.789: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 27 12:58:47.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1269" for this suite. 02/27/23 12:58:47.821
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":356,"skipped":6494,"failed":0}
------------------------------
• [3.552 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:58:44.289
    Feb 27 12:58:44.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename kubectl 02/27/23 12:58:44.291
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:44.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:44.349
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 02/27/23 12:58:44.36
    Feb 27 12:58:44.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 create -f -'
    Feb 27 12:58:44.721: INFO: stderr: ""
    Feb 27 12:58:44.721: INFO: stdout: "pod/pause created\n"
    Feb 27 12:58:44.721: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Feb 27 12:58:44.721: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1269" to be "running and ready"
    Feb 27 12:58:44.732: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.50241ms
    Feb 27 12:58:44.732: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-172-31-7-167.eu-central-1.compute.internal' to be 'Running' but was 'Pending'
    Feb 27 12:58:46.791: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.070509652s
    Feb 27 12:58:46.791: INFO: Pod "pause" satisfied condition "running and ready"
    Feb 27 12:58:46.791: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 02/27/23 12:58:46.792
    Feb 27 12:58:46.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 label pods pause testing-label=testing-label-value'
    Feb 27 12:58:46.994: INFO: stderr: ""
    Feb 27 12:58:46.994: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 02/27/23 12:58:46.994
    Feb 27 12:58:46.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 get pod pause -L testing-label'
    Feb 27 12:58:47.136: INFO: stderr: ""
    Feb 27 12:58:47.136: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 02/27/23 12:58:47.136
    Feb 27 12:58:47.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 label pods pause testing-label-'
    Feb 27 12:58:47.285: INFO: stderr: ""
    Feb 27 12:58:47.285: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 02/27/23 12:58:47.285
    Feb 27 12:58:47.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 get pod pause -L testing-label'
    Feb 27 12:58:47.410: INFO: stderr: ""
    Feb 27 12:58:47.410: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 02/27/23 12:58:47.41
    Feb 27 12:58:47.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 delete --grace-period=0 --force -f -'
    Feb 27 12:58:47.547: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 12:58:47.547: INFO: stdout: "pod \"pause\" force deleted\n"
    Feb 27 12:58:47.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 get rc,svc -l name=pause --no-headers'
    Feb 27 12:58:47.679: INFO: stderr: "No resources found in kubectl-1269 namespace.\n"
    Feb 27 12:58:47.679: INFO: stdout: ""
    Feb 27 12:58:47.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3629291266 --namespace=kubectl-1269 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 27 12:58:47.789: INFO: stderr: ""
    Feb 27 12:58:47.789: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 27 12:58:47.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1269" for this suite. 02/27/23 12:58:47.821
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:58:47.843
Feb 27 12:58:47.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/27/23 12:58:47.845
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:47.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:47.903
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 02/27/23 12:58:47.915
STEP: Creating hostNetwork=false pod 02/27/23 12:58:47.916
Feb 27 12:58:47.942: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3187" to be "running and ready"
Feb 27 12:58:47.965: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.951561ms
Feb 27 12:58:47.965: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:58:49.973: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.030657828s
Feb 27 12:58:49.973: INFO: The phase of Pod test-pod is Running (Ready = true)
Feb 27 12:58:49.973: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 02/27/23 12:58:49.982
Feb 27 12:58:49.995: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3187" to be "running and ready"
Feb 27 12:58:50.004: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.1663ms
Feb 27 12:58:50.004: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:58:52.021: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026382488s
Feb 27 12:58:52.021: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Feb 27 12:58:52.021: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 02/27/23 12:58:52.035
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/27/23 12:58:52.036
Feb 27 12:58:52.036: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:58:52.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:58:52.037: INFO: ExecWithOptions: Clientset creation
Feb 27 12:58:52.037: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 27 12:58:52.177: INFO: Exec stderr: ""
Feb 27 12:58:52.177: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:58:52.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:58:52.178: INFO: ExecWithOptions: Clientset creation
Feb 27 12:58:52.178: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 27 12:58:52.314: INFO: Exec stderr: ""
Feb 27 12:58:52.314: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:58:52.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:58:52.315: INFO: ExecWithOptions: Clientset creation
Feb 27 12:58:52.316: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 27 12:58:52.458: INFO: Exec stderr: ""
Feb 27 12:58:52.458: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:58:52.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:58:52.459: INFO: ExecWithOptions: Clientset creation
Feb 27 12:58:52.459: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 27 12:58:52.595: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/27/23 12:58:52.596
Feb 27 12:58:52.596: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:58:52.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:58:52.597: INFO: ExecWithOptions: Clientset creation
Feb 27 12:58:52.597: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 27 12:58:52.729: INFO: Exec stderr: ""
Feb 27 12:58:52.729: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:58:52.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:58:52.730: INFO: ExecWithOptions: Clientset creation
Feb 27 12:58:52.730: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 27 12:58:52.852: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/27/23 12:58:52.852
Feb 27 12:58:52.852: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:58:52.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:58:52.853: INFO: ExecWithOptions: Clientset creation
Feb 27 12:58:52.853: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 27 12:58:53.007: INFO: Exec stderr: ""
Feb 27 12:58:53.007: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:58:53.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:58:53.009: INFO: ExecWithOptions: Clientset creation
Feb 27 12:58:53.009: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 27 12:58:53.142: INFO: Exec stderr: ""
Feb 27 12:58:53.142: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:58:53.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:58:53.143: INFO: ExecWithOptions: Clientset creation
Feb 27 12:58:53.143: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 27 12:58:53.328: INFO: Exec stderr: ""
Feb 27 12:58:53.328: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 12:58:53.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
Feb 27 12:58:53.329: INFO: ExecWithOptions: Clientset creation
Feb 27 12:58:53.329: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 27 12:58:53.457: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Feb 27 12:58:53.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3187" for this suite. 02/27/23 12:58:53.473
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":357,"skipped":6497,"failed":0}
------------------------------
• [SLOW TEST] [5.644 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:58:47.843
    Feb 27 12:58:47.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/27/23 12:58:47.845
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:47.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:47.903
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 02/27/23 12:58:47.915
    STEP: Creating hostNetwork=false pod 02/27/23 12:58:47.916
    Feb 27 12:58:47.942: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3187" to be "running and ready"
    Feb 27 12:58:47.965: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.951561ms
    Feb 27 12:58:47.965: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:58:49.973: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.030657828s
    Feb 27 12:58:49.973: INFO: The phase of Pod test-pod is Running (Ready = true)
    Feb 27 12:58:49.973: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 02/27/23 12:58:49.982
    Feb 27 12:58:49.995: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3187" to be "running and ready"
    Feb 27 12:58:50.004: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.1663ms
    Feb 27 12:58:50.004: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:58:52.021: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026382488s
    Feb 27 12:58:52.021: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Feb 27 12:58:52.021: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 02/27/23 12:58:52.035
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/27/23 12:58:52.036
    Feb 27 12:58:52.036: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:58:52.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:58:52.037: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:58:52.037: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 27 12:58:52.177: INFO: Exec stderr: ""
    Feb 27 12:58:52.177: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:58:52.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:58:52.178: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:58:52.178: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 27 12:58:52.314: INFO: Exec stderr: ""
    Feb 27 12:58:52.314: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:58:52.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:58:52.315: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:58:52.316: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 27 12:58:52.458: INFO: Exec stderr: ""
    Feb 27 12:58:52.458: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:58:52.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:58:52.459: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:58:52.459: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 27 12:58:52.595: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/27/23 12:58:52.596
    Feb 27 12:58:52.596: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:58:52.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:58:52.597: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:58:52.597: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 27 12:58:52.729: INFO: Exec stderr: ""
    Feb 27 12:58:52.729: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:58:52.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:58:52.730: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:58:52.730: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 27 12:58:52.852: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/27/23 12:58:52.852
    Feb 27 12:58:52.852: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:58:52.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:58:52.853: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:58:52.853: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 27 12:58:53.007: INFO: Exec stderr: ""
    Feb 27 12:58:53.007: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:58:53.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:58:53.009: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:58:53.009: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 27 12:58:53.142: INFO: Exec stderr: ""
    Feb 27 12:58:53.142: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:58:53.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:58:53.143: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:58:53.143: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 27 12:58:53.328: INFO: Exec stderr: ""
    Feb 27 12:58:53.328: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3187 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 12:58:53.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    Feb 27 12:58:53.329: INFO: ExecWithOptions: Clientset creation
    Feb 27 12:58:53.329: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3187/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 27 12:58:53.457: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Feb 27 12:58:53.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-3187" for this suite. 02/27/23 12:58:53.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 12:58:53.488
Feb 27 12:58:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename secrets 02/27/23 12:58:53.489
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:53.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:53.562
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-bbb38222-6dce-472c-8fc5-3973aa35700f 02/27/23 12:58:53.584
STEP: Creating secret with name s-test-opt-upd-c2195b97-7beb-41a0-95a5-aa6785104baa 02/27/23 12:58:53.595
STEP: Creating the pod 02/27/23 12:58:53.607
Feb 27 12:58:53.627: INFO: Waiting up to 5m0s for pod "pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149" in namespace "secrets-1084" to be "running and ready"
Feb 27 12:58:53.636: INFO: Pod "pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149": Phase="Pending", Reason="", readiness=false. Elapsed: 8.861899ms
Feb 27 12:58:53.636: INFO: The phase of Pod pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:58:55.649: INFO: Pod "pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021744933s
Feb 27 12:58:55.649: INFO: The phase of Pod pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 12:58:57.655: INFO: Pod "pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149": Phase="Running", Reason="", readiness=true. Elapsed: 4.027502293s
Feb 27 12:58:57.655: INFO: The phase of Pod pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149 is Running (Ready = true)
Feb 27 12:58:57.655: INFO: Pod "pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-bbb38222-6dce-472c-8fc5-3973aa35700f 02/27/23 12:58:57.734
STEP: Updating secret s-test-opt-upd-c2195b97-7beb-41a0-95a5-aa6785104baa 02/27/23 12:58:57.749
STEP: Creating secret with name s-test-opt-create-24cbe66b-2024-47fe-a445-9800d2b6319f 02/27/23 12:58:57.76
STEP: waiting to observe update in volume 02/27/23 12:58:57.771
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 27 13:00:13.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1084" for this suite. 02/27/23 13:00:13.059
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":358,"skipped":6511,"failed":0}
------------------------------
• [SLOW TEST] [79.584 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 12:58:53.488
    Feb 27 12:58:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename secrets 02/27/23 12:58:53.489
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 12:58:53.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 12:58:53.562
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-bbb38222-6dce-472c-8fc5-3973aa35700f 02/27/23 12:58:53.584
    STEP: Creating secret with name s-test-opt-upd-c2195b97-7beb-41a0-95a5-aa6785104baa 02/27/23 12:58:53.595
    STEP: Creating the pod 02/27/23 12:58:53.607
    Feb 27 12:58:53.627: INFO: Waiting up to 5m0s for pod "pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149" in namespace "secrets-1084" to be "running and ready"
    Feb 27 12:58:53.636: INFO: Pod "pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149": Phase="Pending", Reason="", readiness=false. Elapsed: 8.861899ms
    Feb 27 12:58:53.636: INFO: The phase of Pod pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:58:55.649: INFO: Pod "pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021744933s
    Feb 27 12:58:55.649: INFO: The phase of Pod pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 12:58:57.655: INFO: Pod "pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149": Phase="Running", Reason="", readiness=true. Elapsed: 4.027502293s
    Feb 27 12:58:57.655: INFO: The phase of Pod pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149 is Running (Ready = true)
    Feb 27 12:58:57.655: INFO: Pod "pod-secrets-adf1edb2-04b0-481e-9c4d-c083b481b149" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-bbb38222-6dce-472c-8fc5-3973aa35700f 02/27/23 12:58:57.734
    STEP: Updating secret s-test-opt-upd-c2195b97-7beb-41a0-95a5-aa6785104baa 02/27/23 12:58:57.749
    STEP: Creating secret with name s-test-opt-create-24cbe66b-2024-47fe-a445-9800d2b6319f 02/27/23 12:58:57.76
    STEP: waiting to observe update in volume 02/27/23 12:58:57.771
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 27 13:00:13.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1084" for this suite. 02/27/23 13:00:13.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 13:00:13.081
Feb 27 13:00:13.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename services 02/27/23 13:00:13.082
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 13:00:13.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 13:00:13.244
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 02/27/23 13:00:13.282
STEP: waiting for available Endpoint 02/27/23 13:00:13.3
STEP: listing all Endpoints 02/27/23 13:00:13.305
STEP: updating the Endpoint 02/27/23 13:00:13.315
STEP: fetching the Endpoint 02/27/23 13:00:13.332
STEP: patching the Endpoint 02/27/23 13:00:13.34
STEP: fetching the Endpoint 02/27/23 13:00:13.367
STEP: deleting the Endpoint by Collection 02/27/23 13:00:13.379
STEP: waiting for Endpoint deletion 02/27/23 13:00:13.393
STEP: fetching the Endpoint 02/27/23 13:00:13.405
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 27 13:00:13.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7492" for this suite. 02/27/23 13:00:13.433
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":359,"skipped":6555,"failed":0}
------------------------------
• [0.375 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 13:00:13.081
    Feb 27 13:00:13.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename services 02/27/23 13:00:13.082
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 13:00:13.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 13:00:13.244
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 02/27/23 13:00:13.282
    STEP: waiting for available Endpoint 02/27/23 13:00:13.3
    STEP: listing all Endpoints 02/27/23 13:00:13.305
    STEP: updating the Endpoint 02/27/23 13:00:13.315
    STEP: fetching the Endpoint 02/27/23 13:00:13.332
    STEP: patching the Endpoint 02/27/23 13:00:13.34
    STEP: fetching the Endpoint 02/27/23 13:00:13.367
    STEP: deleting the Endpoint by Collection 02/27/23 13:00:13.379
    STEP: waiting for Endpoint deletion 02/27/23 13:00:13.393
    STEP: fetching the Endpoint 02/27/23 13:00:13.405
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 27 13:00:13.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7492" for this suite. 02/27/23 13:00:13.433
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 13:00:13.46
Feb 27 13:00:13.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename events 02/27/23 13:00:13.461
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 13:00:13.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 13:00:13.54
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 02/27/23 13:00:13.564
Feb 27 13:00:13.590: INFO: created test-event-1
Feb 27 13:00:13.603: INFO: created test-event-2
Feb 27 13:00:13.616: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 02/27/23 13:00:13.616
STEP: delete collection of events 02/27/23 13:00:13.629
Feb 27 13:00:13.629: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/27/23 13:00:13.69
Feb 27 13:00:13.691: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Feb 27 13:00:13.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5986" for this suite. 02/27/23 13:00:13.77
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":360,"skipped":6571,"failed":0}
------------------------------
• [0.344 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 13:00:13.46
    Feb 27 13:00:13.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename events 02/27/23 13:00:13.461
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 13:00:13.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 13:00:13.54
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 02/27/23 13:00:13.564
    Feb 27 13:00:13.590: INFO: created test-event-1
    Feb 27 13:00:13.603: INFO: created test-event-2
    Feb 27 13:00:13.616: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 02/27/23 13:00:13.616
    STEP: delete collection of events 02/27/23 13:00:13.629
    Feb 27 13:00:13.629: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/27/23 13:00:13.69
    Feb 27 13:00:13.691: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Feb 27 13:00:13.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5986" for this suite. 02/27/23 13:00:13.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 13:00:13.806
Feb 27 13:00:13.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename aggregator 02/27/23 13:00:13.808
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 13:00:13.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 13:00:13.887
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Feb 27 13:00:13.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 02/27/23 13:00:13.905
Feb 27 13:00:14.936: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 27 13:00:17.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:19.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:21.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:23.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:25.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:27.075: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:29.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:31.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:33.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:35.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:37.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:39.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:41.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:43.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:45.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:47.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:49.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:51.077: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:53.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:55.075: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:57.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:00:59.281: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:01.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:03.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:05.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:07.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:09.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:11.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:13.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:15.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:17.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:19.078: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:21.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:23.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:25.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:27.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 13:01:29.263: INFO: Waited 157.789023ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 02/27/23 13:01:29.518
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/27/23 13:01:29.528
STEP: List APIServices 02/27/23 13:01:29.544
Feb 27 13:01:29.559: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Feb 27 13:01:30.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-205" for this suite. 02/27/23 13:01:30.911
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":361,"skipped":6580,"failed":0}
------------------------------
• [SLOW TEST] [77.123 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 13:00:13.806
    Feb 27 13:00:13.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename aggregator 02/27/23 13:00:13.808
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 13:00:13.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 13:00:13.887
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Feb 27 13:00:13.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 02/27/23 13:00:13.905
    Feb 27 13:00:14.936: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Feb 27 13:00:17.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:19.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:21.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:23.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:25.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:27.075: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:29.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:31.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:33.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:35.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:37.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:39.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:41.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:43.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:45.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:47.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:49.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:51.077: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:53.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:55.075: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:57.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:00:59.281: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:01.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:03.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:05.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:07.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:09.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:11.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:13.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:15.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:17.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:19.078: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:21.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:23.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:25.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:27.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 13, 0, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 13, 0, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 13:01:29.263: INFO: Waited 157.789023ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 02/27/23 13:01:29.518
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/27/23 13:01:29.528
    STEP: List APIServices 02/27/23 13:01:29.544
    Feb 27 13:01:29.559: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Feb 27 13:01:30.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-205" for this suite. 02/27/23 13:01:30.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/27/23 13:01:30.932
Feb 27 13:01:30.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
STEP: Building a namespace api object, basename events 02/27/23 13:01:30.934
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 13:01:30.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 13:01:30.978
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 02/27/23 13:01:30.988
STEP: get a list of Events with a label in the current namespace 02/27/23 13:01:31.024
STEP: delete a list of events 02/27/23 13:01:31.032
Feb 27 13:01:31.032: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/27/23 13:01:31.083
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Feb 27 13:01:31.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8149" for this suite. 02/27/23 13:01:31.108
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":362,"skipped":6609,"failed":0}
------------------------------
• [0.187 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/27/23 13:01:30.932
    Feb 27 13:01:30.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3629291266
    STEP: Building a namespace api object, basename events 02/27/23 13:01:30.934
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 13:01:30.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 13:01:30.978
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 02/27/23 13:01:30.988
    STEP: get a list of Events with a label in the current namespace 02/27/23 13:01:31.024
    STEP: delete a list of events 02/27/23 13:01:31.032
    Feb 27 13:01:31.032: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/27/23 13:01:31.083
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Feb 27 13:01:31.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8149" for this suite. 02/27/23 13:01:31.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Feb 27 13:01:31.154: INFO: Running AfterSuite actions on all nodes
Feb 27 13:01:31.167: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Feb 27 13:01:31.167: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Feb 27 13:01:31.167: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Feb 27 13:01:31.175: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Feb 27 13:01:31.175: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Feb 27 13:01:31.175: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Feb 27 13:01:31.187: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Feb 27 13:01:31.187: INFO: Running AfterSuite actions on node 1
Feb 27 13:01:31.187: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.034 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Feb 27 13:01:31.154: INFO: Running AfterSuite actions on all nodes
    Feb 27 13:01:31.167: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Feb 27 13:01:31.167: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Feb 27 13:01:31.167: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Feb 27 13:01:31.175: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Feb 27 13:01:31.175: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Feb 27 13:01:31.175: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Feb 27 13:01:31.187: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Feb 27 13:01:31.187: INFO: Running AfterSuite actions on node 1
    Feb 27 13:01:31.187: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.001 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.261 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 6116.369 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h41m57.171296134s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

